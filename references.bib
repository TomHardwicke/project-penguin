
@report{lonsdorf_multiverse_2021,
	title = {Multiverse analyses in fear conditioning research},
	url = {https://psyarxiv.com/2z6pd/},
	abstract = {There is heterogeneity in and a lack of consensus on the preferred statistical analyses for
analyzing fear conditioning effects in light of a multitude of potentially equally justifiable
statistical approaches. Here, we introduce the concept of multiverse analysis for fear
conditioning research. We also present a model multiverse approach specifically tailored to
fear conditioning research and introduce the novel and easy to use R package ‘multifear’ that
allows to run all the models though a single line of code. Model specifications and data
reduction approaches employed in the ‘multifear’ package were identified through a
representative systematic literature search. The heterogeneity of statistical models identified
included Bayesian {ANOVA} and t-tests as well as frequentist {ANOVA}, t-test as well as mixed
models with a variety of data reduction approaches (i.e., number of trials, trial blocks,
averages) as input. We illustrate the power of a multiverse analysis for fear conditioning data
based on two pre-existing data sets with partial (data set 1) and 100\% reinforcement rate
(data set 2) by using {CS} discrimination in skin conductance responses ({SCRs}) during fear
acquisition and extinction training as case examples. Both the effect size and the direction of
effect was impacted by choice of the model and data reduction techniques. We anticipate
that an increase in multiverse-type of studies in the field of fear conditioning research and
their extension to other outcome measures as well as data and design multiverse analyses
will aid the development of formal theories through the accumulation of empirical evidence.
This may contribute to facilitated and more successful clinical translation.},
	institution = {{PsyArXiv}},
	author = {Lonsdorf, Tina and Gerlicher, Anna and Klingelhöfer-Jens, Maren and Krypotos, Angelos-Miltiadis},
	urldate = {2021-03-04},
	date = {2021-03-03},
	doi = {10.31234/osf.io/2z6pd},
	note = {type: article},
	keywords = {Meta-science, anxiety disorders, Anxiety Disorders, Clinical Psychology, good research practices, p-hacking, Physiology, questionable research practices, Social and Behavioral Sciences, transparency},
	file = {Lonsdorf_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lonsdorf_etal_2021.pdf:application/pdf},
}

@article{maccallum_model_1992,
	title = {Model modifications in covariance structure analysis: The problem of capitalization on chance},
	volume = {111},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/0033-2909.111.3.490},
	shorttitle = {Model modifications in covariance structure analysis},
	abstract = {In applications of covariance structure modeling in which an initial model does not fit sample data well, it has become common practice to modify that model to improve its fit. Because this process is data driven, it is inherently susceptible to capitalization on chance characteristics of the data, thus raising the question of whether model modifications generalize to other samples or to the population. This issue is discussed in detail and is explored empirically through sampling studies using 2 large sets of data. Results demonstrate that over repeated samples, model modifications may be very inconsistent and cross-validation results may behave erratically. These findings lead to skepticism about generalizability of models resulting from data-driven modifications of an initial model. The use of alternative a priori models is recommended as a preferred strategy. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {490--504},
	number = {3},
	journaltitle = {Psychological Bulletin},
	author = {{MacCallum}, Robert C. and Roznowski, Mary and Necowitz, Lawrence B.},
	date = {1992},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Analysis of Covariance, Goodness of Fit, Population (Statistics), Statistical Probability, Statistical Samples},
	file = {MacCallum et al. - 1992 - Model modifications in covariance structure analys.pdf:/Users/tom/Zotero/storage/WE7RGFZQ/MacCallum et al. - 1992 - Model modifications in covariance structure analys.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/MAXNLKS3/1992-25917-001.html:text/html},
}

@article{avidan_independent_2019,
	title = {Independent discussion sections for improving inferential reproducibility in published research},
	volume = {122},
	issn = {0007-0912, 1471-6771},
	url = {https://bjanaesthesia.org/article/S0007-0912(18)31378-3/abstract},
	doi = {10.1016/j.bja.2018.12.010},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}There is a reproducibility crisis in science. There are many potential contributors to replication failure in research across the translational continuum. In this perspective piece, we focus on the narrow topic of inferential reproducibility. Although replication of methods and results is necessary to demonstrate reproducibility, it is not sufficient. Also fundamental is consistent interpretation in the Discussion section. Current deficiencies in the Discussion sections of manuscripts might limit the inferential reproducibility of scientific research. Lack of contextualisation using systematic reviews, overinterpretation and misinterpretation of results, and insufficient acknowledgement of limitations are common problems in Discussion sections; these deficiencies can harm the translational process. Proposed solutions include eliminating or not reading Discussions, writing accompanying editorials, and post-publication review and comments; however, none of these solutions works very well. A second Discussion written by an independent author with appropriate expertise in research methodology is a new testable solution that could help probe inferential reproducibility, and address some deficiencies in primary Discussion sections.{\textless}/p{\textgreater}},
	pages = {413--420},
	number = {4},
	journaltitle = {British Journal of Anaesthesia},
	shortjournal = {British Journal of Anaesthesia},
	author = {Avidan, Michael S. and Ioannidis, John P. A. and Mashour, George A.},
	urldate = {2021-03-04},
	date = {2019-04-01},
	pmid = {30857597},
	note = {Publisher: Elsevier},
	file = {Avidan_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Avidan_etal_2019.pdf:application/pdf},
}

@article{steegen_increasing_2016,
	title = {Increasing transparency through a multiverse analysis},
	volume = {11},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691616658637},
	doi = {10.1177/1745691616658637},
	abstract = {Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.},
	pages = {702--712},
	number = {5},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and Vanpaemel, Wolf},
	urldate = {2020-09-21},
	date = {2016-09},
	langid = {english},
	file = {Steegen et al. - 2016 - Increasing Transparency Through a Multiverse Analy.pdf:/Users/tom/Zotero/storage/LZKBIFWF/Steegen et al. - 2016 - Increasing Transparency Through a Multiverse Analy.pdf:application/pdf},
}

@online{dehaven_preregistration_2017,
	title = {Preregistration: a plan, not a prison},
	url = {https://www.cos.io/blog/preregistration-plan-not-prison},
	shorttitle = {Preregistration},
	abstract = {Preregistration},
	author = {{DeHaven}, Alexander},
	urldate = {2020-09-29},
	date = {2017},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/G6K7H4I6/preregistration-plan-not-prison.html:text/html},
}

@article{haven_preregistering_2020,
	title = {Preregistering qualitative research: a Delphi study},
	volume = {19},
	url = {https://osf.io/pz9jr},
	doi = {10.31235/osf.io/pz9jr},
	shorttitle = {Preregistering qualitative research},
	abstract = {Preregistrations—records made a priori about study designs and analysis plans and placed in open repositories—are thought to strengthen the credibility and transparency of research. Different authors have put forth arguments in favor of introducing this practice in qualitative research and made suggestions for what to include in a qualitative preregistration form. The goal of this study was to gauge and understand what parts of preregistration templates qualitative researchers would find helpful and informative. We used an online Delphi study design consisting of two rounds with feedback reports in between. In total, 48 researchers participated (response rate: 16\%). In round 1, panelists considered 14 proposed items relevant to include in the preregistration form, but two items had relevance scores just below our predefined criterion (68\%) with mixed argument and were put forth again. We combined items where possible, leading to 11 revised items. In round 2, panelists agreed on including the two remaining items. Panelists also converged on suggested terminology and elaborations, except for two terms for which they provided clear arguments. The result is an agreement-based form for the preregistration of qualitative studies that consists of 13 items. The form will be made available as a registration option on Open Science Framework (osf.io). We believe it is important to assure that the strength of qualitative research, which is its flexibility to adapt, adjust and respond, is not lost in preregistration. The preregistration should provide a systematic starting point.},
	pages = {1--13},
	journaltitle = {International Journal of Qualitative Methods},
	author = {Haven, Tamarinde L. and Errington, Timothy M. and Gleditsch, Kristian and van Grootel, Leonie and Jacobs, Alan M. and Kern, Florian and Piñeiro, Rafael and Rosenblatt, Fernando and Mokkink, Lidwine},
	urldate = {2020-12-11},
	date = {2020-07-06},
	langid = {english},
	file = {Haven et al. - 2020 - Preregistering Qualitative Research A Delphi Stud.pdf:/Users/tom/Zotero/storage/SG7MM2YJ/Haven et al. - 2020 - Preregistering Qualitative Research A Delphi Stud.pdf:application/pdf},
}

@article{fiedler_creative_2018,
	title = {The creative cycle and the growth of psychological science},
	rights = {© The Author(s) 2018},
	url = {https://journals.sagepub.com/doi/10.1177/1745691617745651},
	doi = {10.1177/1745691617745651},
	shorttitle = {The creative cycle and the growth of psychological science},
	abstract = {Scientific progress relies on the dialectics of loosening and tightening processes. Although most gripping accomplishments in psychological science testify to t...},
	journaltitle = {Perspectives on Psychological Science},
	author = {Fiedler, Klaus},
	urldate = {2020-09-21},
	date = {2018-07-02},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Fiedler_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Fiedler_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9BSEX52X/1745691617745651.html:text/html},
}

@book{shadish_experimental_2001,
	location = {Boston},
	title = {Experimental and quasi-experimental designs for generalized causal inference},
	isbn = {978-0-395-61556-0},
	pagetotal = {623},
	publisher = {Houghton Mifflin},
	author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald T.},
	date = {2001},
	langid = {english},
	keywords = {Causation, Experiments},
	file = {Shadish et al. - 2001 - Experimental and quasi-experimental designs for ge.pdf:/Users/tom/Zotero/storage/IB9FQCD8/Shadish et al. - 2001 - Experimental and quasi-experimental designs for ge.pdf:application/pdf},
}

@collection{lilienfeld_psychological_2017,
	location = {Hoboken},
	title = {Psychological science under scrutiny},
	isbn = {978-1-118-66104-8 978-1-118-66108-6},
	pagetotal = {1},
	publisher = {Wiley Blackwell},
	editor = {Lilienfeld, Scott O. and Waldman, Irwin D.},
	date = {2017},
	langid = {english},
	keywords = {Psychology},
	file = {Lilienfeld and Waldman - 2017 - Psychological science under scrutiny.pdf:/Users/tom/Zotero/storage/96BSJPUF/Lilienfeld and Waldman - 2017 - Psychological science under scrutiny.pdf:application/pdf},
}

@article{klein_practical_2018,
	title = {A practical guide for transparency in psychological science},
	volume = {4},
	rights = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2474-7394},
	url = {http://www.collabra.org/article/10.1525/collabra.158/},
	doi = {10.1525/collabra.158},
	abstract = {Article: A Practical Guide for Transparency in Psychological Science},
	pages = {20},
	number = {1},
	journaltitle = {Collabra: Psychology},
	author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and Ijzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
	urldate = {2020-05-15},
	date = {2018-06-29},
	note = {Number: 1
Publisher: The Regents of the University of California},
	file = {Klein_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Klein_etal_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9MUHFKY7/collabra.158.html:text/html},
}

@article{rouder_minimizing_2019,
	title = {Minimizing mistakes in psychological science},
	volume = {2},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918801915},
	doi = {10.1177/2515245918801915},
	abstract = {Developing and implementing best practices in organizing a lab is challenging, especially in the face of new cultural norms, such as the open-science movement. Part of this challenge in today’s landscape is using new technologies, including cloud storage and computer automation. In this article, we discuss a few practices designed to increase the reliability of scientific labs, focusing on ways to minimize common, ordinary mistakes. We borrow principles from the theory of high-reliability organizations, which has been used to characterize operational practices in high-risk environments, such as aviation and health care. Guided by these principles, we focus on five strategies: (a) implementing a lab culture focused on learning from mistakes, (b) using computer automation in data and metadata collection whenever possible, (c) standardizing organizational strategies, (d) using coded rather than menu-driven analyses, and (e) developing expanded documents that record how analyses were performed.},
	pages = {3--11},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Rouder, Jeffrey N. and Haaf, Julia M. and Snyder, Hope K.},
	urldate = {2020-05-15},
	date = {2019-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@article{lin_standard_2016,
	title = {Standard operating procedures: a safety net for pre-analysis plans},
	volume = {49},
	issn = {1049-0965, 1537-5935},
	url = {http://www.journals.cambridge.org/abstract_S1049096516000810},
	doi = {10.1017/S1049096516000810},
	shorttitle = {Standard operating procedures},
	abstract = {Across the social sciences, growing concerns about research transparency have led to calls for pre-analysis plans ({PAPs}) that specify in advance how researchers intend to analyze the data they are about to gather. {PAPs} promote transparency and credibility by helping readers distinguish between exploratory and conﬁrmatory analyses. However, {PAPs} are time-consuming to write and may fail to anticipate contingencies that arise in the course of data collection. This article proposes the use of “standard operating procedures” ({SOPs})—default practices to guide decisions when issues arise that were not anticipated in the {PAP}. We oﬀer an example of an {SOP} that can be adapted by other researchers seeking a safety net to support their {PAPs}.},
	pages = {495--500},
	number = {3},
	journaltitle = {{PS}: Political Science \& Politics},
	shortjournal = {{APSC}},
	author = {Lin, Winston and Green, Donald P.},
	urldate = {2020-09-22},
	date = {2016-07},
	langid = {english},
	file = {Lin and Green - 2016 - Standard Operating Procedures A Safety Net for Pr.pdf:/Users/tom/Zotero/storage/9L5585YT/Lin and Green - 2016 - Standard Operating Procedures A Safety Net for Pr.pdf:application/pdf},
}

@article{tackett_its_2017,
	title = {It’s time to broaden the replicability conversation: thoughts for and from clinical psychological science:},
	rights = {© The Author(s) 2017},
	url = {https://journals.sagepub.com/doi/10.1177/1745691617690042},
	doi = {10.1177/1745691617690042},
	shorttitle = {It’s time to broaden the replicability conversation},
	abstract = {Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psycho...},
	journaltitle = {Perspectives on Psychological Science},
	author = {Tackett, Jennifer L. and Lilienfeld, Scott O. and Patrick, Christopher J. and Johnson, Sheri L. and Krueger, Robert F. and Miller, Joshua D. and Oltmanns, Thomas F. and Shrout, Patrick E.},
	urldate = {2020-09-21},
	date = {2017-10-03},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/2VVXN7BA/1745691617690042.html:text/html;Tackett_etal_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Tackett_etal_2017.pdf:application/pdf},
}

@article{pronin_bias_2002,
	title = {The bias blind spot: perceptions of bias in self versus others},
	volume = {28},
	issn = {0146-1672},
	url = {https://doi.org/10.1177/0146167202286008},
	doi = {10.1177/0146167202286008},
	shorttitle = {The bias blind spot},
	abstract = {Three studies suggest that individuals see the existence and operation of cognitive and motivational biases much more in others than in themselves. Study 1 provides evidence from three surveys that people rate themselves as less subject to various biases than the “average American,” classmates in a seminar, and fellow airport travelers. Data from the third survey further suggest that such claims arise from the interplay among availability biases and self-enhancement motives. Participants in one follow-up study who showed the better-than-average bias insisted that their self-assessments were accurate and objective even after reading a description of how they could have been affected by the relevant bias. Participants in a final study reported their peer’s self-serving attributions regarding test performance to be biased but their own similarly self-serving attributions to be free of bias. The relevance of these phenomena to naïve realism and to conflict, misunderstanding, and dispute resolution is discussed.},
	pages = {369--381},
	number = {3},
	journaltitle = {Personality and Social Psychology Bulletin},
	shortjournal = {Pers Soc Psychol Bull},
	author = {Pronin, Emily and Lin, Daniel Y. and Ross, Lee},
	urldate = {2020-12-10},
	date = {2002-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Pronin_etal_2002.pdf:/Users/tom/pCloud Drive/Zotero_Library/Pronin_etal_2002.pdf:application/pdf},
}

@article{mazzola_forgetting_2013,
	title = {Forgetting what we learned as graduate students: {HARKing} and selective outcome reporting in I–O journal articles},
	volume = {6},
	rights = {Copyright © 2013 Society for Industrial and Organizational Psychology},
	issn = {1754-9434},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/iops.12049},
	doi = {10.1111/iops.12049},
	shorttitle = {Forgetting what we learned as graduate students},
	pages = {279--284},
	number = {3},
	journaltitle = {Industrial and Organizational Psychology},
	author = {Mazzola, Joseph J. and Deuling, Jacqueline K.},
	urldate = {2021-02-17},
	date = {2013},
	langid = {english},
	note = {\_eprint: https://www.onlinelibrary.wiley.com/doi/pdf/10.1111/iops.12049},
	file = {Mazzola_Deuling_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Mazzola_Deuling_2013.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XAEAZ86Z/iops.html:text/html},
}

@article{wasserstein_asas_2016,
	title = {The {ASA}'s statement on p-values: context, process, and purpose},
	volume = {70},
	doi = {10.1080/00031305.2016.1154108},
	pages = {129--133},
	number = {2},
	journaltitle = {The American Statistician},
	author = {Wasserstein, Ronald L and Lazar, Nicole A},
	date = {2016},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Wasserstein - The ASA's Statement on p-Values Context, Process,.pdf:/Users/tom/Zotero/storage/ABPJI2HN/Wasserstein - The ASA's Statement on p-Values Context, Process,.pdf:application/pdf},
}

@article{pronin_perception_2007,
	title = {Perception and misperception of bias in human judgment},
	volume = {11},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(06)00299-3},
	doi = {10.1016/j.tics.2006.11.001},
	pages = {37--43},
	number = {1},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Pronin, Emily},
	urldate = {2021-03-03},
	date = {2007-01-01},
	pmid = {17129749},
	note = {Publisher: Elsevier},
	file = {Pronin_2007.pdf:/Users/tom/pCloud Drive/Zotero_Library/Pronin_2007.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8NR953NC/S1364-6613(06)00299-3.html:text/html},
}

@article{berry_difficult_2007,
	title = {The difficult and ubiquitous problems of multiplicities},
	volume = {6},
	rights = {Copyright © 2007 John Wiley \& Sons, Ltd.},
	issn = {1539-1612},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/pst.303},
	doi = {10.1002/pst.303},
	abstract = {Multiplicities are ubiquitous. They threaten every inference in every aspect of life. Despite the focus in statistics on multiplicities, statisticians underestimate their importance. One reason is that the focus is on methodology for known multiplicities. Silent multiplicities are much more important and they are insidious. Both frequentists and Bayesians have important contributions to make regarding problems of multiplicities. But neither group has an inside track. Frequentists and Bayesians working together is a promising way of making inroads into this knotty set of problems. Two experiments with identical results may well lead to very different statistical conclusions. So we will never be able to use a software package with default settings to resolve all problems of multiplicities. Every problem has unique aspects. And all problems require understanding the substantive area of application. Copyright © 2007 John Wiley \& Sons, Ltd.},
	pages = {155--160},
	number = {3},
	journaltitle = {Pharmaceutical Statistics},
	author = {Berry, Donald A.},
	urldate = {2021-03-03},
	date = {2007},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.303},
	keywords = {Bayesian statistics, multiple comparisons, multiplicities, subset analyses},
	file = {Berry_2007.pdf:/Users/tom/pCloud Drive/Zotero_Library/Berry_2007.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Y8UF5BMD/pst.html:text/html},
}

@article{kirtley_making_2021,
	title = {Making the black box transparent: a template and tutorial for registration of studies using experience-sampling methods},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245920924686},
	doi = {10.1177/2515245920924686},
	shorttitle = {Making the black box transparent},
	abstract = {A growing interest in understanding complex and dynamic psychological processes as they occur in everyday life has led to an increase in studies using ambulatory assessment techniques, including the experience-sampling method ({ESM}) and ecological momentary assessment. These methods, however, tend to involve numerous forking paths and researcher degrees of freedom, even beyond those typically encountered with other research methodologies. Although a number of researchers working with {ESM} techniques are actively engaged in efforts to increase the methodological rigor and transparency of research that uses them, currently there is little routine implementation of open-science practices in {ESM} research. In this article, we discuss the ways in which {ESM} research is especially vulnerable to threats to transparency, reproducibility, and replicability. We propose that greater use of study registration, a cornerstone of open science, may address some of these threats to the transparency of {ESM} research. Registration of {ESM} research is not without challenges, including model selection, accounting for potential model-convergence issues, and the use of preexisting data sets. As these may prove to be significant barriers for {ESM} researchers, we also discuss ways of overcoming these challenges and of documenting them in a registration. A further challenge is that current general preregistration templates do not adequately capture the unique features of {ESM}. We present a registration template for {ESM} research and also discuss registration of studies using preexisting data.},
	pages = {2515245920924686},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Kirtley, Olivia J. and Lafit, Ginette and Achterhof, Robin and Hiekkaranta, Anu P. and Myin-Germeys, Inez},
	urldate = {2021-03-02},
	date = {2021-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {open science, preregistration, transparency, reproducibility, experience sampling, intensive longitudinal data},
	file = {Kirtley_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kirtley_etal_2021.pdf:application/pdf},
}

@article{sijtsma_playing_2016,
	title = {Playing with data—or how to discourage questionable research practices and stimulate researchers to do things right},
	volume = {81},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/s11336-015-9446-0},
	doi = {10.1007/s11336-015-9446-0},
	abstract = {Recent fraud cases in psychological and medical research have emphasized the need to pay attention to Questionable Research Practices ({QRPs}). Deliberate or not, {QRPs} usually have a deteriorating effect on the quality and the credibility of research results. {QRPs} must be revealed but prevention of {QRPs} is more important than detection. I suggest two policy measures that I expect to be effective in improving the quality of psychological research. First, the research data and the research materials should be made publicly available so as to allow verification. Second, researchers should more readily consider consulting a methodologist or a statistician. These two measures are simple but run against common practice to keep data to oneself and overestimate one’s methodological and statistical skills, thus allowing secrecy and errors to enter research practice.},
	pages = {1--15},
	number = {1},
	journaltitle = {Psychometrika},
	shortjournal = {Psychometrika},
	author = {Sijtsma, Klaas},
	urldate = {2021-03-03},
	date = {2016-03-01},
	langid = {english},
	file = {Sijtsma_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Sijtsma_2016.pdf:application/pdf},
}

@article{giner-sorolla_science_2012,
	title = {Science or art? How aesthetic standards grease the way through the publication bottleneck but undermine science},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612457576},
	doi = {10.1177/1745691612457576},
	shorttitle = {Science or art?},
	abstract = {The current crisis in psychological research involves issues of fraud, replication, publication bias, and false positive results. I argue that this crisis follows the failure of widely adopted solutions to psychology’s similar crisis of the 1970s. The untouched root cause is an information-economic one: Too many studies divided by too few publication outlets equals a bottleneck. Articles cannot pass through just by showing theoretical meaning and methodological rigor; their results must appear to support the hypothesis perfectly. Consequently, psychologists must master the art of presenting perfect-looking results just to survive in the profession. This favors aesthetic criteria of presentation in a way that harms science’s search for truth. Shallow standards of statistical perfection distort analyses and undermine the accuracy of cumulative data; narrative expectations encourage dishonesty about the relationship between results and hypotheses; criteria of novelty suppress replication attempts. Concerns about truth in research are emerging in other sciences and may eventually descend on our heads in the form of difficult and insensitive regulations. I suggest a more palatable solution: to open the bottleneck, putting structures in place to reward broader forms of information sharing beyond the exquisite art of present-day journal publication.},
	pages = {562--571},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Giner-Sorolla, Roger},
	urldate = {2020-09-22},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Giner-Sorolla_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Giner-Sorolla_2012.pdf:application/pdf},
}

@article{benning_registration_2019,
	title = {The registration continuum in clinical science: a guide toward transparent practices},
	volume = {128},
	issn = {0021-843X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6677163/},
	doi = {10.1037/abn0000451},
	shorttitle = {The registration continuum in clinical science},
	abstract = {Clinical scientists can use a continuum of registration efforts that vary in their disclosure and timing relative to data collection and analysis. Broadly speaking, registration benefits investigators by offering stronger, more powerful tests of theory with particular methods in tandem with better control of long-run false positive error rates. Registration helps clinical researchers in thinking through tensions between bandwidth and fidelity that surround recruiting participants, defining clinical phenotypes, handling comorbidity, treating missing data, and analyzing rich and complex data. In particular, registration helps record and justify the reasons behind specific study design decisions, though it also provides the opportunity to register entire decision trees with specific endpoints. Creating ever more faithful registrations and standard operating procedures may offer alternative methods of judging a clinical investigator’s scientific skill and eminence because study registration increases the transparency of clinical researchers’ work., General Scientific Summary: Study registration allows clinical scientists to make their work more credible and transparent to fellow researchers and the general public. We describe the dimensions of disclosure and timing relative to data collection and analysis on which researchers can register their study designs, how registration makes for better science, and the kinds of issues for which registration is particularly helpful in clinical research. We also show how registration permits researchers to flexibly register specific individual decisions or complex decision trees to deal with potential problems in a study.},
	pages = {528--540},
	number = {6},
	journaltitle = {Journal of abnormal psychology},
	shortjournal = {J Abnorm Psychol},
	author = {Benning, Stephen D. and Bachrach, Rachel L. and Smith, Edward A. and Freeman, Andrew J. and Wright, Aidan G. C.},
	urldate = {2020-10-01},
	date = {2019-08},
	pmid = {31368732},
	pmcid = {PMC6677163},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/VULFSRTA/Benning et al. - 2019 - The Registration Continuum in Clinical Science A .pdf:application/pdf},
}

@article{hitchcock_prediction_2004,
	title = {Prediction versus accommodation and the risk of overfitting},
	volume = {55},
	issn = {0007-0882},
	url = {https://academic.oup.com/bjps/article/55/1/1/1547570},
	doi = {10.1093/bjps/55.1.1},
	abstract = {Abstract. When a scientist uses an observation to formulate a theory, it is no surprise that the resulting theory accurately captures that observation. However,},
	pages = {1--34},
	number = {1},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {Br J Philos Sci},
	author = {Hitchcock, Christopher and Sober, Elliott},
	urldate = {2020-12-03},
	date = {2004-03-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Hitchcock_Sober_2004.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hitchcock_Sober_2004.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/S95JEQ3F/1547570.html:text/html;Snapshot:/Users/tom/Zotero/storage/U4MC2CPT/1547570.html:text/html},
}

@article{hudson_whats_2007,
	title = {What's really at issue with novel predictions?},
	volume = {155},
	url = {http://www.jstor.org/stable/27653475},
	doi = {10.1007/s11229-005-6267-1},
	abstract = {In this paper I distinguish two kinds of predictivism, 'timeless' and 'historicized'. The former is the conventional understanding of predictivism. How ever, I argue that its defense in the works of John Worrall (Scerri and Worrall 2001, Studies in History and Philosophy of Science 32, 407-452; Worrall 2002, In the Scope of Logic, Methodology and Philosophy of Science, 1, 191-209) and Patrick Maher (Maher 1988, {PSA} 1988, 1, pp. 273) is wanting. Alternatively, I promote an his toricized predictivism, and briefly defend such a predictivism at the end of the paper.},
	pages = {1--20},
	number = {1},
	journaltitle = {Synthese},
	author = {Hudson, Robert G.},
	date = {2007},
	langid = {english},
	file = {Hudson - 2007 - What's Really at Issue with Novel Predictions.pdf:/Users/tom/Zotero/storage/CZRGLXCR/Hudson - 2007 - What's Really at Issue with Novel Predictions.pdf:application/pdf},
}

@article{mayo_how_2008,
	title = {How to discount double-counting when it counts: some clarifications},
	volume = {59},
	abstract = {The issues of double-counting, use-constructing, and selection effects have long be the subject of debate in the philosophical as well as statistical literature. I have argu that it is the severity, stringency, or probativeness of the test - or lack of it - that sho determine if a double-use of data is admissible. Hitchcock and Sober ([2004]) questio whether this 'severity criterion' can perform its intended job. I argue that their criticis stem from a flawed interpretation of the severity criterion. Taking their criticism springboard, I elucidate some of the central examples that have long been controvers and clarify how the severity criterion is properly applied to them.},
	pages = {857--879},
	number = {4},
	journaltitle = {The British Journal for the Philosophy of Science},
	author = {Mayo, Deborah G.},
	date = {2008},
	langid = {english},
	file = {Mayo - 2008 - How to discount double-counting when it counts so.pdf:/Users/tom/Zotero/storage/IEANV3H8/Mayo - 2008 - How to discount double-counting when it counts so.pdf:application/pdf},
}

@article{vul_puzzlingly_2009,
	title = {Puzzlingly high correlations in fmri studies of emotion, personality, and social cognition},
	volume = {4},
	issn = {1745-6916},
	url = {https://doi.org/10.1111/j.1745-6924.2009.01125.x},
	doi = {10.1111/j.1745-6924.2009.01125.x},
	abstract = {Functional magnetic resonance imaging ({fMRI}) studiesofemotion, personality, and social cognition have drawn much attention in recent years, with high-profile studies frequently reporting extremely high (e.g., {\textgreater}.8) correlations between brain activation and personality measures. We show that these correlations are higher than should be expected given the (evidently limited) reliability of both {fMRI} and personality measures. The high correlations are all the more puzzling because method sections rarely contain much detail about how the correlations were obtained. We surveyed authors of 55 articles that reported findings of this kind to determine a few details on how these correlations were computed. More than half acknowledged using a strategy that computes separate correlations for individual voxels and reports means of only those voxels exceeding chosen thresholds. We show how this nonindependent analysis inflates correlations while yielding reassuring-looking scattergrams. This analysis technique was used to obtain the vast majority of the implausibly high correlations in our survey sample. In addition, we argue that, in some cases, other analysis problems likely created entirely spurious correlations. We outline how the data from these studies could be reanalyzed with unbiased methods to provide accurate estimates of the correlations in question and urge authors to perform such reanalyses. The underlying problems described here appear to be common in {fMRI} research of many kinds—not just in studies of emotion, personality, and social cognition.},
	pages = {274--290},
	number = {3},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Vul, Edward and Harris, Christine and Winkielman, Piotr and Pashler, Harold},
	urldate = {2021-03-02},
	date = {2009-05-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Vul_etal_2009.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vul_etal_2009.pdf:application/pdf},
}

@article{kriegeskorte_circular_2009,
	title = {Circular analysis in systems neuroscience: the dangers of double dipping},
	volume = {12},
	rights = {2009 Nature Publishing Group},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.2303/},
	doi = {10.1038/nn.2303},
	shorttitle = {Circular analysis in systems neuroscience},
	abstract = {This perspective illustrates some of the problems involved in analyzing the complex data yielded by systems neuroscience techniques, such as brain imaging and electrophysiology. Specifically, when test statistics are not independent of the selection criteria, common analyses can produce spurious results. The authors suggest ways to avoid such errors.},
	pages = {535--540},
	number = {5},
	journaltitle = {Nature Neuroscience},
	author = {Kriegeskorte, Nikolaus and Simmons, W. Kyle and Bellgowan, Patrick S. F. and Baker, Chris I.},
	urldate = {2021-03-02},
	date = {2009-05},
	langid = {english},
	note = {Number: 5
Publisher: Nature Publishing Group},
	file = {Kriegeskorte_etal_2009.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kriegeskorte_etal_2009.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/42MVEQUI/nn.2303.html:text/html},
}

@article{bakker_rules_2012,
	title = {The rules of the game called psychological science},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612459060},
	doi = {10.1177/1745691612459060},
	abstract = {If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96\% of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p {\textless} .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.},
	pages = {543--554},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Bakker, Marjan and van Dijk, Annette and Wicherts, Jelte M.},
	urldate = {2021-03-02},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {false positives, power, publication bias, replication, sample size},
	file = {Bakker_etal_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bakker_etal_2012.pdf:application/pdf},
}

@incollection{hahn_what_2014,
	title = {What does it mean to be biased},
	volume = {61},
	isbn = {978-0-12-800283-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128002834000022},
	pages = {41--102},
	booktitle = {Psychology of Learning and Motivation},
	publisher = {Elsevier},
	author = {Hahn, Ulrike and Harris, Adam J.L.},
	urldate = {2021-03-02},
	date = {2014},
	langid = {english},
	doi = {10.1016/B978-0-12-800283-4.00002-2},
	file = {Hahn and Harris - 2014 - What Does It Mean to be Biased.pdf:/Users/tom/Zotero/storage/7DD6HXFF/Hahn and Harris - 2014 - What Does It Mean to be Biased.pdf:application/pdf},
}

@article{lilburn_cultural_2019,
	title = {Cultural problems cannot be solved with technical solutions alone},
	volume = {2},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-019-00036-z},
	doi = {10.1007/s42113-019-00036-z},
	abstract = {A crisis in psychology has provoked researchers to seek remedies for bad practices that might damage the integrity of the discipline as a whole. The ardor for wholesale reform has led to a suite of proposed technical solutions, some of which are considered in the context of computational modeling by the target article. Any technical solution, however, must be placed within a larger cultural and scientific context to be effective (or, indeed, meaningful at all). Many of the suggestions presented in the target article represent good practice in computational cognitive modeling but, even then, still require some amount of nuance in the consideration of the relationship between practice and theory. We consider two examples—model preregistration and bookending—as a means of examining the limits of any proposed technical solution.},
	pages = {170--175},
	number = {3},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Lilburn, Simon D. and Little, Daniel R. and Osth, Adam F. and Smith, Philip L.},
	urldate = {2021-02-24},
	date = {2019-12-01},
	langid = {english},
	file = {Lilburn_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lilburn_etal_2019.pdf:application/pdf},
}

@article{starns_assessing_2019,
	title = {Assessing theoretical conclusions with blinded inference to investigate a potential inference crisis},
	volume = {2},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245919869583},
	doi = {10.1177/2515245919869583},
	abstract = {Scientific advances across a range of disciplines hinge on the ability to make inferences about unobservable theoretical entities on the basis of empirical data patterns. Accurate inferences rely on both discovering valid, replicable data patterns and accurately interpreting those patterns in terms of their implications for theoretical constructs. The replication crisis in science has led to widespread efforts to improve the reliability of research findings, but comparatively little attention has been devoted to the validity of inferences based on those findings. Using an example from cognitive psychology, we demonstrate a blinded-inference paradigm for assessing the quality of theoretical inferences from data. Our results reveal substantial variability in experts’ judgments on the very same data, hinting at a possible inference crisis.},
	pages = {335--349},
	number = {4},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Starns, Jeffrey J. and Cataldo, Andrea M. and Rotello, Caren M. and Annis, Jeffrey and Aschenbrenner, Andrew and Bröder, Arndt and Cox, Gregory and Criss, Amy and Curl, Ryan A. and Dobbins, Ian G. and Dunn, John and Enam, Tasnuva and Evans, Nathan J. and Farrell, Simon and Fraundorf, Scott H. and Gronlund, Scott D. and Heathcote, Andrew and Heck, Daniel W. and Hicks, Jason L. and Huff, Mark J. and Kellen, David and Key, Kylie N. and Kilic, Asli and Klauer, Karl Christoph and Kraemer, Kyle R. and Leite, Fábio P. and Lloyd, Marianne E. and Malejka, Simone and Mason, Alice and {McAdoo}, Ryan M. and {McDonough}, Ian M. and Michael, Robert B. and Mickes, Laura and Mizrak, Eda and Morgan, David P. and Mueller, Shane T. and Osth, Adam and Reynolds, Angus and Seale-Carlisle, Travis M. and Singmann, Henrik and Sloane, Jennifer F. and Smith, Andrew M. and Tillman, Gabriel and van Ravenzwaaij, Don and Weidemann, Christoph T. and Wells, Gary L. and White, Corey N. and Wilson, Jack},
	urldate = {2021-03-01},
	date = {2019-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {Bayesian methods, blinded inference, bootstrap, memory, metascience, modeling, open data},
	file = {Starns_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Starns_etal_2019.pdf:application/pdf},
}

@article{clemens_meaning_2017,
	title = {The meaning of failed replications: a review and proposal},
	volume = {31},
	rights = {© 2015 John Wiley \& Sons Ltd},
	issn = {1467-6419},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12139},
	doi = {10.1111/joes.12139},
	shorttitle = {The meaning of failed replications},
	abstract = {The welcome rise of replication tests in economics has not been accompanied by a consensus standard for determining what constitutes a replication. A discrepant replication, in current usage of the term, can signal anything from an unremarkable disagreement over methods to scientific incompetence or misconduct. This paper proposes a standard for classifying one study as a replication of some other study. It is a standard that places the burden of proof on a study to demonstrate that it should have obtained identical results to the original, a conservative standard that is already used implicitly by many researchers. It contrasts this standard with decades of unsuccessful attempts to harmonize terminology, and argues that many prominent results described as replication tests should not be described as such. Adopting a conservative standard like this one can improve incentives for researchers, encouraging more and better replication tests.},
	pages = {326--342},
	number = {1},
	journaltitle = {Journal of Economic Surveys},
	author = {Clemens, Michael A.},
	urldate = {2021-03-01},
	date = {2017},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/joes.12139},
	keywords = {Ethics, Open data, Replication, Robustness, Transparency},
	file = {Clemens_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Clemens_2017.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JRGV2AU4/joes.html:text/html},
}

@article{ioannidis_why_2008,
	title = {Why most discovered true associations are inflated},
	volume = {19},
	issn = {1531-5487},
	doi = {10.1097/EDE.0b013e31818131e7},
	abstract = {Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated-for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.},
	pages = {640--648},
	number = {5},
	journaltitle = {Epidemiology},
	shortjournal = {Epidemiology},
	author = {Ioannidis, John P. A.},
	date = {2008-09},
	pmid = {18633328},
	keywords = {Clinical Trials as Topic, Data Interpretation, Statistical, Humans, Linkage Disequilibrium, Models, Statistical, Molecular Epidemiology, Sensitivity and Specificity},
	file = {Ioannidis_2008.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ioannidis_2008.pdf:application/pdf},
}

@article{rubin_evaluation_2017,
	title = {An evaluation of four solutions to the forking paths problem: adjusted alpha, preregistration, sensitivity analyses, and abandoning the neyman-pearson approach},
	volume = {21},
	issn = {1089-2680},
	url = {https://doi.org/10.1037/gpr0000135},
	doi = {10.1037/gpr0000135},
	shorttitle = {An evaluation of four solutions to the forking paths problem},
	abstract = {Gelman and Loken (2013, 2014) proposed that when researchers base their statistical analyses on the idiosyncratic characteristics of a specific sample (e.g., a nonlinear transformation of a variable because it is skewed), they open up alternative analysis paths in potential replications of their study that are based on different samples (i.e., no transformation of the variable because it is not skewed). These alternative analysis paths count as additional (multiple) tests and, consequently, they increase the probability of making a Type I error during hypothesis testing. The present article considers this forking paths problem and evaluates four potential solutions that might be used in psychology and other fields: (a) adjusting the prespecified alpha level, (b) preregistration, (c) sensitivity analyses, and (d) abandoning the Neyman-Pearson approach. It is concluded that although preregistration and sensitivity analyses are effective solutions to p-hacking, they are ineffective against result-neutral forking paths, such as those caused by transforming data. Conversely, although adjusting the alpha level cannot address p-hacking, it can be effective for result-neutral forking paths. Finally, abandoning the Neyman-Pearson approach represents a further solution to the forking paths problem.},
	pages = {321--329},
	number = {4},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Rubin, Mark},
	urldate = {2020-12-22},
	date = {2017-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {preregistration, forking paths, null hypothesis significance testing, replication crisis, sensitivity analyses},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/FSZHC2KG/Rubin - 2017 - An Evaluation of Four Solutions to the Forking Pat.pdf:application/pdf},
}

@article{gelman_statistical_2014,
	title = {The statistical crisis in science},
	volume = {102},
	issn = {0003-0996, 1545-2786},
	url = {https://www.americanscientist.org/article/the-statistical-crisis-in-science},
	doi = {10.1511/2014.111.460},
	pages = {460--465},
	number = {6},
	journaltitle = {American Scientist},
	shortjournal = {Am. Sci.},
	author = {Gelman, Andrew and Loken, Eric},
	urldate = {2020-09-20},
	date = {2014},
	file = {Gelman_Loken_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Gelman_Loken_2014.pdf:application/pdf},
}

@article{van_dongen_multiple_2019,
	title = {Multiple perspectives on inference for two simple statistical scenarios},
	volume = {73},
	issn = {0003-1305},
	url = {https://amstat.tandfonline.com/doi/full/10.1080/00031305.2019.1565553},
	doi = {10.1080/00031305.2019.1565553},
	abstract = {When data analysts operate within different statistical frameworks (e.g., frequentist versus Bayesian, emphasis on estimation versus emphasis on testing), how does this impact the qualitative conclusions that are drawn for real data? To study this question empirically we selected from the literature two simple scenarios—involving a comparison of two proportions and a Pearson correlation—and asked four teams of statisticians to provide a concise analysis and a qualitative interpretation of the outcome. The results showed considerable overall agreement; nevertheless, this agreement did not appear to diminish the intensity of the subsequent debate over which statistical framework is more appropriate to address the questions at hand.},
	pages = {328--339},
	issue = {sup1},
	journaltitle = {The American Statistician},
	shortjournal = {null},
	author = {van Dongen, Noah N. N. and van Doorn, Johnny B. and Gronau, Quentin F. and van Ravenzwaaij, Don and Hoekstra, Rink and Haucke, Matthias N. and Lakens, Daniel and Hennig, Christian and Morey, Richard D. and Homer, Saskia and Gelman, Andrew and Sprenger, Jan and Wagenmakers, Eric-Jan},
	urldate = {2020-09-22},
	date = {2019-03-29},
	note = {Publisher: Taylor \& Francis},
	file = {Snapshot:/Users/tom/Zotero/storage/63I5M2YX/00031305.2019.html:text/html;van Dongen_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/van Dongen_etal_2019.pdf:application/pdf},
}

@article{berry_multiplicities_2012,
	title = {Multiplicities in cancer research: ubiquitous and necessary evils},
	volume = {104},
	issn = {0027-8874},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4614276/},
	doi = {10.1093/jnci/djs301},
	shorttitle = {Multiplicities in cancer research},
	abstract = {Scientific inquiry involves observations and measurements, some of which are planned and some of which are not. The most interesting or unusual observations might be regarded as discoveries and therefore particularly worthy of publication. However, the observational process is fraught with inferential land mines, especially if the discoveries are serendipitous. Multiple observations increase the probability of false-positive conclusions and have led to many false and otherwise misleading publications. Statisticians recommend adjustments to final inferences with the goal of reducing the rate of false positives, a strategy that increases the rate of false negatives. Some scientists object to making such adjustments, arguing that it should not be more difficult to determine the validity of a discovery simply because other observations were made. Which tack is right? How does one decide that any particular scientific discovery is real? Unfortunately, there is no panacea, no one-size-fits-all approach. The goal of this commentary is to elucidate the issues and provide recommendations for conducting and reporting results of empirical studies, with emphasis on the problems of multiple comparisons and other types of multiplicities, including what I call “silent multiplicities.” Because of the many observations, outcomes, subsets, treatments, etc, that are typically made or addressed in epidemiology and biomarker research, these recommendations may be particularly relevant for such studies. However, the lessons apply quite generally. I consider both frequentist and Bayesian statistical approaches.},
	pages = {1125--1133},
	number = {15},
	journaltitle = {{JNCI} Journal of the National Cancer Institute},
	shortjournal = {J Natl Cancer Inst},
	author = {Berry, Donald},
	urldate = {2021-03-01},
	date = {2012-08-08},
	pmid = {22859849},
	pmcid = {PMC4614276},
	file = {Berry_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Berry_2012.pdf:application/pdf},
}

@article{giudice_travelers_2021,
	title = {A traveler’s guide to the multiverse: promises, pitfalls, and a framework for the evaluation of analytic decisions},
	volume = {4},
	doi = {https://doi.org/10.1177/2515245920954925},
	abstract = {Decisions made by researchers while analyzing data (e.g., how variables are measured, how outliers are handled) are sometimes arbitrary, with no alternative objectively justified over another. Multiverse-style methods (e.g., specification curve, vibration of effects) estimate an effect across an entire set of possible specifications, to expose the impact of hidden degrees of freedom and/or obtain robust, less biased estimates of the effect of interest. However, if specifications are not truly arbitrary, multiverse-style analyses can produce misleading results, potentially hiding meaningful effects within a mass of poorly justified alternatives. So far, a key question has received scant attention: How does one decide whether alternatives are arbitrary? We offer a framework and conceptual tools for doing so. We discuss three kinds of a priori nonequivalence among alternatives—measurement non-equivalence, effect non-equivalence, and power/precision non-equivalence. The criteria we review lead to three decision scenarios: Type E decisions (principled equivalence), Type N decisions (principled non-equivalence), and Type U decisions (uncertainty). In uncertain scenarios, multiverse-style analysis should be conducted in a deliberately exploratory fashion. The framework is discussed with reference to published examples and illustrated with the help of a simulated dataset. Our framework will help researchers reap the benefits of multiverse-style methods while avoiding their pitfalls.},
	pages = {1--15},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Giudice, M Del and Gangestad, {SW}},
	date = {2021},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Giudice and Gangestad - A Traveler’s Guide to the Multiverse Promises, Pi.pdf:/Users/tom/Zotero/storage/CMNKYCTR/Giudice and Gangestad - A Traveler’s Guide to the Multiverse Promises, Pi.pdf:application/pdf},
}

@article{garcia-perez_statistical_2012,
	title = {Statistical conclusion validity: some common threats and simple remedies},
	volume = {3},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00325/full},
	doi = {10.3389/fpsyg.2012.00325},
	shorttitle = {Statistical conclusion validity},
	abstract = {The ultimate goal of research is to produce dependable knowledge or to provide the evidence that may guide practical decisions. Statistical conclusion validity ({SCV}) holds when the conclusions of a research study are founded on an adequate analysis of the data, generally meaning that adequate statistical methods are used whose small-sample behavior is accurate, besides being logically capable of providing an answer to the research question. Compared to the three other traditional aspects of research validity (external validity, internal validity, and construct validity), interest in {SCV} has recently grown on evidence that inadequate data analyses are sometimes carried out which yield conclusions that a proper analysis of the data would not have supported. This paper discusses evidence of three common threats to {SCV} that arise from widespread recommendations or practices in data analysis, namely, the use of repeated testing and optional stopping without control of Type-I error rates, the recommendation to check the assumptions of statistical tests, and the use of regression whenever a bivariate relation or the equivalence between two variables is studied. For each of these threats, examples are presented and alternative practices that safeguard {SCV} are discussed. Educational and editorial changes that may improve the {SCV} of published research are also discussed.},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {García-Pérez, Miguel A.},
	urldate = {2021-03-01},
	date = {2012},
	note = {Publisher: Frontiers},
	keywords = {data analysis, preliminary tests, regression, stopping rules, validity of research},
	file = {García-Pérez_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/García-Pérez_2012.pdf:application/pdf},
}

@article{jarvinen_blinded_2014,
	title = {Blinded interpretation of study results can feasibly and effectively diminish interpretation bias},
	volume = {67},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435613004861},
	doi = {10.1016/j.jclinepi.2013.11.011},
	abstract = {Objective
Controversial and misleading interpretation of data from randomized trials is common. How to avoid misleading interpretation has received little attention. Herein, we describe two applications of an approach that involves blinded interpretation of the results by study investigators.
Study Design and Settings
The approach involves developing two interpretations of the results on the basis of a blinded review of the primary outcome data (experimental treatment A compared with control treatment B). One interpretation assumes that A is the experimental intervention and another assumes that A is the control. After agreeing that there will be no further changes, the investigators record their decisions and sign the resulting document. The randomization code is then broken, the correct interpretation chosen, and the manuscript finalized. Review of the document by an external authority before finalization can provide another safeguard against interpretation bias.
Results
We found the blinded preparation of a summary of data interpretation described in this article practical, efficient, and useful.
Conclusions
Blinded data interpretation may decrease the frequency of misleading data interpretation. Widespread adoption of blinded data interpretation would be greatly facilitated were it added to the minimum set of recommendations outlining proper conduct of randomized controlled trials (eg, the Consolidated Standards of Reporting Trials statement).},
	pages = {769--772},
	number = {7},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Järvinen, Teppo L. N. and Sihvonen, Raine and Bhandari, Mohit and Sprague, Sheila and Malmivaara, Antti and Paavola, Mika and Schünemann, Holger J. and Guyatt, Gordon H.},
	urldate = {2021-03-01},
	date = {2014-07-01},
	langid = {english},
	keywords = {Bias, Data interpretations, Double-blind method, Drug evaluation/methods, Randomized controlled trials as topic/methods, Research design},
	file = {Järvinen_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Järvinen_etal_2014.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/NZB2TXZW/S0895435613004861.html:text/html},
}

@article{chambers_instead_2014,
	title = {Instead of “playing the game” it is time to change the rules: Registered Reports at {AIMS} Neuroscience and beyond},
	volume = {1},
	issn = {2373-7972},
	url = {http://www.aimspress.com/article/10.3934/Neuroscience.2014.1.4},
	doi = {10.3934/Neuroscience.2014.1.4},
	shorttitle = {Instead of “playing the game” it is time to change the rules},
	pages = {4--17},
	number = {1},
	journaltitle = {{AIMS} Neuroscience},
	author = {Chambers, Christopher D. and Feredoes, Eva and Muthukumaraswamy, Suresh D. and Etchells, Peter J.},
	urldate = {2020-10-01},
	date = {2014},
	langid = {english},
	file = {Chambers et al. - 2014 - Instead of “playing the game” it is time to change.pdf:/Users/tom/Zotero/storage/VIA9IL5N/Chambers et al. - 2014 - Instead of “playing the game” it is time to change.pdf:application/pdf},
}

@article{fabrigar_validity-based_2020,
	title = {A validity-based framework for understanding replication in psychology},
	volume = {24},
	issn = {1088-8683, 1532-7957},
	url = {http://journals.sagepub.com/doi/10.1177/1088868320931366},
	doi = {10.1177/1088868320931366},
	abstract = {In recent years, psychology has wrestled with the broader implications of disappointing rates of replication of previously demonstrated effects. This article proposes that many aspects of this pattern of results can be understood within the classic framework of four proposed forms of validity: statistical conclusion validity, internal validity, construct validity, and external validity. The article explains the conceptual logic for how differences in each type of validity across an original study and a subsequent replication attempt can lead to replication “failure.” Existing themes in the replication literature related to each type of validity are also highlighted. Furthermore, empirical evidence is considered for the role of each type of validity in nonreplication. The article concludes with a discussion of broader implications of this classic validity framework for improving replication rates in psychological research.},
	pages = {316--244},
	number = {4},
	journaltitle = {Personality and Social Psychology Review},
	shortjournal = {Pers Soc Psychol Rev},
	author = {Fabrigar, Leandre R. and Wegener, Duane T. and Petty, Richard E.},
	urldate = {2020-08-03},
	date = {2020},
	langid = {english},
	file = {Fabrigar et al. - 2020 - A Validity-Based Framework for Understanding Repli.pdf:/Users/tom/Zotero/storage/PHZS9QZT/Fabrigar et al. - 2020 - A Validity-Based Framework for Understanding Repli.pdf:application/pdf},
}

@collection{salmon_introduction_1999,
	location = {Indianapolis, {IN}},
	edition = {Reprinted},
	title = {Introduction to the philosophy of science},
	isbn = {978-0-87220-450-8 978-0-87220-451-5},
	pagetotal = {458},
	publisher = {Hackett Publ},
	editor = {Salmon, Merrilee H. and University of Pittsburgh},
	date = {1999},
	langid = {english},
	note = {{OCLC}: 245745444},
	file = {Salmon and University of Pittsburgh - 1999 - Introduction to the philosophy of science.pdf:/Users/tom/Zotero/storage/KHMH7LNK/Salmon and University of Pittsburgh - 1999 - Introduction to the philosophy of science.pdf:application/pdf},
}

@book{silver_signal_2020,
	title = {The signal and the noise: why so many predictions fail-- but some don't},
	isbn = {978-1-101-59595-4},
	url = {http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=6057299},
	shorttitle = {The signal and the noise},
	abstract = {The author has built an innovative system for predicting baseball performance, predicted the 2008 election within a hair's breadth, and has become a national sensation as a blogger. Drawing on his own groundbreaking work, he examines the world of prediction.},
	author = {Silver, Nate},
	urldate = {2021-02-26},
	date = {2020},
	langid = {english},
	note = {{OCLC}: 1194646714},
	file = {Silver - 2020 - The signal and the noise why so many predictions .pdf:/Users/tom/Zotero/storage/A44MSJU7/Silver - 2020 - The signal and the noise why so many predictions .pdf:application/pdf},
}

@report{hoffmann_multiplicity_2020,
	title = {The multiplicity of analysis strategies jeopardizes replicability: lessons learned across disciplines},
	url = {https://osf.io/preprints/metaarxiv/afb9p/},
	shorttitle = {The multiplicity of analysis strategies jeopardizes replicability},
	abstract = {For a given research question, there are usually a large variety of possible analysis strategies acceptable according to the scientific standards of the field, and there are concerns that this multiplicity of analysis strategies plays an important role in the non-replicability of research findings. Here, we define a general framework on common sources of uncertainty arising in computational analyses that lead to this multiplicity, and apply this framework within an overview of approaches proposed across disciplines to address the issue. Armed with this framework, and a set of recommendations derived therefrom, researchers will be able to recognize strategies applicable to their field and use them to generate findings more likely to be replicated in future studies, ultimately improving the credibility of the scientific process.},
	institution = {{MetaArXiv}},
	author = {Hoffmann, Sabine and Schönbrodt, Felix and Elsas, Ralf and Wilson, Rory and Strasser, Ulrich and Boulesteix, Anne-Laure},
	urldate = {2021-02-26},
	date = {2020-02-19},
	doi = {10.31222/osf.io/afb9p},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Medicine and Health Sciences, Physical Sciences and Mathematics},
	file = {Hoffmann_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hoffmann_etal_2020.pdf:application/pdf},
}

@article{scheel_excess_2020,
	title = {An excess of positive results: Comparing the standard Psychology literature with Registered Reports},
	volume = {4},
	url = {https://psyarxiv.com/p6e9c/},
	doi = {10.31234/osf.io/p6e9c},
	shorttitle = {An excess of positive results},
	abstract = {When studies with positive results that support the tested hypotheses have a higher probability of being published than studies with negative results, the literature will give a distorted view of the evidence for scientific claims. Psychological scientists have been concerned about the degree of distortion in their literature due to publication bias and inflated Type-1 error rates. Registered Reports were developed with the goal to minimise such biases: In this new publication format, peer review and the decision to publish take place before the study results are known. We compared the results in the full population of published Registered Reports in Psychology (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) by searching 633 journals for the phrase ‘test* the hypothes*’ (replicating a method by Fanelli, 2010). Analysing the first hypothesis reported in each paper, we found 96\% positive results in standard reports, but only 44\% positive results in Registered Reports. The difference remained nearly as large when direct replications were excluded from the analysis (96\% vs 50\% positive results). This large gap suggests that psychologists underreport negative results to an extent that threatens cumulative science. Although our study did not directly test the effectiveness of Registered Reports at reducing bias, these results show that the introduction of Registered Reports has led to a much larger proportion of negative results appearing in the published literature compared to standard reports.},
	pages = {1--12},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Scheel, Anne M. and Schijen, Mitchell and Lakens, Daniel},
	urldate = {2021-02-25},
	date = {2020-02-05},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, other, Psychology, publication bias, metascience, hypothesis testing, {MetaSciLog}, Registered Reports},
	file = {25152459211007467.pdf:/Users/tom/Zotero/storage/Y6J7NTC4/25152459211007467.pdf:application/pdf},
}

@article{srivastava_sound_2018,
	title = {Sound inference in complicated research: a multi-strategy approach},
	url = {https://psyarxiv.com/bwr48/},
	doi = {10.31234/osf.io/bwr48},
	shorttitle = {Sound inference in complicated research},
	abstract = {Researchers are increasingly paying attention to practices that can capitalize on chance. Capitalizing on chance can lead to unreplicable results in the form of false-positive tests, biased estimates, or overfit models. Preregistration of analysis plans, where a researcher anticipates all analysis decisions in advance and makes a plan for each one, has been promoted as one way to make inferences more sound and findings more replicable. Preregistration works by creating decision independence: analytic decisions are the same regardless of the potentially spurious features of the specific dataset being analyzed, instead of being overfit to them. But preregistration can be difficult in practice for some complicated research paradigms, including longitudinal studies, statistical modeling, machine learning, and other large multivariate designs and analyses where key decisions may be difficult to anticipate or make in advance. When simple preregistration is not practical, other strategies that create decision independence can be incorporated into it or used in place of it. This manuscript describes six such strategies: standardization, blind analysis, data partitioning, supporting studies, coordinated analysis, and multiverse analysis. All of them can be used to create adaptive preregistrations, and several can also be used when decisions arise that were not anticipated. Used together, they can form an integrative, multi-strategy approach to drawing sound inferences in a wide range of research.},
	author = {Srivastava, Sanjay},
	urldate = {2020-09-21},
	date = {2018-11-21},
	note = {Publisher: {PsyArXiv}},
	file = {Snapshot:/Users/tom/Zotero/storage/DRRTZSLW/bwr48.html:text/html;Srivastava_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Srivastava_2018.pdf:application/pdf},
}

@article{veldkamp_statistical_2014,
	title = {Statistical reporting errors and collaboration on statistical analyses in psychological science},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0114876},
	doi = {10.1371/journal.pone.0114876},
	abstract = {Statistical analysis is error prone. A best practice for researchers using statistics would therefore be to share data among co-authors, allowing double-checking of executed tasks just as co-pilots do in aviation. To document the extent to which this ‘co-piloting’ currently occurs in psychology, we surveyed the authors of 697 articles published in six top psychology journals and asked them whether they had collaborated on four aspects of analyzing data and reporting results, and whether the described data had been shared between the authors. We acquired responses for 49.6\% of the articles and found that co-piloting on statistical analysis and reporting results is quite uncommon among psychologists, while data sharing among co-authors seems reasonably but not completely standard. We then used an automated procedure to study the prevalence of statistical reporting errors in the articles in our sample and examined the relationship between reporting errors and co-piloting. Overall, 63\% of the articles contained at least one p-value that was inconsistent with the reported test statistic and the accompanying degrees of freedom, and 20\% of the articles contained at least one p-value that was inconsistent to such a degree that it may have affected decisions about statistical significance. Overall, the probability that a given p-value was inconsistent was over 10\%. Co-piloting was not found to be associated with reporting errors.},
	pages = {e114876},
	number = {12},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Veldkamp, Coosje L. S. and Nuijten, Michèle B. and Dominguez-Alvarez, Linda and Assen, Marcel A. L. M. van and Wicherts, Jelte M.},
	urldate = {2020-05-11},
	date = {2014-12-10},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Applied psychology, Clinical psychology, Cognitive neuroscience, Personality, Social psychology, Statistical data, Surveys},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ICECX9R7/Veldkamp et al. - 2014 - Statistical Reporting Errors and Collaboration on .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Z6TUTAWS/article.html:text/html},
}

@article{wicherts_willingness_2011,
	title = {Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results},
	volume = {6},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0026828},
	doi = {10.1371/journal.pone.0026828},
	abstract = {Background: The widespread reluctance to share published research data is often hypothesized to be due to the authors’ fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically.
Methods and Findings: We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance.
Conclusions: Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.},
	pages = {e26828},
	number = {11},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Wicherts, Jelte M. and Bakker, Marjan and Molenaar, Dylan},
	editor = {Tractenberg, Rochelle E.},
	urldate = {2020-05-14},
	date = {2011-11-02},
	langid = {english},
	file = {Wicherts et al. - 2011 - Willingness to Share Research Data Is Related to t.PDF:/Users/tom/Zotero/storage/GURMMKIB/Wicherts et al. - 2011 - Willingness to Share Research Data Is Related to t.PDF:application/pdf},
}

@article{nuijten_journal_2017,
	title = {Journal data sharing policies and statistical reporting inconsistencies in psychology},
	volume = {3},
	issn = {2474-7394},
	url = {http://www.collabra.org/article/10.1525/collabra.102/},
	doi = {10.1525/collabra.102},
	pages = {31},
	number = {1},
	journaltitle = {Collabra: Psychology},
	author = {Nuijten, Michèle B. and Borghuis, Jeroen and Veldkamp, Coosje L. S. and Dominguez-Alvarez, Linda and Van Assen, Marcel A. L. M. and Wicherts, Jelte M.},
	urldate = {2020-05-14},
	date = {2017-12-15},
	langid = {english},
	file = {Nuijten et al. - 2017 - Journal Data Sharing Policies and Statistical Repo.pdf:/Users/tom/Zotero/storage/B43TWINC/Nuijten et al. - 2017 - Journal Data Sharing Policies and Statistical Repo.pdf:application/pdf},
}

@article{wicherts_degrees_2016,
	title = {Degrees of freedom in planning, running, analyzing, and reporting psychological studies: a checklist to avoid p-hacking},
	volume = {7},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01832/full},
	doi = {10.3389/fpsyg.2016.01832},
	shorttitle = {Degrees of freedom in planning, running, analyzing, and reporting psychological studies},
	abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and van Aert, Robbie C. M. and van Assen, Marcel A. L. M.},
	urldate = {2020-09-18},
	date = {2016},
	note = {Publisher: Frontiers},
	keywords = {p-hacking, questionable research practices, Bias, Experimental design (study designs), Research methods education, significance chasing, Significance testing},
	file = {Wicherts_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wicherts_etal_2016.pdf:application/pdf},
}

@article{wagenmakers_agenda_2012,
	title = {An agenda for purely confirmatory research},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612463078},
	doi = {10.1177/1745691612463078},
	abstract = {The veracity of substantive research claims hinges on the way experimental data are collected and analyzed. In this article, we discuss an uncomfortable fact that threatens the core of psychology’s academic enterprise: almost without exception, psychologists do not commit themselves to a method of data analysis before they see the actual data. It then becomes tempting to fine tune the analysis to the data in order to obtain a desired result—a procedure that invalidates the interpretation of the common statistical tests. The extent of the fine tuning varies widely across experiments and experimenters but is almost impossible for reviewers and readers to gauge. To remedy the situation, we propose that researchers preregister their studies and indicate in advance the analyses they intend to conduct. Only these analyses deserve the label “confirmatory,” and only for these analyses are the common statistical tests valid. Other analyses can be carried out but these should be labeled “exploratory.” We illustrate our proposal with a confirmatory replication attempt of a study on extrasensory perception.},
	pages = {632--638},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and van der Maas, Han L. J. and Kievit, Rogier A.},
	urldate = {2020-09-18},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Wagenmakers_etal_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wagenmakers_etal_2012.pdf:application/pdf},
}

@article{altman_poor-quality_2002,
	title = {Poor-quality medical research: what can journals do?},
	volume = {287},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.287.21.2765},
	doi = {10.1001/jama.287.21.2765},
	abstract = {The aim of medical research is to advance scientific knowledge and hence—directly
or indirectly—lead to improvements in the treatment and prevention of
disease. Each research project should continue systematically from previous
research and feed into future research. Each project should contribute beneficially
to a slowly evolving body of research. A study should not mislead; otherwise
it could adversely affect clinical practice and future research. In 1994 I
observed that research papers commonly contain methodological errors, report
results selectively, and draw unjustified conclusions. Here I revisit the
topic and suggest how journal editors can help.},
	pages = {2765--2767},
	number = {21},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Altman, Douglas G.},
	urldate = {2021-02-24},
	date = {2002-06-05},
	file = {Altman_2002.pdf:/Users/tom/pCloud Drive/Zotero_Library/Altman_2002.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VJRUC8J6/194993.html:text/html},
}

@article{marcus_what_2014,
	title = {What studies of retractions tell us},
	volume = {15},
	issn = {1935-7877, 1935-7885},
	url = {https://www.asmscience.org/content/journal/jmbe/10.1128/jmbe.v15i2.855},
	doi = {10.1128/jmbe.v15i2.855},
	abstract = {The retraction is receiving a growing amount of attention as an important event in scientific and scholarly publishing. Not only are some journals becoming increasingly open in their handling of the articles they withdraw—allowing researchers to gain important insights into the work of their colleagues—but scholars, too, have greater access to the reasons for retractions, information that is dramatically reshaping our understanding of such events. As this article will demonstrate, recent research has inverted the accepted lore about why retractions happen and their impact.},
	pages = {151--154},
	number = {2},
	journaltitle = {Journal of Microbiology \& Biology Education},
	author = {Marcus, Adam and Oransky, Ivan},
	urldate = {2021-02-20},
	date = {2014-12-15},
	langid = {english},
	note = {Company: asm Pub2Web
Distributor: asm Pub2Web
Institution: asm Pub2Web
Label: asm Pub2Web
Publisher: American Society of Microbiology},
	file = {Marcus_Oransky_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Marcus_Oransky_2014.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/A5Z5PT6E/jmbe.v15i2.html:text/html},
}

@article{nissen_publication_2016,
	title = {Publication bias and the canonization of false facts},
	volume = {5},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.21451},
	doi = {10.7554/eLife.21451},
	abstract = {Science is facing a “replication crisis” in which many experimental findings cannot be replicated and are likely to be false. Does this imply that many scientific facts are false as well? To find out, we explore the process by which a claim becomes fact. We model the community’s confidence in a claim as a Markov process with successive published results shifting the degree of belief. Publication bias in favor of positive findings influences the distribution of published results. We find that unless a sufficient fraction of negative results are published, false claims frequently can become canonized as fact. Data-dredging, p-hacking, and similar behaviors exacerbate the problem. Should negative results become easier to publish as a claim approaches acceptance as a fact, however, true and false claims would be more readily distinguished. To the degree that the model reflects the real world, there may be serious concerns about the validity of purported facts in some disciplines.},
	pages = {e21451},
	journaltitle = {{eLife}},
	author = {Nissen, Silas Boye and Magidson, Tali and Gross, Kevin and Bergstrom, Carl T},
	editor = {Rodgers, Peter},
	urldate = {2021-02-22},
	date = {2016-12-20},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {publication bias, replication crisis, hypothesis testing, false positive},
	file = {Nissen_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Nissen_etal_2016.pdf:application/pdf},
}

@book{mayo_error_1996,
	location = {Chicago},
	title = {Error and the growth of experimental knowledge},
	isbn = {978-0-226-51197-9 978-0-226-51198-6},
	series = {Science and its conceptual foundations},
	pagetotal = {493},
	publisher = {University of Chicago Press},
	author = {Mayo, Deborah G.},
	date = {1996},
	langid = {english},
	keywords = {Bayesian statistical decision theory, Error analysis (Mathematics), Philosophy, Science},
	file = {Mayo - 1996 - Error and the growth of experimental knowledge.pdf:/Users/tom/Zotero/storage/VBI3DJCP/Mayo - 1996 - Error and the growth of experimental knowledge.pdf:application/pdf},
}

@article{decullier_visibility_2013,
	title = {Visibility of retractions: a cross-sectional one-year study},
	volume = {6},
	issn = {1756-0500},
	url = {https://doi.org/10.1186/1756-0500-6-238},
	doi = {10.1186/1756-0500-6-238},
	shorttitle = {Visibility of retractions},
	abstract = {Retraction in Medline medical literature experienced a tenfold increase between 1999 and 2009, however retraction remains a rare event since it represents 0.02\% of publications. Retractions used to be handled following informal practices until they were formalized in 2009 by the Committee on Publication Ethics ({COPE}). The objective of our study was to describe the compliance to these guidelines.},
	pages = {238},
	number = {1},
	journaltitle = {{BMC} Research Notes},
	shortjournal = {{BMC} Research Notes},
	author = {Decullier, Evelyne and Huot, Laure and Samson, Géraldine and Maisonneuve, Hervé},
	urldate = {2021-02-20},
	date = {2013-06-19},
	keywords = {Guidelines, Retraction of publication, Scientific misconduct},
	file = {Decullier_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Decullier_etal_2013.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/LI78SW9R/1756-0500-6-238.html:text/html},
}

@article{steen_retractions_2011,
	title = {Retractions in the scientific literature: is the incidence of research fraud increasing?},
	volume = {37},
	rights = {© 2011, Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions.},
	issn = {0306-6800, 1473-4257},
	url = {https://jme.bmj.com/content/37/4/249},
	doi = {10.1136/jme.2010.040923},
	shorttitle = {Retractions in the scientific literature},
	abstract = {{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater} {\textless}p{\textgreater}Scientific papers are retracted for many reasons including fraud (data fabrication or falsification) or error (plagiarism, scientific mistake, ethical problems). Growing attention to fraud in the lay press suggests that the incidence of fraud is increasing.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater} {\textless}p{\textgreater}The reasons for retracting 742 English language research papers retracted from the {PubMed} database between 2000 and 2010 were evaluated. Reasons for retraction were initially dichotomised as fraud or error and then analysed to determine specific reasons for retraction.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater} {\textless}p{\textgreater}Error was more common than fraud (73.5\% of papers were retracted for error (or an undisclosed reason) vs 26.6\% retracted for fraud). Eight reasons for retraction were identified; the most common reason was scientific mistake in 234 papers (31.5\%), but 134 papers (18.1\%) were retracted for ambiguous reasons. Fabrication (including data plagiarism) was more common than text plagiarism. Total papers retracted per year have increased sharply over the decade (r=0.96; p\&lt;0.001), as have retractions specifically for fraud (r=0.89; p\&lt;0.001). Journals now reach farther back in time to retract, both for fraud (r=0.87; p\&lt;0.001) and for scientific mistakes (r=0.95; p\&lt;0.001). Journals often fail to alert the naïve reader; 31.8\% of retracted papers were not noted as retracted in any way.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions{\textless}/h3{\textgreater} {\textless}p{\textgreater}Levels of misconduct appear to be higher than in the past. This may reflect either a real increase in the incidence of fraud or a greater effort on the part of journals to police the literature. However, research bias is rarely cited as a reason for retraction.{\textless}/p{\textgreater}},
	pages = {249--253},
	number = {4},
	journaltitle = {Journal of Medical Ethics},
	author = {Steen, R. Grant},
	urldate = {2021-02-20},
	date = {2011-04-01},
	langid = {english},
	pmid = {21186208},
	note = {Publisher: Institute of Medical Ethics
Section: Research ethics},
	keywords = {data fabrication, data falsification, fraud, Plagiarism, professional misconduct, scientific research},
	file = {Snapshot:/Users/tom/Zotero/storage/FDPFQ4C9/249.html:text/html;Steen_2011.pdf:/Users/tom/pCloud Drive/Zotero_Library/Steen_2011.pdf:application/pdf},
}

@article{weissgerber_automated_2021,
	title = {Automated screening of {COVID}-19 preprints: can we help authors to improve transparency and reproducibility?},
	volume = {27},
	rights = {2021 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-020-01203-7},
	doi = {10.1038/s41591-020-01203-7},
	shorttitle = {Automated screening of {COVID}-19 preprints},
	pages = {6--7},
	number = {1},
	journaltitle = {Nature Medicine},
	author = {Weissgerber, Tracey and Riedel, Nico and Kilicoglu, Halil and Labbé, Cyril and Eckmann, Peter and ter Riet, Gerben and Byrne, Jennifer and Cabanac, Guillaume and Capes-Davis, Amanda and Favier, Bertrand and Saladi, Shyam and Grabitz, Peter and Bannach-Brown, Alexandra and Schulz, Robert and {McCann}, Sarah and Bernard, Rene and Bandrowski, Anita},
	urldate = {2021-02-20},
	date = {2021-01},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	file = {Snapshot:/Users/tom/Zotero/storage/3AMIC6MN/s41591-020-01203-7.html:text/html;Weissgerber_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Weissgerber_etal_2021.pdf:application/pdf},
}

@article{davis_persistence_2012,
	title = {The persistence of error: a study of retracted articles on the Internet and in personal libraries},
	volume = {100},
	issn = {1536-5050},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3411255/},
	doi = {10.3163/1536-5050.100.3.008},
	shorttitle = {The persistence of error},
	abstract = {Objective:
To determine the accessibility of retracted articles residing on non-publisher websites and in personal libraries.

Methods:
Searches were performed to locate Internet copies of 1,779 retracted articles identified in {MEDLINE}, published between 1973 and 2010, excluding the publishers' website. Found copies were classified by article version and location. Mendeley (a bibliographic software) was searched for copies residing in personal libraries.

Results:
Non-publisher websites provided 321 publicly accessible copies for 289 retracted articles: 304 (95\%) copies were the publisher' versions, and 13 (4\%) were final manuscripts. {PubMed} Central had 138 (43\%) copies; educational websites 94 (29\%); commercial websites 24 (7\%); advocacy websites 16 (5\%); and institutional repositories 10 (3\%). Just 15 (5\%) full-article views included a retraction statement. Personal Mendeley libraries contained records for 1,340 (75\%) retracted articles, shared by 3.4 users, on average.

Conclusions:
The benefits of decentralized access to scientific articles may come with the cost of promoting incorrect, invalid, or untrustworthy science. Automated methods to deliver status updates to readers may reduce the persistence of error in the scientific literature.},
	pages = {184--189},
	number = {3},
	journaltitle = {Journal of the Medical Library Association : {JMLA}},
	shortjournal = {J Med Libr Assoc},
	author = {Davis, Philip M.},
	urldate = {2021-02-20},
	date = {2012-07},
	pmid = {22879807},
	pmcid = {PMC3411255},
	file = {Davis_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Davis_2012.pdf:application/pdf},
}

@article{bar-ilan_post_2017,
	title = {Post retraction citations in context: a case study},
	volume = {113},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-017-2242-0},
	doi = {10.1007/s11192-017-2242-0},
	shorttitle = {Post retraction citations in context},
	abstract = {This study examines the nature of citations to articles that were retracted in 2014. Out of 987 retracted articles found in {ScienceDirect}, an Elsevier full text database, we selected all articles that received more than 10 citations between January 2015 and March 2016. Since the retraction year was known for only about 83\% of the retracted articles, we chose to concentrate on recent citations, that for certain appeared after the cited paper was retracted. Overall, we analyzed 238 citing documents and identified the context of each citation as positive, negative or neutral. Our results show that the vast majority of citations to retracted articles are positive despite of the clear retraction notice on the publisher’s platform and regardless of the reason for retraction. Positive citations can be also seen to articles that were retracted due to ethical misconduct, data fabrication and false reports. In light of these results, we listed some recommendations for publishers that could potentially minimize the referral to retracted studies as valid.},
	pages = {547--565},
	number = {1},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Bar-Ilan, Judit and Halevi, Gali},
	urldate = {2021-02-20},
	date = {2017-10-01},
	langid = {english},
	file = {Bar-Ilan_Halevi_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bar-Ilan_Halevi_2017.pdf:application/pdf},
}

@article{nath_retractions_2006,
	title = {Retractions in the research literature: misconduct or mistakes?},
	volume = {185},
	rights = {© 2006 {AMPCo} Pty Ltd. All rights reserved},
	issn = {1326-5377},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.5694/j.1326-5377.2006.tb00504.x},
	doi = {10.5694/j.1326-5377.2006.tb00504.x},
	shorttitle = {Retractions in the research literature},
	abstract = {Objective: To determine how commonly articles are retracted on the basis of unintentional mistakes, and whether these articles differ from those retracted for scientific misconduct in authorship, funding, type of study, publication, and time to retraction. Data source and study selection: All retractions of English language publications indexed in {MEDLINE} between 1982 and 2002 were extracted. Data extraction: Two reviewers categorised the reasons for retraction of each article as misconduct (falsification, fabrication, or plagiarism) or unintentional error (mistakes in sampling, procedures, or data analysis; failure to reproduce findings; accidental omission of information about methods or data analysis). Data synthesis: Of the 395 articles retracted between 1982 and 2002, 107 (27.1\%) were retracted because of scientific misconduct, 244 (61.8\%) because of unintentional errors, and 44 (11.1\%) could not be categorised. Compared with articles retracted because of misconduct, articles with unintentional mistakes were more likely to have multiple authors, no reported funding source, and to be published in frequently cited journals. They were more likely to be retracted by the author(s) of the article, and the retraction was more likely to occur more promptly (mean, 2.0 years; 95\% {CI}, 1.8–2.2) than articles withdrawn because of misconduct (mean, 3.3 years; 95\% {CI}, 2.7–3.9) (P {\textless} 0.05 for all comparisons). Conclusions: Retractions in the biomedical literature were more than twice as likely to result from unintentional mistakes than from scientific misconduct. The different characteristics of articles retracted for misconduct and for mistakes reflect distinct causes and, potentially, distinct solutions.},
	pages = {152--154},
	number = {3},
	journaltitle = {Medical Journal of Australia},
	author = {Nath, Sara B. and Marcus, Steven C. and Druss, Benjamin G.},
	urldate = {2021-02-20},
	date = {2006},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.5694/j.1326-5377.2006.tb00504.x},
	keywords = {Information, science},
	file = {Nath_etal_2006.pdf:/Users/tom/pCloud Drive/Zotero_Library/Nath_etal_2006.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3F7CZW6Q/j.1326-5377.2006.tb00504.html:text/html},
}

@report{aczel_role_2020,
	title = {The role of human fallibility in psychological research: A survey of mistakes in data management},
	url = {https://psyarxiv.com/xcykz/},
	shorttitle = {The role of human fallibility in psychological research},
	abstract = {Errors are an inevitable consequence of human fallibility and researchers are no exception. Most researchers can recall major frustrations or serious time delays due to human errors while collecting, analyzing, or reporting data. The present study is an exploration of mistakes made during the data management process in psychological research. We surveyed 464 researchers regarding the type, frequency, seriousness, and outcome of mistakes that have occurred in their research team during the last 5 years. The majority of respondents suggested that mistakes occurred with very low or low frequency. Most respondents reported that the most frequent mistakes led to insignificant or minor consequences, such as time loss or frustration. The most serious mistakes caused insignificant or minor consequences for about a third of respondents, moderate consequences for almost half of respondents, and major or extreme consequences for about one-fifth of respondents. The most frequently reported types of mistakes were ‘ambiguous naming/defining of data’, ‘incorrect data processing/analysis’, and ‘version control error’. Most mistakes were reportedly due to poor project planning or management and/or personal issues (physical or cognitive constraints). These initial exploratory findings lay the groundwork for a systematic investigation of human fallibility in research data management and the development of solutions that may reduce errors and mitigate their impact.},
	institution = {{PsyArXiv}},
	author = {Aczel, Balazs and Kovacs, Marton and Hoekstra, Rink},
	urldate = {2021-02-20},
	date = {2020-11-05},
	doi = {10.31234/osf.io/xcykz},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, other, Psychology, data management mistakes, human error, life-cycle of the data, research workflow},
	file = {Aczel_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Aczel_etal_2020.pdf:application/pdf},
}

@article{roberts_how_2000,
	title = {How persuasive is a good fit? A comment on theory testing.},
	volume = {107},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.107.2.358},
	doi = {10.1037/0033-295X.107.2.358},
	shorttitle = {How persuasive is a good fit?},
	pages = {358--367},
	number = {2},
	journaltitle = {Psychological Review},
	shortjournal = {Psychological Review},
	author = {Roberts, Seth and Pashler, Harold},
	urldate = {2021-02-19},
	date = {2000},
	langid = {english},
	file = {Roberts and Pashler - 2000 - How persuasive is a good fit A comment on theory .pdf:/Users/tom/Zotero/storage/Z5PN3S9P/Roberts and Pashler - 2000 - How persuasive is a good fit A comment on theory .pdf:application/pdf},
}

@article{borsboom_theory_2021,
	title = {Theory Construction Methodology: A Practical Framework for Building Theories in Psychology},
	issn = {1745-6916},
	url = {https://journals.sagepub.com/doi/abs/10.1177/1745691620969647},
	doi = {10.1177/1745691620969647},
	shorttitle = {Theory Construction Methodology},
	abstract = {This article aims to improve theory formation in psychology by developing a practical methodology for constructing explanatory theories: theory construction methodology ({TCM}). {TCM} is a sequence of five steps. First, the theorist identifies a domain of empirical phenomena that becomes the target of explanation. Second, the theorist constructs a prototheory, a set of theoretical principles that putatively explain these phenomena. Third, the prototheory is used to construct a formal model, a set of model equations that encode explanatory principles. Fourth, the theorist investigates the explanatory adequacy of the model by formalizing its empirical phenomena and assessing whether it indeed reproduces these phenomena. Fifth, the theorist studies the overall adequacy of the theory by evaluating whether the identified phenomena are indeed reproduced faithfully and whether the explanatory principles are sufficiently parsimonious and substantively plausible. We explain {TCM} with an example taken from research on intelligence (the mutualism model of intelligence), in which key elements of the method have been successfully implemented. We discuss the place of {TCM} in the larger scheme of scientific research and propose an outline for a university curriculum that can systematically educate psychologists in the process of theory formation.},
	pages = {1745691620969647},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Borsboom, Denny and van der Maas, Han L. J. and Dalege, Jonas and Kievit, Rogier A. and Haig, Brian D.},
	urldate = {2021-02-19},
	date = {2021-02-16},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {abduction, formal modeling, mutualism, philosophy of science},
	file = {Borsboom_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Borsboom_etal_2021.pdf:application/pdf},
}

@article{hitzig_problem_2020,
	title = {The problem of new evidence: p-hacking and pre-analysis plans},
	volume = {17},
	rights = {Copyright (c) 2020 Zoe Hitzig, Jacob Stegenga},
	issn = {1733-5566},
	url = {https://www.diametros.iphils.uj.edu.pl/diametros},
	doi = {10.33392/diam.1587},
	shorttitle = {The problem of new evidence},
	pages = {10--33},
	number = {66},
	journaltitle = {Diametros},
	author = {Hitzig, Zoe and Stegenga, Jacob},
	urldate = {2020-10-06},
	date = {2020-10-05},
	langid = {english},
	keywords = {p-hacking, replication crisis, Bayesian confirmation theory, pre-analysis plans, predictivism},
	file = {Full Text PDF:/Users/tom/Zotero/storage/SJVDFMS2/Hitzig and Stegenga - 2020 - The Problem of New Evidence P-Hacking and Pre-Ana.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BWLDZYLR/1587.html:text/html},
}

@article{leung_presenting_2011,
	title = {Presenting post hoc hypotheses as a priori: ethical and theoretical issues},
	volume = {7},
	issn = {1740-8776, 1740-8784},
	url = {http://www.cambridge.org/core/journals/management-and-organization-review/article/presenting-post-hoc-hypotheses-as-a-priori-ethical-and-theoretical-issues/1686099DCE63A015505357BC11F59F45},
	doi = {10.1111/j.1740-8784.2011.00222.x},
	shorttitle = {Presenting post hoc hypotheses as a priori},
	abstract = {Presenting post hoc hypotheses based on empirical findings as if they had been developed a priori seems common in management papers. The pure form of this practice is likely to breach research ethics and impede theoretical development by suppressing the falsification process. Two other forms may be more tolerable: deletion of rejected hypotheses and refinement of hypotheses inspired by empirical findings. To address this problem, the field should provide stronger recognition of replication, descriptive research, rejected and post hoc hypotheses, and critical tests of competing hypotheses. These positive changes require the concerted effort of researchers, management associations, and journal editors and reviewers.
         ,},
	pages = {471--479},
	number = {3},
	journaltitle = {Management and Organization Review},
	author = {Leung, Kwok},
	urldate = {2021-02-17},
	date = {2011-11},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {hypothetic-deductive method, positivity bias, post hoc hypothesis, 事后假设, 假设ー演绎法, 积极性偏差},
	file = {Leung_2011.pdf:/Users/tom/pCloud Drive/Zotero_Library/Leung_2011.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZCMNXMGQ/1686099DCE63A015505357BC11F59F45.html:text/html},
}

@article{franco_underreporting_2016,
	title = {Underreporting in psychology experiments: evidence from a study registry},
	volume = {7},
	issn = {1948-5506, 1948-5514},
	url = {http://journals.sagepub.com/doi/10.1177/1948550615598377},
	doi = {10.1177/1948550615598377},
	shorttitle = {Underreporting in psychology experiments},
	abstract = {Many scholars have raised concerns about the credibility of empirical findings in psychology, arguing that the proportion of false positives reported in the published literature dramatically exceeds the rate implied by standard significance levels. A major contributor of false positives is the practice of reporting a subset of the potentially relevant statistical analyses pertaining to a research project. This study is the first to provide direct evidence of selective underreporting in psychology experiments. To overcome the problem that the complete experimental design and full set of measured variables are not accessible for most published research, we identify a population of published psychology experiments from a competitive grant program for which questionnaires and data are made publicly available because of an institutional rule. We find that about 40\% of studies fail to fully report all experimental conditions and about 70\% of studies do not report all outcome variables included in the questionnaire. Reported effect sizes are about twice as large as unreported effect sizes and are about 3 times more likely to be statistically significant.},
	pages = {8--12},
	number = {1},
	journaltitle = {Social Psychological and Personality Science},
	shortjournal = {Social Psychological and Personality Science},
	author = {Franco, A. and Malhotra, N. and Simonovits, G.},
	urldate = {2020-09-21},
	date = {2016-01},
	langid = {english},
	file = {Franco et al. - 2016 - Underreporting in Psychology Experiments Evidence.pdf:/Users/tom/Zotero/storage/BDLKYEAV/Franco et al. - 2016 - Underreporting in Psychology Experiments Evidence.pdf:application/pdf},
}

@article{tierney_individual_2015,
	title = {Individual Participant Data ({IPD}) Meta-analyses of Randomised Controlled Trials: Guidance on Their Use},
	volume = {12},
	issn = {1549-1676},
	url = {https://dx.plos.org/10.1371/journal.pmed.1001855},
	doi = {10.1371/journal.pmed.1001855},
	shorttitle = {Individual Participant Data ({IPD}) Meta-analyses of Randomised Controlled Trials},
	pages = {e1001855},
	number = {7},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLoS} Med},
	author = {Tierney, Jayne F. and Vale, Claire and Riley, Richard and Smith, Catrin Tudur and Stewart, Lesley and Clarke, Mike and Rovers, Maroeska},
	urldate = {2021-02-17},
	date = {2015-07-21},
	langid = {english},
	file = {Tierney et al. - 2015 - Individual Participant Data (IPD) Meta-analyses of.pdf:/Users/tom/Zotero/storage/IVG4Z8A7/Tierney et al. - 2015 - Individual Participant Data (IPD) Meta-analyses of.pdf:application/pdf},
}

@article{moher_assessing_2018,
	title = {Assessing scientists for hiring, promotion, and tenure},
	volume = {16},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.2004089},
	doi = {10.1371/journal.pbio.2004089},
	abstract = {Assessment of researchers is necessary for decisions of hiring, promotion, and tenure. A burgeoning number of scientific leaders believe the current system of faculty incentives and rewards is misaligned with the needs of society and disconnected from the evidence about the causes of the reproducibility crisis and suboptimal quality of the scientific publication record. To address this issue, particularly for the clinical and life sciences, we convened a 22-member expert panel workshop in Washington, {DC}, in January 2017. Twenty-two academic leaders, funders, and scientists participated in the meeting. As background for the meeting, we completed a selective literature review of 22 key documents critiquing the current incentive system. From each document, we extracted how the authors perceived the problems of assessing science and scientists, the unintended consequences of maintaining the status quo for assessing scientists, and details of their proposed solutions. The resulting table was used as a seed for participant discussion. This resulted in six principles for assessing scientists and associated research and policy implications. We hope the content of this paper will serve as a basis for establishing best practices and redesigning the current approaches to assessing scientists by the many players involved in that process.},
	pages = {e2004089},
	number = {3},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLoS} Biol},
	author = {Moher, David and Naudet, Florian and Cristea, Ioana A. and Miedema, Frank and Ioannidis, John P. A. and Goodman, Steven N.},
	urldate = {2021-02-17},
	date = {2018-03-29},
	langid = {english},
	file = {Moher et al. - 2018 - Assessing scientists for hiring, promotion, and te.pdf:/Users/tom/Zotero/storage/MG37ICI8/Moher et al. - 2018 - Assessing scientists for hiring, promotion, and te.pdf:application/pdf},
}

@article{bar-ilan_temporal_2018,
	title = {Temporal characteristics of retracted articles},
	volume = {116},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-018-2802-y},
	doi = {10.1007/s11192-018-2802-y},
	abstract = {There are three main reasons for retraction: (1) ethical misconduct (e.g. duplicate publication, plagiarism, missing credit, no {IRB}, ownership issues, authorship issues, interference in the review process, citation manipulation); (2) scientific distortion (e.g. data manipulation, fraudulent data, unsupported conclusions, questionable data validity, non-replicability, data errors—even if unintended); (3) administrative error (e.g. article published in wrong issue, not the final version published, publisher errors). The first category, although highly deplorable has almost no effect on the advancement of science, the third category is relatively minor. The papers belonging to the second category are most troublesome from the scientific point of view, as they are misleading and have serious negative implications not only on science but also on society. In this paper, we explore some temporal characteristics of retracted articles, including time of publication, years to retract, growth of post retraction citations over time and social media attention by the three major categories. The data set comprises 995 retracted articles retrieved in October 2014 from Elsevier’s {ScienceDirect}. Citations and Mendeley reader counts were retrieved four times within 4 years, which allowed us to examine post-retraction longitudinal trends not only for citations, but also for Mendeley reader counts. The major findings are that both citation counts and Mendeley reader counts continue to grow after retraction.},
	pages = {1771--1783},
	number = {3},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Bar-Ilan, Judit and Halevi, Gali},
	urldate = {2021-02-15},
	date = {2018-09-01},
	langid = {english},
	file = {Bar-Ilan_Halevi_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bar-Ilan_Halevi_2018.pdf:application/pdf},
}

@article{chen_visual_2013,
	title = {A visual analytic study of retracted articles in scientific literature},
	volume = {64},
	rights = {© 2012 {ASIS}\&T},
	issn = {1532-2890},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/asi.22755},
	doi = {10.1002/asi.22755},
	abstract = {Retracting published scientific articles is increasingly common. Retraction is a self-correction mechanism of the scientific community to maintain and safeguard the integrity of scientific literature. However, a retracted article may pose a profound and long-lasting threat to the credibility of the literature. New articles may unknowingly build their work on false claims made in retracted articles. Such dependencies on retracted articles may become implicit and indirect. Consequently, it becomes increasingly important to detect implicit and indirect threats. In this article, our aim is to raise the awareness of the potential threats of retracted articles even after their retraction and demonstrate a visual analytic study of retracted articles with reference to the rest of the literature and how their citations are influenced by their retraction. The context of highly cited retracted articles is visualized in terms of a co-citation network as well as the distribution of articles that have high-order citation dependencies on retracted articles. Survival analyses of time to retraction and postretraction citation are included. Sentences that explicitly cite retracted articles are extracted from full-text articles. Transitions of topics over time are depicted in topic-flow visualizations. We recommend that new visual analytic and science mapping tools should take retracted articles into account and facilitate tasks specifically related to the detection and monitoring of retracted articles.},
	pages = {234--253},
	number = {2},
	journaltitle = {Journal of the American Society for Information Science and Technology},
	author = {Chen, Chaomei and Hu, Zhigang and Milbank, Jared and Schultz, Timothy},
	urldate = {2021-02-15},
	date = {2013},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.22755},
	keywords = {content analysis, information filtering, visualization (electronic)},
	file = {Chen_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Chen_etal_2013.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZNRIF3LD/asi.html:text/html},
}

@article{bolland_citation_2021,
	title = {Citation of retracted publications: A challenging problem},
	volume = {0},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2021.1886933},
	doi = {10.1080/08989621.2021.1886933},
	shorttitle = {Citation of retracted publications},
	abstract = {Scientific publications with compromised integrity should be retracted. Papers citing retracted publications might need correction if findings depend on the retracted publication. While many studies have reported on post-retraction citations, few have focused on citations made before the retraction. We investigated the citation profile for a research group with 113 published concerns regarding publication integrity ({CRPI}). We identified 376 of their source publications that were cited by 5577 articles, and whether the source publication had a published {CRPI}. Of 6926 references to a source publication in these citing articles, for 3925 (57\%) the source article had a published {CRPI}, while for 3001 (43\%) it did not. Of these 3925 references, 3688 were in citing articles published before the source article {CRPI} was published. 166 citing articles containing 198 references to source publications were published after the corresponding source article {CRPI} was published (range 1–5 such references/article; 19/166 (11\%) articles had {\textgreater}1 reference). In summary, many articles cite retracted publications, with the majority of these references occurring before the retraction. However, very few publications assess the impact of the retracted citations, even though the findings of many might be altered, at least in part, by removal of the retracted citation.},
	pages = {1--8},
	number = {0},
	journaltitle = {Accountability in Research},
	author = {Bolland, Mark J. and Grey, Andrew and Avenell, Alison},
	urldate = {2021-02-15},
	date = {2021-02-09},
	pmid = {33557605},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2021.1886933},
	keywords = {Citation, expression of concern, publication integrity, retraction},
	file = {Bolland_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bolland_etal_2021.pdf:application/pdf},
}

@article{nickerson_confirmation_1998,
	title = {Confirmation bias: a ubiquitous phenomenon in many guises},
	volume = {2},
	doi = {10.1037/1089-2680.2.2.175},
	pages = {175--220},
	number = {2},
	journaltitle = {Review of General Psychology},
	author = {Nickerson, Raymond S},
	date = {1998},
	langid = {english},
	file = {Nickerson - Confirmation Bias A Ubiquitous Phenomenon in Many.pdf:/Users/tom/Zotero/storage/FZZHRTF7/Nickerson - Confirmation Bias A Ubiquitous Phenomenon in Many.pdf:application/pdf},
}

@article{greenberg_how_2009,
	title = {How citation distortions create unfounded authority: analysis of a citation network},
	volume = {339},
	rights = {© Greenberg 2009. This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/339/bmj.b2680},
	doi = {10.1136/bmj.b2680},
	shorttitle = {How citation distortions create unfounded authority},
	abstract = {Objective To understand belief in a specific scientific claim by studying the pattern of citations among papers stating it.
Design A complete citation network was constructed from all {PubMed} indexed English literature papers addressing the belief that β amyloid, a protein accumulated in the brain in Alzheimer’s disease, is produced by and injures skeletal muscle of patients with inclusion body myositis. Social network theory and graph theory were used to analyse this network.
Main outcome measures Citation bias, amplification, and invention, and their effects on determining authority.
Results The network contained 242 papers and 675 citations addressing the belief, with 220 553 citation paths supporting it. Unfounded authority was established by citation bias against papers that refuted or weakened the belief; amplification, the marked expansion of the belief system by papers presenting no data addressing it; and forms of invention such as the conversion of hypothesis into fact through citation alone. Extension of this network into text within grants funded by the National Institutes of Health and obtained through the Freedom of Information Act showed the same phenomena present and sometimes used to justify requests for funding.
Conclusion Citation is both an impartial scholarly method and a powerful form of social communication. Through distortions in its social use that include bias, amplification, and invention, citation can be used to generate information cascades resulting in unfounded authority of claims. Construction and analysis of a claim specific citation network may clarify the nature of a published belief system and expose distorted methods of social citation.},
	pages = {b2680},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Greenberg, Steven A.},
	urldate = {2020-09-03},
	date = {2009-07-21},
	langid = {english},
	pmid = {19622839},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Greenberg - 2009 - How citation distortions create unfounded authorit.pdf:/Users/tom/pCloud Drive/Zotero_Library/Greenberg - 2009 - How citation distortions create unfounded authorit.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/D7CXUFPI/bmj.html:text/html},
}

@report{peterson_self-correction_2020,
	title = {Self-correction in science: the diagnostic and integrative motives for replication},
	url = {https://osf.io/preprints/socarxiv/96qxv/},
	shorttitle = {Self-correction in science},
	abstract = {Research replication, with its potential to diagnose the truth of scientific claims, is argued to be central to social control in science. Yet a series of frauds and failed replications have raised questions regarding the frequency and effectiveness of current practices. Metascientific activists have advocated policies that incentivize replications and make them more diagnostically potent. We argue that both current debates and previous research in the social studies of science have overlooked a key dimension of replication practice. Rather than diagnostic tests, replication is commonly motivated by a practical desire to extend research interests. This motivation for replication, which we label “integrative,” is characterized by a pragmatic flexibility toward protocols. The goal is to appropriate what is useful, not test for truth. Within many experimental cultures, however, integrative replications can produce replications of ambiguous diagnostic power. Based on interviews with 60 members of the Board of Reviewing Editors for Science, we show how the interplay between the diagnostic and integrative motives for replication differs between fields and produces different cultures of replication. We offer six theses that aim to put science studies and science activism into dialogue to show why effective reforms will need to confront issues of disciplinary difference.},
	institution = {{SocArXiv}},
	author = {Peterson, David and Panofsky, Aaron},
	urldate = {2021-01-15},
	date = {2020-12-16},
	doi = {10.31235/osf.io/96qxv},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, replication, metascience, Science, and Technology, Knowledge, self-correction, Sociology, task uncertainty},
	file = {Peterson_Panofsky_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Peterson_Panofsky_2020.pdf:application/pdf},
}

@incollection{lauden_peirce_1981,
	location = {Dordrecht},
	title = {Peirce and the trivialization of the self-corrective thesis},
	pages = {226--251},
	booktitle = {Science and Hypothesis},
	publisher = {Springer},
	author = {Lauden, L},
	date = {1981},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Lauden - Peirce and the trivializa non of the self -correct.pdf:/Users/tom/Zotero/storage/K9JHYXT2/Lauden - Peirce and the trivializa non of the self -correct.pdf:application/pdf},
}

@article{gershman_how_2019,
	title = {How to never be wrong},
	volume = {26},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-018-1488-8},
	doi = {10.3758/s13423-018-1488-8},
	abstract = {Human beliefs have remarkable robustness in the face of disconfirmation. This robustness is often explained as the product of heuristics or motivated reasoning. However, robustness can also arise from purely rational principles when the reasoner has recourse to ad hoc auxiliary hypotheses. Auxiliary hypotheses primarily function as the linking assumptions connecting different beliefs to one another and to observational data, but they can also function as a “protective belt” that explains away disconfirmation by absorbing some of the blame. The present article traces the role of auxiliary hypotheses from philosophy of science to Bayesian models of cognition and a host of behavioral phenomena, demonstrating their wide-ranging implications.},
	pages = {13--28},
	number = {1},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Gershman, Samuel J.},
	urldate = {2021-02-04},
	date = {2019-02-01},
	langid = {english},
	file = {Gershman_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Gershman_2019.pdf:application/pdf},
}

@report{mcdiarmid_self-correction_2021,
	title = {Self-correction in psychological science: how do psychologists update their beliefs in response to replications?},
	url = {https://psyarxiv.com/hjcm4/},
	shorttitle = {Self-correction in psychological science},
	abstract = {Self-correction—a key feature distinguishing science from pseudoscience—requires that scientists update their beliefs in light of new evidence. However, people are often reluctant to change their beliefs. We examined self-correction in action, tracking research psychologists’ beliefs in psychological effects before and after the completion of four large-scale replication projects. We found that psychologists did update their beliefs; they updated as much as they predicted they would, but not as much as our Bayesian model suggests they should if they trust the results. We found no evidence that psychologists became more critical of replications when it would have preserved their pre-existing beliefs. We also found no evidence that personal investment or lack of expertise discouraged belief updating, but people higher on intellectual humility updated their beliefs slightly more. Overall, our results suggest that replication studies can contribute to self-correction within psychology, but psychologists may underweight their evidentiary value.},
	institution = {{PsyArXiv}},
	author = {{McDiarmid}, Alexander and Tullett, Alexa and Whitt, Cassie Marie and Vazire, Simine and Smaldino, Paul E. and Stephens, Eli E.},
	urldate = {2021-02-04},
	date = {2021-02-01},
	doi = {10.31234/osf.io/hjcm4},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, other, Psychology, Replication, Belief Updating, Beliefs, Cognitive Psychology, Judgment and Decision Making, Motivated Reasoning, Scientific Self-correction},
	file = {McDiarmid_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/McDiarmid_etal_2021.pdf:application/pdf},
}

@article{vadillo_ego_2019,
	title = {Ego depletion may disappear by 2020},
	volume = {50},
	issn = {1864-9335, 2151-2590},
	url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000375},
	doi = {10.1027/1864-9335/a000375},
	abstract = {Abstract. Ego depletion has been successfully replicated in hundreds of studies. Yet the most recent large-scale Registered Replication Reports ({RRR}), comprising thousands of participants, have yielded disappointingly small effects, sometimes even failing to reach statistical significance. Although these results may seem surprising, in the present article I suggest that they are perfectly consistent with a long-term decline in the size of the depletion effects that can be traced back to at least 10 years ago, well before any of the {RRR} on ego depletion were conceived. The decline seems to be at least partly due to a parallel trend toward publishing better and less biased research.},
	pages = {282--291},
	number = {5},
	journaltitle = {Social Psychology},
	shortjournal = {Social Psychology},
	author = {Vadillo, Miguel A.},
	urldate = {2021-02-03},
	date = {2019-09},
	langid = {english},
}

@article{ferguson_vast_2012,
	title = {A vast graveyard of undead theories: publication bias and psychological science’s aversion to the null},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612459059},
	doi = {10.1177/1745691612459059},
	shorttitle = {A vast graveyard of undead theories},
	abstract = {Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.},
	pages = {555--561},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Ferguson, Christopher J. and Heene, Moritz},
	urldate = {2021-02-03},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {publication bias, null hypothesis significance testing, fail-safe number, falsification, meta-analyses},
	file = {Ferguson_Heene_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ferguson_Heene_2012.pdf:application/pdf},
}

@article{lewandowsky_misinformation_2012,
	title = {Misinformation and its correction: continued influence and successful debiasing},
	volume = {13},
	issn = {1529-1006},
	url = {https://doi.org/10.1177/1529100612451018},
	doi = {10.1177/1529100612451018},
	shorttitle = {Misinformation and its correction},
	abstract = {The widespread prevalence and persistence of misinformation in contemporary societies, such as the false belief that there is a link between childhood vaccinations and autism, is a matter of public concern. For example, the myths surrounding vaccinations, which prompted some parents to withhold immunization from their children, have led to a marked increase in vaccine-preventable disease, as well as unnecessary public expenditure on research and public-information campaigns aimed at rectifying the situation., We first examine the mechanisms by which such misinformation is disseminated in society, both inadvertently and purposely. Misinformation can originate from rumors but also from works of fiction, governments and politicians, and vested interests. Moreover, changes in the media landscape, including the arrival of the Internet, have fundamentally influenced the ways in which information is communicated and misinformation is spread., We next move to misinformation at the level of the individual, and review the cognitive factors that often render misinformation resistant to correction. We consider how people assess the truth of statements and what makes people believe certain things but not others. We look at people’s memory for misinformation and answer the questions of why retractions of misinformation are so ineffective in memory updating and why efforts to retract misinformation can even backfire and, ironically, increase misbelief. Though ideology and personal worldviews can be major obstacles for debiasing, there nonetheless are a number of effective techniques for reducing the impact of misinformation, and we pay special attention to these factors that aid in debiasing., We conclude by providing specific recommendations for the debunking of misinformation. These recommendations pertain to the ways in which corrections should be designed, structured, and applied in order to maximize their impact. Grounded in cognitive psychological theory, these recommendations may help practitioners—including journalists, health professionals, educators, and science communicators—design effective misinformation retractions, educational tools, and public-information campaigns.},
	pages = {106--131},
	number = {3},
	journaltitle = {Psychological Science in the Public Interest},
	shortjournal = {Psychol Sci Public Interest},
	author = {Lewandowsky, Stephan and Ecker, Ullrich K. H. and Seifert, Colleen M. and Schwarz, Norbert and Cook, John},
	urldate = {2021-02-03},
	date = {2012-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {debiasing, false beliefs, memory updating, misinformation},
	file = {Lewandowsky_etal_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lewandowsky_etal_2012.pdf:application/pdf},
}

@article{bastiaansen_citation_2015,
	title = {Citation distortions in the literature on the serotonin-transporter-linked polymorphic region and amygdala activation},
	volume = {78},
	issn = {00063223},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000632231400986X},
	doi = {10.1016/j.biopsych.2014.12.007},
	pages = {E35--E36},
	number = {8},
	journaltitle = {Biological Psychiatry},
	shortjournal = {Biological Psychiatry},
	author = {Bastiaansen, Jojanneke A. and de Vries, Ymkje Anna and Munafò, Marcus R.},
	urldate = {2021-02-03},
	date = {2015-10},
	langid = {english},
	file = {Bastiaansen et al. - 2015 - Citation Distortions in the Literature on the Sero.pdf:/Users/tom/Zotero/storage/IIZ2S9HZ/Bastiaansen et al. - 2015 - Citation Distortions in the Literature on the Sero.pdf:application/pdf},
}

@article{koehler_influence_1993,
	title = {The influence of prior beliefs on scientific judgments of evidence quality},
	volume = {56},
	issn = {0749-5978},
	url = {http://www.sciencedirect.com/science/article/pii/S0749597883710447},
	doi = {10.1006/obhd.1993.1044},
	abstract = {This paper is concerned with the influence of scientists′ prior beliefs on their judgments of evidence quality. A laboratory experiment using advanced graduate students in the sciences (study 1) and an experimental survey of practicing scientists on opposite sides of a controversial issue (study 2) revealed agreement effects. Research reports that agreed with scientists′ prior beliefs were judged to be of higher quality than those that disagreed. In study 1, a prior belief strength × agreement interaction was found, indicating that the agreement effect was larger among scientists who held strong prior beliefs. In both studies, the agreement effect was larger for general, evaluative judgments (e.g., relevance, methodological quality, results clarity) than for more specific, analytical judgments (e.g., adequacy of randomization procedures). A Bayesian analysis indicates that the pattern of agreement effects found in these studies may be normatively defensible, although arguments against implementing a Bayesian approach to scientific judgment are also advanced.},
	pages = {28--55},
	number = {1},
	journaltitle = {Organizational Behavior and Human Decision Processes},
	shortjournal = {Organizational Behavior and Human Decision Processes},
	author = {Koehler, Jonathan J.},
	urldate = {2020-09-21},
	date = {1993-10-01},
	langid = {english},
	file = {Koehler_1993.pdf:/Users/tom/pCloud Drive/Zotero_Library/Koehler_1993.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/4FZ79ANL/S0749597883710447.html:text/html},
}

@article{malicki_journals_2019,
	title = {Journals’ instructions to authors: A cross-sectional study across scientific disciplines},
	volume = {14},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0222157},
	doi = {10.1371/journal.pone.0222157},
	shorttitle = {Journals’ instructions to authors},
	pages = {e0222157},
	number = {9},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Malički, Mario and Aalbersberg, {IJsbrand} Jan and Bouter, Lex and ter Riet, Gerben},
	editor = {Gao, Chang-Qing},
	urldate = {2020-07-15},
	date = {2019-09-05},
	langid = {english},
	file = {Malički et al. - 2019 - Journals’ instructions to authors A cross-section.pdf:/Users/tom/Zotero/storage/L9N4R8HT/Malički et al. - 2019 - Journals’ instructions to authors A cross-section.pdf:application/pdf},
}

@online{bastian_bias_2017,
	title = {Bias in Open Science Advocacy: The Case of Article Badges for Data Sharing},
	url = {https://absolutelymaybe.plos.org/2017/08/29/bias-in-open-science-advocacy-the-case-of-article-badges-for-data-sharing/},
	shorttitle = {Bias in Open Science Advocacy},
	abstract = {I like badges – I have a lot of them! I’m also an open science advocate. So when a group of…},
	titleaddon = {Absolutely Maybe},
	author = {Bastian, Hilda},
	urldate = {2020-07-08},
	date = {2017-08-29},
	langid = {american},
	note = {Library Catalog: absolutelymaybe.plos.org},
	file = {Snapshot:/Users/tom/Zotero/storage/7X6ATVJZ/bias-in-open-science-advocacy-the-case-of-article-badges-for-data-sharing.html:text/html},
}

@online{noauthor_registered_nodate,
	title = {Registered Reports {\textbar} Royal Society Open Science},
	url = {https://royalsocietypublishing.org/rsos/registered-reports},
	urldate = {2020-06-26},
	file = {Registered Reports | Royal Society Open Science:/Users/tom/Zotero/storage/PAHCFVYT/registered-reports.html:text/html},
}

@book{schwarzer_meta-analysis_2015,
	title = {Meta-Analysis with R},
	isbn = {978-3-319-21415-3},
	url = {https://www.springer.com/gp/book/9783319214153},
	series = {Use R!},
	abstract = {This book provides a comprehensive introduction to performing meta-analysis using the statistical software R. It is intended for quantitative researchers and students in the medical and social sciences who wish to learn how to perform meta-analysis with R. As such, the book introduces the key concepts and models used in meta-analysis. It also includes chapters on the following advanced topics: publication bias and small study effects; missing data; multivariate meta-analysis, network meta-analysis; and meta-analysis of diagnostic studies.},
	publisher = {Springer International Publishing},
	author = {Schwarzer, Guido and Carpenter, James R. and Rücker, Gerta},
	urldate = {2020-06-24},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-21416-0},
	file = {Snapshot:/Users/tom/Zotero/storage/TRCKLEIX/9783319214153.html:text/html},
}

@article{devezer_case_2020,
	title = {The case for formal methodology in scientific reform},
	volume = {8},
	doi = {10.1098/rsos.200805},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Current attempts at methodological reform in sciences come in response to an overall lack of rigor in methodological and scientific practices in experimental sciences. However, some of these reform attempts suffer from the same mistakes and over-generalizations they purport to address. Considering the costs of allowing false claims to become canonized, we argue for more rigor and nuance in methodological reform. By way of example, we present a formal analysis of three common claims in the metascientific literature: (a) that reproducibility is the cornerstone of science; (b) that data must not be used twice in any analysis; and (c) that exploratory projects are characterized by poor statistical practice. We show that none of these three claims are correct in general and we explore when they do and do not hold.{\textless}/p{\textgreater}},
	pages = {200805},
	journaltitle = {Royal Society Open Science},
	author = {Devezer, Berna and Navarro, Danielle J. and Vandekerckhove, Joachim and Buzbas, Erkan Ozge},
	urldate = {2020-06-21},
	date = {2020-04-28},
	langid = {english},
	file = {Devezer_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Devezer_etal_22.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/2UDLLNGM/2020.04.26.048306v1.html:text/html},
}

@article{altman_statistics_1980,
	title = {Statistics and ethics in medical research. Misuse of statistics is unethical.},
	volume = {281},
	issn = {0959-8138, 1468-5833},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.281.6249.1182},
	doi = {10.1136/bmj.281.6249.1182},
	abstract = {The ethical implications of statistically substandard research may be summarised as follows: (1) the misuse of patients by exposing them to unjustified risk and inconvenience; (2) the misuse of resources, including the researchers' time, which could be better employed on more valuable activities; and (3) the consequences of publishing misleading results, which may include the carrying out of unnecessary further work. These are specific and highly undesirable outcomes. Failure to guard against these is surely as unethical as using experimental methods that offend against moral principles, such as failing to obtain fully informed consent from subjects. Surprisingly, this aspect seems to have been totally ignored by books on medical ethics.},
	pages = {1182--1184},
	number = {6249},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Altman, D G},
	urldate = {2020-06-15},
	date = {1980-11-01},
	langid = {english},
	file = {Altman - 1980 - Statistics and ethics in medical research. Misuse .pdf:/Users/tom/Zotero/storage/L6MICJTJ/Altman - 1980 - Statistics and ethics in medical research. Misuse .pdf:application/pdf},
}

@article{teixeira_da_silva_fortifying_2017,
	title = {Fortifying the Corrective Nature of Post-publication Peer Review: Identifying Weaknesses, Use of Journal Clubs, and Rewarding Conscientious Behavior},
	volume = {23},
	issn = {1353-3452, 1471-5546},
	url = {http://link.springer.com/10.1007/s11948-016-9854-2},
	doi = {10.1007/s11948-016-9854-2},
	shorttitle = {Fortifying the Corrective Nature of Post-publication Peer Review},
	abstract = {Most departments in any ﬁeld of science that have a sound academic basis have discussion groups or journal clubs in which pertinent and relevant literature is frequently discussed, as a group. This paper shows how such discussions could help to fortify the post-publication peer review ({PPPR}) movement, and could thus fortify the value of traditional peer review, if their content and conclusions were made known to the wider academic community. Recently, there are some tools available for making {PPPR} viable, either as signed ({PubMed} Commons) or anonymous comments ({PubPeer}), or in a hybrid format (Publons). Thus, limited platforms are currently in place to accommodate and integrate {PPPR} as a supplement to traditional peer review, allowing for the open and public discussion of what is often publicly-funded science. This paper examines ways in which the opinions that emerge from journal clubs or discussion groups could help to fortify the integrity and reliability of science while increasing its accountability. A culture of reward for good and corrective behavior, rather than a culture that protects silence, would beneﬁt science most.},
	pages = {1213--1226},
	number = {4},
	journaltitle = {Science and Engineering Ethics},
	shortjournal = {Sci Eng Ethics},
	author = {Teixeira da Silva, Jaime A. and Al-Khatib, Aceil and Dobránszki, Judit},
	urldate = {2020-06-14},
	date = {2017-08},
	langid = {english},
	file = {Teixeira da Silva et al. - 2017 - Fortifying the Corrective Nature of Post-publicati.pdf:/Users/tom/Zotero/storage/88D5PYKK/Teixeira da Silva et al. - 2017 - Fortifying the Corrective Nature of Post-publicati.pdf:application/pdf},
}

@article{fleiss_measuring_1971,
	title = {Measuring nominal scale agreement among many raters},
	volume = {76},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/h0031619},
	abstract = {Introduced the statistic kappa to measure nominal scale agreement between a fixed pair of raters. Kappa was generalized to the case where each of a sample of 30 patients was rated on a nominal scale by the same number of psychiatrist raters (n = 6), but where the raters rating 1 s were not necessarily the same as those rating another. Large sample standard errors were derived. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {378--382},
	number = {5},
	journaltitle = {Psychological Bulletin},
	author = {Fleiss, Joseph L.},
	date = {1971},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Measurement, Psychiatric Patients, Psychodiagnosis, Statistical Analysis},
	file = {Fleiss_1971.pdf:/Users/tom/pCloud Drive/Zotero_Library/Fleiss_1971.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3AF2FGWZ/1972-05083-001.html:text/html},
}

@article{zwaan_making_2018,
	title = {Making replication mainstream},
	volume = {41},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X17001972/type/journal_article},
	doi = {10.1017/S0140525X17001972},
	abstract = {Many philosophers of science and methodologists have argued that the ability to repeat studies and obtain similar results is an essential component of science. A ﬁnding is elevated from single observation to scientiﬁc evidence when the procedures that were used to obtain it can be reproduced and the ﬁnding itself can be replicated. Recent replication attempts show that some high proﬁle results – most notably in psychology, but in many other disciplines as well – cannot be replicated consistently. These replication attempts have generated a considerable amount of controversy, and the issue of whether direct replications have value has, in particular, proven to be contentious. However, much of this discussion has occurred in published commentaries and social media outlets, resulting in a fragmented discourse. To address the need for an integrative summary, we review various types of replication studies and then discuss the most commonly voiced concerns about direct replication. We provide detailed responses to these concerns and consider different statistical ways to evaluate replications. We conclude there are no theoretical or statistical obstacles to making direct replication a routine aspect of psychological science.},
	pages = {e120},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Zwaan, Rolf A. and Etz, Alexander and Lucas, Richard E. and Donnellan, M. Brent},
	urldate = {2020-05-29},
	date = {2018},
	langid = {english},
	file = {Zwaan et al. - 2018 - Making replication mainstream.pdf:/Users/tom/Zotero/storage/ZXE3LHUP/Zwaan et al. - 2018 - Making replication mainstream.pdf:application/pdf},
}

@article{rothman_planning_2018,
	title = {Planning study size based on precision rather than power},
	volume = {29},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/Abstract/2018/09000/Planning_Study_Size_Based_on_Precision_Rather_Than.1.aspx},
	doi = {10.1097/EDE.0000000000000876},
	abstract = {Study size has typically been planned based on statistical power and therefore has been heavily influenced by the philosophy of statistical hypothesis testing. A worthwhile alternative is to plan study size based on precision, for example by aiming to obtain a desired width of a confidence interval for the targeted effect. This article presents formulas for planning the size of an epidemiologic study based on the desired precision of the basic epidemiologic effect measures.},
	pages = {599--603},
	number = {5},
	journaltitle = {Epidemiology},
	author = {Rothman, Kenneth J. and Greenland, Sander},
	urldate = {2020-05-22},
	date = {2018-09},
	langid = {american},
	file = {Rothman_Greenland_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Rothman_Greenland_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/K5BMT7SV/Planning_Study_Size_Based_on_Precision_Rather_Than.1.html:text/html},
}

@article{wagenmakers_power_2015,
	title = {A power fallacy},
	volume = {47},
	issn = {1554-3528},
	url = {http://link.springer.com/10.3758/s13428-014-0517-4},
	doi = {10.3758/s13428-014-0517-4},
	abstract = {The power fallacy refers to the misconception that what holds on average –across an ensemble of hypothetical experiments– also holds for each case individually. According to the fallacy, high-power experiments always yield more informative data than do low-power experiments. Here we expose the fallacy with concrete examples, demonstrating that a particular outcome from a high-power experiment can be completely uninformative, whereas a particular outcome from a low-power experiment can be highly informative. Although power is useful in planning an experiment, it is less useful—and sometimes even misleading—for making inferences from observed data. To make inferences from data, we recommend the use of likelihood ratios or Bayes factors, which are the extension of likelihood ratios beyond point hypotheses. These methods of inference do not average over hypothetical replications of an experiment, but instead condition on the data that have actually been observed. In this way, likelihood ratios and Bayes factors rationally quantify the evidence that a particular data set provides for or against the null or any other hypothesis.},
	pages = {913--917},
	number = {4},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Wagenmakers, Eric-Jan and Verhagen, Josine and Ly, Alexander and Bakker, Marjan and Lee, Michael D. and Matzke, Dora and Rouder, Jeffrey N. and Morey, Richard D.},
	urldate = {2020-05-22},
	date = {2015-12},
	langid = {english},
	file = {Wagenmakers et al. - 2015 - A power fallacy.pdf:/Users/tom/Zotero/storage/MFNA3G7K/Wagenmakers et al. - 2015 - A power fallacy.pdf:application/pdf},
}

@article{obels_analysis_2019,
	title = {Analysis of open data and computational reproducibility in registered reports in psychology},
	url = {https://psyarxiv.com/fk8vh/},
	doi = {10.31234/osf.io/fk8vh},
	abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to re-use or check published research. However, these benefits will only emerge if researchers can reproduce the analysis reported in published articles and if data is annotated well enough so that it is clear what all variables mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify practices that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature between 2014 and 2018, and attempted to independently computationally reproduce the main results in each article. Of the main results from 62 articles that met our inclusion criteria, data were available for 41 articles, and analysis scripts for 37 articles. For the main results in 36 articles that shared both data and code we could run the scripts for 31 analyses, and reproduce the main results for 21 articles. Although the articles that shared both data and code (36 out of 62, or 58\%) and articles for which main results could be computationally reproduced (21 out of 36, or 58\%) was relatively high compared to other studies, there is clear room for improvement. We provide practical recommendations based on our observations and link to examples of good research practices in the papers we reproduced.},
	author = {Obels, Pepijn and Lakens, Daniel and Coles, Nicholas and Gottfried, Jaroslav and Green, Seth Ariel},
	urldate = {2020-05-18},
	date = {2019-05-23},
	note = {Publisher: {PsyArXiv}},
	file = {Obels_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Obels_etal_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Y633F6DF/fk8vh.html:text/html},
}

@article{sakaluk_analytic_2014,
	title = {Analytic review as a solution to the misreporting of statistical results in psychological science},
	volume = {9},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691614549257},
	doi = {10.1177/1745691614549257},
	abstract = {In this article, we propose analytic review ({AR}) as a solution to the problem of misreporting statistical results in psychological science. {AR} requires authors submitting manuscripts for publication to also submit the data file and syntax used during analyses. Regular reviewers or statistical experts then review reported analyses in order to verify that the analyses reported were actually conducted and that the statistical values are accurately reported. We begin by describing the problem of misreporting in psychology and introduce the basic {AR} process. We then highlight both primary and secondary benefits of adopting {AR} and describe different permutations of the {AR} system, each of which has its own strengths and limitations. We conclude by attempting to dispel three anticipated concerns about {AR}: that it will increase the workload placed on scholars, that it will infringe on the traditional peer-review process, and that it will hurt the image of the discipline of psychology. Although implementing {AR} will add one more step to the bureaucratic publication process, we believe it can be implemented in an efficient manner that would greatly assist in decreasing the frequency and impact of misreporting while also providing secondary benefits in other domains of scientific integrity.},
	pages = {652--660},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Sakaluk, John and Williams, Alexander and Biernat, Monica},
	urldate = {2020-05-15},
	date = {2014-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Sakaluk_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Sakaluk_etal_2014.pdf:application/pdf},
}

@article{nuijten_verify_2018,
	title = {Verify original results through reanalysis before replicating},
	volume = {41},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/verify-original-results-through-reanalysis-before-replicating/0CB8760C66146E0A567E974573BBC2D8},
	doi = {10.1017/S0140525X18000791},
	abstract = {In determining the need to directly replicate, it is crucial to first verify the original results through independent reanalysis of the data. Original results that appear erroneous and that cannot be reproduced by reanalysis offer little evidence to begin with, thereby diminishing the need to replicate. Sharing data and scripts is essential to ensure reproducibility.},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Nuijten, Michèle B. and Bakker, Marjan and Maassen, Esther and Wicherts, Jelte M.},
	urldate = {2020-05-15},
	date = {2018},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Snapshot:/Users/tom/Zotero/storage/TT282U3B/0CB8760C66146E0A567E974573BBC2D8.html:text/html},
}

@article{vines_availability_2014,
	title = {The availability of research data declines rapidly with article age},
	volume = {24},
	issn = {0960-9822},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(13)01400-0},
	doi = {10.1016/j.cub.2013.11.014},
	pages = {94--97},
	number = {1},
	journaltitle = {Current Biology},
	shortjournal = {Current Biology},
	author = {Vines, Timothy H. and Albert, Arianne Y. K. and Andrew, Rose L. and Débarre, Florence and Bock, Dan G. and Franklin, Michelle T. and Gilbert, Kimberly J. and Moore, Jean-Sébastien and Renaut, Sébastien and Rennison, Diana J.},
	urldate = {2020-05-14},
	date = {2014-01-06},
	pmid = {24361065},
	note = {Publisher: Elsevier},
	file = {Snapshot:/Users/tom/Zotero/storage/8MVAMVYV/S0960-9822(13)01400-0.html:text/html;Vines_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vines_etal_2014.pdf:application/pdf},
}

@article{eich_business_2014,
	title = {Business not as usual},
	volume = {25},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797613512465},
	doi = {10.1177/0956797613512465},
	pages = {3--6},
	number = {1},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Eich, Eric},
	urldate = {2020-05-14},
	date = {2014-01},
	langid = {english},
	file = {Eich_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Eich_2014.pdf:application/pdf},
}

@article{sison_simultaneous_1995,
	title = {Simultaneous confidence intervals and sample size determination for multinomial proportions},
	volume = {90},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476521},
	doi = {10.1080/01621459.1995.10476521},
	abstract = {Simultaneous confidence interval procedures for multinomial proportions are used in many areas of science. In this article two new simultaneous confidence interval procedures are introduced. Numerical results are presented to evaluate these procedures and compare their performance with established methods that have been used in statistical literature. From the results presented in this article, it is evident that the new procedures are more accurate than the established ones, where the accuracy of the procedure is measured by the volume of the confidence region corresponding to the nominal coverage probability and the probability of coverage it achieves. In the sample size determination problem, the new procedures provide a sizable amount of savings as compared to the procedures that have been used in many applications. Because both procedures performed equally well, the procedure that requires the least amount of computing time is recommended.},
	pages = {366--369},
	number = {429},
	journaltitle = {Journal of the American Statistical Association},
	author = {Sison, Cristina P. and Glaz, Joseph},
	urldate = {2020-05-11},
	date = {1995-03-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476521},
	keywords = {Coverage probabilities, Multinomial distribution, Probability approximations, Simultaneous inference},
	file = {Snapshot:/Users/tom/Zotero/storage/PMG94VQ4/01621459.1995.html:text/html},
}

@book{field_discovering_2012,
	location = {London ; Thousand Oaks, Calif},
	title = {Discovering statistics using R},
	isbn = {978-1-4462-0046-9 978-1-4462-0045-2},
	pagetotal = {957},
	publisher = {Sage},
	author = {Field, Andy P. and Miles, Jeremy and Field, Zoë},
	date = {2012},
	note = {{OCLC}: ocn760970657},
	keywords = {Computer programs, R (Computer program language), Social sciences, Statistical methods Computer programs, Statistics},
}

@article{hardwicke_data_2018,
	title = {Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal Cognition},
	volume = {5},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.180448},
	doi = {10.1098/rsos.180448},
	shorttitle = {Data availability, reusability, and analytic reproducibility},
	abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (‘analytic reproducibility’). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
	pages = {180448},
	number = {8},
	journaltitle = {Royal Society Open Science},
	shortjournal = {Royal Society Open Science},
	author = {Hardwicke, Tom E. and Mathur, Maya B. and {MacDonald}, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Hofelich Mohr, Alicia and Clayton, Elizabeth and Yoon, Erica J. and Henry Tessler, Michael and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
	urldate = {2020-05-11},
	date = {2018},
	note = {Publisher: Royal Society},
	file = {Full Text PDF:/Users/tom/Zotero/storage/JK4NV4RJ/Hardwicke et al. - Data availability, reusability, and analytic repro.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/WSB7QBPS/rsos.html:text/html},
}

@article{wicherts_poor_2006,
	title = {The poor availability of psychological research data for reanalysis.},
	volume = {61},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0003-066X.61.7.726},
	doi = {10.1037/0003-066X.61.7.726},
	pages = {726--728},
	number = {7},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Wicherts, Jelte M. and Borsboom, Denny and Kats, Judith and Molenaar, Dylan},
	urldate = {2020-05-11},
	date = {2006},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/PT8FVF5W/Wicherts et al. - 2006 - The poor availability of psychological research da.pdf:application/pdf},
}

@article{hardwicke_populating_2018,
	title = {Populating the Data Ark: An attempt to retrieve, preserve, and liberate data from the most highly-cited psychology and psychiatry articles},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201856},
	doi = {10.1371/journal.pone.0201856},
	shorttitle = {Populating the Data Ark},
	abstract = {The vast majority of scientific articles published to-date have not been accompanied by concomitant publication of the underlying research data upon which they are based. This state of affairs precludes the routine re-use and re-analysis of research data, undermining the efficiency of the scientific enterprise, and compromising the credibility of claims that cannot be independently verified. It may be especially important to make data available for the most influential studies that have provided a foundation for subsequent research and theory development. Therefore, we launched an initiative—the Data Ark—to examine whether we could retrospectively enhance the preservation and accessibility of important scientific data. Here we report the outcome of our efforts to retrieve, preserve, and liberate data from 111 of the most highly-cited articles published in psychology and psychiatry between 2006–2011 (n = 48) and 2014–2016 (n = 63). Most data sets were not made available (76/111, 68\%, 95\% {CI} [60, 77]), some were only made available with restrictions (20/111, 18\%, 95\% {CI} [10, 27]), and few were made available in a completely unrestricted form (15/111, 14\%, 95\% {CI} [5, 22]). Where extant data sharing systems were in place, they usually (17/22, 77\%, 95\% {CI} [54, 91]) did not allow unrestricted access. Authors reported several barriers to data sharing, including issues related to data ownership and ethical concerns. The Data Ark initiative could help preserve and liberate important scientific data, surface barriers to data sharing, and advance community discussions on data stewardship.},
	pages = {e0201856},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
	urldate = {2020-05-11},
	date = {2018-08-02},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Attention, Citation analysis, Data acquisition, Health care policy, Mental health and psychiatry, Open science, Science policy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/5ECEZNXB/Hardwicke and Ioannidis - 2018 - Populating the Data Ark An attempt to retrieve, p.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZVL996IY/article.html:text/html},
}

@article{ioannidis_repeatability_2009,
	title = {Repeatability of published microarray gene expression analyses},
	volume = {41},
	rights = {2009 Nature Publishing Group},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/ng.295},
	doi = {10.1038/ng.295},
	abstract = {Four teams of analysts attempted exact reproduction of results of 18 microarray experiments published in the journal in 2005–2006 using the data and analytical methods detailed in the original publications. In addition to {MIAME} criteria, the authors recommend publication of an explicit record of the analytical protocols used.},
	pages = {149--155},
	number = {2},
	journaltitle = {Nature Genetics},
	shortjournal = {Nat Genet},
	author = {Ioannidis, John P. A. and Allison, David B. and Ball, Catherine A. and Coulibaly, Issa and Cui, Xiangqin and Culhane, Aedín C. and Falchi, Mario and Furlanello, Cesare and Game, Laurence and Jurman, Giuseppe and Mangion, Jon and Mehta, Tapan and Nitzberg, Michael and Page, Grier P. and Petretto, Enrico and Noort, Vera van},
	urldate = {2020-05-11},
	date = {2009-02},
	langid = {english},
	note = {Number: 2
Publisher: Nature Publishing Group},
	file = {Snapshot:/Users/tom/Zotero/storage/5WCMSNQW/ng.html:text/html},
}

@article{eubank_lessons_2016,
	title = {Lessons from a Decade of Replications at the Quarterly Journal of Political Science},
	volume = {49},
	issn = {1049-0965, 1537-5935},
	url = {https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/lessons-from-a-decade-of-replications-at-the-quarterly-journal-of-political-science/284B2830BFD99888B42D4CEABC28B9EE},
	doi = {10.1017/S1049096516000196},
	abstract = {To allow researchers to investigate not only whether a paper’s methods are theoretically sound but also whether they have been properly implemented and are robust to alternative specifications, it is necessary that published papers be accompanied by their underlying data and code. This article describes experiences and lessons learned at the Quarterly Journal of Political Science since it began requiring authors to provide this type of replication code in 2005. It finds that of the 24 empirical papers subjected to in-house replication review since September 2012, only four packages did not require any modifications. Most troubling, 14 packages (58\%) had results in the paper that differed from those generated by the author’s own code. Based on these experiences, this article presents a set of guidelines for authors and journals for improving the reliability and usability of replication packages.},
	pages = {273--276},
	number = {2},
	journaltitle = {{PS}: Political Science \& Politics},
	author = {Eubank, Nicholas},
	urldate = {2020-05-11},
	date = {2016-04},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NK94JDLL/Eubank - 2016 - Lessons from a Decade of Replications at the Quart.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AG2VYKVD/284B2830BFD99888B42D4CEABC28B9EE.html:text/html},
}

@report{bollen_social_2015,
	location = {Arlington, {VA}},
	title = {Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science},
	url = {https://www.nsf.gov/sbe/AC_Materials/SBE_Robust_and_Reliable_Research_Report.pdf},
	institution = {National Science Foundation},
	author = {Bollen, K and Cacioppo, J. T. and Kaplan, R. M. and Krosnick, J. A. and Olds, J. L.},
	urldate = {2020-05-11},
	date = {2015},
	file = {SBE_Robust_and_Reliable_Research_Report.pdf:/Users/tom/Zotero/storage/QQTY79WT/SBE_Robust_and_Reliable_Research_Report.pdf:application/pdf},
}

@article{lebel_unified_2018,
	title = {A unified framework to quantify the credibility of scientific findings:},
	url = {https://journals.sagepub.com/doi/10.1177/2515245918787489},
	doi = {10.1177/2515245918787489},
	shorttitle = {A unified framework to quantify the credibility of scientific findings},
	abstract = {Societies invest in scientific studies to better understand the world and attempt to harness such improved understanding to address pressing societal problems. ...},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {{LeBel}, Etienne P. and {McCarthy}, Randy J. and Earp, Brian D. and Elson, Malte and Vanpaemel, Wolf},
	urldate = {2020-05-11},
	date = {2018-08-10},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/E3DBWBDK/2515245918787489.html:text/html;Submitted Version:/Users/tom/Zotero/storage/2KM4CC52/LeBel et al. - 2018 - A Unified Framework to Quantify the Credibility of.pdf:application/pdf},
}

@article{stodden_empirical_2018,
	title = {An empirical analysis of journal policy effectiveness for computational reproducibility},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708290115},
	doi = {10.1073/pnas.1708290115},
	abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (
              i
              ) requesting data and code from authors and (
              ii
              ) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal
              Science
              after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy—author remission of data and code postpublication upon request—an improvement over no policy, but currently insufficient for reproducibility.},
	pages = {2584--2589},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
	urldate = {2020-05-11},
	date = {2018-03-13},
	langid = {english},
	file = {Stodden et al. - 2018 - An empirical analysis of journal policy effectiven.pdf:/Users/tom/Zotero/storage/RURGBULE/Stodden et al. - 2018 - An empirical analysis of journal policy effectiven.pdf:application/pdf},
}

@article{stodden_enhancing_2016,
	title = {Enhancing reproducibility for computational methods},
	volume = {354},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aah6168},
	doi = {10.1126/science.aah6168},
	pages = {1240--1241},
	number = {6317},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Stodden, V. and {McNutt}, M. and Bailey, D. H. and Deelman, E. and Gil, Y. and Hanson, B. and Heroux, M. A. and Ioannidis, J. P. A. and Taufer, M.},
	urldate = {2020-05-11},
	date = {2016-12-09},
	langid = {english},
	file = {Stodden et al. - 2016 - Enhancing reproducibility for computational method.pdf:/Users/tom/Zotero/storage/TMUIBH2J/Stodden et al. - 2016 - Enhancing reproducibility for computational method.pdf:application/pdf},
}

@article{stodden_toward_2013,
	title = {Toward reproducible computational research: an empirical analysis of data and code policy adoption by journals},
	volume = {8},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0067111},
	doi = {10.1371/journal.pone.0067111},
	shorttitle = {Toward reproducible computational research},
	abstract = {Journal policy on research data and code availability is an important part of the ongoing shift toward publishing reproducible computational science. This article extends the literature by studying journal data sharing policies by year (for both 2011 and 2012) for a referent set of 170 journals. We make a further contribution by evaluating code sharing policies, supplemental materials policies, and open access status for these 170 journals for each of 2011 and 2012. We build a predictive model of open data and code policy adoption as a function of impact factor and publisher and find higher impact journals more likely to have open data and code policies and scientific societies more likely to have open data and code policies than commercial publishers. We also find open data policies tend to lead open code policies, and we find no relationship between open data and code policies and either supplemental material policies or open access journal status. Of the journals in this study, 38\% had a data policy, 22\% had a code policy, and 66\% had a supplemental materials policy as of June 2012. This reflects a striking one year increase of 16\% in the number of data policies, a 30\% increase in code policies, and a 7\% increase in the number of supplemental materials policies. We introduce a new dataset to the community that categorizes data and code sharing, supplemental materials, and open access policies in 2011 and 2012 for these 170 journals.},
	pages = {e67111},
	number = {6},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Stodden, Victoria and Guo, Peixuan and Ma, Zhaokun},
	editor = {Zaykin, Dmitri},
	urldate = {2020-05-11},
	date = {2013-06-21},
	langid = {english},
	file = {Stodden et al. - 2013 - Toward Reproducible Computational Research An Emp.pdf:/Users/tom/Zotero/storage/BEXYE6R3/Stodden et al. - 2013 - Toward Reproducible Computational Research An Emp.pdf:application/pdf},
}

@article{stodden_reproducing_2015,
	title = {Reproducing Statistical Results},
	volume = {2},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-010814-020127},
	doi = {10.1146/annurev-statistics-010814-020127},
	abstract = {The reproducibility of statistical ﬁndings has become a concern not only for statisticians, but for all researchers engaged in empirical discovery. Section 2 of this article identiﬁes key reasons statistical ﬁndings may not replicate, including power and sampling issues; misapplication of statistical tests; the instability of ﬁndings under reasonable perturbations of data or models; lack of access to methods, data, or equipment; and cultural barriers such as researcher incentives and rewards. Section 3 discusses ﬁve proposed remedies for these replication failures: improved prepublication and postpublication validation of ﬁndings; the complete disclosure of research steps; assessment of the stability of statistical ﬁndings; providing access to digital research objects, in particular data and software; and ensuring these objects are legally reusable.},
	pages = {1--19},
	number = {1},
	journaltitle = {Annual Review of Statistics and Its Application},
	shortjournal = {Annu. Rev. Stat. Appl.},
	author = {Stodden, Victoria},
	urldate = {2020-05-11},
	date = {2015-04-10},
	langid = {english},
	file = {Stodden - 2015 - Reproducing Statistical Results.pdf:/Users/tom/Zotero/storage/982BA99P/Stodden - 2015 - Reproducing Statistical Results.pdf:application/pdf},
}

@article{marwick_packaging_2018,
	title = {Packaging data analytical work reproducibly using R (and friends)},
	volume = {72},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375986},
	doi = {10.1080/00031305.2017.1375986},
	abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
	pages = {80--88},
	number = {1},
	journaltitle = {The American Statistician},
	shortjournal = {The American Statistician},
	author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
	urldate = {2020-04-28},
	date = {2018-01-02},
	langid = {english},
	file = {Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using .pdf:/Users/tom/Zotero/storage/RUSMZ9FG/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using .pdf:application/pdf},
}

@article{baggerly_deriving_2009,
	title = {Deriving chemosensitivity from cell lines: Forensic bioinformatics and reproducible research in high-throughput biology},
	volume = {3},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/euclid.aoas/1267453942},
	doi = {10.1214/09-AOAS291},
	shorttitle = {Deriving chemosensitivity from cell lines},
	abstract = {High-throughput biological assays such as microarrays let us ask very detailed questions about how diseases operate, and promise to let us personalize therapy. Data processing, however, is often not described well enough to allow for exact reproduction of the results, leading to exercises in “forensic bioinformatics” where aspects of raw data and reported results are used to infer what methods must have been employed. Unfortunately, poor documentation can shift from an inconvenience to an active danger when it obscures not just methods but errors. In this report we examine several related papers purporting to use microarray-based signatures of drug sensitivity derived from cell lines to predict patient response. Patients in clinical trials are currently being allocated to treatment arms on the basis of these results. However, we show in five case studies that the results incorporate several simple errors that may be putting patients at risk. One theme that emerges is that the most common errors are simple (e.g., row or column offsets); conversely, it is our experience that the most simple errors are common. We then discuss steps we are taking to avoid such errors in our own investigations.},
	pages = {1309--1334},
	number = {4},
	journaltitle = {Annals of Applied Statistics},
	shortjournal = {Ann. Appl. Stat.},
	author = {Baggerly, Keith A. and Coombes, Kevin R.},
	urldate = {2020-04-28},
	date = {2009-12},
	mrnumber = {MR2752136},
	zmnumber = {1185.92056},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {reproducibility, forensic bioinformatics, Microarrays},
	file = {Full Text PDF:/Users/tom/Zotero/storage/IHLAHZC4/Baggerly and Coombes - 2009 - Deriving chemosensitivity from cell lines Forensi.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/2TN7PR9N/1267453942.html:text/html},
}

@article{garijo_quantifying_2013,
	title = {Quantifying reproducibility in computational biology: the case of the tuberculosis drugome},
	volume = {8},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0080278},
	shorttitle = {Quantifying reproducibility in computational biology},
	abstract = {How easy is it to reproduce the results found in a typical computational biology paper? Either through experience or intuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify this difficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertise to domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimating the time required to reproduce each of the steps in the method described in the original paper and make them part of an explicit workflow that reproduces the original results. Reproducing the method took several months of effort, and required using new versions and new software that posed challenges to reconstructing and validating the results. The quantification leads to "reproducibility maps" that reveal that novice researchers would only be able to reproduce a few of the steps in the method, and that only expert researchers with advance knowledge of the domain would be able to reproduce the method in its entirety. The workflow itself is published as an online resource together with supporting software and data. The paper concludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and a desiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducing the work of others from published papers, but reproducing work from one's own laboratory.},
	pages = {e80278},
	number = {11},
	journaltitle = {{PloS} One},
	shortjournal = {{PLoS} {ONE}},
	author = {Garijo, Daniel and Kinnings, Sarah and Xie, Li and Xie, Lei and Zhang, Yinliang and Bourne, Philip E. and Gil, Yolanda},
	date = {2013},
	pmid = {24312207},
	pmcid = {PMC3842296},
	keywords = {Humans, Computational Biology, Internet, Reproducibility of Results, Software},
	file = {Full Text:/Users/tom/Zotero/storage/RQ5ITH58/Garijo et al. - 2013 - Quantifying reproducibility in computational biolo.pdf:application/pdf},
}

@article{sholler_enforcing_2019,
	title = {Enforcing public data archiving policies in academic publishing: A study of ecology journals:},
	url = {https://journals.sagepub.com/doi/10.1177/2053951719836258},
	doi = {10.1177/2053951719836258},
	shorttitle = {Enforcing public data archiving policies in academic publishing},
	abstract = {To improve the quality and efficiency of research, groups within the scientific community seek to exploit the value of data sharing. Funders, institutions, and ...},
	journaltitle = {Big Data \& Society},
	author = {Sholler, Dan and Ram, Karthik and Boettiger, Carl and Katz, Daniel S.},
	urldate = {2020-04-27},
	date = {2019-03-25},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/V8MT9V4X/Sholler et al. - 2019 - Enforcing public data archiving policies in academ.html:text/html},
}

@article{assel_statistical_2018,
	title = {Statistical code for clinical research papers in a high-impact specialist medical journal},
	volume = {168},
	issn = {0003-4819},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6705117/},
	doi = {10.7326/M17-2863},
	pages = {832--833},
	number = {11},
	journaltitle = {Annals of internal medicine},
	shortjournal = {Ann Intern Med},
	author = {Assel, Melissa and Vickers, Andrew J.},
	urldate = {2020-04-27},
	date = {2018-06-05},
	pmid = {29404569},
	pmcid = {PMC6705117},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/ZSRX7T5M/Assel and Vickers - 2018 - Statistical code for clinical research papers in a.pdf:application/pdf},
}

@article{coffman_pre-analysis_2015,
	title = {Pre-analysis plans have limited upside, especially where replications are feasible},
	volume = {29},
	issn = {0895-3309},
	url = {http://pubs.aeaweb.org/doi/10.1257/jep.29.3.81},
	doi = {10.1257/jep.29.3.81},
	pages = {81--98},
	number = {3},
	journaltitle = {Journal of Economic Perspectives},
	shortjournal = {Journal of Economic Perspectives},
	author = {Coffman, Lucas C. and Niederle, Muriel},
	urldate = {2020-04-23},
	date = {2015-08},
	langid = {english},
	file = {Coffman and Niederle - 2015 - Pre-Analysis Plans Have Limited Upside, Especially.pdf:/Users/tom/Zotero/storage/229DPSYQ/Coffman and Niederle - 2015 - Pre-Analysis Plans Have Limited Upside, Especially.pdf:application/pdf},
}

@article{john_measuring_2012,
	title = {Measuring the prevalence of questionable research practices with incentives for truth telling},
	volume = {23},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611430953},
	doi = {10.1177/0956797611430953},
	abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
	pages = {524--532},
	number = {5},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
	urldate = {2020-03-09},
	date = {2012-05},
	langid = {english},
	file = {John_etal_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/John_etal_2012.pdf:application/pdf},
}

@article{john_when_2018,
	title = {When and why randomized response techniques (fail to) elicit the truth},
	volume = {148},
	issn = {0749-5978},
	url = {http://www.sciencedirect.com/science/article/pii/S0749597817300523},
	doi = {10.1016/j.obhdp.2018.07.004},
	abstract = {By adding random noise to individual responses, randomized response techniques ({RRTs}) are intended to enhance privacy protection and encourage honest disclosure of sensitive information. Empirical findings on their success in doing so are, however, mixed. In nine experiments, we show that the noise introduced by {RRTs} can make respondents concerned that innocuous responses will be interpreted as admissions, and as a result, yield prevalence estimates that are lower than direct questioning (Studies 1–4, 5A, \& 6), less accurate than direct questioning (Studies 1, 3, 4B, \& 5A), and even nonsensical (i.e., negative; Studies 3–6). Studies 2A and 2B show that the paradox is eliminated when the target behavior is socially desirable, even when it is merely framed as such. Study 3 shows the paradox is driven by respondents’ concerns over response misinterpretation. A simple modification designed to reduce concerns over response misinterpretation reduces the problem (Studies 4 \& 5), particularly when such concerns are heightened (Studies 5 \& 6).},
	pages = {101--123},
	journaltitle = {Organizational Behavior and Human Decision Processes},
	shortjournal = {Organizational Behavior and Human Decision Processes},
	author = {John, Leslie K. and Loewenstein, George and Acquisti, Alessandro and Vosgerau, Joachim},
	urldate = {2020-03-09},
	date = {2018-09-01},
	langid = {english},
	keywords = {Information disclosure, Lying, Privacy, Survey research, Truth-telling},
	file = {John_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/John_etal_2018.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/937UTQZD/S0749597817300523.html:text/html},
}

@article{hardwicke_empirical_2020,
	title = {An empirical assessment of transparency and reproducibility-related research practices in the social sciences (2014–2017)},
	volume = {7},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.190806},
	doi = {10.1098/rsos.190806},
	abstract = {Serious concerns about research quality have catalysed a number of reform initiatives intended to improve transparency and reproducibility and thus facilitate self-correction, increase efficiency and enhance research credibility. Meta-research has evaluated the merits of some individual initiatives; however, this may not capture broader trends reflecting the cumulative contribution of these efforts. In this study, we manually examined a random sample of 250 articles in order to estimate the prevalence of a range of transparency and reproducibility-related indicators in the social sciences literature published between 2014 and 2017. Few articles indicated availability of materials (16/151, 11\% [95\% confidence interval, 7\% to 16\%]), protocols (0/156, 0\% [0\% to 1\%]), raw data (11/156, 7\% [2\% to 13\%]) or analysis scripts (2/156, 1\% [0\% to 3\%]), and no studies were pre-registered (0/156, 0\% [0\% to 1\%]). Some articles explicitly disclosed funding sources (or lack of; 74/236, 31\% [25\% to 37\%]) and some declared no conflicts of interest (36/236, 15\% [11\% to 20\%]). Replication studies were rare (2/156, 1\% [0\% to 3\%]). Few studies were included in evidence synthesis via systematic review (17/151, 11\% [7\% to 16\%]) or meta-analysis (2/151, 1\% [0\% to 3\%]). Less than half the articles were publicly available (101/250, 40\% [34\% to 47\%]). Minimal adoption of transparency and reproducibility-related research practices could be undermining the credibility and efficiency of social science research. The present study establishes a baseline that can be revisited in the future to assess progress.},
	pages = {190806},
	number = {2},
	journaltitle = {Royal Society Open Science},
	shortjournal = {Royal Society Open Science},
	author = {Hardwicke, Tom E. and Wallach, Joshua D. and Kidwell, Mallory C. and Bendixen, Theiss and Crüwell, Sophia and Ioannidis, John P. A.},
	urldate = {2020-03-09},
	date = {2020},
	note = {Publisher: Royal Society},
	file = {Hardwicke_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hardwicke_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZGJRJUQ3/rsos.html:text/html},
}

@article{campbell_factors_1957,
	title = {Factors relevant to the validity of experiments in social settings},
	volume = {54},
	pages = {297--312},
	number = {4},
	journaltitle = {Psychological Bulletin},
	author = {Campbell, Donald T},
	date = {1957},
	langid = {english},
	keywords = {❓ Multiple {DOI}},
	file = {Campbell - 1957 - Factors relevant to the validity of experiments in.pdf:/Users/tom/Zotero/storage/FMEDWSNA/Campbell - 1957 - Factors relevant to the validity of experiments in.pdf:application/pdf},
}

@article{schulz_randomised_1996,
	title = {Randomised trials, human nature, and reporting guidelines},
	volume = {348},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(96)01201-9/abstract},
	doi = {10.1016/S0140-6736(96)01201-9},
	abstract = {A flurry of activity on the reporting of randomised controlled trials ({RCTs}) has culminated
in the {CONSORT} guidelines.1 These recommendations followed the {SORT},2 Asilomar Working
Group,3 and the British Journal of Obstetrics and Gynaecology4 guidelines and a widely
circulated editorial5 on the reporting of {RCTs}. Why this guideline avalanche? {RCTs},
the methodological paragon for assessing evidence, have historically been poorly reported
in general medical and specialist journals.6–13 Little seems to have changed over
the years.},
	pages = {596--598},
	number = {9027},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Schulz, Kenneth F.},
	urldate = {2020-09-21},
	date = {1996-08-31},
	pmid = {8774577},
	note = {Publisher: Elsevier},
	file = {Schulz_1996.pdf:/Users/tom/pCloud Drive/Zotero_Library/Schulz_1996.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BLY57AQG/fulltext.html:text/html},
}

@article{altman_treatment_1999,
	title = {Treatment allocation in controlled trials: why randomise?},
	volume = {318},
	rights = {© 1999 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/318/7192/1209.1},
	doi = {10.1136/bmj.318.7192.1209},
	shorttitle = {Treatment allocation in controlled trials},
	abstract = {Since 1991 the {BMJ} has had a policy of not publishing trials that have not been properly randomised, except in rare cases where this can be justified.1 Why?

The simplest approach to evaluating a new treatment is to compare a single group of patients given the new treatment with a group previously treated with an alternative treatment. Usually such studies compare two consecutive series of patients in the same hospital(s). This approach is seriously flawed. Problems will arise from the mixture of retrospective and prospective studies, and we can never satisfactorily eliminate possible biases due to other factors (apart from treatment) that may have changed over time. Sacks et al compared trials of the same treatments in which randomised or historical controls were used and found a consistent tendency for historically controlled trials to yield more optimistic results than randomised trials.2 The use of historical controls can be justified only in tightly controlled situations of relatively rare conditions, such …},
	pages = {1209--1209},
	number = {7192},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Altman, Douglas G. and Bland, J. Martin},
	urldate = {2020-09-21},
	date = {1999-05-01},
	langid = {english},
	pmid = {10221955},
	note = {Publisher: British Medical Journal Publishing Group
Section: Education and debate},
	file = {Altman_Bland_1999.pdf:/Users/tom/pCloud Drive/Zotero_Library/Altman_Bland_1999.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZZQR8PSP/1209.html:text/html},
}

@article{turner_evolution_2013,
	title = {The evolution of assessing bias in Cochrane systematic reviews of interventions: celebrating methodological contributions of the Cochrane Collaboration},
	volume = {2},
	issn = {2046-4053},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3851839/},
	doi = {10.1186/2046-4053-2-79},
	shorttitle = {The evolution of assessing bias in Cochrane systematic reviews of interventions},
	pages = {79},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {Turner, Lucy and Boutron, Isabelle and Hróbjartsson, Asbjørn and Altman, Douglas G and Moher, David},
	urldate = {2020-09-21},
	date = {2013-09-23},
	pmid = {24059942},
	pmcid = {PMC3851839},
	file = {Turner_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Turner_etal_2013.pdf:application/pdf},
}

@article{page_tools_2018,
	title = {Tools for assessing risk of reporting biases in studies and syntheses of studies: a systematic review},
	volume = {8},
	rights = {© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2018. All rights reserved. No commercial use is permitted unless otherwise expressly granted.. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/},
	issn = {2044-6055, 2044-6055},
	url = {https://bmjopen.bmj.com/content/8/3/e019703},
	doi = {10.1136/bmjopen-2017-019703},
	shorttitle = {Tools for assessing risk of reporting biases in studies and syntheses of studies},
	abstract = {Background Several scales, checklists and domain-based tools for assessing risk of reporting biases exist, but it is unclear how much they vary in content and guidance. We conducted a systematic review of the content and measurement properties of such tools.
Methods We searched for potentially relevant articles in Ovid {MEDLINE}, Ovid Embase, Ovid {PsycINFO} and Google Scholar from inception to February 2017. One author screened all titles, abstracts and full text articles, and collected data on tool characteristics.
Results We identified 18 tools that include an assessment of the risk of reporting bias. Tools varied in regard to the type of reporting bias assessed (eg, bias due to selective publication, bias due to selective non-reporting), and the level of assessment (eg, for the study as a whole, a particular result within a study or a particular synthesis of studies). Various criteria are used across tools to designate a synthesis as being at ‘high’ risk of bias due to selective publication (eg, evidence of funnel plot asymmetry, use of non-comprehensive searches). However, the relative weight assigned to each criterion in the overall judgement is unclear for most of these tools. Tools for assessing risk of bias due to selective non-reporting guide users to assess a study, or an outcome within a study, as ‘high’ risk of bias if no results are reported for an outcome. However, assessing the corresponding risk of bias in a synthesis that is missing the non-reported outcomes is outside the scope of most of these tools. Inter-rater agreement estimates were available for five tools.
Conclusion There are several limitations of existing tools for assessing risk of reporting biases, in terms of their scope, guidance for reaching risk of bias judgements and measurement properties. Development and evaluation of a new, comprehensive tool could help overcome present limitations.},
	pages = {e019703},
	number = {3},
	journaltitle = {{BMJ} Open},
	author = {Page, Matthew J. and {McKenzie}, Joanne E. and Higgins, Julian P. T.},
	urldate = {2020-09-20},
	date = {2018-03-01},
	langid = {english},
	pmid = {29540417},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research methods},
	keywords = {publication bias, bias (epidemiology), checklist, review literature as topic},
	file = {Page et al. - 2018 - Tools for assessing risk of reporting biases in st.pdf:/Users/tom/Zotero/storage/VAAURPJ8/Page et al. - 2018 - Tools for assessing risk of reporting biases in st.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/QCF3JES6/e019703.html:text/html},
}

@article{gentili_case_2020,
	title = {The case for preregistering all region of interest ({ROI}) analyses in neuroimaging research},
	issn = {1460-9568},
	doi = {10.1111/ejn.14954},
	abstract = {In neuroimaging studies, small sample sizes and the resultant reduced statistical power to detect effects that are not large, combined with inadequate analytic choices, concur to produce inflated or false-positive findings. To mitigate these issues, researchers often restrict analyses to specific brain areas, using the region of interest ({ROI}) approach. Crucially, {ROI} analysis assumes the a priori justified definition of the target region. Nonetheless, reports often lack details about where in the timeline, ranging from study conception to the data analysis and interpretation of findings, were {ROIs} selected. Frequently, the rationale for {ROI} selection is vague or inadequately founded on the existing literature. These shortcomings have important implications for {ROI}-based studies, augmenting the risk that observed effects are inflated or even false positives. Tools like preregistration and registered reports could address this problem, ensuring the validity of {ROI}-based studies. The benefits could be enhanced by additional practices such as selection of {ROIs} using quantitative methods (i.e., meta-analysis) and the sharing of whole-brain unthresholded maps of effect size, as well as of binary {ROIs}, in publicly accessible repositories.},
	journaltitle = {The European Journal of Neuroscience},
	shortjournal = {Eur. J. Neurosci.},
	author = {Gentili, Claudio and Cecchetti, Luca and Handjaras, Giacomo and Lettieri, Giada and Cristea, Ioana A.},
	date = {2020-08-27},
	pmid = {32852863},
	file = {Gentili_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Gentili_etal_2020.pdf:application/pdf},
}

@article{kirkham_impact_2010,
	title = {The impact of outcome reporting bias in randomised controlled trials on a cohort of systematic reviews},
	volume = {340},
	rights = {© {BMJ} Publishing Group Ltd 2010},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/340/bmj.c365},
	doi = {10.1136/bmj.c365},
	abstract = {Objective To examine the prevalence of outcome reporting bias—the selection for publication of a subset of the original recorded outcome variables on the basis of the results—and its impact on Cochrane reviews.
Design A nine point classification system for missing outcome data in randomised trials was developed and applied to the trials assessed in a large, unselected cohort of Cochrane systematic reviews. Researchers who conducted the trials were contacted and the reason sought for the non-reporting of data. A sensitivity analysis was undertaken to assess the impact of outcome reporting bias on reviews that included a single meta-analysis of the review primary outcome.
Results More than half (157/283 (55\%)) the reviews did not include full data for the review primary outcome of interest from all eligible trials. The median amount of review outcome data missing for any reason was 10\%, whereas 50\% or more of the potential data were missing in 70 (25\%) reviews. It was clear from the publications for 155 (6\%) of the 2486 assessable trials that the researchers had measured and analysed the review primary outcome but did not report or only partially reported the results. For reports that did not mention the review primary outcome, our classification regarding the presence of outcome reporting bias was shown to have a sensitivity of 88\% (95\% {CI} 65\% to 100\%) and specificity of 80\% (95\% {CI} 69\% to 90\%) on the basis of responses from 62 trialists. A third of Cochrane reviews (96/283 (34\%)) contained at least one trial with high suspicion of outcome reporting bias for the review primary outcome. In a sensitivity analysis undertaken for 81 reviews with a single meta-analysis of the primary outcome of interest, the treatment effect estimate was reduced by 20\% or more in 19 (23\%). Of the 42 meta-analyses with a statistically significant result only, eight (19\%) became non-significant after adjustment for outcome reporting bias and 11 (26\%) would have overestimated the treatment effect by 20\% or more.
Conclusions Outcome reporting bias is an under-recognised problem that affects the conclusions in a substantial proportion of Cochrane reviews. Individuals conducting systematic reviews need to address explicitly the issue of missing outcome data for their review to be considered a reliable source of evidence. Extra care is required during data extraction, reviewers should identify when a trial reports that an outcome was measured but no results were reported or events observed, and contact with trialists should be encouraged.},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Kirkham, Jamie J. and Dwan, Kerry M. and Altman, Douglas G. and Gamble, Carrol and Dodd, Susanna and Smyth, Rebecca and Williamson, Paula R.},
	urldate = {2020-09-20},
	date = {2010-02-15},
	langid = {english},
	pmid = {20156912},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Kirkham_etal_2010.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kirkham_etal_2010.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/FJK4EN4A/bmj.c365.html:text/html},
}

@article{cro_evidence_2020,
	title = {Evidence of unexplained discrepancies between planned and conducted statistical analyses: a review of randomised trials},
	volume = {18},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-020-01590-1},
	doi = {10.1186/s12916-020-01590-1},
	shorttitle = {Evidence of unexplained discrepancies between planned and conducted statistical analyses},
	abstract = {Choosing or altering the planned statistical analysis approach after examination of trial data (often referred to as ‘p-hacking’) can bias the results of randomised trials. However, the extent of this issue in practice is currently unclear. We conducted a review of published randomised trials to evaluate how often a pre-specified analysis approach is publicly available, and how often the planned analysis is changed.},
	pages = {137},
	number = {1},
	journaltitle = {{BMC} Medicine},
	shortjournal = {{BMC} Medicine},
	author = {Cro, Suzie and Forbes, Gordon and Johnson, Nicholas A. and Kahan, Brennan C.},
	urldate = {2020-09-20},
	date = {2020-05-29},
	file = {Cro_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Cro_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5A29F6LS/s12916-020-01590-1.html:text/html},
}

@article{kahan_public_2020,
	title = {Public availability and adherence to prespecified statistical analysis approaches was low in published randomized trials},
	volume = {128},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435620301979},
	doi = {10.1016/j.jclinepi.2020.07.015},
	abstract = {Background and Objective
Prespecification of statistical methods in clinical trial protocols and statistical analysis plans can help to deter bias from p-hacking but is only effective if the prespecified approach is made available.
Study Design and Setting
For 100 randomized trials published in 2018 and indexed in {PubMed}, we evaluated how often a prespecified statistical analysis approach for the trial's primary outcome was publicly available. For each trial with an available prespecified analysis, we compared this with the trial publication to identify whether there were unexplained discrepancies.
Results
Only 12 of 100 trials (12\%) had a publicly available prespecified analysis approach for their primary outcome; this document was dated before recruitment began for only two trials. Of the 12 trials with an available prespecified analysis approach, 11 (92\%) had one or more unexplained discrepancies. Only 4 of 100 trials (4\%) stated that the statistician was blinded until the {SAP} was signed off, and only 10 of 100 (10\%) stated the statistician was blinded until the database was locked.
Conclusion
For most published trials, there is insufficient information available to determine whether the results may be subject to p-hacking. Where information was available, there were often unexplained discrepancies between the prespecified and final analysis methods.},
	pages = {29--34},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Kahan, Brennan C. and Ahmad, Tahania and Forbes, Gordon and Cro, Suzie},
	urldate = {2020-09-20},
	date = {2020-12-01},
	langid = {english},
	keywords = {Transparency, Bias, P-hacking, Prespecification, Protocol, Randomized trial, Statistical analysis plan},
	file = {Kahan_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kahan_etal_3.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/JH432SME/S0895435620301979.html:text/html},
}

@article{page_many_2013,
	title = {Many scenarios exist for selective inclusion and reporting of results in randomized trials and systematic reviews},
	volume = {66},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435612003460},
	doi = {10.1016/j.jclinepi.2012.10.010},
	abstract = {Objective
To collate and categorize the ways in which selective inclusion and reporting can occur in randomized controlled trials ({RCTs}) and systematic reviews.
Study Design and Setting
Searches of the Cochrane Methodology Register, {PubMed}, and {PsycInfo} were conducted in April 2011. Methodological reports describing empirically investigated or hypothetical examples of selective inclusion or reporting were eligible for inclusion. Examples were extracted from the reports by one author and categorized by three authors independently. Discrepancies in categorization were resolved via discussion.
Results
Two hundred ninety reports were included. The majority were empirical method studies (45.5\%) or commentaries (29.3\%). Eight categories (30 examples) of selective reporting in {RCTs}, eight categories (27 examples) of selective inclusion in systematic reviews, and eight categories (33 examples) of selective reporting in systematic reviews were collated. Broadly, these describe scenarios in which multiple outcomes or multiple data for the same outcome are available, yet only a subset is included or reported; outcome data are reported with inadequate detail; or outcome data are given different prominence through its placement across or within reports.
Conclusion
An extensive list of examples of selective inclusion and reporting was collated. Increasing trialists’ and systematic reviewers’ awareness of these examples may minimize their occurrence.},
	pages = {524--537},
	number = {5},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Page, Matthew J. and {McKenzie}, Joanne E. and Forbes, Andrew},
	urldate = {2020-09-20},
	date = {2013-05-01},
	langid = {english},
	keywords = {Bias, Outcome reporting bias, Randomized controlled trials, Reporting, Research methodology, Systematic review},
	file = {Page_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Page_etal_2013.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/JDIR9K3L/S0895435612003460.html:text/html},
}

@article{kahan_how_2020,
	title = {How to design a pre-specified statistical analysis approach to limit p-hacking in clinical trials: the Pre-{SPEC} framework},
	volume = {18},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-020-01706-7},
	doi = {10.1186/s12916-020-01706-7},
	shorttitle = {How to design a pre-specified statistical analysis approach to limit p-hacking in clinical trials},
	abstract = {Results from clinical trials can be susceptible to bias if investigators choose their analysis approach after seeing trial data, as this can allow them to perform multiple analyses and then choose the method that provides the most favourable result (commonly referred to as ‘p-hacking’). Pre-specification of the planned analysis approach is essential to help reduce such bias, as it ensures analytical methods are chosen in advance of seeing the trial data. For this reason, guidelines such as {SPIRIT} (Standard Protocol Items: Recommendations for Interventional Trials) and {ICH}-E9 (International Conference for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use) require the statistical methods for a trial’s primary outcome be pre-specified in the trial protocol. However, pre-specification is only effective if done in a way that does not allow p-hacking. For example, investigators may pre-specify a certain statistical method such as multiple imputation, but give little detail on how it will be implemented. Because there are many different ways to perform multiple imputation, this approach to pre-specification is ineffective, as it still allows investigators to analyse the data in different ways before deciding on a final approach. In this article, we describe a five-point framework (the Pre-{SPEC} framework) for designing a pre-specified analysis approach that does not allow p-hacking. This framework was designed based on the principles in the {SPIRIT} and {ICH}-E9 guidelines and is intended to be used in conjunction with these guidelines to help investigators design the statistical analysis strategy for the trial’s primary outcome in the trial protocol.},
	pages = {253},
	number = {1},
	journaltitle = {{BMC} Medicine},
	shortjournal = {{BMC} Medicine},
	author = {Kahan, Brennan C. and Forbes, Gordon and Cro, Suzie},
	urldate = {2020-09-20},
	date = {2020-09-07},
	file = {Kahan_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kahan_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/HWPPQEK3/s12916-020-01706-7.html:text/html},
}

@article{chan_discrepancies_2008,
	title = {Discrepancies in sample size calculations and data analyses reported in randomised trials: comparison of publications with protocols},
	volume = {337},
	rights = {© Chan et al 2008. This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/337/bmj.a2299},
	doi = {10.1136/bmj.a2299},
	shorttitle = {Discrepancies in sample size calculations and data analyses reported in randomised trials},
	abstract = {Objective To evaluate how often sample size calculations and methods of statistical analysis are pre-specified or changed in randomised trials.
Design Retrospective cohort study.
Data source Protocols and journal publications of published randomised parallel group trials initially approved in 1994-5 by the scientific-ethics committees for Copenhagen and Frederiksberg, Denmark (n=70).
Main outcome measure Proportion of protocols and publications that did not provide key information about sample size calculations and statistical methods; proportion of trials with discrepancies between information presented in the protocol and the publication.
Results Only 11/62 trials described existing sample size calculations fully and consistently in both the protocol and the publication. The method of handling protocol deviations was described in 37 protocols and 43 publications. The method of handling missing data was described in 16 protocols and 49 publications. 39/49 protocols and 42/43 publications reported the statistical test used to analyse primary outcome measures. Unacknowledged discrepancies between protocols and publications were found for sample size calculations (18/34 trials), methods of handling protocol deviations (19/43) and missing data (39/49), primary outcome analyses (25/42), subgroup analyses (25/25), and adjusted analyses (23/28). Interim analyses were described in 13 protocols but mentioned in only five corresponding publications.
Conclusion When reported in publications, sample size calculations and statistical methods were often explicitly discrepant with the protocol or not pre-specified. Such amendments were rarely acknowledged in the trial publication. The reliability of trial reports cannot be assessed without having access to the full protocols.},
	pages = {a2299},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Chan, An-Wen and Hróbjartsson, Asbjørn and Jørgensen, Karsten J. and Gøtzsche, Peter C. and Altman, Douglas G.},
	urldate = {2020-09-20},
	date = {2008-12-04},
	langid = {english},
	pmid = {19056791},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Chan_etal_2008.pdf:/Users/tom/pCloud Drive/Zotero_Library/Chan_etal_2008.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UVPVAYWA/bmj.html:text/html},
}

@article{williamson_outcome_2016,
	title = {Outcome selection bias in meta-analysis},
	url = {https://journals.sagepub.com/doi/10.1191/0962280205sm415oa},
	doi = {10.1191/0962280205sm415oa},
	shorttitle = {Outcome selection bias in meta-analysis},
	abstract = {Publication bias has been previously identified as a threat to the validity of a meta-analysis. Recently, new evidence has documented an additional threat to va...},
	journaltitle = {Statistical Methods in Medical Research},
	author = {Williamson, P. R. and Gamble, C. and Altman, D. G. and Hutton, J. L.},
	urldate = {2020-09-20},
	date = {2016-07-02},
	langid = {english},
	note = {Publisher: Sage {PublicationsSage} {CA}: Thousand Oaks, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/U4MTR7JN/0962280205sm415oa.html:text/html;Williamson_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Williamson_etal_2016.pdf:application/pdf},
}

@article{hutton_bias_2000,
	title = {Bias in meta-analysis due to outcome variable selection within studies},
	volume = {49},
	rights = {2000 Royal Statistical Society},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9876.00197},
	doi = {10.1111/1467-9876.00197},
	abstract = {Although bias in meta-analysis arising from selective publication has been studied, within-study selection has received little attention. Chronic diseases often have several possible outcome variables. Methods based on the size of the effect allow results from studies with different outcomes to be combined. However, the possibility of selective reporting of outcomes must be considered. The effect of selective reporting on estimates of the size of the effect and significance levels is presented, and sensitivity analyses are suggested. Substantial publication bias could arise from multiple testing of outcomes in a study, followed by selective reporting. Two meta-analyses, on anthelminth therapy and a treatment for incontinence, are reassessed allowing for within-study selection, as it is clear that more outcomes had been measured than were reported. The sensitivity analyses show that the robustness of the anthelminth results is dependent on what assumption one makes about the reporting strategy for the largest trial. The possible influence of correlation between within-child measurements was such that the conclusions could easily be reversed. The effect of a mild assumption on within-trial selection alone could alter general recommendations about the treatment for incontinence.},
	pages = {359--370},
	number = {3},
	journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Hutton, J. L. and Williamson, Paula R.},
	urldate = {2020-09-20},
	date = {2000},
	langid = {english},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9876.00197},
	keywords = {Bias, Estimated effect size, Meta-analysis, Sensitivity analysis, Within-study variable selection},
	file = {Hutton_Williamson_2000.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hutton_Williamson_2000.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/V7TIJCKU/1467-9876.html:text/html},
}

@article{hahn_investigation_2002,
	title = {Investigation of within-study selective reporting in clinical research: follow-up of applications submitted to a local research ethics committee},
	volume = {8},
	issn = {1365-2753},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1046/j.1365-2753.2002.00314.x},
	doi = {10.1046/j.1365-2753.2002.00314.x},
	shorttitle = {Investigation of within-study selective reporting in clinical research},
	abstract = {Rationale, aims and objectives Within-study selective reporting is widely believed to exist, although to date there have been no empirical studies to assess the extent of the problem in clinical research. The present study aimed to examine this process. Methods We undertook a pilot study, involving a single local research ethics committee ({LREC}), in which we compared the outcomes, analysis and sample size proposed in the original approved study protocol with the results presented in the subsequent study report. Results We received 41 (73\%) replies from lead researchers of 56 projects, which were a complete cohort of clinical research applications approved in a particular time period by the {LREC}. Fifteen of these projects, which were completed and published at the time of our study, were further investigated. Only six (40\%) stated which outcome variables were of primary interest and four (67\%) of these showed consistency in the reports. Eight (53\%) of the 15 studies mentioned an analysis plan. However, seven (88\%) of these eight studies did not follow their prescribed analysis plan: the analysis of outcome variables or associations between certain variables were found to be missing from the report. Conclusions Our pilot study has shown that within-study selective reporting may be examined qualitatively by comparing the study report with the study protocol. Our results suggest that it might well be substantial; however, the bias can only be broadly identified as protocols are not sufficiently precise.},
	pages = {353--359},
	number = {3},
	journaltitle = {Journal of Evaluation in Clinical Practice},
	author = {Hahn, S. and Williamson, P. R. and Hutton, J. L.},
	urldate = {2020-09-20},
	date = {2002},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1046/j.1365-2753.2002.00314.x},
	keywords = {publication bias, meta-analysis, research ethics committee, within-study selective reporting},
	file = {Hahn_etal_2002.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hahn_etal_2002.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7PSD5G5S/j.1365-2753.2002.00314.html:text/html},
}

@article{sim_clinical_2006,
	title = {Clinical trial registration: transparency is the watchword},
	volume = {367},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673606687084},
	doi = {10.1016/S0140-6736(06)68708-4},
	shorttitle = {Clinical trial registration},
	pages = {1631--1633},
	number = {9523},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Sim, Ida and Chan, An-Wen and Gülmezoglu, A Metin and Evans, Tim and Pang, Tikki},
	urldate = {2020-09-20},
	date = {2006-05},
	langid = {english},
	file = {Sim et al. - 2006 - Clinical trial registration transparency is the w.pdf:/Users/tom/Zotero/storage/F3C9D56W/Sim et al. - 2006 - Clinical trial registration transparency is the w.pdf:application/pdf},
}

@article{bauchner_challenges_2019,
	title = {The challenges of sharing data in an era of politicized science},
	volume = {322},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2756117},
	doi = {10.1001/jama.2019.19786},
	abstract = {The goal of making science more transparent—sharing data, posting results on trial registries, use of preprint servers, and open access publishing—may enhance scientific discovery and  improve individual and population health, but it also comes with substantial challenges in an era of politicized...},
	pages = {2290--2291},
	number = {23},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Bauchner, Howard and Fontanarosa, Phil B.},
	urldate = {2020-09-20},
	date = {2019-12-17},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Bauchner_Fontanarosa_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bauchner_Fontanarosa_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3YEKHNAZ/2756117.html:text/html},
}

@article{dickersin_registering_2003,
	title = {Registering Clinical Trials},
	volume = {290},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/196997},
	doi = {10.1001/jama.290.4.516},
	abstract = {That it is not possible to find information about all initiated clinical trials is of international concern. This is a particular worry because scientists tend to publish their positive findings more often than their negative findings (publication bias). A comprehensive register of initiated clinical trials, with each trial assigned a unique identifier, would inform reviewers, physicians, and others (eg, consumers) about which trials had been started and directly address the problem of publication bias. Patients and their clinicians could also know which trials are open for enrollment, thus speeding medical advances. Individuals who participate in clinical trials typically provide consent in the belief that they are contributing to medical knowledge. But if the knowledge gained is never reported, the trust between patients and investigators and that between patients and research ethics review boards are both damaged. Ethical issues are of particular concern if industry is gaining financially from public involvement in trials, but refusing to reciprocate by making information from industry-sponsored trials generally available. All stakeholders—investigators, research organizations and institutions, journal editors, lawmakers, consumers, and others—must act now, together and in their own domains, to ensure comprehensive registration of clinical trials.},
	pages = {516--523},
	number = {4},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Dickersin, Kay and Rennie, Drummond},
	urldate = {2020-09-20},
	date = {2003-07-23},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Dickersin_Rennie_2003.pdf:/Users/tom/pCloud Drive/Zotero_Library/Dickersin_Rennie_2003.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N24MFJKT/196997.html:text/html},
}

@article{chan_spirit_2013,
	title = {{SPIRIT} 2013 explanation and elaboration: guidance for protocols of clinical trials},
	volume = {346},
	rights = {© Chan et al 2013.                      This is an open-access article distributed under the terms of the Creative                         Commons Attribution Non-commercial License, which permits use, distribution,                         and reproduction in any medium, provided the original work is properly                         cited, the use is non commercial and is otherwise in compliance with the                         license. See: http://creativecommons.org/licenses/by-nc/2.0/ and http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/346/bmj.e7586},
	doi = {10.1136/bmj.e7586},
	shorttitle = {{SPIRIT} 2013 explanation and elaboration},
	abstract = {High quality protocols facilitate proper conduct, reporting, and external review of clinical trials. However, the completeness of trial protocols is often inadequate. To help improve the content and quality of protocols, an international group of stakeholders developed the {SPIRIT} 2013 Statement (Standard Protocol Items: Recommendations for Interventional Trials). The {SPIRIT} Statement provides guidance in the form of a checklist of recommended items to include in a clinical trial protocol.
This {SPIRIT} 2013 Explanation and Elaboration paper provides important information to promote full understanding of the checklist recommendations. For each checklist item, we provide a rationale and detailed description; a model example from an actual protocol; and relevant references supporting its importance. We strongly recommend that this explanatory paper be used in conjunction with the {SPIRIT} Statement. A website of resources is also available (www.spirit-statement.org).
The {SPIRIT} 2013 Explanation and Elaboration paper, together with the Statement, should help with the drafting of trial protocols. Complete documentation of key trial elements can facilitate transparency and protocol review for the benefit of all stakeholders.},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Chan, An-Wen and Tetzlaff, Jennifer M. and Gøtzsche, Peter C. and Altman, Douglas G. and Mann, Howard and Berlin, Jesse A. and Dickersin, Kay and Hróbjartsson, Asbjørn and Schulz, Kenneth F. and Parulekar, Wendy R. and Krleža-Jerić, Karmela and Laupacis, Andreas and Moher, David},
	urldate = {2020-09-20},
	date = {2013-01-09},
	langid = {english},
	pmid = {23303884},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Chan_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Chan_etal_2013.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8TQ5KZ2F/bmj.html:text/html},
}

@article{altman_declaration_2013,
	title = {Declaration of transparency for each research article},
	volume = {347},
	rights = {© Altman et al 2013. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See:  http://creativecommons.org/licenses/by-nc/3.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/347/bmj.f4796},
	doi = {10.1136/bmj.f4796},
	abstract = {{\textless}p{\textgreater}An antidote to inadequate reporting of research {\textless}/p{\textgreater}},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Altman, Douglas G. and Moher, David},
	urldate = {2020-09-20},
	date = {2013-08-07},
	langid = {english},
	pmid = {23924655},
	note = {Publisher: British Medical Journal Publishing Group
Section: Editorial},
	file = {Altman_Moher_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Altman_Moher_2013.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/HR2NSQSE/bmj.html:text/html},
}

@article{van_t_veer_pre-registration_2016,
	title = {Pre-registration in social psychology—A discussion and suggested template},
	volume = {67},
	issn = {0022-1031},
	url = {http://www.sciencedirect.com/science/article/pii/S0022103116301925},
	doi = {10.1016/j.jesp.2016.03.004},
	series = {Special Issue: Confirmatory},
	abstract = {Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied—reviewed and unreviewed pre-registration—and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.},
	pages = {2--12},
	journaltitle = {Journal of Experimental Social Psychology},
	shortjournal = {Journal of Experimental Social Psychology},
	author = {van 't Veer, Anna Elisabeth and Giner-Sorolla, Roger},
	urldate = {2020-09-20},
	date = {2016-11-01},
	langid = {english},
	keywords = {Pre-registration, Research methods, Reviewed pre-registration ({RPR}), Solid science, Unreviewed pre-registration ({UPR})},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/VFRHRA4I/S0022103116301925.html:text/html;van 't Veer_Giner-Sorolla_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/van 't Veer_Giner-Sorolla_2016.pdf:application/pdf},
}

@article{monogan_research_2015,
	title = {Research preregistration in political science: the case, counterarguments, and a response to critiques},
	volume = {48},
	issn = {1049-0965, 1537-5935},
	url = {https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/research-preregistration-in-political-science-the-case-counterarguments-and-a-response-to-critiques/E80124ED16BA47D0EA09F03D72B89EC7},
	doi = {10.1017/S1049096515000189},
	shorttitle = {Research preregistration in political science},
	abstract = {This article describes the current debate on the practice of preregistration in political science—that is, publicly releasing a research design before observing outcome data. The case in favor of preregistration maintains that it can restrain four potential causes of publication bias, clearly distinguish deductive and inductive studies, add transparency regarding a researcher’s motivation, and liberate researchers who may be pressured to find specific results. Concerns about preregistration maintain that it is less suitable for the study of historical data, could reduce data exploration, may not allow for contextual problems that emerge in field research, and may increase the difficulty of finding true positive results. This article makes the case that these concerns can be addressed in preregistered studies, and it offers advice to those who would like to pursue study registration in their own work.},
	pages = {425--429},
	number = {3},
	journaltitle = {{PS}: Political Science \& Politics},
	author = {Monogan, James E.},
	urldate = {2020-09-20},
	date = {2015-07},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Monogan_2015.pdf:/Users/tom/pCloud Drive/Zotero_Library/Monogan_2015.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AHAE7994/E80124ED16BA47D0EA09F03D72B89EC7.html:text/html},
}

@article{wood_progress_2009,
	title = {Progress and Deficiencies in the Registration of Clinical Trials},
	volume = {360},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMsr0806582},
	doi = {10.1056/NEJMsr0806582},
	abstract = {In September 2007, President George W. Bush signed into law a bill requiring the registration of clinical trials in phase 2 or beyond and the public reporting of the results of those trials. This article reviews the initial implementation of the law and considers its strengths and weaknesses.},
	pages = {824--830},
	number = {8},
	journaltitle = {New England Journal of Medicine},
	author = {Wood, Alastair J.J.},
	urldate = {2020-09-20},
	date = {2009-02-19},
	pmid = {19228628},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJMsr}0806582},
	file = {Snapshot:/Users/tom/Zotero/storage/6H23JCA4/NEJMsr0806582.html:text/html;Wood_2009.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wood_2009.pdf:application/pdf},
}

@article{zarin_clinicaltrialsgov_2011,
	title = {The {ClinicalTrials}.gov results database — update and key issues},
	volume = {364},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMsa1012065},
	doi = {10.1056/NEJMsa1012065},
	abstract = {This article reviews the history and current status of clinical trial registrations and results data posted on the National Library of Medicine's Web site {ClinicalTrials}.gov.},
	pages = {852--860},
	number = {9},
	journaltitle = {New England Journal of Medicine},
	author = {Zarin, Deborah A. and Tse, Tony and Williams, Rebecca J. and Califf, Robert M. and Ide, Nicholas C.},
	urldate = {2020-09-20},
	date = {2011-03-03},
	pmid = {21366476},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJMsa}1012065},
	file = {Snapshot:/Users/tom/Zotero/storage/K9TH9NND/NEJMsa1012065.html:text/html;Zarin_etal_2011.pdf:/Users/tom/pCloud Drive/Zotero_Library/Zarin_etal_2011.pdf:application/pdf},
}

@article{ellaway-barnard_association_2020,
	title = {The association between registration status and reported outcomes in physiotherapy randomised controlled trials},
	volume = {27},
	url = {https://www.magonlinelibrary.com/doi/abs/10.12968/ijtr.2019.0023},
	doi = {10.12968/ijtr.2019.0023},
	abstract = {Background/{AimsClinical} trial registration has been proposed as a method of mitigating selective reporting in scientific research. It remains unknown whether trial registration is associated with reported outcomes in physiotherapy trials. This study aimed to analyse the association between registration status and outcome (the rejection or acceptance of a primary null hypothesis) for physiotherapy randomised controlled trials.{MethodsAll} randomised controlled trials reporting a physiotherapy intervention in publications listed in {PubMed} between 1 January 2017 and 30 June 2017 were included. Trial registration was determined based on the reporting of a registration number in the primary article or by identifying trials through trial registries.{ResultsOf} the 291 trials analysed, 176 (60.5\%) were registered; 115 (39.5\%) were not. There was no significant association between trial registration and outcome on multivariate analyses (Odds Ratio 1.65; 95\% Confidence Interval (0.92–2.96); P=0.09). Only 22\% of trials were prospectively registered.{ConclusionsRegistration} status and trial outcome are not associated in randomised controlled trials of physiotherapy interventions. The rate of physiotherapy trial registration remains low.},
	pages = {1--15},
	number = {3},
	journaltitle = {International Journal of Therapy and Rehabilitation},
	author = {Ellaway-Barnard, Christopher and Killick, Hannah and Peryer, Guy and Cross, Jane L and Smith, Toby O},
	urldate = {2020-09-20},
	date = {2020-03-02},
	note = {Publisher: Mark Allen Group},
	file = {Snapshot:/Users/tom/Zotero/storage/KIALWWMX/ijtr.2019.html:text/html},
}

@article{won_trial_2019,
	title = {Trial registration as a safeguard against outcome reporting bias and spin? A case study of randomized controlled trials of acupuncture},
	volume = {14},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223305},
	doi = {10.1371/journal.pone.0223305},
	shorttitle = {Trial registration as a safeguard against outcome reporting bias and spin?},
	abstract = {Background and objective Trial registration is widely endorsed as it is considered not only to enhance transparency and quality of reporting but also to help safeguard against outcome reporting bias and probably spin, known as specific reporting that could distort the interpretation of results thus mislead readers. We planned to investigate the current registration status of recently published randomized controlled trials ({RCTs}) of acupuncture, outcome reporting bias in the prospectively registered trials, and the association between trial registration and presence of spin and methodological factors in acupuncture {RCTs}. Methods Acupuncture {RCTs} published in English in recent 5 years (January 2013 to December 2017) were searched in {PubMed}, Cochrane Central Register of Controlled Trials, and {EMBASE}. Trial registration records identified in the publications and trial registries were classified into prospectively registered, retrospectively registered, or unregistered. Primary outcomes were identified and the direction of the results was judged as statistically significant (positive) or statistically nonsignificant (negative). We compared registered and published primary outcomes to assess outcome reporting bias and assessed whether discrepancies favored statistically significant outcomes. Frequency and strategies of spin in published reports with statistically nonsignificant results for primary outcomes were then identified. We also analyzed whether the trial registration status was associated with spin and quality of methodological factors. Results Of the 322 included {RCTs}, 41.9\% (n = 135) were prospectively registered. Among 64 studies that were prospectively registered and specified primary outcomes, 25 trials had the discrepancies between the registered and published primary outcomes and 60\% of them (15 trials) favored the statistically significant findings. Among 169 studies that specified primary outcomes, trial registration status was not associated with the direction of results, i.e., statistically significant or not. Spin was identified in 56.4\% out of 78 studies with statistically nonsignificant primary outcomes and claiming efficacy with no consideration of statistically nonsignificant primary outcomes was the most common strategy for spin. Trial registration status was not statistically different between studies with and without spin. Conclusion While trial registration seemed to have improved over time, primary outcomes in registered records and publications were often inconsistent, tending to favor statistically significant findings and spin was common in studies with statistically nonsignificant primary outcomes. Journal editors and researchers in this field should be alerted to still prevalent reporting bias and spin.},
	pages = {e0223305},
	number = {10},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Won, Jiyoon and Kim, Seoyeon and Bae, Inhu and Lee, Hyangsook},
	urldate = {2020-09-20},
	date = {2019-10-03},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Randomized controlled trials, Acupuncture, Asia, Database searching, Drug therapy, Medical journals, Medical risk factors, Scientific publishing},
	file = {Snapshot:/Users/tom/Zotero/storage/LMEM8CIP/article.html:text/html;Won_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Won_etal_2019.pdf:application/pdf},
}

@article{papageorgiou_registered_2018,
	title = {Registered trials report less beneficial treatment effects than unregistered ones: a meta-epidemiological study in orthodontics},
	volume = {100},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435617311381},
	doi = {10.1016/j.jclinepi.2018.04.017},
	shorttitle = {Registered trials report less beneficial treatment effects than unregistered ones},
	abstract = {Objectives
Clinical trial registration is widely recommended because it allows tracking of trials that helps ensure full and unbiased reporting of their results. The aim of the present overview was to provide empirical evidence on bias associated with trial registration via a meta-epidemiological approach.
Study Design and Settings
Six databases were searched in September 2017 for randomized clinical trials and systematic reviews thereof assessing the effects of orthodontic clinical interventions. After duplicate study selection and data extraction, statistical analysis included a two-step meta-epidemiological approach within- and across-included meta-analyses with a Paule-Mandel random-effects model to calculate differences in standardized mean differences (Δ{SMD}) between registered and unregistered trials and their 95\% confidence intervals ({CI}), followed by subgroup and sensitivity analyses.
Results
A total of 16 meta-analyses with 83 trials and 4,988 patients collectively were finally included, which indicated that registered trials reported less beneficial treatment effects than unregistered trials (Δ{SMD} = −0.36; 95\% {CI} = −0.60, −0.12). Although some small-study effects were identified, sensitivity analyses according to precision and risk of bias indicated robustness.
Conclusion
Signs of bias from lack of trial protocol registration were found with nonregistered trials reporting more beneficial intervention effects than registered ones. Caution is warranted by the interpretation of nonregistered randomized trials or systematic reviews thereof.},
	pages = {44--52},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Papageorgiou, Spyridon N. and Xavier, Guilherme M. and Cobourne, Martyn T. and Eliades, Theodore},
	urldate = {2020-09-20},
	date = {2018-08-01},
	langid = {english},
	keywords = {Meta-analysis, Empirical bias, Meta-epidemiology, Protocol registration, Randomized clinical trials, Reporting bias},
	file = {Papageorgiou_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Papageorgiou_etal_2018.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/NM4QZWHC/S0895435617311381.html:text/html},
}

@article{odutayo_association_2017,
	title = {Association between trial registration and positive study findings: cross sectional study (Epidemiological Study of Randomized Trials—{ESORT})},
	volume = {356},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/content/356/bmj.j917},
	doi = {10.1136/bmj.j917},
	shorttitle = {Association between trial registration and positive study findings},
	abstract = {Objective To assess whether randomised controlled trials ({RCTs}) that were registered were less likely to report positive study findings compared with {RCTs} that were not registered and whether the association varied by funding source.
Design Cross sectional study.
Study sample All primary {RCTs} published in December 2012 and indexed in {PubMed} by November 2013. Trial registration was determined based on the report of a trial registration number in published {RCTs} or the identification of the trial in a search of trial registries. Trials were separated into prospectively and retrospectively registered studies.
Main outcome measure Association between trial registration and positive study findings.
Results 1122 eligible {RCTs} were identified, of which 593 (52.9\%) were registered and 529 (47.1\%) were not registered. Overall, registration was marginally associated with positive study findings (adjusted risk ratio 0.87, 95\% confidence interval 0.78 to 0.98), even with stratification as prospectively and retrospectively registered trials (0.87, 0.74 to 1.03 and 0.88, 0.78 to 1.00, respectively). The interaction term between overall registration and funding source was marginally statistically significant and relative risk estimates were imprecise (0.75, 0.63 to 0.89 for non-industry funded and 1.03, 0.79 to 1.36 for industry funded, P interaction=0.046). Furthermore, a statistically significant interaction was not maintained in sensitivity analyses. Within each stratum of funding source, relative risk estimates were also imprecise for the association between positive study findings and prospective and retrospective registration.
Conclusion Among published {RCTs}, there was little evidence of a difference in positive study findings between registered and non-registered clinical trials, even with stratification by timing of registration. Relative risk estimates were imprecise in subgroups of non-industry and industry funded trials.},
	pages = {j917},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Odutayo, Ayodele and Emdin, Connor A. and Hsiao, Allan J. and Shakir, Mubeen and Copsey, Bethan and Dutton, Susan and Chiocchia, Virginia and Schlussel, Michael and Dutton, Peter and Roberts, Corran and Altman, Douglas G. and Hopewell, Sally},
	urldate = {2020-09-20},
	date = {2017-03-14},
	langid = {english},
	pmid = {28292744},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Odutayo_etal_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Odutayo_etal_2017.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/CC7NW9YC/bmj.html:text/html},
}

@article{dechartres_association_2016,
	title = {Association between trial registration and treatment effect estimates: a meta-epidemiological study},
	volume = {14},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-016-0639-x},
	doi = {10.1186/s12916-016-0639-x},
	shorttitle = {Association between trial registration and treatment effect estimates},
	abstract = {To increase transparency in research, the International Committee of Medical Journal Editors required, in 2005, prospective registration of clinical trials as a condition to publication. However, many trials remain unregistered or retrospectively registered. We aimed to assess the association between trial prospective registration and treatment effect estimates.},
	pages = {1--9},
	number = {100},
	journaltitle = {{BMC} Medicine},
	shortjournal = {{BMC} Medicine},
	author = {Dechartres, Agnès and Ravaud, Philippe and Atal, Ignacio and Riveros, Carolina and Boutron, Isabelle},
	urldate = {2020-09-20},
	date = {2016-07-04},
	file = {Dechartres_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Dechartres_etal_2016.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/336CJW5S/s12916-016-0639-x.html:text/html},
}

@article{chan_empirical_2004,
	title = {Empirical evidence for selective reporting of outcomes in randomized trials: comparison of protocols to published articles},
	volume = {291},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/198809},
	doi = {10.1001/jama.291.20.2457},
	shorttitle = {Empirical evidence for selective reporting of outcomes in randomized trials},
	abstract = {{ContextSelective} reporting of outcomes within published studies based on the nature or direction of their results has been widely suspected, but direct evidence of such bias is currently limited to case reports.{ObjectiveTo} study empirically the extent and nature of outcome reporting bias in a cohort of randomized trials.{DesignCohort} study using protocols and published reports of randomized trials approved by the Scientific-Ethical Committees for Copenhagen and Frederiksberg, Denmark, in 1994-1995. The number and characteristics of reported and unreported trial outcomes were recorded from protocols, journal articles, and a survey of trialists. An outcome was considered incompletely reported if insufficient data were presented in the published articles for meta-analysis. Odds ratios relating the completeness of outcome reporting to statistical significance were calculated for each trial and then pooled to provide an overall estimate of bias. Protocols and published articles were also compared to identify discrepancies in primary outcomes.Main Outcome {MeasuresCompleteness} of reporting of efficacy and harm outcomes and of statistically significant vs nonsignificant outcomes; consistency between primary outcomes defined in the most recent protocols and those defined in published articles.{ResultsOne} hundred two trials with 122 published journal articles and 3736 outcomes were identified. Overall, 50\% of efficacy and 65\% of harm outcomes per trial were incompletely reported. Statistically significant outcomes had a higher odds of being fully reported compared with nonsignificant outcomes for both efficacy (pooled odds ratio, 2.4; 95\% confidence interval [{CI}], 1.4-4.0) and harm (pooled odds ratio, 4.7; 95\% {CI}, 1.8-12.0) data. In comparing published articles with protocols, 62\% of trials had at least 1 primary outcome that was changed, introduced, or omitted. Eighty-six percent of survey responders (42/49) denied the existence of unreported outcomes despite clear evidence to the contrary.{ConclusionsThe} reporting of trial outcomes is not only frequently incomplete but also biased and inconsistent with protocols. Published articles, as well as reviews that incorporate them, may therefore be unreliable and overestimate the benefits of an intervention. To ensure transparency, planned trials should be registered and protocols should be made publicly available prior to trial completion.},
	pages = {2457--2465},
	number = {20},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Chan, An-Wen and Hróbjartsson, Asbjørn and Haahr, Mette T. and Gøtzsche, Peter C. and Altman, Douglas G.},
	urldate = {2020-09-20},
	date = {2004-05-26},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Chan_etal_2004.pdf:/Users/tom/pCloud Drive/Zotero_Library/Chan_etal_2004.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/QLH9PZ5B/198809.html:text/html},
}

@article{chan_association_2017,
	title = {Association of trial registration with reporting of primary outcomes in protocols and publications},
	volume = {318},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2653434},
	doi = {10.1001/jama.2017.13001},
	abstract = {This study characterizes discrepancies between primary outcomes specified in trial protocols, registrations, and publications, and the association between prospective registration and consistency of published outcomes.},
	pages = {1709--1711},
	number = {17},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Chan, An-Wen and Pello, Annukka and Kitchen, Jessica and Axentiev, Anna and Virtanen, Jorma I. and Liu, Annie and Hemminki, Elina},
	urldate = {2020-09-20},
	date = {2017-11-07},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Chan_etal_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Chan_etal_2017.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8YQYNQGR/2653434.html:text/html},
}

@article{boccia_registration_2016,
	title = {Registration practices for observational studies on {ClinicalTrials}.gov indicated low adherence},
	volume = {70},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435615004321},
	doi = {10.1016/j.jclinepi.2015.09.009},
	abstract = {Objective
The study aims to assess the status of registration of observational studies.
Study Design and Setting
We identified studies on cancer research with prospective recruitment of participants that were registered from February 2000 to December 2011 in {ClinicalTrials}.gov. We recorded the dates of registration and start of recruitment, outcomes, and description of statistical method. We searched for publications corresponding to the registered studies through May 31, 2014.
Results
One thousand one hundred nine registered studies were eligible. Primary and secondary outcomes were reported in 809 (73.0\%) and 464 (41.8\%) of them. The date of registration preceded the month of the study start in 145 (13.8\%) and coincided in 205 (19.5\%). A total of 151 publications from 120 (10.8\%) registered studies were identified. In 2 (33.3\%) of the 6 publications where {ClinicalTrials}.gov reported that the study started recruitment after registration, and in 9 (50.0\%) of 18 publications where {ClinicalTrials}.gov reported the same date for registration and start of recruitment, the articles showed that the study had actually started recruiting before registration.
Conclusion
During the period reviewed, few observational studies have been registered. Registration usually occurred after the study started, and prespecification of outcomes and statistical analysis rarely occurred.},
	pages = {176--182},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Boccia, Stefania and Rothman, Kenneth J. and Panic, Nikola and Flacco, Maria Elena and Rosso, Annalisa and Pastorino, Roberta and Manzoli, Lamberto and La Vecchia, Carlo and Villari, Paolo and Boffetta, Paolo and Ricciardi, Walter and Ioannidis, John P. A.},
	urldate = {2020-09-20},
	date = {2016-02-01},
	langid = {english},
	keywords = {Outcome reporting bias, Protocol registration, Cancer protocol registration, {ClinicalTrials}.gov, Registration of observational studies, Registration study records},
	file = {Boccia_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Boccia_etal_2016.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/U47BM6V9/S0895435615004321.html:text/html},
}

@article{williams_registration_2010,
	title = {Registration of observational studies: Is it time?},
	volume = {182},
	rights = {© 2010},
	issn = {0820-3946, 1488-2329},
	url = {https://www.cmaj.ca/content/182/15/1638},
	doi = {10.1503/cmaj.092252},
	shorttitle = {Registration of observational studies},
	abstract = {Observational studies form an important part of the medical evidence base, particularly for assessing rare adverse events and long-term effectiveness of medications and devices. [1][1] However, observational studies, like interventional studies (clinical trials), are subject to publication bias and},
	pages = {1638--1642},
	number = {15},
	journaltitle = {{CMAJ}},
	author = {Williams, Rebecca J. and Tse, Tony and Harlan, William R. and Zarin, Deborah A.},
	urldate = {2020-09-20},
	date = {2010-10-19},
	langid = {english},
	pmid = {20643833},
	note = {Publisher: {CMAJ}
Section: Analysis},
	file = {Snapshot:/Users/tom/Zotero/storage/3MAUDBXJ/1638.html:text/html;Williams_etal_2010.pdf:/Users/tom/pCloud Drive/Zotero_Library/Williams_etal_2010.pdf:application/pdf},
}

@article{vandenbroucke_preregistration_2010,
	title = {Preregistration of Epidemiologic Studies: An Ill-founded Mix of Ideas},
	volume = {21},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/Fulltext/2010/09000/Preregistration_of_Epidemiologic_Studies__An.14.aspx},
	doi = {10.1097/EDE.0b013e3181e942b8},
	shorttitle = {Preregistration of Epidemiologic Studies},
	abstract = {An abstract is unavailable.},
	pages = {619--620},
	number = {5},
	journaltitle = {Epidemiology},
	author = {Vandenbroucke, Jan P.},
	urldate = {2020-09-20},
	date = {2010-09},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/YGEZ82RY/Preregistration_of_Epidemiologic_Studies__An.14.html:text/html;Vandenbroucke_2010.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vandenbroucke_2010.pdf:application/pdf},
}

@article{samet_register_2010,
	title = {To Register or Not To Register:},
	volume = {21},
	issn = {1044-3983},
	url = {http://journals.lww.com/00001648-201009000-00010},
	doi = {10.1097/EDE.0b013e3181e9be54},
	shorttitle = {To Register or Not To Register},
	pages = {610--611},
	number = {5},
	journaltitle = {Epidemiology},
	shortjournal = {Epidemiology},
	author = {Samet, Jonathan M.},
	urldate = {2020-09-20},
	date = {2010-09},
	langid = {english},
	file = {Samet_2010.pdf:/Users/tom/pCloud Drive/Zotero_Library/Samet_2010.pdf:application/pdf},
}

@article{swaen_strengthening_2011,
	title = {Strengthening the reliability and credibility of observational epidemiology studies by creating an Observational Studies Register},
	volume = {64},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435610001708},
	doi = {10.1016/j.jclinepi.2010.04.009},
	abstract = {Objective
To evaluate the need for the creation of a system in which observational epidemiology studies are registered; an Observational Studies Register ({OSR}).
Study Design and Setting
The current scientific process for observational epidemiology studies is described. Next, a parallel is made with the clinical trials area, where the creation of clinical trial registers has greatly restored and improved their credibility and reliability. Next, the advantages and disadvantages of an {OSR} are compared.
Results
The advantages of an {OSR} outweigh its disadvantages.
Conclusion
The creation of an {OSR}, similar to the existing Clinical Trials Registers, will improve the assessment of publication bias and will provide an opportunity to compare the original study protocol with the results reported in the publication. Reliability, credibility, and transparency of observational epidemiology studies are strengthened by the creation of an {OSR}. We propose a structured, collaborative, and coordinated approach for observational epidemiology studies that can provide solutions for existing weaknesses and will strengthen credibility and reliability, similar to the approach currently used in clinical trials, where Clinical Trials Registers have played a key role in strengthening their scientific value.},
	pages = {481--486},
	number = {5},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Swaen, Gerard M. H. and Carmichael, Neil and Doe, John},
	urldate = {2020-09-20},
	date = {2011-05-01},
	langid = {english},
	keywords = {Transparency, Epidemiology, Methods, Publication bias, Register, Risk factors},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/U8H7Y9Q5/S0895435610001708.html:text/html;Swaen_etal_2011.pdf:/Users/tom/pCloud Drive/Zotero_Library/Swaen_etal_2011.pdf:application/pdf},
}

@article{bracken_preregistration_2011,
	title = {Preregistration of Epidemiology Protocols: A Commentary in Support},
	volume = {22},
	issn = {1044-3983},
	url = {http://journals.lww.com/00001648-201103000-00001},
	doi = {10.1097/EDE.0b013e318207fc7c},
	shorttitle = {Preregistration of Epidemiology Protocols},
	pages = {135--137},
	number = {2},
	journaltitle = {Epidemiology},
	shortjournal = {Epidemiology},
	author = {Bracken, Michael B.},
	urldate = {2020-09-20},
	date = {2011-03},
	langid = {english},
	file = {Bracken_2011.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bracken_2011.pdf:application/pdf},
}

@article{savitz_registration_2011,
	title = {Registration of Observational Studies Does Not Enhance Validity},
	volume = {90},
	rights = {© 2011 American Society for Clinical Pharmacology and Therapeutics},
	issn = {1532-6535},
	url = {https://ascpt.onlinelibrary.wiley.com/doi/abs/10.1038/clpt.2011.199},
	doi = {10.1038/clpt.2011.199},
	abstract = {Mandatory registration of observational studies has been proposed to enhance quality of published research and reduce selective publication of positive findings. Enhanced communication of study plans would be welcome, but the alleged benefits to research quality are illusory. In particular, prespecification of hypotheses has no independent effect on data quality or the likelihood that hypotheses are correct. Registration of studies and hypotheses is likely to be misinterpreted as an independent determinant of validity. Clinical Pharmacology \& Therapeutics (2011); 90 5, 646–648. doi:10.1038/clpt.2011.199},
	pages = {646--648},
	number = {5},
	journaltitle = {Clinical Pharmacology \& Therapeutics},
	author = {Savitz, D. A.},
	urldate = {2020-09-20},
	date = {2011},
	langid = {english},
	note = {\_eprint: https://ascpt.onlinelibrary.wiley.com/doi/pdf/10.1038/clpt.2011.199},
	file = {Savitz_2011.pdf:/Users/tom/pCloud Drive/Zotero_Library/Savitz_2011.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TZRXMWF5/clpt.2011.html:text/html},
}

@article{ioannidis_importance_2012,
	title = {The importance of potential studies that have not existed and registration of observational data sets},
	volume = {308},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/1309181},
	doi = {10.1001/jama.2012.8144},
	abstract = {Knowing the complete results of all conducted studies on a question of interest is important to avoid publication and selective outcome reporting biases1 and to obtain a reliable picture of the evidence. However, in many types of research, the problem of selectively missing information does not...},
	pages = {575--576},
	number = {6},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Ioannidis, John P. A.},
	urldate = {2020-09-20},
	date = {2012-08-08},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Ioannidis_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ioannidis_2012.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SJ6TD4FP/1309181.html:text/html},
}

@article{jonge_prevention_2011,
	title = {Prevention of false positive findings in observational studies: registration will not work but replication might},
	volume = {65},
	rights = {© 2011, Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions.},
	issn = {0143-005X, 1470-2738},
	url = {https://jech.bmj.com/content/65/2/95},
	doi = {10.1136/jech.2010.125252},
	shorttitle = {Prevention of false positive findings in observational studies},
	abstract = {Progress in science is built on a balance between curiosity and scepticism, and between creativity and rigour.1 Although progress in science inevitably needs the generating of findings that may not be verified in subsequent studies, there is a considerable risk that exploratory studies introduce bias in the body of scientific knowledge. This risk may be substantial in observational studies, including cohort studies, case–control studies and cross-sectional studies.2 Observational studies play an essential role in medical research as they are often conducted to evaluate research questions that cannot be addressed by clinical trials.3 As shown by Ioannidis, however, the likelihood that any given finding from a published observational study is true in reality is limited.2 Aside from problems of uncontrolled confounding and other biases, observational studies are often generated from databases comprising many variables. Researchers often explore an unknown quantity of potential relationships capitalising on the chance of obtaining positive findings. In this editorial, we will explore two potential solutions to reduce the risk of false positive findings from observational studies: a registration requirement; and a replication requirement.

With a registration requirement, scientific journals would require researchers to register their observational studies in a manner similar to what has become policy for clinical trials.4 5 Study registration would include the recording of well grounded hypotheses that will be tested and data to be collected in a given study, recently …},
	pages = {95--96},
	number = {2},
	journaltitle = {Journal of Epidemiology \& Community Health},
	author = {Jonge, P. de and Conradi, H. J. and Thombs, B. D. and Rosmalen, J. G. M. and Burger, H. and Ormel, J.},
	urldate = {2020-09-20},
	date = {2011-02-01},
	langid = {english},
	pmid = {21113013},
	note = {Publisher: {BMJ} Publishing Group Ltd
Section: Editorial},
	keywords = {replication, bias, bias {ME}, methodology, observational {ME}, Observational studies, registration},
	file = {Jonge_etal_2011.pdf:/Users/tom/pCloud Drive/Zotero_Library/Jonge_etal_2011.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VQLHQ66N/95.html:text/html},
}

@article{de_angelis_clinical_2004,
	title = {Clinical trial registration: A statement from the International Committee of Medical Journal Editors},
	volume = {351},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMe048225},
	doi = {10.1056/NEJMe048225},
	shorttitle = {Clinical Trial Registration},
	abstract = {Altruism and trust lie at the heart of research on human subjects. Altruistic individuals volunteer for research because they trust that their participation will contribute to improved health for others and that researchers will minimize risks to participants. In return for the altruism and trust that make clinical research possible, the research enterprise has an obligation to conduct research ethically and to report it honestly. Honest reporting begins with revealing the existence of all clinical studies, even those that reflect unfavorably on a research sponsor's product. Unfortunately, selective reporting of trials does occur, and it distorts the body of evidence . . .},
	pages = {1250--1251},
	number = {12},
	journaltitle = {New England Journal of Medicine},
	author = {De Angelis, Catherine and Drazen, Jeffrey M. and Frizelle, Frank A. and Haug, Charlotte and Hoey, John and Horton, Richard and Kotzin, Sheldon and Laine, Christine and Marusic, Ana and Overbeke, A. John P.M. and Schroeder, Torben V. and Sox, Hal C. and Weyden, Martin B. Van Der},
	urldate = {2020-09-20},
	date = {2004-09-16},
	pmid = {15356289},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJMe}048225},
	file = {De Angelis_etal_2004.pdf:/Users/tom/pCloud Drive/Zotero_Library/De Angelis_etal_2004.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6IGSTW7N/NEJMe048225.html:text/html},
}

@article{mathieu_use_2013,
	title = {Use of trial register information during the peer review process},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0059910},
	doi = {10.1371/journal.pone.0059910},
	abstract = {Introduction Evidence in the medical literature suggests that trial registration may not be preventing selective reporting of results. We wondered about the place of such information in the peer-review process. Method We asked 1,503 corresponding authors of clinical trials and 1,733 reviewers to complete an online survey soliciting their views on the use of trial registry information during the peer-review process. Results 1,136 authors (n = 713) and reviewers (n = 423) responded (37.5\%); 676 (59.5\%) had reviewed an article reporting a clinical trial in the past 2 years. Among these, 232 (34.3\%) examined information registered on a trial registry. If one or more items (primary outcome, eligibility criteria, etc.) differed between the registry record and the manuscript, 206 (88.8\%) mentioned the discrepancy in their review comments, 46 (19.8\%) advised editors not to accept the manuscript, and 8 did nothing. The reviewers' reasons for not using the trial registry information included a lack of registration number in the manuscript (n = 132; 34.2\%), lack of time (n = 128; 33.2\%), lack of usefulness of registered information for peer review (n = 100; 25.9\%), lack of awareness about registries (n = 54; 14\%), and excessive complexity of the process (n = 39; 10.1\%). Conclusion This survey revealed that only one-third of the peer reviewers surveyed examined registered trial information and reported any discrepancies to journal editors.},
	pages = {e59910},
	number = {4},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Mathieu, Sylvain and Chan, An-Wen and Ravaud, Philippe},
	urldate = {2020-09-20},
	date = {2013-04-10},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Surveys, Internet, Randomized controlled trials, Medical journals, Clinical trial reporting, Clinical trials, Peer review, Research ethics},
	file = {Mathieu_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Mathieu_etal_2013.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TI4MXER5/article.html:text/html},
}

@article{humphreys_fishing_2013,
	title = {Fishing, commitment, and communication: a proposal for comprehensive nonbinding research registration},
	volume = {21},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/fishing-commitment-and-communication-a-proposal-for-comprehensive-nonbinding-research-registration/BD935F7843BF07F338774DAB66E74E3C},
	doi = {10.1093/pan/mps021},
	shorttitle = {Fishing, commitment, and communication},
	abstract = {Social scientists generally enjoy substantial latitude in selecting measures and models for hypothesis testing. Coupled with publication and related biases, this latitude raises the concern that researchers may intentionally or unintentionally select models that yield positive findings, leading to an unreliable body of published research. To combat this “fishing” problem in medical studies, leading journals now require preregistration of designs that emphasize the prior identification of dependent and independent variables. However, we demonstrate here that even with this level of advanced specification, the scope for fishing is considerable when there is latitude over selection of covariates, subgroups, and other elements of an analysis plan. These concerns could be addressed through the use of a form of comprehensive registration. We experiment with such an approach in the context of an ongoing field experiment for which we drafted a complete “mock report” of findings using fake data on treatment assignment. We describe the advantages and disadvantages of this form of registration and propose that a comprehensive but nonbinding approach be adopted as a first step to combat fishing by social scientists. Likely effects of comprehensive but nonbinding registration are discussed, the principal advantage being communication rather than commitment, in particular that it generates a clear distinction between exploratory analyses and genuine tests.},
	pages = {1--20},
	number = {1},
	journaltitle = {Political Analysis},
	author = {Humphreys, Macartan and Sierra, Raul Sanchez de la and Windt, Peter van der},
	urldate = {2020-09-20},
	date = {2013},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Humphreys_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Humphreys_etal_2013.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/X7LTFUVJ/BD935F7843BF07F338774DAB66E74E3C.html:text/html},
}

@article{simes_publication_1986,
	title = {Publication bias: the case for an international registry of clinical trials.},
	volume = {4},
	issn = {0732-183X},
	url = {https://ascopubs.org/doi/10.1200/JCO.1986.4.10.1529},
	doi = {10.1200/JCO.1986.4.10.1529},
	shorttitle = {Publication bias},
	abstract = {A problem in evaluating different therapies from a review of clinical trials is that the published clinical trial literature may be biased in favor of positive or promising results. In this report, a model is proposed for reviewing clinical trial results which is free from publication bias based on the selection of trials registered in advance in a registry. The value of a registry is illustrated by comparing a review of published clinical trials located by a literature search with a review of registered trials contained in a cancer trials registry. Two therapeutic questions are examined: the survival impact of initial alkylating agent ({AA}) v combination chemotherapy ({CC}) in advanced ovarian cancer, and the survival impact of {AA}/prednisone v {CC} in multiple myeloma. In advanced ovarian cancer, a pooled analysis of published clinical trials demonstrates a significant survival advantage for combination chemotherapy (median survival ratio of {CC} to {AA}, 1.16; P = .02). However, no significant difference in survival is demonstrated based on a pooled analysis of registered trials (median survival ratio, 1.05; P = .25). For multiple myeloma, a pooled analysis of published trials also demonstrates a significant survival advantage for {CC} (median survival ratio, 1.26; P = 04), especially for poor risk patients (ratio, 1.66; P = .002). A pooled analysis of registered trials also shows a survival benefit for patients receiving combination chemotherapy (all patients, P = .06; poor risk, P = .03), but the estimated magnitude of the benefit is reduced (all patients: ratio, 1.11; poor risk: ratio, 1.22). These examples illustrate an approach to reviewing the clinical trial literature, which is free from publication bias, and demonstrate the value and importance of an international registry of all clinical trials.},
	pages = {1529--1541},
	number = {10},
	journaltitle = {Journal of Clinical Oncology},
	shortjournal = {{JCO}},
	author = {Simes, R J},
	urldate = {2020-09-20},
	date = {1986-10-01},
	note = {Publisher: American Society of Clinical Oncology},
	file = {Simes_1986.pdf:/Users/tom/pCloud Drive/Zotero_Library/Simes_1986.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ICX2A85N/JCO.1986.4.10.html:text/html},
}

@article{weber_trial_2015,
	title = {Trial registration 10 years on},
	volume = {351},
	rights = {© {BMJ} Publishing Group Ltd 2015},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/351/bmj.h3572},
	doi = {10.1136/bmj.h3572},
	abstract = {{\textless}p{\textgreater}The single most valuable tool we have to ensure unbiased reporting of research studies{\textless}/p{\textgreater}},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Weber, Wim E. J. and Merino, José G. and Loder, Elizabeth},
	urldate = {2020-09-20},
	date = {2015-07-06},
	langid = {english},
	pmid = {26149708},
	note = {Publisher: British Medical Journal Publishing Group
Section: Editorial},
	file = {Snapshot:/Users/tom/Zotero/storage/MGTUGEZ5/bmj.html:text/html;Weber_etal_2015.pdf:/Users/tom/pCloud Drive/Zotero_Library/Weber_etal_2015.pdf:application/pdf},
}

@article{lash_preregistration_2010,
	title = {Preregistration of Study Protocols Is Unlikely to Improve the Yield From Our Science, But Other Strategies Might},
	volume = {21},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/Fulltext/2010/09000/Preregistration_of_Study_Protocols_Is_Unlikely_to.11.aspx},
	doi = {10.1097/EDE.0b013e3181e9bba6},
	abstract = {An abstract is unavailable.},
	pages = {612--613},
	number = {5},
	journaltitle = {Epidemiology},
	author = {Lash, Timothy L.},
	urldate = {2020-09-20},
	date = {2010-09},
	langid = {american},
	file = {Lash_2010.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lash_2010.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZIBSJLNE/Preregistration_of_Study_Protocols_Is_Unlikely_to.11.html:text/html},
}

@article{zarin_update_2017,
	title = {Update on trial registration 11 years after the {ICMJE} policy was established},
	volume = {376},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMsr1601330},
	doi = {10.1056/NEJMsr1601330},
	pages = {383--391},
	number = {4},
	journaltitle = {New England Journal of Medicine},
	shortjournal = {N Engl J Med},
	author = {Zarin, Deborah A. and Tse, Tony and Williams, Rebecca J. and Rajakannan, Thiyagu},
	urldate = {2020-09-20},
	date = {2017-01-26},
	langid = {english},
	file = {Zarin et al. - 2017 - Update on Trial Registration 11 Years after the IC.pdf:/Users/tom/Zotero/storage/2K5V8PEH/Zarin et al. - 2017 - Update on Trial Registration 11 Years after the IC.pdf:application/pdf},
}

@article{greenberg_pre-specification_2018,
	title = {Pre-specification of statistical analysis approaches in published clinical trial protocols was inadequate},
	volume = {101},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435618301616},
	doi = {10.1016/j.jclinepi.2018.05.023},
	abstract = {Objectives
Results from randomized trials can depend on the statistical analysis approach used. It is important to prespecify the analysis approach in the trial protocol to avoid selective reporting of analyses based on those which provide the most favourable results. We undertook a review of published trial protocols to assess how often the statistical analysis of the primary outcome was adequately prespecified.
Methods
We searched protocols of randomized trials indexed in {PubMed} in November 2016. We identified whether the following aspects of the statistical analysis approach for the primary outcome were adequately prespecified: (1) analysis population; (2) analysis model; (3) use of covariates; and (4) method of handling missing data.
Results
We identified 99 eligible protocols. Very few protocols adequately prespecified the analysis population (8/99, 8\%), analysis model (27/99, 27\%), covariates (40/99, 40\%), or approach to handling missing data (10/99, 10\%). Most protocols did not adequately predefine any of these four aspects of their statistical analysis approach (39\%) or predefined only one aspect (36\%). No protocols adequately predefined all four aspects of the analysis.
Conclusion
The statistical analysis approach is rarely prespecified in published trial protocols. This may allow selective reporting of results based on different analyses.},
	pages = {53--60},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Greenberg, Lauren and Jairath, Vipul and Pearse, Rupert and Kahan, Brennan C.},
	urldate = {2020-09-20},
	date = {2018-09-01},
	langid = {english},
	keywords = {Analysis switching, Clinical trial, Pre-specification, Randomized controlled trial, Statistical analysis},
	file = {Greenberg et al. - 2018 - Pre-specification of statistical analysis approach.pdf:/Users/tom/pCloud Drive/Zotero_Library/Greenberg et al. - 2018 - Pre-specification of statistical analysis approach.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/E4AUVSE5/S0895435618301616.html:text/html},
}

@article{gamble_guidelines_2017,
	title = {Guidelines for the content of statistical analysis plans in clinical trials},
	volume = {318},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2666509},
	doi = {10.1001/jama.2017.18556},
	abstract = {{\textless}h3{\textgreater}Importance{\textless}/h3{\textgreater}{\textless}p{\textgreater}While guidance on statistical principles for clinical trials exists, there is an absence of guidance covering the required content of statistical analysis plans ({SAPs}) to support transparency and reproducibility.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater}{\textless}p{\textgreater}To develop recommendations for a minimum set of items that should be addressed in {SAPs} for clinical trials, developed with input from statisticians, previous guideline authors, journal editors, regulators, and funders.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Design{\textless}/h3{\textgreater}{\textless}p{\textgreater}Funders and regulators (n = 39) of randomized trials were contacted and the literature was searched to identify existing guidance; a survey of current practice was conducted across the network of {UK} Clinical Research Collaboration–registered trial units (n = 46, 1 unit had 2 responders) and a Delphi survey (n = 73 invited participants) was conducted to establish consensus on {SAPs}. The Delphi survey was sent to statisticians in trial units who completed the survey of current practice (n = 46), {CONSORT} (Consolidated Standards of Reporting Trials) and {SPIRIT} (Standard Protocol Items: Recommendations for Interventional Trials) guideline authors (n = 16), pharmaceutical industry statisticians (n = 3), journal editors (n = 9), and regulators (n = 2) (3 participants were included in 2 groups each), culminating in a consensus meeting attended by experts (N = 12) with representatives from each group. The guidance subsequently underwent critical review by statisticians from the surveyed trial units and members of the expert panel of the consensus meeting (N = 51), followed by piloting of the guidance document in the {SAPs} of 5 trials.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}No existing guidance was identified. The registered trials unit survey (46 responses) highlighted diversity in current practice and confirmed support for developing guidance. The Delphi survey (54 of 73, 74\% participants completing both rounds) reached consensus on 42\% (n = 46) of 110 items. The expert panel (N = 12) agreed that 63 items should be included in the guidance, with an additional 17 items identified as important but may be referenced elsewhere. Following critical review and piloting, some overlapping items were combined, leaving 55 items.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions and Relevance{\textless}/h3{\textgreater}{\textless}p{\textgreater}Recommendations are provided for a minimum set of items that should be addressed and included in {SAPs} for clinical trials. Trial registration, protocols, and statistical analysis plans are critically important in ensuring appropriate reporting of clinical trials.{\textless}/p{\textgreater}},
	pages = {2337--2343},
	number = {23},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Gamble, Carrol and Krishan, Ashma and Stocken, Deborah and Lewis, Steff and Juszczak, Edmund and Doré, Caroline and Williamson, Paula R. and Altman, Douglas G. and Montgomery, Alan and Lim, Pilar and Berlin, Jesse and Senn, Stephen and Day, Simon and Barbachano, Yolanda and Loder, Elizabeth},
	urldate = {2020-09-20},
	date = {2017-12-19},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Gamble_etal_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Gamble_etal_2017.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RZLV4NHE/2666509.html:text/html},
}

@article{fanelli_meta-assessment_2017,
	title = {Meta-assessment of bias in science},
	volume = {114},
	rights = {©  . Freely available online through the {PNAS} open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/114/14/3714},
	doi = {10.1073/pnas.1618569114},
	abstract = {Numerous biases are believed to affect the scientific literature, but their actual prevalence across disciplines is unknown. To gain a comprehensive picture of the potential imprint of bias in science, we probed for the most commonly postulated bias-related patterns and risk factors, in a large random sample of meta-analyses taken from all disciplines. The magnitude of these biases varied widely across fields and was overall relatively small. However, we consistently observed a significant risk of small, early, and highly cited studies to overestimate effects and of studies not published in peer-reviewed journals to underestimate them. We also found at least partial confirmation of previous evidence suggesting that {US} studies and early studies might report more extreme effects, although these effects were smaller and more heterogeneously distributed across meta-analyses and disciplines. Authors publishing at high rates and receiving many citations were, overall, not at greater risk of bias. However, effect sizes were likely to be overestimated by early-career researchers, those working in small or long-distance collaborations, and those responsible for scientific misconduct, supporting hypotheses that connect bias to situational factors, lack of mutual control, and individual integrity. Some of these patterns and risk factors might have modestly increased in intensity over time, particularly in the social sciences. Our findings suggest that, besides one being routinely cautious that published small, highly-cited, and earlier studies may yield inflated results, the feasibility and costs of interventions to attenuate biases in the literature might need to be discussed on a discipline-specific and topic-specific basis.},
	pages = {3714--3719},
	number = {14},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Fanelli, Daniele and Costas, Rodrigo and Ioannidis, John P. A.},
	urldate = {2020-09-18},
	date = {2017-04-04},
	langid = {english},
	pmid = {28320937},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {meta-analysis, bias, integrity, meta-research, misconduct},
	file = {Fanelli_etal_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Fanelli_etal_2017.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZUYJNC8R/3714.html:text/html},
}

@article{ioannidis_outcome_2017,
	title = {Outcome reporting bias in clinical trials: why monitoring matters},
	volume = {356},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/content/356/bmj.j408},
	doi = {10.1136/bmj.j408},
	shorttitle = {Outcome reporting bias in clinical trials},
	abstract = {{\textless}p{\textgreater}\textbf{John Ioannidis and colleagues} explain how clinical trial outcomes are less fixed than we think and advocate transparency in documenting why they change{\textless}/p{\textgreater}},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Ioannidis, John {PA} and Caplan, Arthur L. and Dal-Ré, Rafael},
	urldate = {2020-09-18},
	date = {2017-02-14},
	langid = {english},
	pmid = {28196819},
	note = {Publisher: British Medical Journal Publishing Group
Section: Analysis},
	file = {Ioannidis_etal_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ioannidis_etal_2017.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RKBPZPP4/bmj.html:text/html},
}

@article{lash_commentary_2012,
	title = {Commentary: should preregistration of epidemiologic study protocols become compulsory? Reflections and a counterproposal},
	volume = {23},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/pages/articleviewer.aspx?year=2012&issue=03000&article=00002&type=Fulltext},
	doi = {10.1097/EDE.0b013e318245c05b},
	shorttitle = {Commentary},
	abstract = {An abstract is unavailable.},
	pages = {184--188},
	number = {2},
	journaltitle = {Epidemiology},
	author = {Lash, Timothy L. and Vandenbroucke, Jan P.},
	urldate = {2020-09-18},
	date = {2012-03},
	langid = {american},
	file = {Lash_Vandenbroucke_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lash_Vandenbroucke_2012.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/H8J7CX4F/articleviewer.html:text/html},
}

@article{dal-re_making_2014,
	title = {Making prospective registration of observational research a reality},
	volume = {6},
	rights = {Copyright © 2014, American Association for the Advancement of Science},
	issn = {1946-6234, 1946-6242},
	url = {https://stm.sciencemag.org/content/6/224/224cm1},
	doi = {10.1126/scitranslmed.3007513},
	abstract = {The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data.
Key information about human observational studies should be publicly available before the study is initiated.
Key information about human observational studies should be publicly available before the study is initiated.},
	pages = {1--4},
	number = {224},
	journaltitle = {Science Translational Medicine},
	author = {Dal-Ré, Rafael and Ioannidis, John P. and Bracken, Michael B. and Buffler, Patricia A. and Chan, An-Wen and Franco, Eduardo L. and Vecchia, Carlo La and Weiderpass, Elisabete},
	urldate = {2020-09-18},
	date = {2014-02-19},
	langid = {english},
	pmid = {24553383},
	note = {Publisher: American Association for the Advancement of Science
Section: Commentary},
	file = {Dal-Ré_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Dal-Ré_etal_2014.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/G9I69IKM/224cm1.html:text/html},
}

@article{dickersin_evolution_2012,
	title = {The evolution of trial registries and their use to assess the clinical trial enterprise},
	volume = {307},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/1150078},
	doi = {10.1001/jama.2012.4230},
	abstract = {The original purpose of registries of clinical trials was to reveal the existence of all trials, published or not, to investigators and systematic reviewers. Trials left unpublished because results were unfavorable to their sponsors, or simply because investigators never submitted them to journals...},
	pages = {1861--1864},
	number = {17},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Dickersin, Kay and Rennie, Drummond},
	urldate = {2020-09-18},
	date = {2012-05-02},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Dickersin_Rennie_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Dickersin_Rennie_2012.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ADPAX9MX/1150078.html:text/html},
}

@article{kimmelman_distinguishing_2014,
	title = {Distinguishing between exploratory and confirmatory preclinical research will improve translation},
	volume = {12},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001863},
	doi = {10.1371/journal.pbio.1001863},
	abstract = {Kimmelman and colleagues argue that the key to improving preclinical research lies in distinguishing between two different modes of research: exploratory vs. confirmatory.},
	pages = {e1001863},
	number = {5},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Kimmelman, Jonathan and Mogil, Jeffrey S. and Dirnagl, Ulrich},
	urldate = {2020-09-18},
	date = {2014-05-20},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Measurement, Science policy, Clinical trials, Animal models, Drug discovery, Drug research and development, Experimental design, Replication studies},
	file = {Kimmelman_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kimmelman_etal_2014.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/C2KSTX7U/article.html:text/html},
}

@article{lakens_value_2019,
	title = {The value of preregistration for psychological science: a conceptual analysis},
	volume = {62},
	url = {https://osf.io/jbh4w},
	doi = {10.31234/osf.io/jbh4w},
	shorttitle = {The value of preregistration for psychological science},
	abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.},
	pages = {272--280},
	number = {3},
	journaltitle = {Japanese Psychological Review},
	author = {Lakens, Daniel},
	urldate = {2020-09-18},
	date = {2019},
	langid = {english},
	file = {Lakens - 2019 - The Value of Preregistration for Psychological Sci.pdf:/Users/tom/Zotero/storage/EULG4MQJ/Lakens - 2019 - The Value of Preregistration for Psychological Sci.pdf:application/pdf},
}

@online{donkin_unpacking_2020,
	title = {Unpacking the Disagreement: Guest Post by Donkin and Szollosi},
	url = {https://www.bayesianspectacles.org/unpacking-the-disagreement-guest-post-by-donkin-and-szollosi/},
	shorttitle = {Unpacking the Disagreement},
	abstract = {This post is a response to the previous post A Breakdown of “Preregistration is Redundant, at Best”. We were delighted to see how interested people were in the short paper we wrote on p…},
	titleaddon = {Bayesian Spectacles},
	author = {Donkin, Chris and Szollosi, Aba},
	urldate = {2020-09-18},
	date = {2020-01-02},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/6CNX2YDI/unpacking-the-disagreement-guest-post-by-donkin-and-szollosi.html:text/html},
}

@online{wagenmakers_breakdown_2019,
	title = {A Breakdown of “Preregistration is Redundant, at Best”},
	url = {https://www.bayesianspectacles.org/a-breakdown-of-preregistration-is-redundant-at-best/},
	abstract = {In this sentence-by-sentence breakdown of the paper “Preregistration is Redundant, at Best”, I argue that preregistration is a pragmatic tool to combat biases that invalidate statistical inference.…},
	titleaddon = {Bayesian Spectacles},
	author = {Wagenmakers, E. J.},
	urldate = {2020-09-18},
	date = {2019-11-05},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/EZ5NVW4T/a-breakdown-of-preregistration-is-redundant-at-best.html:text/html},
}

@article{munafo_scientific_2014,
	title = {Scientific rigor and the art of motorcycle maintenance},
	volume = {32},
	rights = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.3004},
	doi = {10.1038/nbt.3004},
	abstract = {The reliability of scientific research is under scrutiny. A recently convened working group proposes cultural adjustments to incentivize better research practices.},
	pages = {871--873},
	number = {9},
	journaltitle = {Nature Biotechnology},
	author = {Munafò, Marcus and Noble, Simon and Browne, William J. and Brunner, Dani and Button, Katherine and Ferreira, Joaquim and Holmans, Peter and Langbehn, Douglas and Lewis, Glyn and Lindquist, Martin and Tilling, Kate and Wagenmakers, Eric-Jan and Blumenstein, Robi},
	urldate = {2020-09-18},
	date = {2014-09},
	langid = {english},
	note = {Number: 9
Publisher: Nature Publishing Group},
	file = {Munafò et al. - 2014 - Scientific rigor and the art of motorcycle mainten.pdf:/Users/tom/Zotero/storage/UI6Y8Z8E/Munafò et al. - 2014 - Scientific rigor and the art of motorcycle mainten.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8PDIULQW/nbt.html:text/html},
}

@article{dienes_how_2016,
	title = {How Bayes factors change scientific practice},
	volume = {72},
	issn = {0022-2496},
	url = {http://www.sciencedirect.com/science/article/pii/S0022249615000607},
	doi = {10.1016/j.jmp.2015.10.003},
	series = {Bayes Factors for Testing Hypotheses in Psychological Research: Practical Relevance and New Developments},
	abstract = {Bayes factors provide a symmetrical measure of evidence for one model versus another (e.g. H1 versus H0) in order to relate theory to data. These properties help solve some (but not all) of the problems underlying the credibility crisis in psychology. The symmetry of the measure of evidence means that there can be evidence for H0 just as much as for H1; or the Bayes factor may indicate insufficient evidence either way. P-values cannot make this three-way distinction. Thus, Bayes factors indicate when the data count against a theory (and when they count for nothing); and thus they indicate when replications actually support H0 or H1 (in ways that power cannot). There is every reason to publish evidence supporting the null as going against it, because the evidence can be measured to be just as strong either way (thus the published record can be more balanced). Bayes factors can be B-hacked but they mitigate the problem because a) they allow evidence in either direction so people will be less tempted to hack in just one direction; b) as a measure of evidence they are insensitive to the stopping rule; c) families of tests cannot be arbitrarily defined; and d) falsely implying a contrast is planned rather than post hoc becomes irrelevant (though the value of pre-registration is not mitigated).},
	pages = {78--89},
	journaltitle = {Journal of Mathematical Psychology},
	shortjournal = {Journal of Mathematical Psychology},
	author = {Dienes, Zoltan},
	urldate = {2020-09-18},
	date = {2016-06-01},
	langid = {english},
	keywords = {Bayes factor, Confidence interval, Multiple comparisons, Null hypothesis, Planned vs post hoc, Stopping rule},
	file = {Dienes_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Dienes_2016.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/I2TF5EEV/S0022249615000607.html:text/html},
}

@online{rooij_psychological_2019,
	title = {Psychological science needs theory development before preregistration},
	url = {https://featuredcontent.psychonomic.org/psychological-science-needs-theory-development-before-preregistration/},
	abstract = {"(…) a substantial proportion of research effort in experimental psychology isn't expended directly in the explanation business; it is expended in the busines},
	titleaddon = {Psychonomic Society Featured Content},
	author = {Rooij, Iris van},
	urldate = {2020-09-18},
	date = {2019-01-18},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/HXYW73BD/psychological-science-needs-theory-development-before-preregistration.html:text/html},
}

@article{navarro_between_2019,
	title = {Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection},
	volume = {2},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-018-0019-z},
	doi = {10.1007/s42113-018-0019-z},
	shorttitle = {Between the Devil and the Deep Blue Sea},
	abstract = {Discussions of model selection in the psychological literature typically frame the issues as a question of statistical inference, with the goal being to determine which model makes the best predictions about data. Within this setting, advocates of leave-one-out cross-validation and Bayes factors disagree on precisely which prediction problem model selection questions should aim to answer. In this comment, I discuss some of these issues from a scientific perspective. What goal does model selection serve when all models are known to be systematically wrong? How might “toy problems” tell a misleading story? How does the scientific goal of explanation align with (or differ from) traditional statistical concerns? I do not offer answers to these questions, but hope to highlight the reasons why psychological researchers cannot avoid asking them.},
	pages = {28--34},
	number = {1},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Navarro, Danielle J.},
	urldate = {2020-09-18},
	date = {2019-03-01},
	langid = {english},
	file = {Navarro_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Navarro_2019.pdf:application/pdf},
}

@article{heinl_rethinking_2020,
	title = {Rethinking the incentive system in science: animal study registries},
	volume = {21},
	issn = {1469-221X},
	url = {https://www.embopress.org/doi/full/10.15252/embr.201949709},
	doi = {10.15252/embr.201949709},
	shorttitle = {Rethinking the incentive system in science},
	abstract = {The Animal Study Registry offers scientists a range of benefits by preregistering their studies. Wider adoption could address the reproducibility problem in biomedical research and enhance animal welfare.},
	pages = {e49709},
	number = {1},
	journaltitle = {{EMBO} reports},
	shortjournal = {{EMBO} reports},
	author = {Heinl, Céline and Chmielewska, Justyna and Olevska, Anastasia and Grune, Barbara and Schönfelder, Gilbert and Bert, Bettina},
	urldate = {2020-09-18},
	date = {2020-01-07},
	note = {Publisher: John Wiley \& Sons, Ltd},
	file = {Heinl_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Heinl_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B5QFZTGY/embr.html:text/html},
}

@article{szollosi_is_2020,
	title = {Is preregistration worthwhile?},
	volume = {24},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661319302852},
	doi = {10.1016/j.tics.2019.11.009},
	pages = {94--95},
	number = {2},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and van Rooij, Iris and Van Zandt, Trisha and Donkin, Chris},
	urldate = {2020-09-18},
	date = {2020-02-01},
	langid = {english},
	keywords = {preregistration, inference, theory development},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/TFZRVX9A/S1364661319302852.html:text/html;Szollosi_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Szollosi_etal_2020.pdf:application/pdf},
}

@article{simmons_false-positive_2011,
	title = {False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant},
	volume = {22},
	rights = {© Association for Psychological Science 2011},
	url = {https://journals.sagepub.com/doi/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	shorttitle = {False-positive psychology},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ ...},
	pages = {1359--1366},
	number = {11},
	journaltitle = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	urldate = {2020-09-18},
	date = {2011-10-17},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Simmons_etal_2011.pdf:/Users/tom/pCloud Drive/Zotero_Library/Simmons_etal_2011.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KCDRWVH5/0956797611417632.html:text/html},
}

@article{de_groot_meaning_2014,
	title = {The meaning of “significance” for different types of research},
	volume = {148},
	issn = {0001-6918},
	url = {http://www.sciencedirect.com/science/article/pii/S0001691814000304},
	doi = {10.1016/j.actpsy.2014.02.001},
	abstract = {Adrianus Dingeman de Groot (1914–2006) was one of the most influential Dutch psychologists. He became famous for his work “Thought and Choice in Chess”, but his main contribution was methodological — De Groot co-founded the Department of Psychological Methods at the University of Amsterdam (together with R. F. van Naerssen), founded one of the leading testing and assessment companies ({CITO}), and wrote the monograph “Methodology” that centers on the empirical-scientific cycle: observation–induction–deduction–testing–evaluation. Here we translate one of De Groot's early articles, published in 1956 in the Dutch journal Nederlands Tijdschrift voor de Psychologie en Haar Grensgebieden. This article is more topical now than it was almost 60years ago. De Groot stresses the difference between exploratory and confirmatory (“hypothesis testing”) research and argues that statistical inference is only sensible for the latter: “One ‘is allowed’ to apply statistical tests in exploratory research, just as long as one realizes that they do not have evidential impact”. De Groot may have also been one of the first psychologists to argue explicitly for preregistration of experiments and the associated plan of statistical analysis. The appendix provides annotations that connect De Groot's arguments to the current-day debate on transparency and reproducibility in psychological science.},
	pages = {188--194},
	journaltitle = {Acta Psychologica},
	shortjournal = {Acta Psychologica},
	author = {de Groot, A. D.},
	translator = {Wagenmakers, Eric-Jan and Borsboom, Denny and Verhagen, Josine and Kievit, Rogier A. and Bakker, Marjan and Cramer, Angélique O. J. and Matzke, Dora and Mellenbergh, Don and van der Maas, Han L. J.},
	urldate = {2020-09-18},
	date = {2014-05-01},
	langid = {english},
	note = {original-date: 1956},
	keywords = {Confirmatory research, De Groot, Exploratory research, Inference and evidence},
	file = {de Groot_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/de Groot_2014.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/Q95G8ZZX/S0001691814000304.html:text/html},
}

@article{bakker_recommendations_2020,
	title = {Recommendations in pre-registrations and internal review board proposals promote formal power analyses but do not increase sample size},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079},
	doi = {10.1371/journal.pone.0236079},
	abstract = {In this preregistered study, we investigated whether the statistical power of a study is higher when researchers are asked to make a formal power analysis before collecting data. We compared the sample size descriptions from two sources: (i) a sample of pre-registrations created according to the guidelines for the Center for Open Science Preregistration Challenge ({PCRs}) and a sample of institutional review board ({IRB}) proposals from Tilburg School of Behavior and Social Sciences, which both include a recommendation to do a formal power analysis, and (ii) a sample of pre-registrations created according to the guidelines for Open Science Framework Standard Pre-Data Collection Registrations ({SPRs}) in which no guidance on sample size planning is given. We found that {PCRs} and {IRBs} (72\%) more often included sample size decisions based on power analyses than the {SPRs} (45\%). However, this did not result in larger planned sample sizes. The determined sample size of the {PCRs} and {IRB} proposals (Md = 90.50) was not higher than the determined sample size of the {SPRs} (Md = 126.00; W = 3389.5, p = 0.936). Typically, power analyses in the registrations were conducted with G*power, assuming a medium effect size, α = .05 and a power of .80. Only 20\% of the power analyses contained enough information to fully reproduce the results and only 62\% of these power analyses pertained to the main hypothesis test in the pre-registration. Therefore, we see ample room for improvements in the quality of the registrations and we offer several recommendations to do so.},
	pages = {e0236079},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Bakker, Marjan and Veldkamp, Coosje L. S. and Akker, Olmo R. van den and Assen, Marcel A. L. M. van and Crompvoets, Elise and Ong, How Hwee and Wicherts, Jelte M.},
	urldate = {2020-09-18},
	date = {2020-07-31},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Social sciences, Open science, Research ethics, Analysis of variance, Computer software, Linear regression analysis, Metaanalysis},
	file = {Bakker_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bakker_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GFLAFGJJ/article.html:text/html},
}

@article{ledgerwood_preregistration_2018,
	title = {The preregistration revolution needs to distinguish between predictions and analyses},
	volume = {115},
	rights = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/45/E10516},
	doi = {10.1073/pnas.1812592115},
	abstract = {Nosek et al. (1) recently joined others in advocating for “widespread adoption of preregistration” as a tool for advancing science. The language they use in making this important argument, however, creates unnecessary confusion: Like many others discussing these issues, they seem to conflate the goal of theory falsification with the goal of constraining type I error. This masks a crucial distinction between two types of preregistration: preregistering a theoretical, a priori, directional prediction (which serves to clarify how a hypothesis is constructed) and preregistering an analysis plan (which serves to clarify how evidence is produced).

Indeed, philosophers of science have identified elements of both how a hypothesis is constructed and how evidence is … 

[↵][1]1Email: aledgerwood\{at\}ucdavis.edu.

 [1]: \#xref-corresp-1-1},
	pages = {E10516--E10517},
	number = {45},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Ledgerwood, Alison},
	urldate = {2020-09-18},
	date = {2018-11-06},
	langid = {english},
	pmid = {30341225},
	note = {Publisher: National Academy of Sciences
Section: Letter},
	file = {Ledgerwood_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ledgerwood_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/H5SSJ938/E10516.html:text/html},
}

@article{rubin_does_2020,
	title = {Does preregistration improve the credibility of research ﬁndings?},
	volume = {16},
	doi = {10.20982/tqmp.16.4.p376},
	abstract = {Preregistration entails researchers registering their planned research hypotheses, methods, and analyses in a time-stamped document before they undertake their data collection and analyses. This document is then made available with the published research report to allow readers to identify discrepancies between what the researchers originally planned to do and what they actually ended up doing. This historical transparency is supposed to facilitate judgments about the credibility of the research ﬁndings. The present article provides a critical review of 17 of the reasons behind this argument. The article covers issues such as {HARKing}, multiple testing, p-hacking, forking paths, optional stopping, researchers’ biases, selective reporting, test severity, publication bias, and replication rates. It is concluded that preregistration’s historical transparency does not facilitate judgments about the credibility of research ﬁndings when researchers provide contemporary transparency in the form of (a) clear rationales for current hypotheses and analytical approaches, (b) public access to research data, materials, and code, and (c) demonstrations of the robustness of research conclusions to alternative interpretations and analytical approaches.},
	pages = {376--390},
	number = {4},
	journaltitle = {The Quantitative Methods for Psychology},
	author = {Rubin, Mark},
	date = {2020},
	langid = {english},
	file = {Rubin - 2020 - Does preregistration improve the credibility of re.pdf:/Users/tom/Zotero/storage/MMWXDAXK/Rubin - 2020 - Does preregistration improve the credibility of re.pdf:application/pdf},
}

@article{cruwell_robust_2019,
	title = {Robust standards in cognitive science},
	volume = {2},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-019-00049-8},
	doi = {10.1007/s42113-019-00049-8},
	abstract = {Recent discussions within the mathematical psychology community have focused on how Open Science practices may apply to cognitive modelling. Lee et al. (2019) sketched an initial approach for adapting Open Science practices that have been developed for experimental psychology research to the unique needs of cognitive modelling. While we welcome the general proposal of Lee et al. (2019), we believe a more fine-grained view is necessary to accommodate the adoption of Open Science practices in the diverse areas of cognitive modelling. Firstly, we suggest a categorization for the diverse types of cognitive modelling, which we argue will allow researchers to more clearly adapt Open Science practices to different types of cognitive modelling. Secondly, we consider the feasibility and usefulness of preregistration and lab notebooks for each of these categories and address potential objections to preregistration in cognitive modelling. Finally, we separate several cognitive modelling concepts that we believe Lee et al. (2019) conflated, which should allow for greater consistency and transparency in the modelling process. At a general level, we propose a framework that emphasizes local consistency in approaches while allowing for global diversity in modelling practices.},
	pages = {255--265},
	number = {3},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Crüwell, Sophia and Stefan, Angelika M. and Evans, Nathan J.},
	urldate = {2020-09-17},
	date = {2019-12-01},
	langid = {english},
	file = {Crüwell et al. - 2019 - Robust Standards in Cognitive Science.pdf:/Users/tom/Zotero/storage/DG4SRGIS/Crüwell et al. - 2019 - Robust Standards in Cognitive Science.pdf:application/pdf},
}

@article{aczel_quantifying_2018,
	title = {Quantifying support for the null hypothesis in psychology: an empirical investigation},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918773742},
	doi = {10.1177/2515245918773742},
	shorttitle = {Quantifying support for the null hypothesis in psychology},
	abstract = {In the traditional statistical framework, nonsignificant results leave researchers in a state of suspended disbelief. In this study, we examined, empirically, the treatment and evidential impact of nonsignificant results. Our specific goals were twofold: to explore how psychologists interpret and communicate nonsignificant results and to assess how much these results constitute evidence in favor of the null hypothesis. First, we examined all nonsignificant findings mentioned in the abstracts of the 2015 volumes of Psychonomic Bulletin \& Review, Journal of Experimental Psychology: General, and Psychological Science (N = 137). In 72\% of these cases, nonsignificant results were misinterpreted, in that the authors inferred that the effect was absent. Second, a Bayes factor reanalysis revealed that fewer than 5\% of the nonsignificant findings provided strong evidence (i.e., {BF}01 {\textgreater} 10) in favor of the null hypothesis over the alternative hypothesis. We recommend that researchers expand their statistical tool kit in order to correctly interpret nonsignificant results and to be able to evaluate the evidence for and against the null hypothesis.},
	pages = {357--366},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Aczel, Balazs and Palfi, Bence and Szollosi, Aba and Kovacs, Marton and Szaszi, Barnabas and Szecsi, Peter and Zrubka, Mark and Gronau, Quentin F. and van den Bergh, Don and Wagenmakers, Eric-Jan},
	urldate = {2020-09-07},
	date = {2018-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/ZL6KSY55/Aczel et al. - 2018 - Quantifying Support for the Null Hypothesis in Psy.pdf:application/pdf},
}

@article{elman_transparent_2018,
	title = {Transparent Social Inquiry: Implications for Political Science},
	volume = {21},
	issn = {1094-2939},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-polisci-091515-025429},
	doi = {10.1146/annurev-polisci-091515-025429},
	shorttitle = {Transparent Social Inquiry},
	abstract = {Political scientists use diverse methods to study important topics. The findings they reach and conclusions they draw can have significant social implications and are sometimes controversial. As a result, audiences can be skeptical about the rigor and relevance of the knowledge claims that political scientists produce. For these reasons, being a political scientist means facing myriad questions about how we know what we claim to know. Transparency can help political scientists address these questions. An emerging literature and set of practices suggest that sharing more data and providing more information about our analytic and interpretive choices can help others understand the rigor and relevance of our claims. At the same time, increasing transparency can be costly and has been contentious. This review describes opportunities created by, and difficulties posed by, attempts to increase transparency. We conclude that, despite the challenges, consensus about the value and practice of transparency is emerging within and across political science's diverse and dynamic research communities.},
	pages = {29--47},
	number = {1},
	journaltitle = {Annual Review of Political Science},
	shortjournal = {Annu. Rev. Polit. Sci.},
	author = {Elman, Colin and Kapiszewski, Diana and Lupia, Arthur},
	urldate = {2020-09-06},
	date = {2018-05-11},
	note = {Publisher: Annual Reviews},
	file = {Full Text PDF:/Users/tom/Zotero/storage/9N3N8KNN/Elman et al. - 2018 - Transparent Social Inquiry Implications for Polit.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/YA2NAT4T/annurev-polisci-091515-025429.html:text/html},
}

@article{kaplan_likelihood_2015,
	title = {Likelihood of null effects of large {NHLBI} clinical trials has increased over time},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382},
	doi = {10.1371/journal.pone.0132382},
	abstract = {Background We explore whether the number of null results in large National Heart Lung, and Blood Institute ({NHLBI}) funded trials has increased over time. Methods We identified all large {NHLBI} supported {RCTs} between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs {\textgreater}\$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality. Results 17 of 30 studies (57\%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8\%) trials published after 2000 (χ2=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings. Conclusions The number {NHLBI} trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in {RCTs}, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.},
	pages = {e0132382},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Kaplan, Robert M. and Irvin, Veronica L.},
	urldate = {2020-09-06},
	date = {2015-08-05},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Randomized controlled trials, Drug therapy, Cardiovascular diseases, Cardiovascular therapy, Coronary heart disease, Myocardial infarction, Sudden cardiac death, Women's health},
	file = {Kaplan and Irvin - 2015 - Likelihood of Null Effects of Large NHLBI Clinical.html:/Users/tom/Zotero/storage/J75ZBA39/Kaplan and Irvin - 2015 - Likelihood of Null Effects of Large NHLBI Clinical.html:text/html;Kaplan and Irvin - 2015 - Likelihood of Null Effects of Large NHLBI Clinical.pdf:/Users/tom/Zotero/storage/XTINYENE/Kaplan and Irvin - 2015 - Likelihood of Null Effects of Large NHLBI Clinical.pdf:application/pdf},
}

@article{piper_exact_2019,
	title = {Exact replication: Foundation of science or game of chance?},
	volume = {17},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000188},
	doi = {10.1371/journal.pbio.3000188},
	shorttitle = {Exact replication},
	abstract = {The need for replication of initial results has been rediscovered only recently in many fields of research. In preclinical biomedical research, it is common practice to conduct exact replications with the same sample sizes as those used in the initial experiments. Such replication attempts, however, have lower probability of replication than is generally appreciated. Indeed, in the common scenario of an effect just reaching statistical significance, the statistical power of the replication experiment assuming the same effect size is approximately 50\%—in essence, a coin toss. Accordingly, we use the provocative analogy of “replicating” a neuroprotective drug animal study with a coin flip to highlight the need for larger sample sizes in replication experiments. Additionally, we provide detailed background for the probability of obtaining a significant p value in a replication experiment and discuss the variability of p values as well as pitfalls of simple binary significance testing in both initial preclinical experiments and replication studies with small sample sizes. We conclude that power analysis for determining the sample size for a replication study is obligatory within the currently dominant hypothesis testing framework. Moreover, publications should include effect size point estimates and corresponding measures of precision, e.g., confidence intervals, to allow readers to assess the magnitude and direction of reported effects and to potentially combine the results of initial and replication study later through Bayesian or meta-analytic approaches.},
	pages = {e3000188},
	number = {4},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Piper, Sophie K. and Grittner, Ulrike and Rex, Andre and Riedel, Nico and Fischer, Felix and Nadon, Robert and Siegerink, Bob and Dirnagl, Ulrich},
	urldate = {2020-09-06},
	date = {2019-04-09},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Drug therapy, Replication studies, Metaanalysis, Animal studies, Forecasting, Head, Neuroprotectives, Scientists},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MQBW6B3S/Piper et al. - 2019 - Exact replication Foundation of science or game o.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KHQ9PXPU/article.html:text/html},
}

@article{kvarven_comparing_2020,
	title = {Comparing meta-analyses and preregistered multiple-laboratory replication projects},
	volume = {4},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-019-0787-z},
	doi = {10.1038/s41562-019-0787-z},
	abstract = {Many researchers rely on meta-analysis to summarize research evidence. However, there is a concern that publication bias and selective reporting may lead to biased meta-analytic effect sizes. We compare the results of meta-analyses to large-scale preregistered replications in psychology carried out at multiple laboratories. The multiple-laboratory replications provide precisely estimated effect sizes that do not suffer from publication bias or selective reporting. We searched the literature and identified 15 meta-analyses on the same topics as multiple-laboratory replications. We find that meta-analytic effect sizes are significantly different from replication effect sizes for 12 out of the 15 meta-replication pairs. These differences are systematic and, on average, meta-analytic effect sizes are almost three times as large as replication effect sizes. We also implement three methods of correcting meta-analysis for bias, but these methods do not substantively improve the meta-analytic results.},
	pages = {423--434},
	number = {4},
	journaltitle = {Nature Human Behaviour},
	author = {Kvarven, Amanda and Strømland, Eirik and Johannesson, Magnus},
	urldate = {2020-09-06},
	date = {2020-04},
	langid = {english},
	note = {Number: 4
Publisher: Nature Publishing Group},
	file = {Kvarven et al. - 2020 - Comparing meta-analyses and preregistered multiple.pdf:/Users/tom/Zotero/storage/CKCN7NDY/Kvarven et al. - 2020 - Comparing meta-analyses and preregistered multiple.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6UIJIVHU/s41562-019-0787-z.html:text/html},
}

@article{anderson_evaluation_2020,
	title = {Evaluation of indicators supporting reproducibility and transparency within cardiology literature},
	rights = {© Author(s) (or their employer(s)) 2020. No commercial re-use. See rights and permissions. Published by {BMJ}.},
	issn = {1355-6037, 1468-201X},
	url = {https://heart.bmj.com/content/early/2020/08/20/heartjnl-2020-316519},
	doi = {10.1136/heartjnl-2020-316519},
	abstract = {Objectives It has been suggested that biomedical research is facing a reproducibility issue, yet the extent of reproducible research within the cardiology literature remains unclear. Thus, our main objective was to assess the quality of research published in cardiology journals by assessing for the presence of eight indicators of reproducibility and transparency.
Methods Using a cross-sectional study design, we conducted an advanced search of the National Library of Medicine catalogue for publications in cardiology journals. We included publications published between 1 January 2014 and 31 December 2019. After the initial list of eligible cardiology publications was generated, we searched for full-text {PDF} versions using Open Access, Google Scholar and {PubMed}. Using a pilot-tested Google Form, a random sample of 532 publications were assessed for the presence of eight indicators of reproducibility and transparency.
Results A total of 232 eligible publications were included in our final analysis. The majority of publications (224/232, 96.6\%) did not provide access to complete and unmodified data sets, all 229/232 (98.7\%) failed to provide step-by-step analysis scripts and 228/232 (98.3\%) did not provide access to complete study protocols.
Conclusions The presentation of studies published in cardiology journals would make reproducing study outcomes challenging, at best. Solutions to increase the reproducibility and transparency of publications in cardiology journals is needed. Moving forward, addressing inadequate sharing of materials, raw data and key methodological details might help to better the landscape of reproducible research within the field.},
	journaltitle = {Heart},
	shortjournal = {Heart},
	author = {Anderson, J. Michael and Wright, Bryan and Rauh, Shelby and Tritz, Daniel and Horn, Jarryd and Parker, Ian and Bergeron, Daniel and Cook, Sharolyn and Vassar, Matt},
	urldate = {2020-09-03},
	date = {2020-08-21},
	langid = {english},
	pmid = {32826286},
	note = {Publisher: {BMJ} Publishing Group Ltd and British Cardiovascular Society
Section: Healthcare delivery, economics and global health},
	keywords = {research approaches, statistics and study design},
	file = {Anderson et al. - 2020 - Evaluation of indicators supporting reproducibilit.pdf:/Users/tom/Zotero/storage/RZCJWYIP/Anderson et al. - 2020 - Evaluation of indicators supporting reproducibilit.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5JVLQ8LA/heartjnl-2020-316519.html:text/html},
}

@book{gelman_data_2007,
	location = {Cambridge ; New York},
	title = {Data analysis using regression and multilevel/hierarchical models},
	isbn = {978-0-521-86706-1 978-0-521-68689-1},
	series = {Analytical methods for social research},
	pagetotal = {625},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer},
	date = {2007},
	note = {{OCLC}: ocm67375137},
	keywords = {Multilevel models (Statistics), Regression analysis},
	file = {Gelman and Hill - 2007 - Data analysis using regression and multilevelhier.pdf:/Users/tom/Zotero/storage/SLJI7Y4I/Gelman and Hill - 2007 - Data analysis using regression and multilevelhier.pdf:application/pdf},
}

@article{ly_bayesian_2018,
	title = {Bayesian reanalyses from summary statistics: a guide for academic consumers:},
	rights = {© The Author(s) 2018},
	url = {http://journals.sagepub.com/doi/10.1177/2515245918779348},
	doi = {10.1177/2515245918779348},
	shorttitle = {Bayesian reanalyses from summary statistics},
	abstract = {Across the social sciences, researchers have overwhelmingly used the classical statistical paradigm to draw conclusions from data, often focusing heavily on a s...},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Ly, Alexander and Raj, Akash and Etz, Alexander and Marsman, Maarten and Gronau, Quentin F. and Wagenmakers, Eric-Jan},
	urldate = {2020-09-02},
	date = {2018-08-13},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Ly_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ly_etal_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3WLNQ2RH/2515245918779348.html:text/html},
}

@article{keysers_using_2020,
	title = {Using Bayes factor hypothesis testing in neuroscience to establish evidence of absence},
	volume = {23},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-020-0660-4},
	doi = {10.1038/s41593-020-0660-4},
	pages = {788--799},
	number = {7},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Keysers, Christian and Gazzola, Valeria and Wagenmakers, Eric-Jan},
	urldate = {2020-09-02},
	date = {2020-07},
	langid = {english},
	file = {Keysers et al. - 2020 - Using Bayes factor hypothesis testing in neuroscie.pdf:/Users/tom/Zotero/storage/QJU5AU5A/Keysers et al. - 2020 - Using Bayes factor hypothesis testing in neuroscie.pdf:application/pdf},
}

@article{nelson_psychologys_2018,
	title = {Psychology's Renaissance},
	volume = {69},
	issn = {0066-4308, 1545-2085},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-122216-011836},
	doi = {10.1146/annurev-psych-122216-011836},
	abstract = {In 2010–2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive ﬁndings. This sparked a period of methodological reﬂection that we review here and call Psychology’s Renaissance. We begin by describing how psychologists’ concerns with publication bias shifted from worrying about ﬁle-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientiﬁc practices of experimental psychologists have improved dramatically.},
	pages = {511--534},
	number = {1},
	journaltitle = {Annual Review of Psychology},
	shortjournal = {Annu. Rev. Psychol.},
	author = {Nelson, Leif D. and Simmons, Joseph and Simonsohn, Uri},
	urldate = {2020-08-25},
	date = {2018-01-04},
	langid = {english},
	file = {Nelson et al. - 2018 - Psychology's Renaissance.pdf:/Users/tom/Zotero/storage/SPX5DA4W/Nelson et al. - 2018 - Psychology's Renaissance.pdf:application/pdf},
}

@article{fraser_questionable_2018,
	title = {Questionable research practices in ecology and evolution},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303},
	doi = {10.1371/journal.pone.0200303},
	abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices ({QRPs}), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known ({HARKing}). We also asked them to estimate the proportion of their colleagues that use each of these {QRPs}. Several of the {QRPs} were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start ({HARKing}). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of {QRPs} found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
	pages = {e0200303},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
	urldate = {2020-08-20},
	date = {2018-07-16},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Statistical data, Behavioral ecology, Community ecology, Evolutionary biology, Evolutionary ecology, Evolutionary rate, Publication ethics},
	file = {Full Text PDF:/Users/tom/Zotero/storage/QCMAKUYJ/Fraser et al. - 2018 - Questionable research practices in ecology and evo.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/CVC54CC8/article.html:text/html},
}

@article{agnoli_questionable_2017,
	title = {Questionable research practices among italian research psychologists},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0172792},
	doi = {10.1371/journal.pone.0172792},
	abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some {QRPs}. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than {US} researchers to consider a practice defensible. Participants’ estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
	pages = {e0172792},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
	urldate = {2020-08-20},
	date = {2017-03-15},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Behavior, Experimental psychology, Italian people, Psychologists, Psychometrics, Questionnaires, United States},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ASH358LP/Agnoli et al. - 2017 - Questionable research practices among italian rese.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SFGEZGY3/article.html:text/html},
}

@article{fanelli_opinion_2018,
	title = {Opinion: Is science really facing a reproducibility crisis, and do we need it to?},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708272114},
	doi = {10.1073/pnas.1708272114},
	shorttitle = {Opinion},
	abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
	pages = {2628--2631},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Fanelli, Daniele},
	urldate = {2020-08-19},
	date = {2018-03-13},
	langid = {english},
	file = {Fanelli - 2018 - Opinion Is science really facing a reproducibilit.pdf:/Users/tom/Zotero/storage/SYJ8SSUL/Fanelli - 2018 - Opinion Is science really facing a reproducibilit.pdf:application/pdf},
}

@article{fiedler_questionable_2016,
	title = {Questionable research practices revisited},
	volume = {7},
	issn = {1948-5506},
	url = {https://doi.org/10.1177/1948550615612150},
	doi = {10.1177/1948550615612150},
	abstract = {The current discussion of questionable research practices ({QRPs}) is meant to improve the quality of science. It is, however, important to conduct {QRP} studies with the same scrutiny as all research. We note problems with overestimates of {QRP} prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed {QRP} prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm ({QRP} is normal) that can counteract the injunctive norm to minimize {QRPs} and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
	pages = {45--52},
	number = {1},
	journaltitle = {Social Psychological and Personality Science},
	shortjournal = {Social Psychological and Personality Science},
	author = {Fiedler, Klaus and Schwarz, Norbert},
	urldate = {2020-08-19},
	date = {2016-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/DJY2FIAL/Fiedler and Schwarz - 2016 - Questionable Research Practices Revisited.pdf:application/pdf},
}

@article{ioannidis_increasing_2014,
	title = {Increasing value and reducing waste in research design, conduct, and analysis},
	volume = {383},
	issn = {0140-6736},
	url = {http://www.sciencedirect.com/science/article/pii/S0140673613622278},
	doi = {10.1016/S0140-6736(13)62227-8},
	abstract = {Correctable weaknesses in the design, conduct, and analysis of biomedical and public health research studies can produce misleading results and waste valuable resources. Small effects can be difficult to distinguish from bias introduced by study design and analyses. An absence of detailed written protocols and poor documentation of research is common. Information obtained might not be useful or important, and statistical precision or power is often too low or used in a misleading way. Insufficient consideration might be given to both previous and continuing studies. Arbitrary choice of analyses and an overemphasis on random extremes might affect the reported findings. Several problems relate to the research workforce, including failure to involve experienced statisticians and methodologists, failure to train clinical researchers and laboratory scientists in research methods and design, and the involvement of stakeholders with conflicts of interest. Inadequate emphasis is placed on recording of research decisions and on reproducibility of research. Finally, reward systems incentivise quantity more than quality, and novelty more than reliability. We propose potential solutions for these problems, including improvements in protocols and documentation, consideration of evidence from studies in progress, standardisation of research efforts, optimisation and training of an experienced and non-conflicted scientific workforce, and reconsideration of scientific reward systems.},
	pages = {166--175},
	number = {9912},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Ioannidis, John P A and Greenland, Sander and Hlatky, Mark A and Khoury, Muin J and Macleod, Malcolm R and Moher, David and Schulz, Kenneth F and Tibshirani, Robert},
	urldate = {2020-08-11},
	date = {2014-01-11},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/DLX5JGD7/Ioannidis et al. - 2014 - Increasing value and reducing waste in research de.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/KKPZYKQP/S0140673613622278.html:text/html},
}

@article{romero_scientific_2020,
	title = {Scientific self-correction: the Bayesian way},
	issn = {0039-7857, 1573-0964},
	url = {http://link.springer.com/10.1007/s11229-020-02697-x},
	doi = {10.1007/s11229-020-02697-x},
	shorttitle = {Scientific self-correction},
	abstract = {The enduring replication crisis in many scientific disciplines casts doubt on the ability of science to estimate effect sizes accurately, and in a wider sense, to self-correct its findings and to produce reliable knowledge. We investigate the merits of a particular countermeasure—replacing null hypothesis significance testing ({NHST}) with Bayesian inference—in the context of the meta-analytic aggregation of effect sizes. In particular, we elaborate on the advantages of this Bayesian reform proposal under conditions of publication bias and other methodological imperfections that are typical of experimental research in the behavioral sciences. Moving to Bayesian statistics would not solve the replication crisis single-handedly. However, the move would eliminate important sources of effect size overestimation for the conditions we study.},
	journaltitle = {Synthese},
	shortjournal = {Synthese},
	author = {Romero, Felipe and Sprenger, Jan},
	urldate = {2020-08-07},
	date = {2020-06-29},
	langid = {english},
	file = {Romero and Sprenger - 2020 - Scientific self-correction the Bayesian way.pdf:/Users/tom/Zotero/storage/BX9HS7TE/Romero and Sprenger - 2020 - Scientific self-correction the Bayesian way.pdf:application/pdf},
}

@article{romero_philosophy_2019,
	title = {Philosophy of science and the replicability crisis},
	volume = {14},
	rights = {© 2019 The Author. Philosophy Compass published by John Wiley \& Sons Ltd},
	issn = {1747-9991},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/phc3.12633},
	doi = {10.1111/phc3.12633},
	abstract = {Replicability is widely taken to ground the epistemic authority of science. However, in recent years, important published findings in the social, behavioral, and biomedical sciences have failed to replicate, suggesting that these fields are facing a “replicability crisis.” For philosophers, the crisis should not be taken as bad news but as an opportunity to do work on several fronts, including conceptual analysis, history and philosophy of science, research ethics, and social epistemology. This article introduces philosophers to these discussions. First, I discuss precedents and evidence for the crisis. Second, I discuss methodological, statistical, and social-structural factors that have contributed to the crisis. Third, I focus on the philosophical issues raised by the crisis. Finally, I discuss several proposals for solutions and highlight the gaps that philosophers could focus on.},
	pages = {e12633},
	number = {11},
	journaltitle = {Philosophy Compass},
	author = {Romero, Felipe},
	urldate = {2020-08-07},
	date = {2019},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/phc3.12633},
	file = {Romero_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Romero_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/EZ4PTCNL/phc3.html:text/html},
}

@article{kenny_unappreciated_2019,
	title = {The unappreciated heterogeneity of effect sizes: Implications for power, precision, planning of research, and replication.},
	volume = {24},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000209},
	doi = {10.1037/met0000209},
	shorttitle = {The unappreciated heterogeneity of effect sizes},
	abstract = {Repeated investigations of the same phenomenon typically yield effect sizes that vary more than one would expect from sampling error alone. Such variation is even found in exact replication studies, suggesting that it is not only because of identifiable moderators but also to subtler random variation across studies. Such heterogeneity of effect sizes is typically ignored, with unfortunate consequences. We consider its implications for power analyses, the precision of estimated effects, and the planning of original and replication research. With heterogeneity and an interest in generalizing to a population of studies, the usual power calculations and confidence intervals are likely misleading, and the preference for single definitive large-N studies is misguided. Researchers and methodologists need to recognize that effects are often heterogeneous and plan accordingly.},
	pages = {578--589},
	number = {5},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Kenny, David A. and Judd, Charles M.},
	urldate = {2020-08-04},
	date = {2019-10},
	langid = {english},
	file = {Kenny and Judd - 2019 - The unappreciated heterogeneity of effect sizes I.pdf:/Users/tom/Zotero/storage/RX3VCZZ9/Kenny and Judd - 2019 - The unappreciated heterogeneity of effect sizes I.pdf:application/pdf},
}

@article{kenny_enhancing_2019,
	title = {Enhancing validity in psychological research.},
	volume = {74},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000531},
	doi = {10.1037/amp0000531},
	abstract = {For psychological research to be applied, its conclusions must be true. Donald T. Campbell and colleagues developed four different ways to assess the validity of scientific research. In this article, I discuss work done by myself and my colleagues on enhancing these four different types of validity.},
	pages = {1018--1028},
	number = {9},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Kenny, David A.},
	urldate = {2020-08-04},
	date = {2019-12},
	langid = {english},
	file = {Kenny - 2019 - Enhancing validity in psychological research..pdf:/Users/tom/Zotero/storage/JJXM28RN/Kenny - 2019 - Enhancing validity in psychological research..pdf:application/pdf},
}

@article{palfi_why_2020,
	title = {Why Bayesian “Evidence for \textit{H} $_{\textrm{1}}$ ” in One Condition and Bayesian “Evidence for \textit{H} $_{\textrm{0}}$ ” in Another Condition Does Not Mean Good-Enough Bayesian Evidence for a Difference Between the Conditions},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/2515245920913019},
	doi = {10.1177/2515245920913019},
	abstract = {Psychologists are often interested in whether an independent variable has a different effect in condition A than in condition B. To test such a question, one needs to directly compare the effect of that variable in the two conditions (i.e., test the interaction). Yet many researchers tend to stop when they find a significant test in one condition and a nonsignificant test in the other condition, deeming this as sufficient evidence for a difference between the two conditions. In this Tutorial, we aim to raise awareness of this inferential mistake when Bayes factors are used with conventional cutoffs to draw conclusions. For instance, some researchers might falsely conclude that there must be good-enough evidence for the interaction if they find good-enough Bayesian evidence for the alternative hypothesis, H1, in condition A and good-enough Bayesian evidence for the null hypothesis, H0, in condition B. The case study we introduce highlights that ignoring the test of the interaction can lead to unjustified conclusions and demonstrates that the principle that any assertion about the existence of an interaction necessitates the direct comparison of the conditions is as true for Bayesian as it is for frequentist statistics. We provide an R script of the analyses of the case study and a Shiny app that can be used with a 2 × 2 design to develop intuitions on this issue, and we introduce a rule of thumb with which one can estimate the sample size one might need to have a well-powered design.},
	pages = {251524592091301},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Palfi, Bence and Dienes, Zoltan},
	urldate = {2020-07-28},
	date = {2020-07-07},
	langid = {english},
	file = {Palfi and Dienes - 2020 - Why Bayesian “Evidence for H 1 ”.pdf:/Users/tom/Zotero/storage/U5SKRB6Q/Palfi and Dienes - 2020 - Why Bayesian “Evidence for H 1 ”.pdf:application/pdf},
}

@article{schriger_content_2006,
	title = {The content of medical journal instructions for authors},
	volume = {48},
	issn = {0196-0644},
	url = {http://www.sciencedirect.com/science/article/pii/S0196064406004781},
	doi = {10.1016/j.annemergmed.2006.03.028},
	abstract = {Study objective
We describe the general and statistical content of the Instructions for Authors of major medical journals.
Methods
This article reports on 2 observational studies. In study 1, we investigated the online versions of Instructions for Authors of 166 journals from 33 specialties for the presence of content about 15 methodologic and statistical topics. In study 2, we categorized the general content of the online versions of the Instructions for Authors of 35 medical journals. Two abstractors independently assigned the content into 18 categories and counted the total number of words devoted to each category. Interrater reliability of the classification was assessed.
Results
Less than half of the 166 Instructions for Authors in study 1 provided any guidance on statistical methods, and the majority failed to cite accepted publication standards such as the International Committee of Medical Journal Editors Uniform Guidelines or {CONSORT}. Only 13\% of journals commented on the content and style of data tables and figures. The 35 Instructions for Authors in study 2 varied greatly in length (mean 3,308; median 2,283; range 885 to 18,927) and, with few exceptions, focused on formatting issues. Forty-three percent of Instructions offered no advice on scientific content, and only 5 journals devoted more than 10\% of their words to scientific content.
Conclusion
There is great heterogeneity among medical journal Instructions for Authors. Instructions provide little guidance about methodologic and statistical issues, and the advice provided is often contradictory among journals.},
	pages = {743--749.e4},
	number = {6},
	journaltitle = {Annals of Emergency Medicine},
	shortjournal = {Annals of Emergency Medicine},
	author = {Schriger, David L. and Arora, Sanjay and Altman, Douglas G.},
	urldate = {2020-07-15},
	date = {2006-12-01},
	langid = {english},
	file = {Schriger et al. - 2006 - The content of medical journal instructions for au.pdf:/Users/tom/Zotero/storage/XVNJ7WFE/Schriger et al. - 2006 - The content of medical journal instructions for au.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/R9FI3NNW/S0196064406004781.html:text/html},
}

@article{loftus_semantic_1978,
	title = {Semantic integration of verbal information into a visual memory},
	volume = {4},
	doi = {10.1037/0278-7393.4.1.19},
	pages = {19--31},
	number = {1},
	journaltitle = {Journal of Experimental Psychology: Human Learning and Memory},
	author = {Loftus, Elizabeth F and Burns, Helen J and Miller, David G},
	date = {1978},
	langid = {english},
	file = {Loftus et al. - Semantic Integration of Verbal Information into a .pdf:/Users/tom/Zotero/storage/RGZQ9WQJ/Loftus et al. - Semantic Integration of Verbal Information into a .pdf:application/pdf},
}

@article{bamford_poppers_1993,
	title = {Popper's Explications of Ad Hocness: Circularity, Empirical Content, and Scientific Practice},
	volume = {44},
	issn = {0007-0882},
	url = {https://academic.oup.com/bjps/article/44/2/335/1442862},
	doi = {10.1093/bjps/44.2.335},
	shorttitle = {Popper's Explications of Ad Hocness},
	abstract = {Abstract.  Karl Popper defines an ad hoc hypothesis as one that is introduced to immunize a theory from some (or all) refutation but which cannot be tested inde},
	pages = {335--355},
	number = {2},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {Br J Philos Sci},
	author = {Bamford, Greg},
	urldate = {2020-10-15},
	date = {1993-06-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Snapshot:/Users/tom/Zotero/storage/T2B9YU8U/1442862.html:text/html},
}

@article{tang_electroconvulsive_2020,
	title = {Electroconvulsive Therapy with a Memory Reactivation Intervention for Post-Traumatic Stress Disorder: A Randomized Controlled Trial},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	issn = {2021-0450},
	url = {https://www.medrxiv.org/content/10.1101/2020.10.10.20210450v1},
	doi = {10.1101/2020.10.10.20210450},
	shorttitle = {Electroconvulsive Therapy with a Memory Reactivation Intervention for Post-Traumatic Stress Disorder},
	abstract = {{\textless}p{\textgreater}Introduction Post-traumatic Stress Disorder ({PTSD}) often does not respond to available treatments. Memories are vulnerable to disruption during reconsolidation, and electroconvulsive therapy ({ECT}) has amnestic effects. We sought to exploit this phenomenon as a potential treatment for {PTSD} with a clinical trial of patients with {PTSD} receiving {ECT}. Methods Patients with severe depression with comorbid {PTSD} referred for {ECT} treatment were randomly assigned to reactivation of a traumatic or non-traumatic memory using script driven imagery prior to each {ECT} treatment. Primary outcomes were change in scores on the Modified {PTSD} Symptom Scale - Self Report ({MPSS}-{SR}) and the Clinician-Administered {PTSD} Scale for {DSM}-5 ({CAPS}-5). Assessments were completed by blinded raters. Secondary outcomes included a comparison of the change in heart rate ({HR}) while listening to the script. Results Twenty-eight participants were randomized, and 25 patients who completed a post-{ECT} assessment were included in the analysis. No significant group differences were found in the {MPSS}-{SR} or {CAPS}-5 scores from pre-{ECT} to post-{ECT} or 3-month follow-ups. However, both groups improved at post-{ECT} and 3-month follow up. Partial eta squared estimates of effect size showed large effect sizes for all outcomes (η2 \&gt; 0.13). Changes in {HR} were not significantly different between groups or over time. Conclusions In this {RCT}, {ECT} paired with pre-treatment traumatic memory reactivation was not more effective for treating {PTSD} symptoms than {ECT} alone. While our primary hypothesis was not supported, our data provides further support for the efficacy of {ECT} for improving symptoms of {PTSD} with comorbid depression.{\textless}/p{\textgreater}},
	pages = {2020.10.10.20210450},
	journaltitle = {{medRxiv}},
	author = {Tang, Victor Mark and Trought, Kathleen and Gicas, Kristina M. and Kozak, Mari and Josselyn, Sheena A. and Daskalakis, Zafiris J. and Blumberger, Daniel M. and Voineskos, Daphne and Knyahnytska, Yuliya and Chung, Yuan and Zhou, Young and Isserles, Moshe and Wong, Albert H. C.},
	urldate = {2020-10-15},
	date = {2020-10-13},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory Press},
	file = {Tang_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Tang_etal_2020.pdf:application/pdf},
}

@article{van_der_naald_publication_2020,
	title = {Publication rate in preclinical research: a plea for preregistration},
	volume = {4},
	issn = {2398-8703},
	url = {https://www.openscience.bmj.com/lookup/doi/10.1136/bmjos-2019-100051},
	doi = {10.1136/bmjos-2019-100051},
	shorttitle = {Publication rate in preclinical research},
	abstract = {Objectives  The ultimate goal of biomedical research is the development of new treatment options for patients. Animal models are used if questions cannot be addressed otherwise. Currently, it is widely believed that a large fraction of performed studies are never published, but there are no data that directly address this question.
Methods  We have tracked a selection of animal study protocols approved in the University Medical Center Utrecht in the Netherlands, to assess whether these have led to a publication with a follow-­up period of 7 years.
Results  We found that 60\% of all animal study protocols led to at least one publication (full text or abstract). A total of 5590 animals were used in these studies, of which 26\% was reported in the resulting publications.
Conclusions  The data presented here underline the need for preclinical preregistration, in view of the risk of reporting and publication bias in preclinical research. We plea that all animal study protocols should be prospectively registered on an online, accessible platform to increase transparency and data sharing. To facilitate this, we have developed a platform dedicated to animal study protocol registration: www.preclinicaltrials.eu.},
	pages = {e100051},
	number = {1},
	journaltitle = {{BMJ} Open Science},
	shortjournal = {{BMJ} Open Science},
	author = {van der Naald, Mira and Wenker, Steven and Doevendans, Pieter A and Wever, Kimberley E and Chamuleau, Steven A J},
	urldate = {2020-10-15},
	date = {2020-08},
	langid = {english},
	file = {van der Naald et al. - 2020 - Publication rate in preclinical research a plea f.pdf:/Users/tom/Zotero/storage/J6MIKM4X/van der Naald et al. - 2020 - Publication rate in preclinical research a plea f.pdf:application/pdf},
}

@article{van_doorn_jasp_2020,
	title = {The {JASP} guidelines for conducting and reporting a Bayesian analysis},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-020-01798-5},
	doi = {10.3758/s13423-020-01798-5},
	abstract = {Despite the increasing popularity of Bayesian inference in empirical research, few practical guidelines provide detailed recommendations for how to apply Bayesian procedures and interpret the results. Here we offer specific guidelines for four different stages of Bayesian statistical reasoning in a research setting: planning the analysis, executing the analysis, interpreting the results, and reporting the results. The guidelines for each stage are illustrated with a running example. Although the guidelines are geared towards analyses performed with the open-source statistical software {JASP}, most guidelines extend to Bayesian inference in general.},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {van Doorn, Johnny and van den Bergh, Don and Böhm, Udo and Dablander, Fabian and Derks, Koen and Draws, Tim and Etz, Alexander and Evans, Nathan J. and Gronau, Quentin F. and Haaf, Julia M. and Hinne, Max and Kucharský, Šimon and Ly, Alexander and Marsman, Maarten and Matzke, Dora and Gupta, Akash R. Komarlu Narendra and Sarafoglou, Alexandra and Stefan, Angelika and Voelkel, Jan G. and Wagenmakers, Eric-Jan},
	urldate = {2020-10-12},
	date = {2020-10-09},
	langid = {english},
	file = {van Doorn_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/van Doorn_etal_2020.pdf:application/pdf},
}

@article{bailar_practice_1995,
	title = {The practice of meta-analysis},
	volume = {48},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/089543569400149K},
	doi = {10.1016/0895-4356(94)00149-K},
	series = {The Potsdam International Consultation on Meta-Analysis},
	abstract = {Meta-analysis seems to have a potentially useful role in carefully selected situations where the primary literature is of good quality, heterogeneity in the response to treatment of the tested population is small and well-understood, interest centers on estimation of a specific, critical parameter of outcome, and the meta-analyst is deeply expert in the subject matter. Other uses can produce, and have produced, results that may be seriously misleading. Five short case studies are presented (diethylstilbestrol and outcome of pregnancy, chlorination of drinking water and cancer, cisplatin and cancer of the ovary, antibiotic therapy for otitis media with effusion, and beta-agonists and asthma).},
	pages = {149--157},
	number = {1},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Bailar, John C.},
	urldate = {2020-10-07},
	date = {1995-01-01},
	langid = {english},
	keywords = {Beta-agonists and asthma, Chlorination, Cisplatin, Diethylstilbestrol, Meta analysis, Otitis media with effusion, Reviewing},
	file = {Bailar_1995.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bailar_1995.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/TIVRCXK8/089543569400149K.html:text/html},
}

@article{easterbrook_publication_1991,
	title = {Publication bias in clinical research},
	volume = {337},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/014067369190201Y},
	doi = {10.1016/0140-6736(91)90201-Y},
	pages = {867--872},
	number = {8746},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Easterbrook, P.J and Gopalan, R and Berlin, J.A and Matthews, D.R},
	urldate = {2020-10-07},
	date = {1991-04},
	langid = {english},
	file = {Easterbrook et al. - 1991 - Publication bias in clinical research.pdf:/Users/tom/Zotero/storage/E352WRK7/Easterbrook et al. - 1991 - Publication bias in clinical research.pdf:application/pdf},
}

@article{strasak_use_2007,
	title = {The use of statistics in medical research: a comparison of \textit{the New England Journal of Medicine} and \textit{Nature Medicine}},
	volume = {61},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313007X170242},
	doi = {10.1198/000313007X170242},
	shorttitle = {The use of statistics in medical research},
	pages = {47--55},
	number = {1},
	journaltitle = {The American Statistician},
	shortjournal = {The American Statistician},
	author = {Strasak, Alexander M and Zaman, Qamruz and Marinell, Gerhard and Pfeiffer, Karl P and Ulmer, Hanno},
	urldate = {2020-10-07},
	date = {2007-02},
	langid = {english},
	file = {Strasak et al. - 2007 - The Use of Statistics in Medical Research A Compa.pdf:/Users/tom/Zotero/storage/ZLNM6CQV/Strasak et al. - 2007 - The Use of Statistics in Medical Research A Compa.pdf:application/pdf},
}

@article{armitage_repeated_1969,
	title = {Repeated significance tests on accumulating data},
	volume = {132},
	rights = {© 1969 The Authors},
	issn = {2397-2327},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2343787},
	doi = {10.2307/2343787},
	abstract = {If significance tests at a fixed level are repeated at stages during the accumulation of data the probability of obtaining a significant result when the null hypothesis is true rises above the nominal significance level. Numerical results are presented for repeated tests on cumulative series of binomial, normal and exponential observations.},
	pages = {235--244},
	number = {2},
	journaltitle = {Journal of the Royal Statistical Society: Series A (General)},
	author = {Armitage, P. and {McPherson}, C. K. and Rowe, B. C.},
	urldate = {2020-10-07},
	date = {1969},
	langid = {english},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2343787},
	file = {Armitage et al. - 1969 - Repeated Significance Tests on Accumulating Data.pdf:/Users/tom/Zotero/storage/FKF7GWQ3/Armitage et al. - 1969 - Repeated Significance Tests on Accumulating Data.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/F7NNYZ2A/2343787.html:text/html},
}

@report{wagenmakers_five_2019,
	title = {Five Bayesian Intuitions for the Stopping Rule Principle},
	url = {https://osf.io/5ntkd},
	abstract = {Is it statistically appropriate to monitor evidence for or against a hypothesis as the data accumulate, and stop whenever this evidence is deemed sufficiently compelling Researchers raised in the tradition of frequentist inference may intuit that such a practice will bias the results and may even lead to "sampling to a foregone conclusion". In contrast, the Bayesian formalism entails that the decision on whether or not to terminate data collection is irrelevant for the assessment of the strength of the evidence. Here we provide five Bayesian intuitions for why the rational updating of beliefs ought not to depend on the decision when to stop data collection, that is, for the Stopping Rule Principle.},
	institution = {{PsyArXiv}},
	type = {preprint},
	author = {Wagenmakers, Eric-Jan and Gronau, Quentin Frederik and Vandekerckhove, Joachim},
	urldate = {2020-10-07},
	date = {2019-03-07},
	doi = {10.31234/osf.io/5ntkd},
	file = {Wagenmakers et al. - 2019 - Five Bayesian Intuitions for the Stopping Rule Pri.pdf:/Users/tom/Zotero/storage/U69TPQBJ/Wagenmakers et al. - 2019 - Five Bayesian Intuitions for the Stopping Rule Pri.pdf:application/pdf},
}

@article{rouder_optional_2014,
	title = {Optional stopping: No problem for Bayesians},
	volume = {21},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-014-0595-4},
	doi = {10.3758/s13423-014-0595-4},
	shorttitle = {Optional stopping},
	abstract = {Optional stopping refers to the practice of peeking at data and then, based on the results, deciding whether or not to continue an experiment. In the context of ordinary significance-testing analysis, optional stopping is discouraged, because it necessarily leads to increased type I error rates over nominal values. This article addresses whether optional stopping is problematic for Bayesian inference with Bayes factors. Statisticians who developed Bayesian methods thought not, but this wisdom has been challenged by recent simulation results of Yu, Sprenger, Thomas, and Dougherty (2013) and Sanborn and Hills (2013). In this article, I show through simulation that the interpretation of Bayesian quantities does not depend on the stopping rule. Researchers using Bayesian methods may employ optional stopping in their own research and may provide Bayesian analysis of secondary data regardless of the employed stopping rule. I emphasize here the proper interpretation of Bayesian quantities as measures of subjective belief on theoretical positions, the difference between frequentist and Bayesian interpretations, and the difficulty of using frequentist intuition to conceptualize the Bayesian approach.},
	pages = {301--308},
	number = {2},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Rouder, Jeffrey N.},
	urldate = {2020-10-07},
	date = {2014-04},
	langid = {english},
	file = {Rouder - 2014 - Optional stopping No problem for Bayesians.pdf:/Users/tom/Zotero/storage/8U2RTTBZ/Rouder - 2014 - Optional stopping No problem for Bayesians.pdf:application/pdf},
}

@article{wagenmakers_practical_2007,
	title = {A practical solution to the pervasive problems ofp values},
	volume = {14},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/BF03194105},
	doi = {10.3758/BF03194105},
	abstract = {In the field of psychology, the practice ofp value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of thep value procedure. In particular,p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover,p values do not quantify statistical evidence. This article reviews thesep value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to thesep value problems is to adopt a model selection perspective and use the Bayesian information criterion ({BIC}) for statistical inference (Raftery, 1995). The {BIC} provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from {SPSS} output.},
	pages = {779--804},
	number = {5},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychonomic Bulletin \& Review},
	author = {Wagenmakers, Eric-Jan},
	urldate = {2020-10-07},
	date = {2007-10-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/ZEXKT3UQ/Wagenmakers - 2007 - A practical solution to the pervasive problems ofp.pdf:application/pdf},
}

@article{elahi_electroconvulsive_2020,
	title = {Electroconvulsive shock does not impair the reconsolidation of cued and contextual pavlovian threat memory},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1422-0067/21/19/7072},
	doi = {10.3390/ijms21197072},
	abstract = {Existing memories, when retrieved under certain circumstances, can undergo modification through the protein synthesis-dependent process of reconsolidation. Disruption of this process can lead to the weakening of a memory trace, an approach which is being examined as a potential treatment for disorders characterized by pathological memories, such as Post-Traumatic Stress Disorder. The success of this approach relies upon the ability to robustly attenuate reconsolidation; however, the available literature brings into question the reliability of the various drugs used to achieve such a blockade. The identification of a drug or intervention that can reliably disrupt reconsolidation without requiring intracranial access for administration would be extremely useful. Electroconvulsive shock ({ECS}) delivered after memory retrieval has been demonstrated in some studies to disrupt memory reconsolidation; however, there exists a paucity of literature characterizing its effects on Pavlovian fear memory. Considering this, we chose to examine {ECS} as an inexpensive and facile means to impair reconsolidation in rats. Here we show that electroconvulsive seizure induction, when administered after memory retrieval, (immediately, after 30 min, or after 1 h), does not impair the reconsolidation of cued or contextual Pavlovian fear memories. On the contrary, {ECS} administration immediately after extinction training may modestly impair the consolidation of fear extinction memory.},
	pages = {7072},
	number = {19},
	journaltitle = {International Journal of Molecular Sciences},
	author = {Elahi, Hajira and Hong, Veronica and Ploski, Jonathan E.},
	urldate = {2020-10-06},
	date = {2020-01},
	langid = {english},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {consolidation, {ECS}, {ECT}, electroconvulsive shock, fear conditioning, fear extinction, reconsolidation},
	file = {Elahi et al. - 2020 - Electroconvulsive Shock Does Not Impair the Recons.pdf:/Users/tom/Zotero/storage/IH5FWSWE/Elahi et al. - 2020 - Electroconvulsive Shock Does Not Impair the Recons.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/S2SB6AWC/Elahi et al. - 2020 - Electroconvulsive Shock Does Not Impair the Recons.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5EPQUHV6/7072.html:text/html},
}

@article{ioannidis_importance_2012-1,
	title = {The Importance of Potential Studies That Have Not Existed and Registration of Observational Data Sets},
	volume = {308},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2012.8144},
	doi = {10.1001/jama.2012.8144},
	pages = {575},
	number = {6},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Ioannidis, John P. A.},
	urldate = {2020-10-06},
	date = {2012-08-08},
	langid = {english},
	file = {Ioannidis - 2012 - The Importance of Potential Studies That Have Not .pdf:/Users/tom/Zotero/storage/RE4UYWJB/Ioannidis - 2012 - The Importance of Potential Studies That Have Not .pdf:application/pdf},
}

@article{mahmic-kaknjo_motivations_2020,
	title = {Motivations for performing scholarly prepublication peer review: A scoping review},
	volume = {0},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2020.1822170},
	doi = {10.1080/08989621.2020.1822170},
	shorttitle = {Motivations for performing scholarly prepublication peer review},
	abstract = {Prepublication peer review is a cornerstone of science. Overburdened reviewers invest millions of hours in this voluntary activity. In this scoping review, we aimed at identifying motivations for performing prepublication peer review of scholarly manuscripts. Original research studies investigating actual peer reviewers’ motivations were included. We excluded modeling studies, studies related to other types of peer review, guidelines, peer review processes in particular journals. Medline, {WoS}, and Scopus were searched in February 2016, with no language or time limitations, and the search was updated in July 2019. The search yielded 5,250 records, and 382 were chosen for full text analysis, out of which 10 were appropriate for synthesis. Reference snowballing identified one eligible study. Eleven studies were appropriate for synthesis: four qualitative, four mixed qualitative/quantitative, and three qualitative studies, published from 1998 to 2018, involving 6,667 respondents. Major internal incentive was “communal obligations and reciprocity.” Major external incentives were “career advancement,” “being recognized as an expert,” and “building relationships with journals and editors.” Major disincentive was the “lack of time.” Editors could incentivize peer review process by choosing highest quality articles, improving communication with peer reviewers, in order to make the process of peer review as short and efficient as possible. The gaps in research concern disincentives to review.},
	pages = {1--33},
	number = {0},
	journaltitle = {Accountability in Research},
	author = {Mahmić-Kaknjo, Mersiha and Utrobičić, Ana and Marušić, Ana},
	urldate = {2020-10-06},
	date = {2020-09-09},
	pmid = {32907396},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2020.1822170},
	file = {Mahmić-Kaknjo et al. - 2020 - Motivations for performing scholarly prepublicatio.pdf:/Users/tom/Zotero/storage/TBUZYZFU/Mahmić-Kaknjo et al. - 2020 - Motivations for performing scholarly prepublicatio.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9NST7NYD/08989621.2020.html:text/html},
}

@article{haven_preregistering_2019,
	title = {Preregistering qualitative research},
	volume = {26},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2019.1580147},
	doi = {10.1080/08989621.2019.1580147},
	abstract = {The threat to reproducibility and awareness of current rates of research misbehavior sparked initiatives to better academic science. One initiative is preregistration of quantitative research. We investigate whether the preregistration format could also be used to boost the credibility of qualitative research. A crucial distinction underlying preregistration is that between prediction and postdiction. In qualitative research, data are used to decide which way interpretation should move forward, using data to generate hypotheses and new research questions. Qualitative research is thus a real-life example of postdiction research. Some may object to the idea of preregistering qualitative studies because qualitative research generally does not test hypotheses, and because qualitative research design is typically flexible and subjective. We rebut these objections, arguing that making hypotheses explicit is just one feature of preregistration, that flexibility can be tracked using preregistration, and that preregistration would provide a check on subjectivity. We then contextualize preregistrations alongside another initiative to enhance credibility in qualitative research: the confirmability audit. Besides, preregistering qualitative studies is practically useful to combating dissemination bias and could incentivize qualitative researchers to report constantly on their study's development. We conclude with suggested modifications to the Open Science Framework preregistration form to tailor it for qualitative studies.},
	pages = {229--244},
	number = {3},
	journaltitle = {Accountability in Research},
	author = {Haven, Tamarinde L. and Grootel, Dr Leonie Van},
	urldate = {2020-10-06},
	date = {2019-04-03},
	pmid = {30741570},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2019.1580147},
	keywords = {transparency, Preregistration, qualitative research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/GYKNUUAK/Haven and Grootel - 2019 - Preregistering qualitative research.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9KLKKC42/08989621.2019.html:text/html},
}

@article{weston_recommendations_2019,
	title = {Recommendations for Increasing the Transparency of Analysis of Preexisting Data Sets:},
	rights = {© The Author(s) 2019},
	url = {https://journals.sagepub.com/doi/10.1177/2515245919848684},
	doi = {10.1177/2515245919848684},
	shorttitle = {Recommendations for Increasing the Transparency of Analysis of Preexisting Data Sets},
	abstract = {Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more tru...},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Weston, Sara J. and Ritchie, Stuart J. and Rohrer, Julia M. and Przybylski, Andrew K.},
	urldate = {2020-10-06},
	date = {2019-06-11},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/V3SPDQMD/2515245919848684.html:text/html;Weston et al. - 2019 - Recommendations for Increasing the Transparency of.pdf:/Users/tom/Zotero/storage/A9XBF9IE/Weston et al. - 2019 - Recommendations for Increasing the Transparency of.pdf:application/pdf},
}

@article{mathison_why_1988,
	title = {Why Triangulate?},
	volume = {17},
	issn = {0013-189X},
	url = {https://doi.org/10.3102/0013189X017002013},
	doi = {10.3102/0013189X017002013},
	abstract = {This article discusses triangulation as a strategy for increasing the validity of evaluation and research findings. Typically, through triangulating we expect various data sources and methods to lead to a singular proposition about the phenomenon being studied. That this is not the case is obvious to most researchers and evaluators. Given that this expectation is unrealistic, an alternative perspective of triangulation is presented. This alternative perspective takes into account that triangulation results in convergent, inconsistent, and contradictory evidence that must be rendered sensible by the researcher or evaluator.},
	pages = {13--17},
	number = {2},
	journaltitle = {Educational Researcher},
	shortjournal = {Educational Researcher},
	author = {Mathison, Sandra},
	urldate = {2020-10-05},
	date = {1988-03-01},
	langid = {english},
	note = {Publisher: American Educational Research Association},
}

@article{lawlor_triangulation_2016,
	title = {Triangulation in aetiological epidemiology},
	volume = {45},
	issn = {0300-5771},
	url = {https://academic.oup.com/ije/article/45/6/1866/2930550},
	doi = {10.1093/ije/dyw314},
	abstract = {Abstract.  Triangulation is the practice of obtaining more reliable answers to research questions through integrating results from several different approaches,},
	pages = {1866--1886},
	number = {6},
	journaltitle = {International Journal of Epidemiology},
	shortjournal = {Int J Epidemiol},
	author = {Lawlor, Debbie A. and Tilling, Kate and Davey Smith, George},
	urldate = {2020-10-05},
	date = {2016-12-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XLP4I3QW/Lawlor et al. - 2016 - Triangulation in aetiological epidemiology.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VY6FDFPP/2930550.html:text/html},
}

@article{phillips_publication_2004,
	title = {Publication bias in situ},
	volume = {4},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-4-20},
	doi = {10.1186/1471-2288-4-20},
	abstract = {Publication bias, as typically defined, refers to the decreased likelihood of studies' results being published when they are near the null, not statistically significant, or otherwise "less interesting." But choices about how to analyze the data and which results to report create a publication bias within the published results, a bias I label "publication bias in situ" ({PBIS}).},
	pages = {20},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Phillips, Carl V.},
	urldate = {2020-10-05},
	date = {2004-08-05},
	file = {Full Text:/Users/tom/Zotero/storage/RDXUGFDL/Phillips - 2004 - Publication bias in situ.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/MFYCHLND/1471-2288-4-20.html:text/html},
}

@article{glasziou_reducing_2014,
	title = {Reducing waste from incomplete or unusable reports of biomedical research},
	volume = {383},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014067361362228X},
	doi = {10.1016/S0140-6736(13)62228-X},
	pages = {267--276},
	number = {9913},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Glasziou, Paul and Altman, Douglas G and Bossuyt, Patrick and Boutron, Isabelle and Clarke, Mike and Julious, Steven and Michie, Susan and Moher, David and Wager, Elizabeth},
	urldate = {2020-10-05},
	date = {2014-01},
	langid = {english},
	file = {Glasziou et al. - 2014 - Reducing waste from incomplete or unusable reports.pdf:/Users/tom/Zotero/storage/2NTTU89S/Glasziou et al. - 2014 - Reducing waste from incomplete or unusable reports.pdf:application/pdf},
}

@article{noauthor_entering_nodate,
	title = {Entering New Fields: Exploratory Uses of Experimentation on {JSTOR}},
	url = {https://www.jstor.org/stable/188390?seq=1#metadata_info_tab_contents},
	urldate = {2020-10-05},
	file = {Entering New Fields Exploratory Uses of Experimen.pdf:/Users/tom/Zotero/storage/HTWA77DD/Entering New Fields Exploratory Uses of Experimen.pdf:application/pdf;Entering New Fields\: Exploratory Uses of Experimentation on JSTOR:/Users/tom/Zotero/storage/WHR7SL4T/188390.html:text/html},
}

@article{busse_boundary_2016,
	title = {Boundary conditions: what they are, how to explore them, why we need them, and when to consider them},
	rights = {© The Author(s) 2016},
	url = {https://journals.sagepub.com/doi/10.1177/1094428116641191},
	doi = {10.1177/1094428116641191},
	shorttitle = {Boundary conditions},
	abstract = {Boundary conditions ({BC}) have long been discussed as an important element in theory development, referring to the “who, where, when” aspects of a theory. Howeve...},
	journaltitle = {Organizational Research Methods},
	author = {Busse, Christian and Kach, Andrew P. and Wagner, Stephan M.},
	urldate = {2020-10-05},
	date = {2016-04-14},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/35WBBUY7/1094428116641191.html:text/html},
}

@article{scheel_why_2020,
	title = {Why hypothesis testers should spend less time testing hypotheses},
	doi = {https://doi.org/10.1177/1745691620966795},
	abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable (Meehl, 1990). In response to the replication crisis, reforms in psychology have focused on formalising procedures for testing hypotheses. These reforms were necessary and impactful. However, as an unexpected consequence, psychologists have begun to realise that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound ‘derivation chain’ between test and theory is counterproductive. Instead, various non-confirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and their functional form, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognised and incentivised as a crucial goal in and of itself. In this article, we discuss how shifting the focus to non-confirmatory research can tie together many loose ends of psychology’s reform movement and help us lay the foundation to develop strong, testable theories, as Paul Meehl urged us to.},
	journaltitle = {Perspectives on Psychological Science},
	author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Daniel},
	urldate = {2020-10-05},
	date = {2020-09-06},
	note = {Publisher: {PsyArXiv}},
	file = {Full Text PDF:/Users/tom/Zotero/storage/JY9RIM24/Scheel et al. - 2020 - Why hypothesis testers should spend less time test.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/WDPL9246/vekpu.html:text/html},
}

@article{gigerenzer_surrogates_1998,
	title = {Surrogates for theories},
	volume = {8},
	url = {https://journals.sagepub.com/doi/10.1177/0959354398082006},
	doi = {10.1177/0959354398082006},
	shorttitle = {Surrogates for Theories},
	abstract = {I first discuss several strategies that serve as surrogates for theories in psychology: one-word explanation, redescription, drawing vague dichotomies, and data...},
	pages = {195--204},
	number = {2},
	journaltitle = {Theory \& Psychology},
	author = {Gigerenzer, Gerd},
	urldate = {2020-10-05},
	date = {1998},
	langid = {english},
	note = {Publisher: Sage {PublicationsSage} {CA}: Thousand Oaks, {CA}},
	file = {Gigerenzer - 2016 - Surrogates for Theories.pdf:/Users/tom/Zotero/storage/3P5BIBSW/Gigerenzer - 2016 - Surrogates for Theories.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DUL9QUU7/0959354398082006.html:text/html},
}

@article{muthukrishna_problem_2019,
	title = {A problem in theory},
	volume = {3},
	rights = {2019 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0522-1},
	doi = {10.1038/s41562-018-0522-1},
	abstract = {The replication crisis facing the psychological sciences is widely regarded as rooted in methodological or statistical shortcomings. We argue that a large part of the problem is the lack of a cumulative theoretical framework or frameworks. Without an overarching theoretical framework that generates hypotheses across diverse domains, empirical programs spawn and grow from personal intuitions and culturally biased folk theories. By providing ways to develop clear predictions, including through the use of formal modelling, theoretical frameworks set expectations that determine whether a new finding is confirmatory, nicely integrating with existing lines of research, or surprising, and therefore requiring further replication and scrutiny. Such frameworks also prioritize certain research foci, motivate the use diverse empirical approaches and, often, provide a natural means to integrate across the sciences. Thus, overarching theoretical frameworks pave the way toward a more general theory of human behaviour. We illustrate one such a theoretical framework: dual inheritance theory.},
	pages = {221--229},
	number = {3},
	journaltitle = {Nature Human Behaviour},
	author = {Muthukrishna, Michael and Henrich, Joseph},
	urldate = {2020-10-05},
	date = {2019-03},
	langid = {english},
	note = {Number: 3
Publisher: Nature Publishing Group},
	file = {Muthukrishna and Henrich - 2019 - A problem in theory.pdf:/Users/tom/Zotero/storage/NGMBFDHT/Muthukrishna and Henrich - 2019 - A problem in theory.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/F5UUIDY4/s41562-018-0522-1.html:text/html},
}

@article{maxwell_is_2015,
	title = {Is psychology suffering from a replication crisis? What does “failure to replicate” really mean?},
	volume = {70},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039400},
	doi = {10.1037/a0039400},
	shorttitle = {Is psychology suffering from a replication crisis?},
	abstract = {Psychology has recently been viewed as facing a replication crisis because efforts to replicate past study ﬁndings frequently do not show the same result. Often, the ﬁrst study showed a statistically signiﬁcant result but the replication does not. Questions then arise about whether the ﬁrst study results were false positives, and whether the replication study correctly indicates that there is truly no effect after all. This article suggests these so-called failures to replicate may not be failures at all, but rather are the result of low statistical power in single replication studies, and the result of failure to appreciate the need for multiple replications in order to have enough power to identify true effects. We provide examples of these power problems and suggest some solutions using Bayesian statistics and metaanalysis. Although the need for multiple replication studies may frustrate those who would prefer quick answers to psychology’s alleged crisis, the large sample sizes typically needed to provide ﬁrm evidence will almost always require concerted efforts from multiple investigators. As a result, it remains to be seen how many of the recently claimed failures to replicate will be supported or instead may turn out to be artifacts of inadequate sample sizes and single study replications.},
	pages = {487--498},
	number = {6},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Maxwell, Scott E. and Lau, Michael Y. and Howard, George S.},
	urldate = {2020-10-05},
	date = {2015},
	langid = {english},
	file = {Maxwell et al. - 2015 - Is psychology suffering from a replication crisis.pdf:/Users/tom/Zotero/storage/XLCVCKX7/Maxwell et al. - 2015 - Is psychology suffering from a replication crisis.pdf:application/pdf},
}

@article{klahr_studies_1999,
	title = {Studies of scientific discovery: Complementary approaches and convergent findings},
	volume = {125},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/0033-2909.125.5.524},
	shorttitle = {Studies of scientific discovery},
	abstract = {This review integrates 4 major approaches to the study of science—historical accounts of scientific discoveries, psychological experiments with nonscientists working on tasks related to scientific discoveries, direct observation of ongoing scientific laboratories, and computational modeling of scientific discovery processes—by viewing them through the lens of the theory of human problem solving. The authors provide a brief justification for the study of scientific discovery, a summary of the major approaches, and criteria for comparing and contrasting them. Then, they apply these criteria to the different approaches and indicate their complementarities. Finally, they provide several examples of convergent principles of the process of scientific discovery. ({PsycINFO} Database Record (c) 2019 {APA}, all rights reserved)},
	pages = {524--543},
	number = {5},
	journaltitle = {Psychological Bulletin},
	author = {Klahr, David and Simon, Herbert A.},
	date = {1999},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Computer Simulation, Direct Observation, Experimentation, History, Methodology, Observation Methods, Sciences},
	file = {Klahr and Simon - 1999 - Studies of scientific discovery Complementary app.pdf:/Users/tom/Zotero/storage/EAKC7LYS/Klahr and Simon - 1999 - Studies of scientific discovery Complementary app.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/EAS6A5NN/1999-03909-002.html:text/html},
}

@article{smith_data_2002,
	title = {Data dredging, bias, or confounding},
	volume = {325},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1124898/},
	pages = {1437--1438},
	number = {7378},
	journaltitle = {{BMJ} : British Medical Journal},
	shortjournal = {{BMJ}},
	author = {Smith, George Davey and Ebrahim, Shah},
	urldate = {2020-10-05},
	date = {2002-12-21},
	pmid = {12493654},
	pmcid = {PMC1124898},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/68KKRS65/Smith and Ebrahim - 2002 - Data dredging, bias, or confounding.pdf:application/pdf},
}

@article{tierney_creative_2020,
	title = {Creative destruction in science},
	volume = {161},
	issn = {0749-5978},
	url = {http://www.sciencedirect.com/science/article/pii/S0749597820303678},
	doi = {10.1016/j.obhdp.2020.07.002},
	abstract = {Drawing on the concept of a gale of creative destruction in a capitalistic economy, we argue that initiatives to assess the robustness of findings in the organizational literature should aim to simultaneously test competing ideas operating in the same theoretical space. In other words, replication efforts should seek not just to support or question the original findings, but also to replace them with revised, stronger theories with greater explanatory power. Achieving this will typically require adding new measures, conditions, and subject populations to research designs, in order to carry out conceptual tests of multiple theories in addition to directly replicating the original findings. To illustrate the value of the creative destruction approach for theory pruning in organizational scholarship, we describe recent replication initiatives re-examining culture and work morality, working parents’ reasoning about day care options, and gender discrimination in hiring decisions.
Significance statement
It is becoming increasingly clear that many, if not most, published research findings across scientific fields are not readily replicable when the same method is repeated. Although extremely valuable, failed replications risk leaving a theoretical void— reducing confidence the original theoretical prediction is true, but not replacing it with positive evidence in favor of an alternative theory. We introduce the creative destruction approach to replication, which combines theory pruning methods from the field of management with emerging best practices from the open science movement, with the aim of making replications as generative as possible. In effect, we advocate for a Replication 2.0 movement in which the goal shifts from checking on the reliability of past findings to actively engaging in competitive theory testing and theory building.
Scientific transparency statement
The materials, code, and data for this article are posted publicly on the Open Science Framework, with links provided in the article.},
	pages = {291--309},
	journaltitle = {Organizational Behavior and Human Decision Processes},
	shortjournal = {Organizational Behavior and Human Decision Processes},
	author = {Tierney, Warren and Hardy, Jay H. and Ebersole, Charles R. and Leavitt, Keith and Viganola, Domenico and Clemente, Elena Giulia and Gordon, Michael and Dreber, Anna and Johannesson, Magnus and Pfeiffer, Thomas and Uhlmann, Eric Luis},
	urldate = {2020-10-04},
	date = {2020-11-01},
	langid = {english},
	keywords = {Replication, Conceptual replication, Cultural differences, Direct replication, Falsification, Gender discrimination, Hiring decisions, Protestant work ethic, Theory pruning, Theory testing, Work values, Work-family conflict},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/5AHJNU7W/Tierney et al. - 2020 - Creative destruction in science.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/S9QX6FIA/S0749597820303678.html:text/html},
}

@article{chalmers_underreporting_1990,
	title = {Underreporting research is scientific misconduct},
	volume = {263},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/380971},
	doi = {10.1001/jama.1990.03440100121018},
	abstract = {{\textless}p{\textgreater}Substantial numbers of clinical trials are never reported in print, and among those that are, many are not reported in sufficient detail to enable judgments to be made about the validity of their results. Failure to publish an adequate account of a well-designed clinical trial is a form of scientific misconduct that can lead those caring for patients to make inappropriate treatment decisions. Investigators, research ethics committees, funding bodies, and scientific editors all have responsibilities to reduce underreporting of clinical trials. An extended use of prospective registration of trials at inception, as well as benefiting clinical research in other ways, could help people to play their respective roles in reducing underreporting of clinical trials.{\textless}/p{\textgreater}{\textless}p{\textgreater}(\textit{{JAMA}}. 1990;263:1405-1408){\textless}/p{\textgreater}},
	pages = {1405--1408},
	number = {10},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Chalmers, Lain},
	urldate = {2020-10-02},
	date = {1990-03-09},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Chalmers - 1990 - Underreporting Research Is Scientific Misconduct.pdf:/Users/tom/Zotero/storage/XRZSHDIQ/Chalmers - 1990 - Underreporting Research Is Scientific Misconduct.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JZN2M5RA/380971.html:text/html},
}

@article{montori_randomized_2005,
	title = {Randomized trials stopped early for benefit: a systematic review},
	volume = {294},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.294.17.2203},
	doi = {10.1001/jama.294.17.2203},
	shorttitle = {Randomized trials stopped early for benefit},
	pages = {2203},
	number = {17},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Montori, Victor M. and Devereaux, P. J. and Adhikari, Neill K. J. and Burns, Karen E. A. and Eggert, Christoph H. and Briel, Matthias and Lacchetti, Christina and Leung, Teresa W. and Darling, Elizabeth and Bryant, Dianne M. and Bucher, Heiner C. and Schünemann, Holger J. and Meade, Maureen O. and Cook, Deborah J. and Erwin, Patricia J. and Sood, Amit and Sood, Richa and Lo, Benjamin and Thompson, Carly A. and Zhou, Qi and Mills, Edward and Guyatt, Gordon H.},
	urldate = {2020-10-02},
	date = {2005-11-02},
	langid = {english},
	file = {Montori et al. - 2005 - Randomized Trials Stopped Early for Benefit A Sys.pdf:/Users/tom/Zotero/storage/VUHYSAXH/Montori et al. - 2005 - Randomized Trials Stopped Early for Benefit A Sys.pdf:application/pdf},
}

@article{dwan_comparison_2011,
	title = {Comparison of protocols and registry entries to published reports for randomised controlled trials},
	issn = {1465-1858},
	url = {https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.MR000031.pub2/full},
	doi = {10.1002/14651858.MR000031.pub2},
	number = {1},
	journaltitle = {Cochrane Database of Systematic Reviews},
	author = {Dwan, Kerry and Altman, Douglas G. and Cresswell, Lynne and Blundell, Michaela and Gamble, Carrol L. and Williamson, Paula R.},
	urldate = {2020-10-02},
	date = {2011},
	langid = {english},
	note = {Publisher: John Wiley \& Sons, Ltd},
	file = {Dwan et al. - 2011 - Comparison of protocols and registry entries to pu.pdf:/Users/tom/Zotero/storage/A22X43UF/Dwan et al. - 2011 - Comparison of protocols and registry entries to pu.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BYP7RFLP/full.html:text/html},
}

@article{wallach_evaluation_2017,
	title = {Evaluation of evidence of statistical support and corroboration of subgroup claims in randomized clinical trials},
	volume = {177},
	issn = {2168-6106},
	url = {http://archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2016.9125},
	doi = {10.1001/jamainternmed.2016.9125},
	pages = {554--560},
	number = {4},
	journaltitle = {{JAMA} Internal Medicine},
	shortjournal = {{JAMA} Intern Med},
	author = {Wallach, Joshua D. and Sullivan, Patrick G. and Trepanowski, John F. and Sainani, Kristin L. and Steyerberg, Ewout W. and Ioannidis, John P. A.},
	urldate = {2020-10-02},
	date = {2017-04-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/H6P4T7FW/Wallach et al. - 2017 - Evaluation of Evidence of Statistical Support and .pdf:application/pdf;jamainternal_wallach_2017_oi_160123.pdf:/Users/tom/Zotero/storage/H6LN6TR6/jamainternal_wallach_2017_oi_160123.pdf:application/pdf},
}

@article{schriger_inadequate_2010,
	title = {Inadequate post-publication review of medical research},
	volume = {341},
	rights = {© {BMJ} Publishing Group Ltd 2010},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/341/bmj.c3803},
	doi = {10.1136/bmj.c3803},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Schriger, David L. and Altman, Douglas G.},
	urldate = {2020-10-02},
	date = {2010-08-11},
	langid = {english},
	pmid = {20702543},
	note = {Publisher: British Medical Journal Publishing Group
Section: Editorial},
	file = {Snapshot:/Users/tom/Zotero/storage/Q9FB2FKR/bmj.html:text/html},
}

@article{boot_pervasive_2013,
	title = {The pervasive problem with placebos in psychology: why active control groups are not sufficient to rule out placebo effects},
	volume = {8},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691613491271},
	doi = {10.1177/1745691613491271},
	shorttitle = {The pervasive problem with placebos in psychology},
	abstract = {To draw causal conclusions about the efficacy of a psychological intervention, researchers must compare the treatment condition with a control group that accounts for improvements caused by factors other than the treatment. Using an active control helps to control for the possibility that improvement by the experimental group resulted from a placebo effect. Although active control groups are superior to “no-contact” controls, only when the active control group has the same expectation of improvement as the experimental group can we attribute differential improvements to the potency of the treatment. Despite the need to match expectations between treatment and control groups, almost no psychological interventions do so. This failure to control for expectations is not a minor omission—it is a fundamental design flaw that potentially undermines any causal inference. We illustrate these principles with a detailed example from the video-game-training literature showing how the use of an active control group does not eliminate expectation differences. The problem permeates other interventions as well, including those targeting mental health, cognition, and educational achievement. Fortunately, measuring expectations and adopting alternative experimental designs makes it possible to control for placebo effects, thereby increasing confidence in the causal efficacy of psychological interventions.},
	pages = {445--454},
	number = {4},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Boot, Walter R. and Simons, Daniel J. and Stothart, Cary and Stutts, Cassie},
	urldate = {2020-10-02},
	date = {2013-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/BFCFN6YH/Boot et al. - 2013 - The Pervasive Problem With Placebos in Psychology.pdf:application/pdf},
}

@article{heaven_ai_2018,
	title = {{AI} peer reviewers unleashed to ease publishing grind},
	volume = {563},
	rights = {2020 Nature},
	url = {https://www.nature.com/articles/d41586-018-07245-9},
	doi = {10.1038/d41586-018-07245-9},
	abstract = {Automated tools could speed up and improve the review process, but humans are still in the driving seat.},
	pages = {609--610},
	number = {7733},
	journaltitle = {Nature},
	author = {Heaven, Douglas},
	urldate = {2020-10-01},
	date = {2018-11-22},
	langid = {english},
	note = {Number: 7733
Publisher: Nature Publishing Group},
	file = {Full Text:/Users/tom/Zotero/storage/R9H2VKGN/Heaven - 2018 - AI peer reviewers unleashed to ease publishing gri.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BQ5PQKRA/d41586-018-07245-9.html:text/html},
}

@article{goodman_what_2016,
	title = {What does research reproducibility mean?},
	volume = {8},
	rights = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {1946-6234, 1946-6242},
	url = {https://stm.sciencemag.org/content/8/341/341ps12},
	doi = {10.1126/scitranslmed.aaf5027},
	abstract = {The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.},
	pages = {341ps12--341ps12},
	number = {341},
	journaltitle = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	urldate = {2020-10-01},
	date = {2016-06-01},
	langid = {english},
	pmid = {27252173},
	note = {Publisher: American Association for the Advancement of Science
Section: Perspective},
	file = {Full Text PDF:/Users/tom/Zotero/storage/8Q4SI23P/Goodman et al. - 2016 - What does research reproducibility mean.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XV2ZTK7W/341ps12.html:text/html},
}

@article{sackett_bias_1979,
	title = {Bias in analytic research},
	volume = {32},
	issn = {0021-9681},
	url = {http://www.sciencedirect.com/science/article/pii/0021968179900122},
	doi = {10.1016/0021-9681(79)90012-2},
	pages = {51--63},
	number = {1},
	journaltitle = {Journal of Chronic Diseases},
	shortjournal = {Journal of Chronic Diseases},
	author = {Sackett, David L.},
	urldate = {2020-10-01},
	date = {1979-01-01},
	langid = {english},
	file = {Sackett - 1979 - Bias in analytic research.pdf:/Users/tom/Zotero/storage/L3446JWT/Sackett - 1979 - Bias in analytic research.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/L86WYIRZ/0021968179900122.html:text/html},
}

@article{sharpe_meta-analysis_2020,
	title = {Meta-analysis as a response to the replication crisis.},
	issn = {1878-7304, 0708-5591},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/cap0000215},
	doi = {10.1037/cap0000215},
	journaltitle = {Canadian Psychology/Psychologie canadienne},
	shortjournal = {Canadian Psychology/Psychologie canadienne},
	author = {Sharpe, Donald and Poets, Sarena},
	urldate = {2020-10-01},
	date = {2020-08-13},
	langid = {english},
	file = {Sharpe and Poets - 2020 - Meta-analysis as a response to the replication cri.pdf:/Users/tom/Zotero/storage/YCC7LYDK/Sharpe and Poets - 2020 - Meta-analysis as a response to the replication cri.pdf:application/pdf},
}

@article{monogan_case_2013,
	title = {A Case for Registering Studies of Political Outcomes: An Application in the 2010 House Elections},
	volume = {21},
	issn = {1047-1987},
	url = {https://www.jstor.org/stable/23359688},
	doi = {10.1093/pan/mps022},
	shorttitle = {A Case for Registering Studies of Political Outcomes},
	abstract = {This article makes the case for the systematic registration of political studies. By proposing a research design before an outcome variable is observed, a researcher commits him- or herself to a theoretically motivated method for studying the object of interest. Further, study registration prompts peers of the discipline to evaluate a study's quality on its own merits, reducing norms to accept significant results and reject null findings. To advance this idea, the Political Science Registered Studies Dataverse (http://dvn.iq.harvard.edu/dvn/dv/registration) has been created, in which scholars may create a permanent record of a research design before completing a study. This article also illustrates the method of registration through a study of the impact of the immigration issue in the 2010 election for the U.S. House of Representatives. Prior to the election, a design for this study was posted on the Society for Political Methodology website (http://polmeth.wustl.edu/{mediaDetail}.php?{docId}=1258). After the votes were counted, the study was completed in accord with the design. The treatment effect in this theoretically specified design was indiscernible, but a specification search could yield a significant result. Hence, this article illustrates the argument for study registration through a case in which the result could easily be manipulated.},
	pages = {21--37},
	number = {1},
	journaltitle = {Political Analysis},
	author = {Monogan, James E.},
	urldate = {2020-10-01},
	date = {2013},
	note = {Publisher: Oxford University Press},
}

@article{casey_reshaping_2012,
	title = {Reshaping institutions: evidence on aid impacts using a preanalysis plan*},
	volume = {127},
	issn = {0033-5533},
	url = {https://academic.oup.com/qje/article/127/4/1755/1841616},
	doi = {10.1093/qje/qje027},
	shorttitle = {Reshaping institutions},
	abstract = {Abstract.   Despite their importance, there is limited evidence on how institutions can be strengthened. Evaluating the effects of specific reforms is complicat},
	pages = {1755--1812},
	number = {4},
	journaltitle = {The Quarterly Journal of Economics},
	shortjournal = {Q J Econ},
	author = {Casey, Katherine and Glennerster, Rachel and Miguel, Edward},
	urldate = {2020-10-01},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Full Text PDF:/Users/tom/Zotero/storage/IAXY4697/Casey et al. - 2012 - Reshaping Institutions Evidence on Aid Impacts Us.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/4BCE98HW/1841616.html:text/html},
}

@article{kline_post_2020,
	title = {Post p value education in graduate statistics: Preparing tomorrow’s psychology researchers for a postcrisis future.},
	issn = {1878-7304, 0708-5591},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/cap0000200},
	doi = {10.1037/cap0000200},
	shorttitle = {Post p value education in graduate statistics},
	abstract = {How to better educate student researchers in psychology and related disciplines in modern methods of statistics and data analysis that also promote better practices, such as replication, is the focus of this work. Suggestions about how to modernize course content and deal with potential obstacles that can arise in teaching graduate-level statistics courses are also considered.},
	journaltitle = {Canadian Psychology/Psychologie canadienne},
	shortjournal = {Canadian Psychology/Psychologie canadienne},
	author = {Kline, Rex B.},
	urldate = {2020-10-01},
	date = {2020-08-13},
	langid = {english},
	file = {Kline - 2020 - Post p value education in graduate statistics Pre.pdf:/Users/tom/Zotero/storage/ZQZYZBAA/Kline - 2020 - Post p value education in graduate statistics Pre.pdf:application/pdf},
}

@article{friedman_working_2020,
	title = {Working with psychology journal editors to correct problems in the scientific literature.},
	issn = {1878-7304, 0708-5591},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/cap0000248},
	doi = {10.1037/cap0000248},
	abstract = {Psychological science undergirds all of the field’s applications, yet it suffers from widespread replicability and reproducibility failures; even many of its foundational studies have been shown to be flawed. Change is needed to restore psychology’s viability, which requires addressing its dysfunctional scientific culture, including those aspects that discourage journal editors from publishing corrections to the scientific literature. Several cases of editorial resistance to publishing reports debunking findings that could not be reproduced are examined, and recommendations are offered to encourage editors, and others, to embrace, rather than obstruct, self-correcting psychological science.},
	journaltitle = {Canadian Psychology/Psychologie canadienne},
	shortjournal = {Canadian Psychology/Psychologie canadienne},
	author = {Friedman, Harris L. and {MacDonald}, Douglas A. and Coyne, James C.},
	urldate = {2020-10-01},
	date = {2020-08-13},
	langid = {english},
	file = {Friedman et al. - 2020 - Working with psychology journal editors to correct.pdf:/Users/tom/Zotero/storage/56J4HVBD/Friedman et al. - 2020 - Working with psychology journal editors to correct.pdf:application/pdf},
}

@article{schimmack_meta-psychological_2020,
	title = {A meta-psychological perspective on the decade of replication failures in social psychology.},
	issn = {1878-7304, 0708-5591},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/cap0000246},
	doi = {10.1037/cap0000246},
	abstract = {This article examines explanations for the low replicability of social psychology. Using a new statistical method, z-curve, it is shown that low statistical power and selective reporting of significant results are the main factor. As a result, original results in social psychology journals report inflated effect sizes that cannot be replicated. To improve the credibility of social psychology, researchers must increase statistical power and disclose nonsignificant results.},
	journaltitle = {Canadian Psychology/Psychologie canadienne},
	shortjournal = {Canadian Psychology/Psychologie canadienne},
	author = {Schimmack, Ulrich},
	urldate = {2020-10-01},
	date = {2020-08-13},
	langid = {english},
	file = {Schimmack - 2020 - A meta-psychological perspective on the decade of .pdf:/Users/tom/Zotero/storage/8WRKFHJD/Schimmack - 2020 - A meta-psychological perspective on the decade of .pdf:application/pdf},
}

@article{lilienfeld_psychological_2020,
	title = {Psychological measurement and the replication crisis: Four sacred cows.},
	issn = {1878-7304, 0708-5591},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/cap0000236},
	doi = {10.1037/cap0000236},
	shorttitle = {Psychological measurement and the replication crisis},
	abstract = {This article outlines four widely held but erroneous measurement assumptions that may adversely affect the accuracy and replicability of psychological findings. The effects of questionable measurement practices stemming from these assumptions are discussed, and new data bearing on the prevalence of these assumptions in academic journals are presented. In addition, this article offers several potential remedies that researchers and journals can implement to improve the measurement of psychological constructs.},
	journaltitle = {Canadian Psychology/Psychologie canadienne},
	shortjournal = {Canadian Psychology/Psychologie canadienne},
	author = {Lilienfeld, Scott O. and Strother, Adele N.},
	urldate = {2020-10-01},
	date = {2020-08-13},
	langid = {english},
	file = {Lilienfeld and Strother - 2020 - Psychological measurement and the replication cris.pdf:/Users/tom/Zotero/storage/758FDS2J/Lilienfeld and Strother - 2020 - Psychological measurement and the replication cris.pdf:application/pdf},
}

@article{daston_scientific_2005,
	title = {Scientific Error and the Ethos of Belief},
	volume = {72},
	pages = {1--28},
	number = {1},
	journaltitle = {Social Research},
	author = {Daston, Lorraine},
	date = {2005},
	langid = {english},
	file = {2020 - Scientific Error and the Ethos of Belief.pdf:/Users/tom/Zotero/storage/7MS563CF/2020 - Scientific Error and the Ethos of Belief.pdf:application/pdf},
}

@article{sedlmeier_studies_1989,
	title = {Do studies of statistical power have an effect on the power of studies?},
	volume = {105},
	pages = {309--316},
	number = {2},
	journaltitle = {Psychological Bulletin},
	author = {Sedlmeier, Peter and Gigerenzer, Gerd},
	date = {1989},
	langid = {english},
	keywords = {❓ Multiple {DOI}},
	file = {Sedlmeier and Gigerenzer - Do Studies of Statistical Power Have an Effect on .pdf:/Users/tom/Zotero/storage/43KGXF28/Sedlmeier and Gigerenzer - Do Studies of Statistical Power Have an Effect on .pdf:application/pdf},
}

@article{gigerenzer_surrogate_nodate,
	title = {Surrogate science: the idol of a universal method for scientific inference},
	doi = {10.1177/0149206314547522},
	pages = {20},
	author = {Gigerenzer, Gerd and Marewski, Julian N},
	langid = {english},
	file = {Gigerenzer and Marewski - Surrogate Science The Idol of a Universal Method .pdf:/Users/tom/Zotero/storage/3NLTXILP/Gigerenzer and Marewski - Surrogate Science The Idol of a Universal Method .pdf:application/pdf},
}

@article{gigerenzer_mindless_2004,
	title = {Mindless statistics},
	volume = {33},
	issn = {1053-5357},
	url = {http://www.sciencedirect.com/science/article/pii/S1053535704000927},
	doi = {10.1016/j.socec.2004.09.033},
	series = {Statistical Significance},
	abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
	pages = {587--606},
	number = {5},
	journaltitle = {The Journal of Socio-Economics},
	shortjournal = {The Journal of Socio-Economics},
	author = {Gigerenzer, Gerd},
	urldate = {2020-10-01},
	date = {2004-11-01},
	langid = {english},
	keywords = {Collective illusions, Editors, Rituals, Statistical significance, Textbooks},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/T8IYGDFG/Gigerenzer - 2004 - Mindless statistics.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/SW5AS63M/S1053535704000927.html:text/html},
}

@article{winker_promise_2015,
	title = {The promise of post-publication peer review: how do we get there from here?},
	volume = {28},
	issn = {09531513, 17414857},
	url = {http://doi.wiley.com/10.1087/20150209},
	doi = {10.1087/20150209},
	shorttitle = {The promise of post-publication peer review},
	pages = {143--145},
	number = {2},
	journaltitle = {Learned Publishing},
	shortjournal = {Learn. Pub.},
	author = {Winker, Margaret A.},
	urldate = {2020-10-01},
	date = {2015-04-01},
	langid = {english},
	file = {Winker - 2015 - The promise of post-publication peer review how d.pdf:/Users/tom/Zotero/storage/TVATPV5D/Winker - 2015 - The promise of post-publication peer review how d.pdf:application/pdf},
}

@article{altman_unjustified_2005,
	title = {Unjustified restrictions on letters to the editor},
	volume = {2},
	issn = {1549-1676},
	url = {https://dx.plos.org/10.1371/journal.pmed.0020126},
	doi = {10.1371/journal.pmed.0020126},
	pages = {e126},
	number = {5},
	journaltitle = {{PLoS} Medicine},
	shortjournal = {{PLoS} Med},
	author = {Altman, Douglas G},
	urldate = {2020-10-01},
	date = {2005-05-31},
	langid = {english},
	file = {Altman - 2005 - Unjustified Restrictions on Letters to the Editor.pdf:/Users/tom/Zotero/storage/23PZWSJ8/Altman - 2005 - Unjustified Restrictions on Letters to the Editor.pdf:application/pdf},
}

@article{gotzsche_adequacy_2010,
	title = {Adequacy of authors' replies to criticism raised in electronic letters to the editor: cohort study},
	volume = {341},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.c3926},
	doi = {10.1136/bmj.c3926},
	shorttitle = {Adequacy of authors' replies to criticism raised in electronic letters to the editor},
	abstract = {Objective To investigate whether substantive criticism in electronic letters to the editor, defined as a problem that could invalidate the research or reduce its reliability, is adequately addressed by the authors. Design Cohort study. {SettingBMJ} between October 2005 and September 2007. Inclusion criteria Research papers generating substantive criticism in the rapid responses section on bmj.com. Main outcome measures Severity of criticism (minor, moderate, or major) as judged by two editors and extent to which the criticism was addressed by authors (fully, partly, or not) as judged by two editors and the critics.
Results A substantive criticism was raised against 105 of 350 (30\%, 95\% confidence interval 25\% to 35\%) included research papers, and of these the authors had responded to 47 (45\%, 35\% to 54\%). The severity of the criticism was the same in those papers as in the 58 without author replies (mean score 2.2 in both groups, P=0.72). For the 47 criticisms with replies, there was no relation between the severity of the criticism and the adequacy of the reply, neither as judged by the editors (P=0.88 and P=0.95, respectively) nor by the critics (P=0.83; response rate 85\%). However, the critics were much more critical of the replies than the editors (average score 2.3 v 1.4, P{\textless}0.001).
Conclusions Authors are reluctant to respond to criticisms of their work, although they are not less likely to respond when criticisms are severe. Editors should ensure that authors take relevant criticism seriously and respond adequately to it.},
	pages = {c3926--c3926},
	issue = {aug10 2},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Gotzsche, Peter. C. and Delamothe, T. and Godlee, F. and Lundh, A.},
	urldate = {2020-10-01},
	date = {2010-08-10},
	langid = {english},
	file = {Gotzsche et al. - 2010 - Adequacy of authors' replies to criticism raised i.pdf:/Users/tom/Zotero/storage/HTYH54PH/Gotzsche et al. - 2010 - Adequacy of authors' replies to criticism raised i.pdf:application/pdf},
}

@article{gigerenzer_statistical_2018,
	title = {Statistical rituals: the replication delusion and how we got there},
	volume = {1},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/2515245918771329},
	doi = {10.1177/2515245918771329},
	shorttitle = {Statistical rituals},
	pages = {198--218},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Gigerenzer, Gerd},
	urldate = {2020-10-01},
	date = {2018-06},
	langid = {english},
	file = {Gigerenzer - 2018 - Statistical Rituals The Replication Delusion and .pdf:/Users/tom/Zotero/storage/89BLVF7Q/Gigerenzer - 2018 - Statistical Rituals The Replication Delusion and .pdf:application/pdf},
}

@article{walker_discrepancies_2014,
	title = {Discrepancies between registration and publication of randomised controlled trials: an observational study:},
	rights = {© The Author(s) 2014 Reprints and permissions: sagepub.co.uk/{journalsPermissions}.nav},
	url = {https://journals.sagepub.com/doi/10.1177/2042533313517688},
	doi = {10.1177/2042533313517688},
	shorttitle = {Discrepancies between registration and publication of randomised controlled trials},
	abstract = {Summary Objectives To determine the consistency between information contained in the registration and publication of randomised controlled trials ({RCTs}). Design...},
	journaltitle = {{JRSM} Open},
	author = {Walker, Kate F. and Stevenson, Graham and Thornton, James G.},
	urldate = {2020-10-01},
	date = {2014-05-01},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {UK}: London, England},
	file = {Full Text:/Users/tom/Zotero/storage/6S5ZAV5D/Walker et al. - 2014 - Discrepancies between registration and publication.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/QS5XG2B9/2042533313517688.html:text/html},
}

@article{jones_comparison_2015,
	title = {Comparison of registered and published outcomes in randomized controlled trials: a systematic review},
	volume = {13},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-015-0520-3},
	doi = {10.1186/s12916-015-0520-3},
	shorttitle = {Comparison of registered and published outcomes in randomized controlled trials},
	abstract = {Clinical trial registries can improve the validity of trial results by facilitating comparisons between prospectively planned and reported outcomes. Previous reports on the frequency of planned and reported outcome inconsistencies have reported widely discrepant results. It is unknown whether these discrepancies are due to differences between the included trials, or to methodological differences between studies. We aimed to systematically review the prevalence and nature of discrepancies between registered and published outcomes among clinical trials.},
	pages = {282},
	number = {1},
	journaltitle = {{BMC} Medicine},
	shortjournal = {{BMC} Medicine},
	author = {Jones, Christopher W. and Keil, Lukas G. and Holland, Wesley C. and Caughey, Melissa C. and Platts-Mills, Timothy F.},
	urldate = {2020-10-01},
	date = {2015-11-18},
	file = {Full Text:/Users/tom/Zotero/storage/D7LP7W7A/Jones et al. - 2015 - Comparison of registered and published outcomes in.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5S8FTAN6/s12916-015-0520-3.html:text/html},
}

@article{shaw_measurement_2020,
	title = {Measurement practices in large-scale replications: Insights from Many Labs 2.},
	issn = {1878-7304, 0708-5591},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/cap0000220},
	doi = {10.1037/cap0000220},
	shorttitle = {Measurement practices in large-scale replications},
	abstract = {Valid measurement is a key aspect of conducting robust, reproducible research. Our review indicates that current measurement practices in original and replication studies are lacking rigour. We discuss the resulting implications and recommend steps that the field can take to improve the validity of measurement in original and replication research.},
	journaltitle = {Canadian Psychology/Psychologie canadienne},
	shortjournal = {Canadian Psychology/Psychologie canadienne},
	author = {Shaw, Mairead and Cloos, Leonie J. R. and Luong, Raymond and Elbaz, Sasha and Flake, Jessica Kay},
	urldate = {2020-10-01},
	date = {2020-08-13},
	langid = {english},
	file = {Shaw et al. - 2020 - Measurement practices in large-scale replications.pdf:/Users/tom/Zotero/storage/7J2ILPGB/Shaw et al. - 2020 - Measurement practices in large-scale replications.pdf:application/pdf},
}

@article{tackett_bringing_2020,
	title = {Bringing the (pre)registration revolution to graduate training.},
	issn = {1878-7304, 0708-5591},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/cap0000221},
	doi = {10.1037/cap0000221},
	abstract = {Preregistration— or the public posting of plans for a study prior to its completion—is a tool that shows great promise for increasing scientific rigor. However, this practice has not yet been adopted by the majority of psychology researchers. In this article, the authors detail their approach to creating a workshop-based class on preregistration that makes preregistration accessible to multiple areas of psychology.},
	journaltitle = {Canadian Psychology/Psychologie canadienne},
	shortjournal = {Canadian Psychology/Psychologie canadienne},
	author = {Tackett, Jennifer L. and Brandes, Cassandra M. and Dworak, Elizabeth M. and Shields, Allison N.},
	urldate = {2020-10-01},
	date = {2020-08-13},
	langid = {english},
	file = {Tackett et al. - 2020 - Bringing the (pre)registration revolution to gradu.pdf:/Users/tom/Zotero/storage/93WPZJ7L/Tackett et al. - 2020 - Bringing the (pre)registration revolution to gradu.pdf:application/pdf},
}

@article{lilienfeld_psychological_nodate,
	title = {Psychological Treatments That Cause Harm},
	volume = {2},
	abstract = {The phrase primum non nocere (‘‘ﬁrst, do no harm’’) is a well-accepted credo of the medical and mental health professions. Although emerging data indicate that several psychological treatments may produce harm in significant numbers of individuals, psychologists have until recently paid little attention to the problem of hazardous treatments. I critically evaluate and update earlier conclusions regarding deterioration effects in psychotherapy, outline methodological obstacles standing in the way of identifying potentially harmful therapies ({PHTs}), provide a provisional list of {PHTs}, discuss the implications of {PHTs} for clinical science and practice, and delineate fruitful areas for further research on {PHTs}. A heightened emphasis on {PHTs} should narrow the scientist–practitioner gap and safeguard mental health consumers against harm. Moreover, the literature on {PHTs} may provide insight into underlying mechanisms of change that cut across many domains of psychotherapy. The ﬁeld of psychology should prioritize its efforts toward identifying {PHTs} and place greater emphasis on potentially dangerous than on empirically supported therapies.},
	pages = {18},
	number = {1},
	author = {Lilienfeld, Scott O},
	langid = {english},
	file = {Lilienfeld - Psychological Treatments That Cause Harm.pdf:/Users/tom/Zotero/storage/NNLPSX7V/Lilienfeld - Psychological Treatments That Cause Harm.pdf:application/pdf},
}

@article{stefan_tutorial_2019,
	title = {A tutorial on Bayes Factor Design Analysis using an informed prior},
	volume = {51},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-018-01189-8},
	doi = {10.3758/s13428-018-01189-8},
	abstract = {Well-designed experiments are likely to yield compelling evidence with efficient sample sizes. Bayes Factor Design Analysis ({BFDA}) is a recently developed methodology that allows researchers to balance the informativeness and efficiency of their experiment (Schönbrodt \& Wagenmakers, Psychonomic Bulletin \& Review, 25(1), 128–142 2018). With {BFDA}, researchers can control the rate of misleading evidence but, in addition, they can plan for a target strength of evidence. {BFDA} can be applied to fixed-N and sequential designs. In this tutorial paper, we provide an introduction to {BFDA} and analyze how the use of informed prior distributions affects the results of the {BFDA}. We also present a user-friendly web-based {BFDA} application that allows researchers to conduct {BFDAs} with ease. Two practical examples highlight how researchers can use a {BFDA} to plan for informative and efficient research designs.},
	pages = {1042--1058},
	number = {3},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Stefan, Angelika M. and Gronau, Quentin F. and Schönbrodt, Felix D. and Wagenmakers, Eric-Jan},
	urldate = {2020-09-29},
	date = {2019-06-01},
	langid = {english},
	file = {Stefan_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Stefan_etal_2019.pdf:application/pdf},
}

@article{mcphetres_what_2020,
	title = {What should a preregistration contain?},
	url = {https://psyarxiv.com/cj5mh/},
	doi = {10.31234/osf.io/cj5mh},
	abstract = {A large amount of variation exists in beliefs about the purpose and benefits of preregistration, making it difficult to implement and evaluate, and limiting its usefulness. Additionally, no single resource exists to describe what a preregistration should contain or how it should be used. In this paper, I describe what an effective preregistration should contain and when it should be used. Specifically, preregistration should 1) restrict as many researcher degrees of freedom as possible, 2) detail all aspects of a study’s method and analysis, 3) detail information on decisions made during the planning stages, and 4) specify how the results will be used and interpreted. Further, a preregistration must be publicly verifiable and permanent. Finally, I argue that preregistration should be used in any situation where researchers intend to collect data in order to make a claim, description, decision, or inference based on that data. I also note that preregistrations which do not address each of these points do more harm than good by falsely signalling credibility and quality.},
	author = {{McPhetres}, Jonathon},
	urldate = {2020-09-23},
	date = {2020-06-01},
	note = {Publisher: {PsyArXiv}},
	file = {McPhetres_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/McPhetres_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/X69ND8EE/cj5mh.html:text/html},
}

@article{vadillo_unconscious_2020,
	title = {Unconscious or underpowered? Probabilistic cuing of visual attention.},
	volume = {149},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000632},
	doi = {10.1037/xge0000632},
	shorttitle = {Unconscious or underpowered?},
	abstract = {Recent debate about the reliability of psychological research has raised concerns about the prevalence of false positives in our discipline. However, false negatives can be just as concerning in areas of research that depend on finding support for the absence of an effect. This risk is particularly high in unconscious learning experiments, where researchers commonly seek to demonstrate that people can learn to perform a task in the absence of any explicit knowledge of the information that drives performance. The fact that some unconscious learning effects are typically studied with small samples and unreliable awareness measures makes false negatives especially likely. In the present article we focus on a popular unconscious learning paradigm, probabilistic cuing of visual attention, as a case study. First, we show that, at the meta-analytic level, previous experiments reveal positive signs of participant awareness, although individual studies are severely underpowered to detect this. Second, we report the results of 2 empirical studies in which participants’ awareness was tested with alternative and more sensitive dependent measures, both of which manifest positive evidence of awareness. We also show that, based on the predictions of a formal model of probabilistic cuing and given the reliabilities of the dependent measures collected in these experiments, any statistical test aimed at detecting a significant correlation between learning and awareness is doomed to return a nonsignificant result, even if at the latent level both constructs are actually related and participants’ knowledge is completely explicit.},
	pages = {160--181},
	number = {1},
	journaltitle = {Journal of Experimental Psychology: General},
	shortjournal = {Journal of Experimental Psychology: General},
	author = {Vadillo, Miguel A. and Linssen, Douglas and Orgaz, Cristina and Parsons, Stephanie and Shanks, David R.},
	urldate = {2020-09-22},
	date = {2020-01},
	langid = {english},
	file = {Vadillo et al. - 2020 - Unconscious or underpowered Probabilistic cuing o.pdf:/Users/tom/Zotero/storage/3C5GW6GW/Vadillo et al. - 2020 - Unconscious or underpowered Probabilistic cuing o.pdf:application/pdf},
}

@article{rosenthal_file_1979,
	title = {The file drawer problem and tolerance for null results},
	volume = {86},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/0033-2909.86.3.638},
	abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {638--641},
	number = {3},
	journaltitle = {Psychological Bulletin},
	author = {Rosenthal, Robert},
	date = {1979},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Statistical Probability, Experimentation, Scientific Communication, Statistical Tests, Type I Errors},
	file = {Snapshot:/Users/tom/Zotero/storage/C6E48WMI/1979-27602-001.html:text/html},
}

@article{oboyle_chrysalis_2017,
	title = {The chrysalis effect: how ugly initial results metamorphosize into beautiful articles},
	volume = {43},
	issn = {0149-2063},
	url = {https://doi.org/10.1177/0149206314527133},
	doi = {10.1177/0149206314527133},
	shorttitle = {The chrysalis effect},
	abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices ({QRPs}) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of {QRPs} in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the “Chrysalis Effect.”},
	pages = {376--399},
	number = {2},
	journaltitle = {Journal of Management},
	shortjournal = {Journal of Management},
	author = {O’Boyle, Ernest Hugh and Banks, George Christopher and Gonzalez-Mulé, Erik},
	urldate = {2020-09-22},
	date = {2017-02-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@article{botvinik-nezer_variability_2020,
	title = {Variability in the analysis of a single neuroimaging dataset by many teams},
	volume = {582},
	rights = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2314-9},
	doi = {10.1038/s41586-020-2314-9},
	abstract = {Data analysis workflows in many scientific domains have become increasingly complex and flexible. Here we assess the effect of this flexibility on the results of functional magnetic resonance imaging by asking 70 independent teams to analyse the same dataset, testing the same 9 ex-ante hypotheses1. The flexibility of analytical approaches is exemplified by the fact that no two teams chose identical workflows to analyse the data. This flexibility resulted in sizeable variation in the results of hypothesis tests, even for teams whose statistical maps were highly correlated at intermediate stages of the analysis pipeline. Variation in reported results was related to several aspects of analysis methodology. Notably, a meta-analytical approach that aggregated information across teams yielded a significant consensus in activated regions. Furthermore, prediction markets of researchers in the field revealed an overestimation of the likelihood of significant findings, even by researchers with direct knowledge of the dataset2–5. Our findings show that analytical flexibility can have substantial effects on scientific conclusions, and identify factors that may be related to variability in the analysis of functional magnetic resonance imaging. The results emphasize the importance of validating and sharing complex analysis workflows, and demonstrate the need for performing and reporting multiple analyses of the same data. Potential approaches that could be used to mitigate issues related to analytical variability are discussed.},
	pages = {84--88},
	number = {7810},
	journaltitle = {Nature},
	author = {Botvinik-Nezer, Rotem and Holzmeister, Felix and Camerer, Colin F. and Dreber, Anna and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Iwanir, Roni and Mumford, Jeanette A. and Adcock, R. Alison and Avesani, Paolo and Baczkowski, Blazej M. and Bajracharya, Aahana and Bakst, Leah and Ball, Sheryl and Barilari, Marco and Bault, Nadège and Beaton, Derek and Beitner, Julia and Benoit, Roland G. and Berkers, Ruud M. W. J. and Bhanji, Jamil P. and Biswal, Bharat B. and Bobadilla-Suarez, Sebastian and Bortolini, Tiago and Bottenhorn, Katherine L. and Bowring, Alexander and Braem, Senne and Brooks, Hayley R. and Brudner, Emily G. and Calderon, Cristian B. and Camilleri, Julia A. and Castrellon, Jaime J. and Cecchetti, Luca and Cieslik, Edna C. and Cole, Zachary J. and Collignon, Olivier and Cox, Robert W. and Cunningham, William A. and Czoschke, Stefan and Dadi, Kamalaker and Davis, Charles P. and Luca, Alberto De and Delgado, Mauricio R. and Demetriou, Lysia and Dennison, Jeffrey B. and Di, Xin and Dickie, Erin W. and Dobryakova, Ekaterina and Donnat, Claire L. and Dukart, Juergen and Duncan, Niall W. and Durnez, Joke and Eed, Amr and Eickhoff, Simon B. and Erhart, Andrew and Fontanesi, Laura and Fricke, G. Matthew and Fu, Shiguang and Galván, Adriana and Gau, Remi and Genon, Sarah and Glatard, Tristan and Glerean, Enrico and Goeman, Jelle J. and Golowin, Sergej A. E. and González-García, Carlos and Gorgolewski, Krzysztof J. and Grady, Cheryl L. and Green, Mikella A. and Guassi Moreira, João F. and Guest, Olivia and Hakimi, Shabnam and Hamilton, J. Paul and Hancock, Roeland and Handjaras, Giacomo and Harry, Bronson B. and Hawco, Colin and Herholz, Peer and Herman, Gabrielle and Heunis, Stephan and Hoffstaedter, Felix and Hogeveen, Jeremy and Holmes, Susan and Hu, Chuan-Peng and Huettel, Scott A. and Hughes, Matthew E. and Iacovella, Vittorio and Iordan, Alexandru D. and Isager, Peder M. and Isik, Ayse I. and Jahn, Andrew and Johnson, Matthew R. and Johnstone, Tom and Joseph, Michael J. E. and Juliano, Anthony C. and Kable, Joseph W. and Kassinopoulos, Michalis and Koba, Cemal and Kong, Xiang-Zhen and Koscik, Timothy R. and Kucukboyaci, Nuri Erkut and Kuhl, Brice A. and Kupek, Sebastian and Laird, Angela R. and Lamm, Claus and Langner, Robert and Lauharatanahirun, Nina and Lee, Hongmi and Lee, Sangil and Leemans, Alexander and Leo, Andrea and Lesage, Elise and Li, Flora and Li, Monica Y. C. and Lim, Phui Cheng and Lintz, Evan N. and Liphardt, Schuyler W. and Losecaat Vermeer, Annabel B. and Love, Bradley C. and Mack, Michael L. and Malpica, Norberto and Marins, Theo and Maumet, Camille and {McDonald}, Kelsey and {McGuire}, Joseph T. and Melero, Helena and Méndez Leal, Adriana S. and Meyer, Benjamin and Meyer, Kristin N. and Mihai, Glad and Mitsis, Georgios D. and Moll, Jorge and Nielson, Dylan M. and Nilsonne, Gustav and Notter, Michael P. and Olivetti, Emanuele and Onicas, Adrian I. and Papale, Paolo and Patil, Kaustubh R. and Peelle, Jonathan E. and Pérez, Alexandre and Pischedda, Doris and Poline, Jean-Baptiste and Prystauka, Yanina and Ray, Shruti and Reuter-Lorenz, Patricia A. and Reynolds, Richard C. and Ricciardi, Emiliano and Rieck, Jenny R. and Rodriguez-Thompson, Anais M. and Romyn, Anthony and Salo, Taylor and Samanez-Larkin, Gregory R. and Sanz-Morales, Emilio and Schlichting, Margaret L. and Schultz, Douglas H. and Shen, Qiang and Sheridan, Margaret A. and Silvers, Jennifer A. and Skagerlund, Kenny and Smith, Alec and Smith, David V. and Sokol-Hessner, Peter and Steinkamp, Simon R. and Tashjian, Sarah M. and Thirion, Bertrand and Thorp, John N. and Tinghög, Gustav and Tisdall, Loreen and Tompson, Steven H. and Toro-Serey, Claudio and Torre Tresols, Juan Jesus and Tozzi, Leonardo and Truong, Vuong and Turella, Luca and van ‘t Veer, Anna E. and Verguts, Tom and Vettel, Jean M. and Vijayarajah, Sagana and Vo, Khoi and Wall, Matthew B. and Weeda, Wouter D. and Weis, Susanne and White, David J. and Wisniewski, David and Xifra-Porxas, Alba and Yearling, Emily A. and Yoon, Sangsuk and Yuan, Rui and Yuen, Kenneth S. L. and Zhang, Lei and Zhang, Xu and Zosky, Joshua E. and Nichols, Thomas E. and Poldrack, Russell A. and Schonberg, Tom},
	urldate = {2020-09-22},
	date = {2020-06},
	langid = {english},
	note = {Number: 7810
Publisher: Nature Publishing Group},
	file = {Botvinik-Nezer_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Botvinik-Nezer_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GJT2VLYQ/s41586-020-2314-9.html:text/html},
}

@article{wiseman_registered_2019,
	title = {Registered reports: an early example and analysis},
	volume = {7},
	issn = {2167-8359},
	url = {https://peerj.com/articles/6232},
	doi = {10.7717/peerj.6232},
	shorttitle = {Registered reports},
	abstract = {The recent ‘replication crisis’ in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting ‘Registered Reports’, wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology ({EJP}). A retrospective and observational comparison of Registered and non-Registered Reports published in the {EJP} during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson’s pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.},
	pages = {e6232},
	journaltitle = {{PeerJ}},
	shortjournal = {{PeerJ}},
	author = {Wiseman, Richard and Watt, Caroline and Kornbrot, Diana},
	urldate = {2020-09-21},
	date = {2019-01-16},
	langid = {english},
	note = {Publisher: {PeerJ} Inc.},
	file = {Snapshot:/Users/tom/Zotero/storage/C3CHJW4P/6232.html:text/html;Wiseman_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wiseman_etal_2019.pdf:application/pdf},
}

@article{franco_publication_2014,
	title = {Publication bias in the social sciences: Unlocking the file drawer},
	volume = {345},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1255484},
	doi = {10.1126/science.1255484},
	shorttitle = {Publication bias in the social sciences},
	pages = {1502--1505},
	number = {6203},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Franco, A. and Malhotra, N. and Simonovits, G.},
	urldate = {2020-09-21},
	date = {2014-09-19},
	langid = {english},
	file = {Franco et al. - 2014 - Publication bias in the social sciences Unlocking.pdf:/Users/tom/Zotero/storage/4HTJU2X9/Franco et al. - 2014 - Publication bias in the social sciences Unlocking.pdf:application/pdf},
}

@article{chambers_protocol_2018,
	title = {Protocol transparency is vital for registered reports},
	volume = {2},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-018-0449-6},
	doi = {10.1038/s41562-018-0449-6},
	pages = {791--792},
	number = {11},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Chambers, Christopher D. and Mellor, David T.},
	urldate = {2020-09-21},
	date = {2018-11},
	langid = {english},
	file = {Chambers and Mellor - 2018 - Protocol transparency is vital for registered repo.pdf:/Users/tom/Zotero/storage/48A7FAYA/Chambers and Mellor - 2018 - Protocol transparency is vital for registered repo.pdf:application/pdf},
}

@article{hardwicke_mapping_2018,
	title = {Mapping the universe of registered reports},
	volume = {2},
	rights = {2018 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0444-y},
	doi = {10.1038/s41562-018-0444-y},
	abstract = {Registered reports present a substantial departure from traditional publishing models with the goal of enhancing the transparency and credibility of the scientific literature. We map the evolving universe of registered reports to assess their growth, implementation and shortcomings at journals across scientific disciplines.},
	pages = {793--796},
	number = {11},
	journaltitle = {Nature Human Behaviour},
	author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
	urldate = {2020-09-21},
	date = {2018-11},
	langid = {english},
	note = {Number: 11
Publisher: Nature Publishing Group},
	file = {Hardwicke and Ioannidis - 2018 - Mapping the universe of registered reports.pdf:/Users/tom/Zotero/storage/37VYP33X/Hardwicke and Ioannidis - 2018 - Mapping the universe of registered reports.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/948GTQQ9/s41562-018-0444-y.html:text/html},
}

@online{rouder_invariances_2018,
	title = {Invariances: Preregistration: Try it (Or not)},
	url = {https://jeffrouder.blogspot.com/2018/11/preregistration-try-it-to-see-if-you.html},
	shorttitle = {Invariances},
	titleaddon = {Invariances},
	author = {Rouder, Jeff},
	urldate = {2020-09-21},
	date = {2018-11-19},
	file = {Blogspot Snapshot:/Users/tom/Zotero/storage/BR7FID5G/preregistration-try-it-to-see-if-you.html:text/html},
}

@article{ware_significance_2015,
	title = {Significance chasing in research practice: causes, consequences and possible solutions},
	volume = {110},
	rights = {© 2014 Society for the Study of Addiction},
	issn = {1360-0443},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/add.12673},
	doi = {10.1111/add.12673},
	shorttitle = {Significance chasing in research practice},
	abstract = {Background and Aims The low reproducibility of findings within the scientific literature is a growing concern. This may be due to many findings being false positives which, in turn, can misdirect research effort and waste money. Methods We review factors that may contribute to poor study reproducibility and an excess of ‘significant’ findings within the published literature. Specifically, we consider the influence of current incentive structures and the impact of these on research practices. Results The prevalence of false positives within the literature may be attributable to a number of questionable research practices, ranging from the relatively innocent and minor (e.g. unplanned post-hoc tests) to the calculated and serious (e.g. fabrication of data). These practices may be driven by current incentive structures (e.g. pressure to publish), alongside the preferential emphasis placed by journals on novelty over veracity. There are a number of potential solutions to poor reproducibility, such as new publishing formats that emphasize the research question and study design, rather than the results obtained. This has the potential to minimize significance chasing and non-publication of null findings. Conclusions Significance chasing, questionable research practices and poor study reproducibility are the unfortunate consequence of a ‘publish or perish’ culture and a preference among journals for novel findings. It is likely that top–down change implemented by those with the ability to modify current incentive structure (e.g. funders and journals) will be required to address problems of poor reproducibility.},
	pages = {4--8},
	number = {1},
	journaltitle = {Addiction},
	author = {Ware, Jennifer J. and Munafò, Marcus R.},
	urldate = {2020-09-21},
	date = {2015},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/add.12673},
	keywords = {reproducibility, publication bias, significance chasing, fraud, False positive, pre-registration},
	file = {Snapshot:/Users/tom/Zotero/storage/QFYLSUE4/add.html:text/html;Ware_Munafò_2015.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ware_Munafò_2015.pdf:application/pdf},
}

@article{ioannidis_handling_2015,
	title = {Handling the fragile vase of scientific practices},
	volume = {110},
	rights = {© 2014 Society for the Study of Addiction},
	issn = {1360-0443},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/add.12720},
	doi = {10.1111/add.12720},
	pages = {9--10},
	number = {1},
	journaltitle = {Addiction},
	author = {Ioannidis, John P. A.},
	urldate = {2020-09-21},
	date = {2015},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/add.12720},
	keywords = {reproducibility, bias, registration, pre-registration, peer review, reporting, research},
	file = {Ioannidis_2015.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ioannidis_2015.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/794U9SUY/add.html:text/html},
}

@article{berlin_measuring_1999,
	title = {Measuring the quality of trials: the quality of quality scales},
	volume = {282},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/191580},
	doi = {10.1001/jama.282.11.1083},
	shorttitle = {Measuring the quality of trials},
	abstract = {Physicians seeking the best information about particular interventions
often turn to the results of meta-analyses. Meta-analyses, if done correctly
according to explicit rules, will include all relevant studies that meet specified
criteria, even those unpublished, to produce an unbiased estimate...},
	pages = {1083--1085},
	number = {11},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Berlin, Jesse A. and Rennie, Drummond},
	urldate = {2020-09-21},
	date = {1999-09-15},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Berlin_Rennie_1999.pdf:/Users/tom/pCloud Drive/Zotero_Library/Berlin_Rennie_1999.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GLH6K4YD/191580.html:text/html},
}

@article{edwards_disconfirmation_1996,
	title = {A disconfirmation bias in the evaluation of arguments},
	volume = {71},
	pages = {5--24},
	number = {1},
	journaltitle = {Attitudes and Social Cognition},
	author = {Edwards, Kari and Smith, Edward E},
	date = {1996},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Edwards and Smith - ATTITUDES AND SOCIAL COGNITION.pdf:/Users/tom/Zotero/storage/JJGYWTYE/Edwards and Smith - ATTITUDES AND SOCIAL COGNITION.pdf:application/pdf},
}

@article{mynatt_confirmation_1977,
	title = {Confirmation bias in a simulated research environment: an experimental study of scientific inference:},
	rights = {© 1977 Experimental Pscyhology Society},
	url = {https://journals.sagepub.com/doi/10.1080/00335557743000053},
	doi = {10.1080/00335557743000053},
	shorttitle = {Confirmation bias in a simulated research environment},
	abstract = {Numerous authors (e.g., Popper, 1959) argue that scientists should try to falsify rather than confirm theories. However, recent empirical work (Wason and Johnso...},
	journaltitle = {Quarterly Journal of Experimental Psychology},
	author = {Mynatt, Clifford R. and Doherty, Michael E. and Tweney, Ryan D.},
	urldate = {2020-09-21},
	date = {1977},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {UK}: London, England},
	file = {Mynatt_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Mynatt_etal_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JWNNWZ2Y/00335557743000053.html:text/html},
}

@article{fugelsang_theory_2004,
	title = {Theory and data interactions of the scientific mind: evidence from the molecular and the cognitive laboratory},
	volume = {58},
	issn = {1196-1961},
	doi = {10.1037/h0085799},
	shorttitle = {Theory and data interactions of the scientific mind},
	abstract = {A number of researchers and scholars have stressed the importance of disconfirmation in the quest for the development of scientific knowledge (e.g., Popper, 1959). Paradoxically, studies examining human reasoning in the laboratory have typically found that people display a confirmation bias in that they are more likely to seek out and attend to data consistent rather than data inconsistent with their initial theory (Wason, 1968). We examine the strategies that scientists and students use to evaluate data that are either consistent or inconsistent with their expectations. First, we present findings from scientists reasoning "live" in their laboratory meetings. We show that scientists often show an initial reluctance to consider inconsistent data as "real." However, this initial reluctance is often overcome with repeated observations of the inconsistent data such that they modify their theories to account for the new data. We further examine these issues in a controlled scientific causal thinking simulation specifically developed to examine the reasoning strategies we observed in the natural scientific environment. Like the scientists, we found that participants in our simulation initially displayed a propensity to discount data inconsistent with a theory provided. However, with repeated observations of the inconsistent data, the students, like the scientists, began to see the once anomalous data as "real" and the initial bias to discount that data was significantly diminished.},
	pages = {86--95},
	number = {2},
	journaltitle = {Canadian Journal of Experimental Psychology = Revue Canadienne De Psychologie Experimentale},
	shortjournal = {Can J Exp Psychol},
	author = {Fugelsang, Jonathan A. and Stein, Courtney B. and Green, Adam E. and Dunbar, Kevin N.},
	date = {2004-06},
	pmid = {15285598},
	keywords = {Humans, Science, Adult, Analysis of Variance, Causality, Cognitive Science, Female, Male, Molecular Biology, Psychological Theory, Thinking},
	file = {Fugelsang_etal_2004.pdf:/Users/tom/pCloud Drive/Zotero_Library/Fugelsang_etal_2004.pdf:application/pdf},
}

@article{brewer_scientists_1994,
	title = {Scientists' responses to anomalous data: evidence from psychology, history, and philosophy of science},
	volume = {1994},
	issn = {0270-8647},
	url = {https://www.jstor.org/stable/193035},
	doi = {10.1086/psaprocbienmeetp.1994.1.193035},
	shorttitle = {Scientists' responses to anomalous data},
	abstract = {This paper presents an analysis of the forms of response that scientists make when confronted with anomalous data. We postulate that there are seven ways in which an individual who currently holds a theory can respond to anomalous data: (1) ignore the data; (2) reject the data; (3) exclude the data from the domain of the current theory; (4) hold the data in abeyance; (5) reinterpret the data; (6) make peripheral changes to the current theory; or (7) change the theory. We analyze psychological experiments and cases from the history of science to support this proposal. Implications for the philosophy of science are discussed.},
	pages = {304--313},
	journaltitle = {{PSA}: Proceedings of the Biennial Meeting of the Philosophy of Science Association},
	author = {Brewer, William F. and Chinn, Clark A.},
	urldate = {2020-09-21},
	date = {1994},
	note = {Publisher: [University of Chicago Press, Springer, Philosophy of Science Association]},
	file = {Brewer_Chinn_1994.pdf:/Users/tom/pCloud Drive/Zotero_Library/Brewer_Chinn_1994.pdf:application/pdf},
}

@article{strevens_notes_nodate,
	title = {Notes on Bayesian Con rmation Theory},
	pages = {151},
	author = {Strevens, Michael},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Strevens - Notes on Bayesian Con rmation Theory.pdf:/Users/tom/Zotero/storage/JQ5PRJBQ/Strevens - Notes on Bayesian Con rmation Theory.pdf:application/pdf},
}

@article{strevens_bayesian_2005,
	title = {The Bayesian Treatment of Auxiliary Hypotheses: Reply to Fitelson and Waterman},
	volume = {56},
	issn = {1464-3537, 0007-0882},
	url = {http://academic.oup.com/bjps/article/56/4/913/1451509/The-Bayesian-Treatment-of-Auxiliary-Hypotheses},
	doi = {10.1093/bjps/axi133},
	shorttitle = {The Bayesian Treatment of Auxiliary Hypotheses},
	pages = {913--918},
	number = {4},
	journaltitle = {The British Journal for the Philosophy of Science},
	author = {Strevens, Michael},
	urldate = {2020-09-21},
	date = {2005-12-01},
	langid = {english},
	file = {Strevens - 2005 - The Bayesian Treatment of Auxiliary Hypotheses Re.pdf:/Users/tom/Zotero/storage/IDTH2LLS/Strevens - 2005 - The Bayesian Treatment of Auxiliary Hypotheses Re.pdf:application/pdf},
}

@article{colquhoun_scoping_2014,
	title = {Scoping reviews: time for clarity in definition, methods, and reporting},
	volume = {67},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435614002108},
	doi = {10.1016/j.jclinepi.2014.03.013},
	shorttitle = {Scoping reviews},
	abstract = {Objectives
The scoping review has become increasingly popular as a form of knowledge synthesis. However, a lack of consensus on scoping review terminology, definition, methodology, and reporting limits the potential of this form of synthesis. In this article, we propose recommendations to further advance the field of scoping review methodology.
Study Design and Setting
We summarize current understanding of scoping review publication rates, terms, definitions, and methods. We propose three recommendations for clarity in term, definition and methodology.
Results
We recommend adopting the terms “scoping review” or “scoping study” and the use of a proposed definition. Until such time as further guidance is developed, we recommend the use of the methodological steps outlined in the Arksey and O'Malley framework and further enhanced by Levac et al. The development of reporting guidance for the conduct and reporting of scoping reviews is underway.
Conclusion
Consistency in the proposed domains and methodologies of scoping reviews, along with the development of reporting guidance, will facilitate methodological advancement, reduce confusion, facilitate collaboration and improve knowledge translation of scoping review findings.},
	pages = {1291--1294},
	number = {12},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Colquhoun, Heather L. and Levac, Danielle and O'Brien, Kelly K. and Straus, Sharon and Tricco, Andrea C. and Perrier, Laure and Kastner, Monika and Moher, David},
	urldate = {2020-09-21},
	date = {2014-12-01},
	langid = {english},
	keywords = {Reporting, Methodology, {EQUATOR}, Knowledge synthesis, Scoping review, Scoping study, Terminology},
	file = {Colquhoun_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Colquhoun_etal_2014.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/AZS78KMR/S0895435614002108.html:text/html},
}

@article{grand_outcome_2018,
	title = {From Outcome to Process Focus: Fostering a More Robust Psychological Science Through Registered Reports and Results-Blind Reviewing},
	volume = {13},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691618767883},
	doi = {10.1177/1745691618767883},
	shorttitle = {From Outcome to Process Focus},
	abstract = {A variety of alternative mechanisms, strategies, and “ways of doing” have been proposed for improving the rigor and robustness of published research in the psychological sciences in recent years. In this article, we describe two existing but underused publication models—registered reporting ({RR}) and results-blind reviewing ({RBR})—that we believe would contribute in important ways to improving both the conduct and evaluation of psychological research. We first outline the procedures and distinguishing features of both publication pathways and note their value for promoting positive changes to current scientific practices. We posit that a significant value of {RR} and {RBR} is their potential to promote a greater focus on the research process (i.e., how and why research is conducted) relative to research outcomes (i.e., what was observed or concluded from research). We conclude by discussing what we perceive to be five common beliefs about {RR} and {RBR} practices and attempt to provide a balanced perspective of the realities likely to be experienced with these systems.},
	pages = {448--456},
	number = {4},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Grand, James A. and Rogelberg, Steven G. and Banks, George C. and Landis, Ronald S. and Tonidandel, Scott},
	urldate = {2020-09-21},
	date = {2018-07},
	langid = {english},
	file = {Grand et al. - 2018 - From Outcome to Process Focus Fostering a More Ro.pdf:/Users/tom/Zotero/storage/5XNFD36L/Grand et al. - 2018 - From Outcome to Process Focus Fostering a More Ro.pdf:application/pdf},
}

@article{vries_cumulative_2018,
	title = {The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression},
	volume = {48},
	issn = {0033-2917, 1469-8978},
	url = {https://www.cambridge.org/core/journals/psychological-medicine/article/cumulative-effect-of-reporting-and-citation-biases-on-the-apparent-efficacy-of-treatments-the-case-of-depression/71D73CADE32C0D3D996DABEA3FCDBF57},
	doi = {10.1017/S0033291718001873},
	shorttitle = {The cumulative effect of reporting and citation biases on the apparent efficacy of treatments},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0033291718001873/resource/name/{firstPage}-S0033291718001873a.jpg},
	pages = {2453--2455},
	number = {15},
	journaltitle = {Psychological Medicine},
	author = {Vries, Y. A. de and Roest, A. M. and Jonge, P. de and Cuijpers, P. and Munafò, M. R. and Bastiaansen, J. A.},
	urldate = {2020-09-21},
	date = {2018-11},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {bias, Antidepressants, citation bias, depression, psychotherapy, reporting bias},
	file = {Snapshot:/Users/tom/Zotero/storage/M9UJM556/71D73CADE32C0D3D996DABEA3FCDBF57.html:text/html;Vries_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vries_etal_2018.pdf:application/pdf},
}

@article{stoll_discrepancies_2020,
	title = {Discrepancies from registered protocols and spin occurred frequently in randomized psychotherapy trials—A meta-epidemiologic study},
	volume = {128},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435620302080},
	doi = {10.1016/j.jclinepi.2020.08.013},
	abstract = {Objectives
This study aimed to investigate the relationship between trial registration, trial discrepancy from registered protocol, and spin in nonpharmacological trials.
Study Design and Setting
Recent psychotherapy trials on depression (2015–2018) were analyzed regarding their registration status and its relationship to discrepancies between registered and published primary outcomes and to spin (discrepancy between the nonsignificant finding in a study and an overly beneficial interpretation of the effect of the treatment).
Results
A total of 196 trials were identified, of which 78 (40\%) had been registered prospectively and 56 (29\%) had been registered retrospectively. In 102 (76\%) of 134 registered trials, discrepancies between trial and protocol were present. Of 72 trials with a nonsignificant difference between treatments for the primary outcome, 68 trials (94\%) showed spin. Discrepancies from protocol were less frequent in prospectively than in retrospectively registered trials (odds ratio= 0.19; 95\% confidence interval [{CI}]: 0.07–0.52), but regarding the amount of spin, there was no difference between prospectively and retrospectively registered trials (rb = −0.12; 95\% {CI}: −0.41 to 0.19) or between registered and unregistered trials (rb = −0.22, 95\% {CI} −0.49 to 0.08).
Conclusion
Protocol discrepancies and spin have a high prevalence in psychotherapy outcome research. The results show no relation between registration and spin, but prospective registration may prevent discrepancies from protocol.},
	pages = {49--56},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Stoll, Marlene and Mancini, Alexander and Hubenschmid, Lara and Dreimüller, Nadine and König, Jochem and Cuijpers, Pim and Barth, Jürgen and Lieb, Klaus},
	urldate = {2020-09-21},
	date = {2020-12-01},
	langid = {english},
	keywords = {Reporting bias, Conflict of interest, Depression, Psychotherapy, Review, Spin in research},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/LDX6G87Q/S0895435620302080.html:text/html;Stoll_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Stoll_etal_2020.pdf:application/pdf},
}

@article{goldin-meadow_why_2016,
	title = {Why Preregistration Makes Me Nervous},
	volume = {29},
	url = {https://www.psychologicalscience.org/observer/why-preregistration-makes-me-nervous},
	abstract = {I must admit that when I first heard of the effort to get psychological scientists to preregister their studies (that is, to submit to a journal a study’s hypotheses and a plan for how the …},
	number = {7},
	journaltitle = {{APS} Observer},
	author = {Goldin-Meadow, Susan},
	urldate = {2020-09-21},
	date = {2016-08-31},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/8HPLZZCF/why-preregistration-makes-me-nervous.html:text/html},
}

@article{finkel_best_2015,
	title = {Best research practices in psychology: Illustrating epistemological and pragmatic considerations with the case of relationship science},
	volume = {108},
	issn = {1939-1315(Electronic),0022-3514(Print)},
	doi = {10.1037/pspi0000007},
	shorttitle = {Best research practices in psychology},
	abstract = {In recent years, a robust movement has emerged within psychology to increase the evidentiary value of our science. This movement, which has analogs throughout the empirical sciences, is broad and diverse, but its primary emphasis has been on the reduction of statistical false positives. The present article addresses epistemological and pragmatic issues that we, as a field, must consider as we seek to maximize the scientific value of this movement. Regarding epistemology, this article contrasts the false-positives-reduction ({FPR}) approach with an alternative, the error balance ({EB}) approach, which argues that any serious consideration of optimal scientific practice must contend simultaneously with both false-positive and false-negative errors. Regarding pragmatics, the movement has devoted a great deal of attention to issues that frequently arise in laboratory experiments and one-shot survey studies, but it has devoted less attention to issues that frequently arise in intensive and/or longitudinal studies. We illustrate these epistemological and pragmatic considerations with the case of relationship science, one of the many research domains that frequently employ intensive and/or longitudinal methods. Specifically, we examine 6 research prescriptions that can help to reduce false-positive rates: preregistration, prepublication sharing of materials, postpublication sharing of data, close replication, avoiding piecemeal publication, and increasing sample size. For each, we offer concrete guidance not only regarding how researchers can improve their research practices and balance the risk of false-positive and false-negative errors, but also how the movement can capitalize upon insights from research practices within relationship science to make the movement stronger and more inclusive. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {275--297},
	number = {2},
	journaltitle = {Journal of Personality and Social Psychology},
	author = {Finkel, Eli J. and Eastwick, Paul W. and Reis, Harry T.},
	date = {2015},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Experimentation, Type I Errors, Best Practices, Epistemology, Interpersonal Relationships, Social Psychology},
	file = {Finkel_etal_2015.pdf:/Users/tom/pCloud Drive/Zotero_Library/Finkel_etal_2015.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RE7CZ5WZ/2015-02331-005.html:text/html},
}

@article{kunda_case_1990,
	title = {The case for motivated reasoning.},
	volume = {108},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.108.3.480},
	doi = {10.1037/0033-2909.108.3.480},
	pages = {480--498},
	number = {3},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Kunda, Ziva},
	urldate = {2020-09-21},
	date = {1990},
	langid = {english},
	file = {Kunda_1990.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kunda_1990.pdf:application/pdf},
}

@article{bakan_test_1966,
	title = {The test of significance in psychological research},
	volume = {66},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/h0020412},
	abstract = {The test of significance does not provide the information concerning psychological phenomena characteristically attributed to it; and a great deal of mischief has been associated with its use. The basic logic associated with the test of significance is reviewed. The null hypothesis is characteristically false under any circumstances. Publication practices foster the reporting of small effects in populations. Psychologists have "adjusted" by misinterpretation, taking the p value as a "measure," assuming that the test of significance provides automaticity of inference, and confusing the aggregate with the general. The difficulties are illuminated by bringing to bear the contributions from the decision-theory school on the Fisher approach. The Bayesian approach is suggested. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {423--437},
	number = {6},
	journaltitle = {Psychological Bulletin},
	author = {Bakan, David},
	date = {1966},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Statistical Analysis, Experimentation, Statistical Tests, Null Hypothesis Testing, Statistical Significance},
	file = {Bakan_1966.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bakan_1966.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/QF32VXAZ/2005-10045-001.html:text/html},
}

@collection{robertson_blinding_2016,
	location = {London, {UK} ; San Diego, {CA}, {USA}},
	title = {Blinding as a solution to bias: strengthening biomedical science, forensic science, and law},
	isbn = {978-0-12-802460-7},
	shorttitle = {Blinding as a solution to bias},
	pagetotal = {371},
	publisher = {Academic Press is an imprint of Elsevier},
	editor = {Robertson, Christopher T. and Kesselheim, Aaron S.},
	date = {2016},
	note = {{OCLC}: ocn932174343},
	keywords = {United States, Bias (Law), Discrimination, Prejudices, Social perception, Stereotypes (Social psychology)},
	file = {Robertson and Kesselheim - 2016 - Blinding as a solution to bias strengthening biom.pdf:/Users/tom/Zotero/storage/SRT65F2W/Robertson and Kesselheim - 2016 - Blinding as a solution to bias strengthening biom.pdf:application/pdf},
}

@article{may_g_s_randomized_1981,
	title = {The randomized clinical trial: bias in analysis.},
	volume = {64},
	url = {https://www.ahajournals.org/doi/10.1161/01.cir.64.4.669},
	doi = {10.1161/01.CIR.64.4.669},
	shorttitle = {The randomized clinical trial},
	abstract = {The realization that bias in patient selection may influence the results of clinical studies has helped to establish the randomized controlled clinical trial in medical research. However, bias can be equally important at other stages of a trial, especially at the time of analysis. Withdrawing patients from consideration in the analysis because of ineligibility on account of study entry criteria, lack of compliance to the protocol, or data of poor quality may be a source of systematic error. Examples to illustrate the possible consequences are taken from trials in the cardiovascular field. We recommended that reported study results should include outcome data from all subjects randomized in the group to which they were originally assigned.},
	pages = {669--673},
	number = {4},
	journaltitle = {Circulation},
	shortjournal = {Circulation},
	author = {{May G S} and {DeMets D L} and {Friedman L M} and {Furberg C} and {Passamani E}},
	urldate = {2020-09-21},
	date = {1981-10-01},
	note = {Publisher: American Heart Association},
	file = {May G S_etal_1981.pdf:/Users/tom/pCloud Drive/Zotero_Library/May G S_etal_1981.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DS2TYVMY/01.cir.64.4.html:text/html},
}

@article{patel_assessment_2015,
	title = {Assessment of vibration of effects due to model specification can demonstrate the instability of observational associations},
	volume = {68},
	issn = {0895-4356},
	url = {http://www.sciencedirect.com/science/article/pii/S0895435615002772},
	doi = {10.1016/j.jclinepi.2015.05.029},
	abstract = {Objectives
Model specification—what adjusting variables are analytically modeled—may influence results of observational associations. We present a standardized approach to quantify the variability of results obtained with choices of adjustments called the “vibration of effects” ({VoE}).
Study Design and Setting
We estimated the {VoE} for 417 clinical, environmental, and physiological variables in association with all-cause mortality using National Health and Nutrition Examination Survey data. We selected 13 variables as adjustment covariates and computed 8,192 Cox models for each of 417 variables' associations with all-cause mortality.
Results
We present the {VoE} by assessing the variance of the effect size and in the −log10(P-value) obtained by different combinations of adjustments. We present whether there are multimodality patterns in effect sizes and P-values and the trajectory of results with increasing adjustments. For 31\% of the 417 variables, we observed a Janus effect, with the effect being in opposite direction in the 99th versus the 1st percentile of analyses. For example, the vitamin E variant α-tocopherol had a {VoE} that indicated higher and lower risk for mortality.
Conclusion
Estimating {VoE} offers empirical estimates of associations are under different model specifications. When {VoE} is large, claims for observational associations should be very cautious.},
	pages = {1046--1058},
	number = {9},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Patel, Chirag J. and Burford, Belinda and Ioannidis, John P. A.},
	urldate = {2020-09-21},
	date = {2015-09-01},
	langid = {english},
	keywords = {Biostatistics, Confounding, Environment-wide association study, Model specification, Observational association, Vibration of effects},
	file = {Patel_etal_2015.pdf:/Users/tom/pCloud Drive/Zotero_Library/Patel_etal_2015.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/L9ISCYNU/S0895435615002772.html:text/html},
}

@article{simonsohn_specification_2015,
	title = {Specification curve: descriptive and inferential statistics on all reasonable specifications},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2694998},
	doi = {10.2139/ssrn.2694998},
	shorttitle = {Specification curve},
	abstract = {Empirical results hinge on analytic decisions that are defensible, arbitrary, and motivated. These decisions probably introduce bias (towards the narrative put forward by the authors), and certainly involve variability not reflected by standard errors. To address this source of noise and bias, we introduce Specification Curve Analysis, which consists of three steps: (i) identifying the set of theoretically justified, statistically valid, and non-redundant specifications, (ii) displaying the results graphically, allowing readers to identify consequential specifications decisions, and (iii) conducting joint inference across all specifications. We illustrate the usefulness of this technique by applying specification curve to three findings from two different papers, one investigating discrimination based on distinctively black names, and the other investigating the effect of assigning female vs. male names to hurricanes). Specification curve reveals that one finding is robust, one is weak, one is not robust at all.},
	journaltitle = {{SSRN} Electronic Journal},
	shortjournal = {{SSRN} Journal},
	author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
	urldate = {2020-09-21},
	date = {2015},
	langid = {english},
	file = {Simonsohn et al. - 2015 - Specification Curve Descriptive and Inferential S.pdf:/Users/tom/Zotero/storage/HHJJSWU7/Simonsohn et al. - 2015 - Specification Curve Descriptive and Inferential S.pdf:application/pdf},
}

@article{maccoun_blind_2015,
	title = {Blind analysis: Hide results to seek the truth},
	volume = {526},
	url = {http://www.nature.com/news/blind-analysis-hide-results-to-seek-the-truth-1.18510},
	doi = {10.1038/526187a},
	shorttitle = {Blind analysis},
	abstract = {More fields should, like particle physics, adopt blind analysis to thwart bias, urge Robert {MacCoun} and Saul Perlmutter.},
	pages = {187--189},
	number = {7572},
	journaltitle = {Nature News},
	author = {{MacCoun}, Robert and Perlmutter, Saul},
	urldate = {2020-09-21},
	date = {2015-10-08},
	langid = {english},
	note = {Section: Comment},
	file = {MacCoun_Perlmutter_2015.pdf:/Users/tom/pCloud Drive/Zotero_Library/MacCoun_Perlmutter_2015.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/4P4WRIM3/blind-analysis-hide-results-to-seek-the-truth-1.html:text/html},
}

@article{gotzsche_blinding_1996,
	title = {Blinding during data analysis and writing of manuscripts},
	volume = {17},
	issn = {0197-2456},
	url = {http://www.sciencedirect.com/science/article/pii/0197245695002634},
	doi = {10.1016/0197-2456(95)00263-4},
	pages = {285--290},
	number = {4},
	journaltitle = {Controlled Clinical Trials},
	shortjournal = {Controlled Clinical Trials},
	author = {Gøtzsche, Peter C.},
	urldate = {2020-09-21},
	date = {1996-08-01},
	langid = {english},
	file = {Gøtzsche_1996.pdf:/Users/tom/pCloud Drive/Zotero_Library/Gøtzsche_1996.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/TK5ZMRRK/0197245695002634.html:text/html},
}

@article{greenland_bias_2001,
	title = {On the bias produced by quality scores in meta‐analysis, and a hierarchical view of proposed solutions},
	volume = {2},
	issn = {1465-4644},
	url = {https://academic.oup.com/biostatistics/article/2/4/463/321492},
	doi = {10.1093/biostatistics/2.4.463},
	abstract = {Abstract.  Results from better quality studies should in some sense be more valid or more accurate than results from other studies, and as a consequence should},
	pages = {463--471},
	number = {4},
	journaltitle = {Biostatistics},
	shortjournal = {Biostatistics},
	author = {Greenland, Sander and O'rourke, Keith},
	urldate = {2020-09-21},
	date = {2001-12-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Greenland_O'rourke_2001.pdf:/Users/tom/pCloud Drive/Zotero_Library/Greenland_O'rourke_2001.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/NC7WSHM5/321492.html:text/html},
}

@article{juni_assessing_2001,
	title = {Assessing the quality of controlled clinical trials},
	volume = {323},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1120670/},
	pages = {42--46},
	number = {7303},
	journaltitle = {{BMJ} : British Medical Journal},
	shortjournal = {{BMJ}},
	author = {Jüni, Peter and Altman, Douglas G and Egger, Matthias},
	urldate = {2020-09-21},
	date = {2001-07-07},
	pmid = {11440947},
	pmcid = {PMC1120670},
	file = {Jüni_etal_2001.pdf:/Users/tom/pCloud Drive/Zotero_Library/Jüni_etal_2001.pdf:application/pdf},
}

@article{rohrer_subtle_2015,
	title = {Do subtle reminders of money change people’s political views?},
	volume = {144},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000058},
	doi = {10.1037/xge0000058},
	abstract = {A number of researchers have reported studies showing that subtle reminders of money can alter behaviors and beliefs that are seemingly unrelated to money. In 1 set of studies published in this journal, Caruso, Vohs, Baxter, and Waytz (2013) found that incidental exposures to money led subjects to indicate greater support for inequality, socioeconomic differences, group-based discrimination, and free market economies. We conducted high-powered replication attempts of these 4 money priming effects and found no evidence of priming (weighted Cohen’s d ϭ 0.03). We later learned that Caruso et al. also found several null effects in their line of research that were not reported in the original article. In addition, the money priming effect observed in the first study of Caruso et al. was included in the Many Labs Replication Project (Klein et al., 2014), and only 1 of the 36 labs was able to find the effect.},
	pages = {e73--e85},
	number = {4},
	journaltitle = {Journal of Experimental Psychology: General},
	shortjournal = {Journal of Experimental Psychology: General},
	author = {Rohrer, Doug and Pashler, Harold and Harris, Christine R.},
	urldate = {2021-02-03},
	date = {2015-08},
	langid = {english},
	file = {Rohrer et al. - 2015 - Do subtle reminders of money change people’s polit.pdf:/Users/tom/Zotero/storage/GQCNNE4B/Rohrer et al. - 2015 - Do subtle reminders of money change people’s polit.pdf:application/pdf},
}

@article{shanks_priming_2013,
	title = {Priming Intelligent Behavior: An Elusive Phenomenon},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0056515},
	doi = {10.1371/journal.pone.0056515},
	shorttitle = {Priming Intelligent Behavior},
	abstract = {Can behavior be unconsciously primed via the activation of attitudes, stereotypes, or other concepts? A number of studies have suggested that such priming effects can occur, and a prominent illustration is the claim that individuals' accuracy in answering general knowledge questions can be influenced by activating intelligence-related concepts such as professor or soccer hooligan. In 9 experiments with 475 participants we employed the procedures used in these studies, as well as a number of variants of those procedures, in an attempt to obtain this intelligence priming effect. None of the experiments obtained the effect, although financial incentives did boost performance. A Bayesian analysis reveals considerable evidential support for the null hypothesis. The results conform to the pattern typically obtained in word priming experiments in which priming is very narrow in its generalization and unconscious (subliminal) influences, if they occur at all, are extremely short-lived. We encourage others to explore the circumstances in which this phenomenon might be obtained.},
	pages = {e56515},
	number = {4},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Shanks, David R. and Newell, Ben R. and Lee, Eun Hee and Balakrishnan, Divya and Ekelund, Lisa and Cenac, Zarus and Kavvadia, Fragkiski and Moore, Christopher},
	urldate = {2021-02-03},
	date = {2013-04-24},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Experimental psychology, Bayesian method, Intelligence, Intelligence tests, Priming (psychology), Sociology of knowledge, Sports},
	file = {Shanks_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Shanks_etal_2013.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RANFTJHP/article.html:text/html},
}

@article{carter_series_2015,
	title = {A series of meta-analytic tests of the depletion effect: Self-control does not seem to rely on a limited resource},
	volume = {144},
	issn = {1939-2222},
	doi = {10.1037/xge0000083},
	shorttitle = {A series of meta-analytic tests of the depletion effect},
	abstract = {Failures of self-control are thought to underlie various important behaviors (e.g., addiction, violence, obesity, poor academic achievement). The modern conceptualization of self-control failure has been heavily influenced by the idea that self-control functions as if it relied upon a limited physiological or cognitive resource. This view of self-control has inspired hundreds of experiments designed to test the prediction that acts of self-control are more likely to fail when they follow previous acts of self-control (the depletion effect). Here, we evaluated the empirical evidence for this effect with a series of focused, meta-analytic tests that address the limitations in prior appraisals of the evidence. We find very little evidence that the depletion effect is a real phenomenon, at least when assessed with the methods most frequently used in the laboratory. Our results strongly challenge the idea that self-control functions as if it relies on a limited psychological or physical resource.},
	pages = {796--815},
	number = {4},
	journaltitle = {Journal of Experimental Psychology. General},
	shortjournal = {J Exp Psychol Gen},
	author = {Carter, Evan C. and Kofler, Lilly M. and Forster, Daniel E. and {McCullough}, Michael E.},
	date = {2015-08},
	pmid = {26076043},
	keywords = {Humans, Ego, Self-Control},
	file = {Full Text:/Users/tom/Zotero/storage/PWDDC6RX/Carter et al. - 2015 - A series of meta-analytic tests of the depletion e.pdf:application/pdf},
}

@article{coles_meta-analysis_2019,
	title = {A meta-analysis of the facial feedback literature: Effects of facial feedback on emotional experience are small and variable.},
	volume = {145},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000194},
	doi = {10.1037/bul0000194},
	shorttitle = {A meta-analysis of the facial feedback literature},
	pages = {610--651},
	number = {6},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Coles, Nicholas A. and Larsen, Jeff T. and Lench, Heather C.},
	urldate = {2021-02-03},
	date = {2019-06},
	langid = {english},
	file = {Coles et al. - 2019 - A meta-analysis of the facial feedback literature.pdf:/Users/tom/Zotero/storage/45RTVJP8/Coles et al. - 2019 - A meta-analysis of the facial feedback literature.pdf:application/pdf},
}

@article{haaf_bayesian_2020,
	title = {A bayesian multiverse analysis of Many Labs 4: quantifying the evidence against mortality salience},
	url = {https://osf.io/cb9er},
	doi = {10.31234/osf.io/cb9er},
	shorttitle = {A bayesian multiverse analysis of many labs 4},
	abstract = {Many Labs projects have become the gold standard for assessing the replicability of key findings in psychological science. The Many Labs 4 project recently failed to replicate the mortality salience effect where being reminded of one’s own death strengthens the own cultural identity. Here, we provide a Bayesian reanalysis of Many Labs 4 using meta-analytic and hierarchical modeling approaches and model comparison with Bayes factors. In a multiverse analysis we assess the robustness of the results with varying data inclusion criteria and prior settings. Bayesian model comparison results largely converge to a common conclusion: We find evidence against a mortality salience effect across the majority of our analyses. Even when ignoring the Bayesian model comparison results we estimate overall effect sizes so small (between d = 0.03 and d = 0.18) that it renders the entire field of mortality salience studies as uninformative.},
	author = {Haaf, Julia M. and Hoogeveen, Suzanne and Berkhout, Sophie and Gronau, Quentin Frederik and Wagenmakers, Eric-Jan},
	urldate = {2020-09-21},
	date = {2020-04-14},
}

@article{dutilh_flexible_2019,
	title = {Flexible yet fair: blinding analyses in experimental psychology},
	doi = {https://doi.org/10.1007/s11229-019-02456-7},
	shorttitle = {Flexible yet fair},
	abstract = {The replicability of findings in experimental psychology can be improved by distinguishing sharply between hypothesis-generating research and hypothesis-testing research. This distinction can be achieved by preregistration, a method that has recently attracted widespread attention. Although preregistration is fair in the sense that it inoculates researchers against hindsight bias and confirmation bias, preregistration does not allow researchers to analyze the data flexibly without the analysis being demoted to exploratory. To alleviate this concern we discuss how researchers may conduct blinded analyses ({MacCoun} \& Perlmutter, 2015). As with preregistration, blinded analyses break the feedback loop between the analysis plan and analysis outcome, thereby preventing cherry-picking and significance seeking. However, blinded analyses retain the flexibility to account for unexpected peculiarities in the data. We discuss different methods of blinding, offer recommendations for blinding of popular experimental designs, and introduce the design for an online blinding protocol.},
	journaltitle = {Synthese},
	author = {Dutilh, Gilles and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
	urldate = {2020-09-21},
	date = {2019-08-10},
	note = {Publisher: {PsyArXiv}},
	file = {Dutilh_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Dutilh_etal_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/R5RC5I5U/d79r8.html:text/html},
}

@article{vadillo_selection_2016,
	title = {Selection bias, vote counting, and money-priming effects: A comment on Rohrer, Pashler, and Harris (2015) and Vohs (2015).},
	volume = {145},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000157},
	doi = {10.1037/xge0000157},
	shorttitle = {Selection bias, vote counting, and money-priming effects},
	pages = {655--663},
	number = {5},
	journaltitle = {Journal of Experimental Psychology: General},
	shortjournal = {Journal of Experimental Psychology: General},
	author = {Vadillo, Miguel A. and Hardwicke, Tom E. and Shanks, David R.},
	urldate = {2021-02-03},
	date = {2016-05},
	langid = {english},
	file = {Vadillo et al. - 2016 - Selection bias, vote counting, and money-priming effects A comment on Rohrer, Pashler, and Harris (.pdf:/Users/tom/Zotero/storage/TAG3RVWR/Vadillo et al. - 2016 - Selection bias, vote counting, and money-priming effects A comment on Rohrer, Pashler, and Harris (.pdf:application/pdf},
}

@article{simons_introduction_2014,
	title = {An introduction to Registered Replication Reports at \textit{Perspectives on Psychological Science}},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691614543974},
	doi = {10.1177/1745691614543974},
	pages = {552--555},
	number = {5},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Simons, Daniel J. and Holcombe, Alex O. and Spellman, Barbara A.},
	urldate = {2021-01-12},
	date = {2014-09},
	langid = {english},
	file = {Simons et al. - 2014 - An Introduction to Registered Replication Reports .pdf:/Users/tom/Zotero/storage/3TEKT293/Simons et al. - 2014 - An Introduction to Registered Replication Reports .pdf:application/pdf},
}

@article{tatsioni_persistence_2007,
	title = {Persistence of contradicted claims in the literature},
	volume = {298},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/209653},
	doi = {10.1001/jama.298.21.2517},
	abstract = {{ContextSome} research findings based on observational epidemiology are contradicted by randomized trials, but may nevertheless still be supported in some scientific circles.{ObjectivesTo} evaluate the change over time in the content of citations for 2 highly cited epidemiological studies that proposed major cardiovascular benefits associated with vitamin E in 1993; and to understand how these benefits continued being defended in the literature, despite strong contradicting evidence from large randomized clinical trials ({RCTs}). To examine the generalizability of these findings, we also examined the extent of persistence of supporting citations for the highly cited and contradicted protective effects of beta-carotene on cancer and of estrogen on Alzheimer disease.Data {SourcesFor} vitamin E, we sampled articles published in 1997, 2001, and 2005 (before, early, and late after publication of refuting evidence) that referenced the highly cited epidemiological studies and separately sampled articles published in 2005 and referencing the major contradicting {RCT} ({HOPE} trial). We also sampled articles published in 2006 that referenced highly cited articles proposing benefits associated with beta-carotene for cancer (published in 1981 and contradicted long ago by {RCTs} in 1994-1996) and estrogen for Alzheimer disease (published in 1996 and contradicted recently by {RCTs} in 2004).Data {ExtractionThe} stance of the citing articles was rated as favorable, equivocal, and unfavorable to the intervention. We also recorded the range of counterarguments raised to defend effectiveness against contradicting evidence.{ResultsFor} the 2 vitamin E epidemiological studies, even in 2005, 50\% of citing articles remained favorable. A favorable stance was independently less likely in more recent articles, specifically in articles that also cited the {HOPE} trial (odds ratio for 2001, 0.05 [95\% confidence interval, 0.01-0.19; P \&lt; .001] and the odds ratio for 2005, 0.06 [95\% confidence interval, 0.02-0.24; P \&lt; .001], as compared with 1997), and in general/internal medicine vs specialty journals. Among articles citing the {HOPE} trial in 2005, 41.4\% were unfavorable. In 2006, 62.5\% of articles referencing the highly cited article that had proposed beta-carotene and 61.7\% of those referencing the highly cited article on estrogen effectiveness were still favorable; 100\% and 96\%, respectively, of the citations appeared in specialty journals; and citations were significantly less favorable (P = .001 and P = .009, respectively) when the major contradicting trials were also mentioned. Counterarguments defending vitamin E or estrogen included diverse selection and information biases and genuine differences across studies in participants, interventions, cointerventions, and outcomes. Favorable citations to beta-carotene, long after evidence contradicted its effectiveness, did not consider the contradicting evidence.{ConclusionClaims} from highly cited observational studies persist and continue to be supported in the medical literature despite strong contradictory evidence from randomized trials.},
	pages = {2517--2526},
	number = {21},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Tatsioni, Athina and Bonitsis, Nikolaos G. and Ioannidis, John P. A.},
	urldate = {2020-09-03},
	date = {2007-12-05},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Snapshot:/Users/tom/Zotero/storage/MP4MJI6Y/209653.html:text/html;Tatsioni_etal_2007.pdf:/Users/tom/pCloud Drive/Zotero_Library/Tatsioni_etal_2007.pdf:application/pdf},
}

@article{ioannidis_why_2012,
	title = {Why science is not necessarily self-correcting},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612464056},
	doi = {10.1177/1745691612464056},
	abstract = {The ability to self-correct is considered a hallmark of science. However, self-correction does not always happen to scientific evidence by default. The trajectory of scientific credibility can fluctuate over time, both for defined scientific fields and for science at-large. History suggests that major catastrophes in scientific credibility are unfortunately possible and the argument that “it is obvious that progress is made” is weak. Careful evaluation of the current status of credibility of various scientific fields is important in order to understand any credibility deficits and how one could obtain and establish more trustworthy results. Efficient and unbiased replication mechanisms are essential for maintaining high levels of scientific credibility. Depending on the types of results obtained in the discovery and replication phases, there are different paradigms of research: optimal, self-correcting, false nonreplication, and perpetuated fallacy. In the absence of replication efforts, one is left with unconfirmed (genuine) discoveries and unchallenged fallacies. In several fields of investigation, including many areas of psychological science, perpetuated and unchallenged fallacies may comprise the majority of the circulating evidence. I catalogue a number of impediments to self-correction that have been empirically studied in psychological science. Finally, I discuss some proposed solutions to promote sound replication practices enhancing the credibility of scientific results as well as some potential disadvantages of each of them. Any deviation from the principle that seeking the truth has priority over any other goals may be seriously damaging to the self-correcting functions of science.},
	pages = {645--654},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Ioannidis, John P. A.},
	urldate = {2020-09-03},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Ioannidis - 2012 - Why science is not necessarily self-correcting.pdf:/Users/tom/Zotero/storage/PNGY8KGP/Ioannidis - 2012 - Why science is not necessarily self-correcting.pdf:application/pdf},
}

@article{moonesinghe_most_2007,
	title = {Most published research findings are false—but a little replication goes a long way},
	volume = {4},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040028},
	doi = {10.1371/journal.pmed.0040028},
	abstract = {While the authors agree with John Ioannidis that "most research findings are false," here they show that replication of research findings enhances the positive predictive value of research findings being true.},
	pages = {e28},
	number = {2},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Moonesinghe, Ramal and Khoury, Muin J. and Janssens, A. Cecile J. W.},
	urldate = {2020-12-02},
	date = {2007-02-27},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Metaanalysis, Forecasting, Publication ethics, Genetics, Genetics of disease, Mixtures, Population genetics, Research errors},
	file = {Moonesinghe_etal_2007.pdf:/Users/tom/pCloud Drive/Zotero_Library/Moonesinghe_etal_2007.pdf:application/pdf},
}

@article{earp_replication_2015,
	title = {Replication, falsification, and the crisis of confidence in social psychology},
	volume = {6},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00621/full},
	doi = {10.3389/fpsyg.2015.00621},
	abstract = {The (latest) “crisis in confidence” in social psychology has generated much heated discussion about the importance of replication, including how such replication should be carried out as well as interpreted by scholars in the field. What does it mean if a replication attempt “fails”—does it mean that the original results, or the theory that predicted them, have been falsified? And how should “failed” replications affect our belief in the validity of the original research? In this paper, we consider the “replication” debate from a historical and philosophical perspective, and provide a conceptual analysis of both replication and falsification as they pertain to this important discussion. Along the way, we introduce a Bayesian framework for assessing “failed” replications in terms of how they should affect our confidence in purported findings.},
	pages = {621},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Earp, Brian D. and Trafimow, David},
	urldate = {2020-12-01},
	date = {2015},
	note = {Publisher: Frontiers},
	keywords = {Psychology, Replication, Falsification, crisis of replicability, Philosophy of science, Social},
	file = {Earp_Trafimow_2015.pdf:/Users/tom/pCloud Drive/Zotero_Library/Earp_Trafimow_2015.pdf:application/pdf},
}

@article{bishop_psychology_2019,
	title = {The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture:},
	volume = {73},
	rights = {© Experimental Psychology Society 2019},
	url = {https://journals.sagepub.com/doi/10.1177/1747021819886519},
	doi = {10.1177/1747021819886519},
	shorttitle = {The psychology of experimental psychologists},
	abstract = {Like many other areas of science, experimental psychology is affected by a “replication crisis” that is causing concern in many fields of research. Approaches t...},
	pages = {1--19},
	number = {1},
	journaltitle = {Quarterly Journal of Experimental Psychology},
	author = {Bishop, Dorothy {VM}},
	urldate = {2020-09-22},
	date = {2019-11-14},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {UK}: London, England},
	file = {Bishop_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bishop_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ALIIZ3W5/1747021819886519.html:text/html},
}

@book{collins_changing_1985,
	location = {{CA}: Beverly Hills},
	title = {Changing order: replication and induction in scientific practice},
	isbn = {978-0-8039-9757-8 978-0-8039-9717-2},
	shorttitle = {Changing order},
	pagetotal = {187},
	publisher = {Sage Publications},
	author = {Collins, H. M.},
	date = {1985},
	keywords = {Philosophy, Science, Social aspects},
	file = {Collins_1985.pdf:/Users/tom/pCloud Drive/Zotero_Library/Collins_1985.pdf:application/pdf},
}

@report{halina_replications_2020,
	title = {Replications in comparative psychology},
	url = {https://psyarxiv.com/sqxah/},
	abstract = {In order to assess the status of replication in comparative psychology, it is important to clarify what constitutes a replicated experiment. In this paper, I adopt the Resampling Account of replication recently advanced by the philosopher Edouard Machery. I apply this account to an area of comparative psychology—namely, nonhuman primate theory of mind research. Two key findings emerge from this analysis. First, under the account of replication advanced here, genuine replications are common in comparative psychology. Second, I argue that different types of replications offer different epistemic benefits to researchers. This finding diverges from Machery’s view. Finally, I suggest that community-level change is needed in order to promote a wide range of replications and their associated diversity of epistemic benefits.},
	institution = {{PsyArXiv}},
	author = {Halina, Marta},
	urldate = {2020-12-31},
	date = {2020-08-25},
	doi = {10.31234/osf.io/sqxah},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Replication, Theory and Philosophy of Science, Cognitive Psychology, Animal Learning and Behavior, Chimpanzees, Comparative Psychology, Resampling Account, Theory of Mind},
	file = {Halina_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Halina_2020.pdf:application/pdf},
}

@article{hagger_multilab_2016,
	title = {A multilab preregistered replication of the ego-depletion effect},
	volume = {11},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691616652873},
	doi = {10.1177/1745691616652873},
	abstract = {Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion. Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95\% confidence intervals ({CIs}) that encompassed zero (d = 0.04, 95\% {CI} [−0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.},
	pages = {546--573},
	number = {4},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Hagger, M. S. and Chatzisarantis, N. L. D. and Alberts, H. and Anggono, C. O. and Batailler, C. and Birt, A. R. and Brand, R. and Brandt, M. J. and Brewer, G. and Bruyneel, S. and Calvillo, D. P. and Campbell, W. K. and Cannon, P. R. and Carlucci, M. and Carruth, N. P. and Cheung, T. and Crowell, A. and De Ridder, D. T. D. and Dewitte, S. and Elson, M. and Evans, J. R. and Fay, B. A. and Fennis, B. M. and Finley, A. and Francis, Z. and Heise, E. and Hoemann, H. and Inzlicht, M. and Koole, S. L. and Koppel, L. and Kroese, F. and Lange, F. and Lau, K. and Lynch, B. P. and Martijn, C. and Merckelbach, H. and Mills, N. V. and Michirev, A. and Miyake, A. and Mosser, A. E. and Muise, M. and Muller, D. and Muzi, M. and Nalis, D. and Nurwanti, R. and Otgaar, H. and Philipp, M. C. and Primoceri, P. and Rentzsch, K. and Ringos, L. and Schlinkert, C. and Schmeichel, B. J. and Schoch, S. F. and Schrama, M. and Schütz, A. and Stamos, A. and Tinghög, G. and Ullrich, J. and {vanDellen}, M. and Wimbarti, S. and Wolff, W. and Yusainy, C. and Zerhouni, O. and Zwienenberg, M.},
	urldate = {2021-01-12},
	date = {2016-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {meta-analysis, energy model, resource depletion, self-regulation, strength model},
	file = {Hagger_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hagger_etal_2016.pdf:application/pdf},
}

@article{carter_single_2011,
	title = {A single exposure to the american flag shifts support toward republicanism up to 8 months later},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611414726},
	doi = {10.1177/0956797611414726},
	abstract = {There is scant evidence that incidental cues in the environment significantly alter people’s political judgments and behavior in a durable way. We report that a brief exposure to the American flag led to a shift toward Republican beliefs, attitudes, and voting behavior among both Republican and Democratic participants, despite their overwhelming belief that exposure to the flag would not influence their behavior. In Experiment 1, which was conducted online during the 2008 U.S. presidential election, a single exposure to an American flag resulted in a significant increase in participants’ Republican voting intentions, voting behavior, political beliefs, and implicit and explicit attitudes, with some effects lasting 8 months after the exposure to the prime. In Experiment 2, we replicated the findings more than a year into the current Democratic presidential term. These results constitute the first evidence that nonconscious priming effects from exposure to a national flag can bias the citizenry toward one political party and can have considerable durability.},
	pages = {1011--1018},
	number = {8},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Carter, Travis J. and Ferguson, Melissa J. and Hassin, Ran R.},
	urldate = {2021-01-12},
	date = {2011-08},
	langid = {english},
	file = {Carter et al. - 2011 - A Single Exposure to the American Flag Shifts Supp.pdf:/Users/tom/Zotero/storage/8BQV8KWJ/Carter et al. - 2011 - A Single Exposure to the American Flag Shifts Supp.pdf:application/pdf},
}

@article{sripada_methylphenidate_2014,
	title = {Methylphenidate blocks effort-induced depletion of regulatory control in healthy volunteers},
	volume = {25},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797614526415},
	doi = {10.1177/0956797614526415},
	abstract = {A recent wave of studies—over 100 conducted over the last decade—shows that exerting effort at controlling impulses or behavioral tendencies leaves a person depleted and less able to engage in subsequent rounds of regulation. Regulatory depletion is thought to play an important role in everyday problems (e.g., excessive spending, overeating) as well as psychiatric conditions, but its neurophysiological basis is poorly understood. Using a placebo-controlled, double-blind design, we demonstrate that the psychostimulant methylphenidate (commonly known as ‘Ritalin’), a catecholamine reuptake blocker that increases dopamine and norepinephrine at the synaptic cleft, fully blocks effort-induced depletion of regulatory control. Spectral analysis of trial-by-trial reaction times found specificity of methylphenidate effects on regulatory depletion in the slow-4 frequency band. This band is associated with the operation of resting state brain networks that produce mind wandering, raising potential connections between our results and recent brain network-based models of control over attention.},
	pages = {1227--1234},
	number = {6},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Sripada, Chandra and Kessler, Daniel and Jonides, John},
	urldate = {2021-01-12},
	date = {2014-06},
	langid = {english},
	file = {Sripada et al. - 2014 - Methylphenidate Blocks Effort-Induced Depletion of.pdf:/Users/tom/Zotero/storage/PEKS25AD/Sripada et al. - 2014 - Methylphenidate Blocks Effort-Induced Depletion of.pdf:application/pdf},
}

@book{xie_dynamic_2017,
	title = {Dynamic documents with R and knitr},
	isbn = {978-1-315-36327-1},
	abstract = {Quickly and Easily Write Dynamic Documents Suitable for both beginners and advanced users, Dynamic Documents with R and knitr, Second Edition makes writing statistical reports easier by integrating computing directly with reporting. Reports range from homework, projects, exams, books, blogs, and web pages to virtually any documents related to statistical graphics, computing, and data analysis. The book covers basic applications for beginners while guiding power users in understanding the extensibility of the knitr package. New to the Second Edition  A new chapter that introduces R Markdown v2 Changes that reflect improvements in the knitr package New sections on generating tables, defining custom printing methods for objects in code chunks, the C/Fortran engines, the Stan engine, running engines in a persistent session, and starting a local server to serve dynamic documents Boost Your Productivity in Statistical Report Writing and Make Your Scientific Computing with R Reproducible Like its highly praised predecessor, this edition shows you how to improve your efficiency in writing reports. The book takes you from program output to publication-quality reports, helping you fine-tune every aspect of your report.},
	pagetotal = {295},
	publisher = {{CRC} Press},
	author = {Xie, Yihui},
	date = {2017-07-12},
	langid = {english},
	note = {Google-Books-{ID}: 5EQPEAAAQBAJ},
	keywords = {Business \& Economics / Statistics, Mathematics / Probability \& Statistics / General},
}

@article{tukey_we_1980,
	title = {We need both exploratory and confirmatory},
	volume = {34},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2682991},
	doi = {10.2307/2682991},
	abstract = {We often forget how science and engineering function. Ideas come from previous exploration more often than from lightning strokes. Important questions can demand the most careful planning for confirmatory analysis. Broad general inquiries are also important. Finding the question is often more important than finding the answer. Exploratory data analysis is an attitude, a flexibility, and a reliance on display, {NOT} a bundle of techniques, and should be so taught. Confirmatory data analysis, by contrast, is easier to teach and easier to computerize. We need to teach both; to think about science and engineering more broadly; to be prepared to randomize and avoid multiplicity.},
	pages = {23--25},
	number = {1},
	journaltitle = {The American Statistician},
	author = {Tukey, John W.},
	urldate = {2021-01-17},
	date = {1980},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	file = {Tukey_1980.pdf:/Users/tom/pCloud Drive/Zotero_Library/Tukey_1980.pdf:application/pdf},
}

@article{levenstein_data_2018,
	title = {Data: Sharing is caring},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918758319},
	doi = {10.1177/2515245918758319},
	shorttitle = {Data},
	abstract = {Data sharing promotes scientific progress by permitting replication of prior scientific analyses and by increasing the return on the human and financial investments made in data collection. The costs of data sharing can be reduced through the implementation of best practices in data management across the research life cycle; this article provides specific guidance on these practices. The benefits of data sharing will be reaped when researchers who share their data are rewarded with citations and recognition of the intellectual value inherent in producing new scientific data.},
	pages = {95--103},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Levenstein, Margaret C. and Lyle, Jared A.},
	urldate = {2021-01-18},
	date = {2018-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {reproducibility, replication, data citation, data preservation, data sharing, data stewardship},
	file = {Levenstein_Lyle_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Levenstein_Lyle_2018.pdf:application/pdf},
}

@article{guest_how_2021,
	title = {How computational modeling can force theory building in psychological science},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620970585},
	doi = {10.1177/1745691620970585},
	abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined—what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
	pages = {1745691620970585},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Guest, Olivia and Martin, Andrea E.},
	urldate = {2021-01-23},
	date = {2021-01-22},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {open science, computational model, scientific inference, theoretical psychology},
	file = {Guest_Martin_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Guest_Martin_2021.pdf:application/pdf},
}

@article{schmidt_shall_2009,
	title = {Shall we really do it again? The powerful concept of replication is neglected in the social sciences},
	volume = {13},
	issn = {1089-2680},
	url = {https://doi.org/10.1037/a0015108},
	doi = {10.1037/a0015108},
	shorttitle = {Shall we really do it again?},
	abstract = {Replication is one of the most important tools for the verification of facts within the empirical sciences. A detailed examination of the notion of replication reveals that there are many different meanings to this concept and the relevant procedures, but hardly any systematic literature. This paper analyzes the concept of replication from a theoretical point of view. It demonstrates that the theoretical demands are scarcely met in everyday work within the social sciences. Some demands are just not feasible, whereas others are constricted by restrictions relating to publication. A new classification scheme based on a functional approach that distinguishes between different types of replication is proposed. Next, it will be argued that replication addresses the important connection between existing and new knowledge. To do so it has to be applied explicitly and systematically. The paper ends with a description of procedures how this could be done and a set of recommendations how to handle the concept of replication in the future to exploit its potential to the full.},
	pages = {90--100},
	number = {2},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Schmidt, Stefan},
	urldate = {2021-01-22},
	date = {2009-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication, philosophy of science, design, functional approach, sociology of science},
}

@article{strack_inhibiting_1988,
	title = {Inhibiting and facilitating conditions of the human smile: A nonobtrusive test of the Facial Feedback Hypothesis},
	volume = {54},
	doi = {10.1037/0022-3514.54.5.768},
	pages = {768--777},
	number = {5},
	journaltitle = {Journal of Personality and Social Psychology},
	author = {Strack, Fritz and Martin, Leonard L and Stepper, Sabine},
	date = {1988},
	langid = {english},
	file = {Strack et al. - 1988 - Inhibiting and facilitating conditions of the huma.pdf:/Users/tom/Zotero/storage/L4NXGCG7/Strack et al. - 1988 - Inhibiting and facilitating conditions of the huma.pdf:application/pdf},
}

@online{zotero_retracted_2019,
	title = {Retracted item notifications with retraction watch integration},
	url = {https://www.zotero.org/blog/retracted-item-notifications},
	author = {Zotero},
	urldate = {2021-01-30},
	date = {2019-06-14},
}

@article{munafo_research_2020,
	title = {Research culture and reproducibility},
	volume = {24},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661319302906},
	doi = {10.1016/j.tics.2019.12.002},
	abstract = {There is ongoing debate regarding the robustness and credibility of published scientific research. We argue that these issues stem from two broad causal mechanisms: the cognitive biases of researchers and the incentive structures within which researchers operate. The {UK} Reproducibility Network ({UKRN}) is working with researchers, institutions, funders, publishers, and other stakeholders to address these issues.},
	pages = {91--93},
	number = {2},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Munafò, Marcus R. and Chambers, Christopher D. and Collins, Alexandra M. and Fortunato, Laura and Macleod, Malcolm R.},
	urldate = {2021-01-31},
	date = {2020-02-01},
	langid = {english},
	keywords = {replication crisis, {UK} Reproducibility Network},
	file = {Munafò_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Munafò_etal_2020.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/9BPDX5MR/S1364661319302906.html:text/html},
}

@article{wigboldus_encourage_2016,
	title = {Encourage playing with data and discourage questionable reporting practices},
	volume = {81},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/s11336-015-9445-1},
	doi = {10.1007/s11336-015-9445-1},
	pages = {27--32},
	number = {1},
	journaltitle = {Psychometrika},
	shortjournal = {Psychometrika},
	author = {Wigboldus, Daniel H. J. and Dotsch, Ron},
	urldate = {2021-02-01},
	date = {2016-03-01},
	langid = {english},
	file = {Wigboldus_Dotsch_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wigboldus_Dotsch_2016.pdf:application/pdf},
}

@article{budd_phenomena_1998,
	title = {Phenomena of retraction: reasons for retraction and citations to the publications},
	volume = {280},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/187739},
	doi = {10.1001/jama.280.3.296},
	shorttitle = {Phenomena of retraction},
	abstract = {Context.—This study examined the impact of retracted articles on biomedical communication.Objective.—To examine publications identified in the biomedical literature as having been retracted, to ascertain why and by whom the publications were retracted and to what extent citations of later-retracted articles continue to be incorporated in subsequent work.Design.—A search of {MEDLINE} from 1966 through August 1997 for articles that had been retracted.Main Outcome Measures.—Characteristics of retractions and citations to articles after retraction.Results.—A total of 235 articles had been retracted. Error was acknowledged in relation to 91 articles; results could not be replicated in 38; misconduct was evident in 86; and no clear reason was given in 20. Of the 235 articles, 190 were retracted by some or all of the authors; 45 were retracted by a person or organization other than the author(s). The 235 retracted articles were cited 2034 times after the retraction notice. Examination of 299 of those citations reveals that in only 19 instances was the retraction noted; the remaining 280 citations treated the retracted article either explicitly (n=17) or implicitly (n=263) as though it were valid research.Conclusion.—Retracted articles continue to be cited as valid work in the biomedical literature after publication of the retraction; these citations signal potential problems for biomedical science.},
	pages = {296--297},
	number = {3},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Budd, John M. and Sievert, {MaryEllen} and Schultz, Tom R.},
	urldate = {2020-09-03},
	date = {1998-07-15},
	langid = {english},
	note = {Publisher: American Medical Association},
	file = {Budd et al. - 1998 - Phenomena of Retraction Reasons for Retraction an.pdf:/Users/tom/pCloud Drive/Zotero_Library/Budd et al. - 1998 - Phenomena of Retraction Reasons for Retraction an.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Q2WIBGX6/187739.html:text/html},
}

@article{neuliep_everyone_1993,
	title = {Everyone was wrong: there are lots of replications out there},
	volume = {8},
	issn = {0886-1641},
	url = {https://search.proquest.com/docview/1292304122/citation/9871F9658F274ABFPQ/1},
	shorttitle = {Everyone was wrong},
	pages = {1--8},
	number = {6},
	journaltitle = {Journal of Social Behavior and Personality},
	author = {Neuliep, J. W. and Crandall, R.},
	urldate = {2021-01-30},
	date = {1993-01-01},
	note = {Num Pages: 8
Place: Corte Madera, {CA}, United States
Publisher: Select Press},
	keywords = {Psychology, Sociology, Social Sciences (General)},
	file = {Neuliep_Crandall_1993.pdf:/Users/tom/pCloud Drive/Zotero_Library/Neuliep_Crandall_1993.pdf:application/pdf},
}

@article{polanin_transparency_2020,
	title = {Transparency and reproducibility of meta-analyses in psychology: a meta-review},
	volume = {15},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620906416},
	doi = {10.1177/1745691620906416},
	shorttitle = {Transparency and reproducibility of meta-analyses in psychology},
	abstract = {Systematic review and meta-analysis are possible as viable research techniques only through transparent reporting of primary research; thus, one might expect meta-analysts to demonstrate best practice in their reporting of results and have a high degree of transparency leading to reproducibility of their work. This assumption has yet to be fully tested in the psychological sciences. We therefore aimed to assess the transparency and reproducibility of psychological meta-analyses. We conducted a meta-review by sampling 150 studies from Psychological Bulletin to extract information about each review’s transparent and reproducible reporting practices. The results revealed that authors reported on average 55\% of criteria and that transparent reporting practices increased over the three decades studied (b = 1.09, {SE} = 0.24, t = 4.519, p {\textless} .001). Review authors consistently reported eligibility criteria, effect-size information, and synthesis techniques. Review authors, however, on average, did not report specific search results, screening and extraction procedures, and most importantly, effect-size and moderator information from each individual study. Far fewer studies provided statistical code required for complete analytical replication. We argue that the field of psychology and research synthesis in general should require review authors to report these elements in a transparent and reproducible manner.},
	pages = {1026--1041},
	number = {4},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Polanin, Joshua R. and Hennessy, Emily A. and Tsuji, Sho},
	urldate = {2021-01-30},
	date = {2020-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {reproducibility, meta-analysis, research synthesis, research transparency},
	file = {Polanin_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Polanin_etal_2020.pdf:application/pdf},
}

@article{kuhl_is_2019,
	title = {Is learning with elaborative interrogation less desirable when learners are depleted?},
	volume = {10},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.00707/full},
	doi = {10.3389/fpsyg.2019.00707},
	pages = {707},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Kühl, Tim and Bertrams, Alex},
	urldate = {2021-01-30},
	date = {2019-03-29},
	file = {Kühl_Bertrams_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kühl_Bertrams_2019.pdf:application/pdf},
}

@article{maassen_reproducibility_2020,
	title = {Reproducibility of individual effect sizes in meta-analyses in psychology},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0233107},
	doi = {10.1371/journal.pone.0233107},
	pages = {e0233107},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Maassen, Esther and van Assen, Marcel A. L. M. and Nuijten, Michèle B. and Olsson-Collentine, Anton and Wicherts, Jelte M.},
	editor = {Gnambs, Timo},
	urldate = {2021-01-30},
	date = {2020-05-27},
	langid = {english},
	file = {Maassen_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Maassen_etal_2020.pdf:application/pdf},
}

@article{siddaway_how_2019,
	title = {How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses},
	volume = {70},
	issn = {0066-4308},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-010418-102803},
	doi = {10.1146/annurev-psych-010418-102803},
	shorttitle = {How to do a systematic review},
	abstract = {Systematic reviews are characterized by a methodical and replicable methodology and presentation. They involve a comprehensive search to locate all relevant published and unpublished work on a subject; a systematic integration of search results; and a critique of the extent, nature, and quality of evidence in relation to a particular research question. The best reviews synthesize studies to draw broad theoretical conclusions about what a literature means, linking theory to evidence and evidence to theory. This guide describes how to plan, conduct, organize, and present a systematic review of quantitative (meta-analysis) or qualitative (narrative review, meta-synthesis) information. We outline core standards and principles and describe commonly encountered problems. Although this guide targets psychological scientists, its high level of abstraction makes it potentially relevant to any subject area or discipline. We argue that systematic reviews are a key methodology for clarifying whether and how research findings replicate and for explaining possible inconsistencies, and we call for researchers to conduct systematic reviews to help elucidate whether there is a replication crisis.},
	pages = {747--770},
	number = {1},
	journaltitle = {Annual Review of Psychology},
	shortjournal = {Annu. Rev. Psychol.},
	author = {Siddaway, Andy P. and Wood, Alex M. and Hedges, Larry V.},
	urldate = {2020-08-05},
	date = {2019-01-04},
	note = {Publisher: Annual Reviews},
	keywords = {toread},
	file = {Siddaway_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Siddaway_etal_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZNY6AVKY/annurev-psych-010418-102803.html:text/html},
}

@article{dwan_evidence_2014,
	title = {Evidence for the selective reporting of analyses and discrepancies in clinical trials: a systematic review of cohort studies of clinical trials},
	volume = {11},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001666},
	doi = {10.1371/journal.pmed.1001666},
	shorttitle = {Evidence for the selective reporting of analyses and discrepancies in clinical trials},
	abstract = {In a systematic review of cohort studies, Kerry Dwan and colleagues examine the evidence for selective reporting and discrepancies in analyses between journal publications and other documents for clinical trials. Please see later in the article for the Editors' Summary},
	pages = {e1001666},
	number = {6},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Dwan, Kerry and Altman, Douglas G. and Clarke, Mike and Gamble, Carrol and Higgins, Julian P. T. and Sterne, Jonathan A. C. and Williamson, Paula R. and Kirkham, Jamie J.},
	urldate = {2020-09-20},
	date = {2014-06-24},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Statistical data, Randomized controlled trials, Drug therapy, Medical journals, Clinical trial reporting, Clinical trials, Systematic reviews, Cohort studies},
	file = {Dwan_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Dwan_etal_2014.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B5V468BQ/article.html:text/html},
}

@article{carter_correcting_2019,
	title = {Correcting for bias in psychology: a comparison of meta-analytic methods},
	volume = {2},
	doi = {10.1177/2515245919847196},
	abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in metaanalysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shiny apps.org/apps/{metaExplorer}/.},
	pages = {115--144},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Carter, Evan C and Schönbrodt, Felix D and Gervais, Will M and Hilgard, Joseph},
	date = {2019},
	langid = {english},
	file = {Carter_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Carter_etal_2019.pdf:application/pdf},
}

@article{schroyens_reactivation-dependent_2021,
	title = {Reactivation-dependent amnesia for contextual fear memories: evidence for publication bias},
	volume = {8},
	rights = {Copyright © 2021 Schroyens et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
	issn = {2373-2822},
	url = {https://www.eneuro.org/content/8/1/ENEURO.0108-20.2020},
	doi = {10.1523/ENEURO.0108-20.2020},
	shorttitle = {Reactivation-dependent amnesia for contextual fear memories},
	abstract = {Research on memory reconsolidation has been booming in the last two decades, with numerous high-impact publications reporting promising amnestic interventions in rodents and humans. However, our own recently-published failed replication attempts of reactivation-dependent amnesia for fear memories in rats suggest that such amnestic effects are not always readily found and that they depend on subtle and possibly uncontrollable parameters. The discrepancy between our observations and published studies in rodents suggests that the literature in this field might be biased. The aim of the current study was to gauge the presence of publication bias in a well-delineated part of the reconsolidation literature. To this end, we performed a systematic review of the literature on reactivation-dependent amnesia for contextual fear memories in rodents, followed by a statistical assessment of publication bias in this sample. In addition, relevant researchers were contacted for unpublished results, which were included in the current analyses. The obtained results support the presence of publication bias, suggesting that the literature provides an overly optimistic overall estimate of the size and reproducibility of amnestic effects. Reactivation-dependent amnesia for contextual fear memories in rodents is thus less robust than what is projected by the literature. The moderate success of clinical studies may be in line with this conclusion, rather than reflecting translational issues. For the field to evolve, replication and non-biased publication of obtained results are essential. A set of tools that can create opportunities to increase transparency, reproducibility and credibility of research findings is provided.},
	number = {1},
	journaltitle = {{eNeuro}},
	shortjournal = {{eNeuro}},
	author = {Schroyens, Natalie and Sigwald, Eric L. and Noortgate, Wim Van Den and Beckers, Tom and Luyten, Laura},
	urldate = {2021-01-25},
	date = {2021-01-01},
	langid = {english},
	pmid = {33355289},
	note = {Publisher: Society for Neuroscience
Section: Research Article: New Research - Registered Report},
	keywords = {publication bias, reconsolidation, amnesia, contextual fear memory, pharmacology, rodents},
	file = {Schroyens_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Schroyens_etal_2021.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8VWSAKYQ/ENEURO.0108-20.html:text/html},
}

@article{strack_data_2017,
	title = {From data to truth in psychological science. A personal perspective.},
	volume = {8},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2017.00702/full},
	doi = {10.3389/fpsyg.2017.00702},
	pages = {702},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Strack, Fritz},
	urldate = {2021-01-30},
	date = {2017-05-16},
	file = {Strack_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Strack_2017.pdf:application/pdf},
}

@article{soderkvist_how_2018,
	title = {How the experience of emotion is modulated by facial feedback},
	volume = {42},
	issn = {0191-5886, 1573-3653},
	url = {http://link.springer.com/10.1007/s10919-017-0264-1},
	doi = {10.1007/s10919-017-0264-1},
	pages = {129--151},
	number = {1},
	journaltitle = {Journal of Nonverbal Behavior},
	shortjournal = {J Nonverbal Behav},
	author = {Söderkvist, Sven and Ohlén, Kajsa and Dimberg, Ulf},
	urldate = {2021-01-30},
	date = {2018-03},
	langid = {english},
	file = {Söderkvist_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Söderkvist_etal_2018.pdf:application/pdf},
}

@article{lewis_interactions_2018,
	title = {The interactions between botulinum-toxin-based facial treatments and embodied emotions},
	volume = {8},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-018-33119-1},
	doi = {10.1038/s41598-018-33119-1},
	pages = {14720},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Lewis, Michael B.},
	urldate = {2021-01-30},
	date = {2018-12},
	langid = {english},
	file = {Lewis_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lewis_2018.pdf:application/pdf},
}

@article{muller_self-leadership_2018,
	title = {Self-leadership and self-control strength in the work context},
	volume = {33},
	issn = {0268-3946},
	url = {https://www.emerald.com/insight/content/doi/10.1108/JMP-04-2017-0149/full/html},
	doi = {10.1108/JMP-04-2017-0149},
	abstract = {Purpose
              Based on the limited strength model, the purpose of this paper is to examine the relationship of self-leadership strategies (behavior-focused strategies, constructive thought patterns) and qualitative and quantitative overload with subsequent self-control strength.
            
            
              Design/methodology/approach
              The present study is a field study with 142 university affiliates and two measurement occasions during a typical workday (before and after lunch). Self-control strength was measured using a handgrip task.
            
            
              Findings
              Hierarchical regression analyses revealed that self-leadership, quantitative overload, and qualitative overload were not directly associated with self-control strength at either of the two measurement occasions. Qualitative overload moderated the relationship between self-leadership and self-control strength, such that self-leadership was associated with lower self-control strength at both measurement occasions when individuals experienced high qualitative overload in the morning.
            
            
              Practical implications
              Employees and employers should be aware of the possibly depleting characteristics of self-leadership in order to be able to create a work environment allowing for the recovery and replenishment of self-control strength.
            
            
              Originality/value
              The present field study theoretically and methodologically contributes to the literature on self-leadership and self-control strength in the work context by investigating the depleting nature of self-leadership and workload.},
	pages = {74--92},
	number = {1},
	journaltitle = {Journal of Managerial Psychology},
	shortjournal = {{JMP}},
	author = {Müller, Teresa and Niessen, Cornelia},
	urldate = {2021-01-30},
	date = {2018-02-12},
	langid = {english},
}

@article{vadillo_underpowered_2016,
	title = {Underpowered samples, false negatives, and unconscious learning},
	volume = {23},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-015-0892-6},
	doi = {10.3758/s13423-015-0892-6},
	abstract = {The scientific community has witnessed growing concern about the high rate of false positives and unreliable results within the psychological literature, but the harmful impact of false negatives has been largely ignored. False negatives are particularly concerning in research areas where demonstrating the absence of an effect is crucial, such as studies of unconscious or implicit processing. Research on implicit processes seeks evidence of above-chance performance on some implicit behavioral measure at the same time as chance-level performance (that is, a null result) on an explicit measure of awareness. A systematic review of 73 studies of contextual cuing, a popular implicit learning paradigm, involving 181 statistical analyses of awareness tests, reveals how underpowered studies can lead to failure to reject a false null hypothesis. Among the studies that reported sufficient information, the meta-analytic effect size across awareness tests was dz= 0.31 (95 \% {CI} 0.24–0.37), showing that participants’ learning in these experiments was conscious. The unusually large number of positive results in this literature cannot be explained by selective publication. Instead, our analyses demonstrate that these tests are typically insensitive and underpowered to detect medium to small, but true, effects in awareness tests. These findings challenge a widespread and theoretically important claim about the extent of unconscious human cognition.},
	pages = {87--102},
	number = {1},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Vadillo, Miguel A. and Konstantinidis, Emmanouil and Shanks, David R.},
	urldate = {2021-01-24},
	date = {2016-02-01},
	langid = {english},
	file = {Vadillo_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vadillo_etal_2016.pdf:application/pdf},
}

@article{zwaan_participant_2018,
	title = {Participant Nonnaiveté and the reproducibility of cognitive psychology},
	volume = {25},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-017-1348-y},
	doi = {10.3758/s13423-017-1348-y},
	abstract = {Many argue that there is a reproducibility crisis in psychology. We investigated nine well-known effects from the cognitive psychology literature—three each from the domains of perception/action, memory, and language, respectively—and found that they are highly reproducible. Not only can they be reproduced in online environments, but they also can be reproduced with nonnaïve participants with no reduction of effect size. Apparently, some cognitive tasks are so constraining that they encapsulate behavior from external influences, such as testing situation and prior recent experience with the experiment to yield highly robust effects.},
	pages = {1968--1972},
	number = {5},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Zwaan, Rolf A. and Pecher, Diane and Paolacci, Gabriele and Bouwmeester, Samantha and Verkoeijen, Peter and Dijkstra, Katinka and Zeelenberg, René},
	urldate = {2021-01-24},
	date = {2018-10-01},
	langid = {english},
	file = {s13423-017-1348-y.pdf:/Users/tom/Zotero/storage/S5DFAD6P/s13423-017-1348-y.pdf:application/pdf},
}

@online{machery_what_2019,
	title = {What is a replication?},
	url = {http://philsci-archive.pitt.edu/16553/},
	abstract = {This article develops a new, general account of replication (“the Resampling Account of replication”): I argue that a replication is an experiment that resamples the experimental components of an original experiment that are treated as random factors and that the function of replications is, narrowly, to assess the reliability of the replicated experiments. On this basis, I argue that the common notion of conceptual replication is confused, and that the on-going controversy about the relative value of direct and conceptual replications should be dissolved.},
	type = {Preprint},
	author = {Machery, Edouard},
	urldate = {2021-01-22},
	date = {2019},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/W9WLQFNP/16553.html:text/html},
}

@article{hardwicke_analytic_2021,
	title = {Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: an observational study},
	volume = {8},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.201494},
	doi = {10.1098/rsos.201494},
	shorttitle = {Analytic reproducibility in articles receiving open data badges at the journal Psychological Science},
	abstract = {For any scientific report, repeating the original analyses upon the original data should yield the original outcomes. We evaluated analytic reproducibility in 25 Psychological Science articles awarded open data badges between 2014 and 2015. Initially, 16 (64\%, 95\% confidence interval [43,81]) articles contained at least one ‘major numerical discrepancy' ({\textgreater}10\% difference) prompting us to request input from original authors. Ultimately, target values were reproducible without author involvement for 9 (36\% [20,59]) articles; reproducible with author involvement for 6 (24\% [8,47]) articles; not fully reproducible with no substantive author response for 3 (12\% [0,35]) articles; and not fully reproducible despite author involvement for 7 (28\% [12,51]) articles. Overall, 37 major numerical discrepancies remained out of 789 checked values (5\% [3,6]), but original conclusions did not appear affected. Non-reproducibility was primarily caused by unclear reporting of analytic procedures. These results highlight that open data alone is not sufficient to ensure analytic reproducibility.},
	pages = {201494},
	number = {1},
	journaltitle = {Royal Society Open Science},
	shortjournal = {Royal Society Open Science},
	author = {Hardwicke, Tom E. and Bohn, Manuel and {MacDonald}, Kyle and Hembacher, Emily and Nuijten, Michèle B. and Peloquin, Benjamin N. and {deMayo}, Benjamin E. and Long, Bria and Yoon, Erica J. and Frank, Michael C.},
	urldate = {2021-01-21},
	date = {2021},
	note = {Publisher: Royal Society},
	file = {Hardwicke_etal_.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hardwicke_etal_.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6324JKAL/rsos.html:text/html},
}

@article{rowhani-farid_did_2020,
	title = {Did awarding badges increase data sharing in {BMJ} Open? A randomized controlled trial},
	volume = {7},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.191818},
	doi = {10.1098/rsos.191818},
	shorttitle = {Did awarding badges increase data sharing in {BMJ} Open?},
	abstract = {Sharing data and code are important components of reproducible research. Data sharing in research is widely discussed in the literature; however, there are no well-established evidence-based incentives that reward data sharing, nor randomized studies that demonstrate the effectiveness of data sharing policies at increasing data sharing. A simple incentive, such as an Open Data Badge, might provide the change needed to increase data sharing in health and medical research. This study was a parallel group randomized controlled trial (protocol registration: doi:10.17605/{OSF}.{IO}/{PXWZQ}) with two groups, control and intervention, with 80 research articles published in {BMJ} Open per group, with a total of 160 research articles. The intervention group received an email offer for an Open Data Badge if they shared their data along with their final publication and the control group received an email with no offer of a badge if they shared their data with their final publication. The primary outcome was the data sharing rate. Badges did not noticeably motivate researchers who published in {BMJ} Open to share their data; the odds of awarding badges were nearly equal in the intervention and control groups (odds ratio = 0.9, 95\% {CI} [0.1, 9.0]). Data sharing rates were low in both groups, with just two datasets shared in each of the intervention and control groups. The global movement towards open science has made significant gains with the development of numerous data sharing policies and tools. What remains to be established is an effective incentive that motivates researchers to take up such tools to share their data.},
	pages = {191818},
	number = {3},
	journaltitle = {Royal Society Open Science},
	shortjournal = {Royal Society Open Science},
	author = {Rowhani-Farid, Anisa and Aldcroft, Adrian and Barnett, Adrian G.},
	urldate = {2020-11-10},
	date = {2020},
	note = {Publisher: Royal Society},
	file = {Rowhani-Farid_etal_.pdf:/Users/tom/pCloud Drive/Zotero_Library/Rowhani-Farid_etal_.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/V3DZ55UC/rsos.html:text/html},
}

@software{aust_papaja_2020,
	title = {Papaja: Create {APA} manuscripts with rmarkdown.},
	url = {https://github.com/crsh/papaja},
	author = {Aust, F. and Barth, M.},
	date = {2020},
}

@article{craig_study_2021,
	title = {A study on episodic memory reconsolidation that tells us more about consolidation},
	volume = {28},
	issn = {1072-0502, 1549-5485},
	url = {http://learnmem.cshlp.org/content/28/2/30},
	doi = {10.1101/lm.052274.120},
	abstract = {Awake quiescence immediately after encoding is conducive to episodic memory consolidation. Retrieval can render episodic memories labile again, but reconsolidation can modify and restrengthen them. It remained unknown whether awake quiescence after retrieval supports episodic memory reconsolidation. We sought to examine this question via an object-location memory paradigm. We failed to probe the effect of quiescence on reconsolidation, but we did observe an unforeseen “delayed” effect of quiescence on consolidation. Our findings reveal that the beneficial effect of quiescence on episodic memory consolidation is not restricted to immediately following encoding but can be achieved at a delayed stage and even following a period of task engagement.},
	pages = {30--33},
	number = {2},
	journaltitle = {Learning \& Memory},
	shortjournal = {Learn. Mem.},
	author = {Craig, Michael and Knowles, Christopher and Hill, Stephanie and Dewar, Michaela},
	urldate = {2021-01-15},
	date = {2021-01-02},
	langid = {english},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Institution: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Publisher: Cold Spring Harbor Lab},
	file = {Snapshot:/Users/tom/Zotero/storage/XAZJW47M/30.html:text/html},
}

@incollection{musgrave_falsification_1970,
	location = {Cambridge},
	title = {Falsification and the methodology of scientific research programmes},
	volume = {4},
	isbn = {978-0-521-09623-2},
	url = {https://www.cambridge.org/core/books/criticism-and-the-growth-of-knowledge/falsification-and-the-methodology-of-scientific-research-programmes/B1AAD974814D6E7BF35E6449691AA58F},
	pages = {91--196},
	booktitle = {Criticism and the Growth of Knowledge},
	publisher = {Cambridge University Press},
	author = {Lakatos, I.},
	editor = {Musgrave, Alan and Lakatos, I.},
	urldate = {2021-01-13},
	date = {1970},
	doi = {10.1017/CBO9781139171434.009},
	file = {Snapshot:/Users/tom/Zotero/storage/JEU3HPM7/B1AAD974814D6E7BF35E6449691AA58F.html:text/html},
}

@article{etz_bayesian_2016,
	title = {A Bayesian perspective on the Reproducibility Project: Psychology},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0149794},
	doi = {10.1371/journal.pone.0149794},
	shorttitle = {A Bayesian Perspective on the Reproducibility Project},
	abstract = {We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors—a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis—for a large subset (N = 72) of the original papers and their corresponding replication attempts. In our computation, we take into account the likely scenario that publication bias had distorted the originally published results. Overall, 75\% of studies gave qualitatively similar results in terms of the amount of evidence provided. However, the evidence was often weak (i.e., Bayes factor {\textless} 10). The majority of the studies (64\%) did not provide strong evidence for either the null or the alternative hypothesis in either the original or the replication, and no replication attempts provided strong evidence in favor of the null. In all cases where the original paper provided strong evidence but the replication did not (15\%), the sample size in the replication was smaller than the original. Where the replication provided strong evidence but the original did not (10\%), the replication sample size was larger. We conclude that the apparent failure of the Reproducibility Project to replicate many target effects can be adequately explained by overestimation of effect sizes (or overestimation of evidence against the null hypothesis) due to small sample sizes and publication bias in the psychological literature. We further conclude that traditional sample sizes are insufficient and that a more widespread adoption of Bayesian methods is desirable.},
	pages = {e0149794},
	number = {2},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Etz, Alexander and Vandekerckhove, Joachim},
	urldate = {2021-01-11},
	date = {2016-02-26},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Statistical data, Reproducibility, Replication studies, Publication ethics, Experimental psychology, Analysts, Statistical distributions},
	file = {Etz_Vandekerckhove_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Etz_Vandekerckhove_2016.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XULG2QGY/article.html:text/html},
}

@article{baumeister_ego_1998,
	title = {Ego depletion: Is the active self a limited resource?},
	volume = {74},
	doi = {10.1037/0022-3514.74.5.1252},
	pages = {1252--1265},
	number = {5},
	journaltitle = {Journal of Personality and Social Psychology},
	author = {Baumeister, Roy F and Bratslavsky, Ellen and Muraven, Mark and Tice, Dianne M},
	date = {1998},
	langid = {english},
	file = {Baumeister et al. - Ego Depletion Is the Active Self a Limited Resour.pdf:/Users/tom/Zotero/storage/NH2R85DR/Baumeister et al. - Ego Depletion Is the Active Self a Limited Resour.pdf:application/pdf},
}

@article{caruso_mere_2013,
	title = {Mere exposure to money increases endorsement of free-market systems and social inequality.},
	volume = {142},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029288},
	doi = {10.1037/a0029288},
	abstract = {The present research tested whether incidental exposure to money affects people’s endorsement of social systems that legitimize social inequality. We found that subtle reminders of the concept of money, relative to nonmoney concepts, led participants to endorse more strongly the existing social system in the United States in general (Experiment 1) and free-market capitalism in particular (Experiment 4), to assert more strongly that victims deserve their fate (Experiment 2), and to believe more strongly that socially advantaged groups should dominate socially disadvantaged groups (Experiment 3). We further found that reminders of money increased preference for a free-market system of organ transplants that benefited the wealthy at the expense of the poor even though this was not the prevailing system (Experiment 5) and that this effect was moderated by participants’ nationality. These results demonstrate how merely thinking about money can influence beliefs about the social order and the extent to which people deserve their station in life.},
	pages = {301--306},
	number = {2},
	journaltitle = {Journal of Experimental Psychology: General},
	shortjournal = {Journal of Experimental Psychology: General},
	author = {Caruso, Eugene M. and Vohs, Kathleen D. and Baxter, Brittani and Waytz, Adam},
	urldate = {2021-01-12},
	date = {2013},
	langid = {english},
	file = {Caruso et al. - 2013 - Mere exposure to money increases endorsement of fr.pdf:/Users/tom/Zotero/storage/KXCAR7ZJ/Caruso et al. - 2013 - Mere exposure to money increases endorsement of fr.pdf:application/pdf},
}

@article{wagenmakers_registered_2016,
	title = {Registered Replication Report: Strack, Martin, \& Stepper (1988)},
	volume = {11},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691616674458},
	doi = {10.1177/1745691616674458},
	shorttitle = {Registered Replication Report},
	abstract = {According to the facial feedback hypothesis, people’s affective responses can be influenced by their own facial expression (e.g., smiling, pouting), even when their expression did not result from their emotional experiences. For example, Strack, Martin, and Stepper (1988) instructed participants to rate the funniness of cartoons using a pen that they held in their mouth. In line with the facial feedback hypothesis, when participants held the pen with their teeth (inducing a “smile”), they rated the cartoons as funnier than when they held the pen with their lips (inducing a “pout”). This seminal study of the facial feedback hypothesis has not been replicated directly. This Registered Replication Report describes the results of 17 independent direct replications of Study 1 from Strack et al. (1988), all of which followed the same vetted protocol. A meta-analysis of these studies examined the difference in funniness ratings between the “smile” and “pout” conditions. The original Strack et al. (1988) study reported a rating difference of 0.82 units on a 10-point Likert scale. Our meta-analysis revealed a rating difference of 0.03 units with a 95\% confidence interval ranging from −0.11 to 0.16.},
	pages = {917--928},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Wagenmakers, E.-J. and Beek, T. and Dijkhoff, L. and Gronau, Q. F. and Acosta, A. and Adams, R. B. and Albohn, D. N. and Allard, E. S. and Benning, S. D. and Blouin-Hudon, E.-M. and Bulnes, L. C. and Caldwell, T. L. and Calin-Jageman, R. J. and Capaldi, C. A. and Carfagno, N. S. and Chasten, K. T. and Cleeremans, A. and Connell, L. and {DeCicco}, J. M. and Dijkstra, K. and Fischer, A. H. and Foroni, F. and Hess, U. and Holmes, K. J. and Jones, J. L. H. and Klein, O. and Koch, C. and Korb, S. and Lewinski, P. and Liao, J. D. and Lund, S. and Lupianez, J. and Lynott, D. and Nance, C. N. and Oosterwijk, S. and Ozdoğru, A. A. and Pacheco-Unguetti, A. P. and Pearson, B. and Powis, C. and Riding, S. and Roberts, T.-A. and Rumiati, R. I. and Senden, M. and Shea-Shumsky, N. B. and Sobocko, K. and Soto, J. A. and Steiner, T. G. and Talarico, J. M. and van Allen, Z. M. and Vandekerckhove, M. and Wainwright, B. and Wayand, J. F. and Zeelenberg, R. and Zetzer, E. E. and Zwaan, R. A.},
	urldate = {2021-01-12},
	date = {2016-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {preregistration, replication, facial feedback hypothesis, many-labs},
	file = {Wagenmakers_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wagenmakers_etal_2016.pdf:application/pdf},
}

@article{fletcher_role_2021,
	title = {The role of replication in psychological science},
	volume = {11},
	issn = {1879-4920},
	url = {https://doi.org/10.1007/s13194-020-00329-2},
	doi = {10.1007/s13194-020-00329-2},
	abstract = {The replication or reproducibility crisis in psychological science has renewed attention to philosophical aspects of its methodology. I provide herein a new, functional account of the role of replication in a scientific discipline: to undercut the underdetermination of scientific hypotheses from data, typically by hypotheses that connect data with phenomena. These include hypotheses that concern sampling error, experimental control, and operationalization. How a scientific hypothesis could be underdetermined in one of these ways depends on a scientific discipline’s epistemic goals, theoretical development, material constraints, institutional context, and their interconnections. I illustrate how these apply to the case of psychological science. I then contrast this “bottom-up” account with “top-down” accounts, which assume that the role of replication in a particular science, such as psychology, must follow from a uniform role that it plays in science generally. Aside from avoiding unaddressed problems with top-down accounts, my bottom-up account also better explains the variability of importance of replication of various types across different scientific disciplines.},
	pages = {23},
	number = {1},
	journaltitle = {European Journal for Philosophy of Science},
	shortjournal = {Euro Jnl Phil Sci},
	author = {Fletcher, Samuel C.},
	urldate = {2021-01-09},
	date = {2021-01-08},
	langid = {english},
	file = {Fletcher_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Fletcher_2021.pdf:application/pdf},
}

@article{braun_correcting_2020,
	title = {Correcting eyewitness suggestibility: does explanatory role predict resistance to correction?},
	volume = {0},
	issn = {0965-8211},
	url = {https://doi.org/10.1080/09658211.2020.1854788},
	doi = {10.1080/09658211.2020.1854788},
	shorttitle = {Correcting eyewitness suggestibility},
	abstract = {Many studies have documented that exposure to post event misinformation can lead eyewitnesses to misremember witnessing events they did not see and do so with high confidence. The goal of the present study was to investigate whether reporting of suggested misinformation can be reversed following a correction, and if so, whether misinformation would be more resistant to correction when it serves an explanatory function than when it does not. In two experiments participants witnessed an event, were exposed to a blatantly false suggestion(s) and one week later received a correction followed by a test of their memory for the witnessed event. We found evidence for both the persistence of misinformation following a correction (E1) and the complete reversibility of misinformation effects following a highly salient correction (E2). Although false reporting of the misinformation doubled when it served an explanatory function relative to when it did not (E1 and E2), in both experiments we found no evidence that resistance to correction varied as a function of the misinformation’s explanatory role. Our findings suggest that, with a salient correction provided by a credible source, people are capable of updating their knowledge with new information that reverses what they previously thought.},
	pages = {1--19},
	number = {0},
	journaltitle = {Memory},
	author = {Braun, Blair E. and Zaragoza, Maria S. and Chrobak, Quin M. and Ithisuphalap, Jaruda},
	urldate = {2021-01-03},
	date = {2020-12-08},
	pmid = {33290185},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09658211.2020.1854788},
	keywords = {correcting misinformation, explanatory coherence, explanatory role, Eyewitness suggestibility, false memory},
	file = {Snapshot:/Users/tom/Zotero/storage/6JSKHQ6H/09658211.2020.html:text/html},
}

@article{orlandi_behavioral_2020,
	title = {Behavioral tagging underlies memory reconsolidation},
	volume = {117},
	rights = {© 2020 . https://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/30/18029},
	doi = {10.1073/pnas.2009517117},
	abstract = {Memory reconsolidation occurs when a retrieving event destabilizes transiently a consolidated memory, triggering thereby a new process of restabilization that ensures memory persistence. Although this phenomenon has received wide attention, the effect of new information cooccurring with the reconsolidation process has been less explored. Here we demonstrate that a memory-retrieving event sets a neural tag, which enables the reconsolidation of memory after binding proteins provided by the original or a different contiguous experience. We characterized the specific temporal window during which this association is effective and identified the protein kinase A ({PKA}) and the extracellular signal-regulated kinase 1 and 2 ({ERK} 1/2) pathways as the mechanisms related to the setting of the reconsolidation tag and the synthesis of proteins. Our results show, therefore, that memory reconsolidation is mediated by a “behavioral tagging” process, which is common to different memory forms. They represent a significant advance in understanding the fate of memories reconsolidated while being adjacent to other events, and provide a tool for designing noninvasive strategies to attenuate (pathological/traumatic) or improve (education-related) memories.},
	pages = {18029--18036},
	number = {30},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Orlandi, Iván Rabinovich and Fullio, Camila L. and Schroeder, Matías Nicolás and Giurfa, Martin and Ballarini, Fabricio and Moncada, Diego},
	urldate = {2020-12-31},
	date = {2020-07-28},
	langid = {english},
	pmid = {32665437},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {memory, reconsolidation, behavioral tagging},
	file = {Orlandi_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Orlandi_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SNZ3R8DU/18029.html:text/html},
}

@article{wessel_multiverse_2020,
	title = {A multiverse analysis of early attempts to replicate memory suppression with the Think/No-think Task},
	volume = {28},
	issn = {0965-8211},
	url = {https://doi.org/10.1080/09658211.2020.1797095},
	doi = {10.1080/09658211.2020.1797095},
	abstract = {In 2001, Anderson and Green [2001. Suppressing unwanted memories by executive control. Nature, 410(6826), 366–369] showed memory suppression using a novel Think/No-think ({TNT}) task. When participants attempted to prevent studied words from entering awareness, they reported fewer of those words than baseline words in subsequent cued recall (i.e., suppression effect). The {TNT} literature contains predominantly positive findings and few null-results. Therefore we report unpublished replications conducted in the 2000s (N = 49; N = 36). As the features of the data obtained with the {TNT} task call for a variety of plausible solutions, we report parallel “universes” of data-analyses (i.e., multiverse analysis) testing the suppression effect. Two published studies (Wessel et al., 2005. Dissociation and memory suppression: A comparison of high and low dissociative individuals’ performance on the Think–No think Task. Personality and Individual Differences, 39(8), 1461–1470, N = 68; Wessel et al., 2010. Cognitive control and suppression of memories of an emotional film. Journal of Behavior Therapy and Experimental Psychiatry, 41(2), 83–89. https://doi.org/10.1016/j.jbtep.2009.10.005, N = 80) were reanalysed in a similar fashion. For recall probed with studied cues (Same Probes, {SP}), some tests (sample 3) or all (samples 2 and 4) showed statistically significant suppression effects, whereas in sample 1, only one test showed significance. Recall probed with novel cues (Independent Probes, {IP}) predominantly rendered non-significant results. The absence of statistically significant {IP} suppression effects raises problems for inhibition theory and its implication that repression is a viable mechanism of forgetting. The pre-registration, materials, data, and code are publicly available (https://osf.io/qgcy5/).},
	pages = {870--887},
	number = {7},
	journaltitle = {Memory},
	author = {Wessel, Ineke and Albers, Casper J. and Zandstra, Anna Roos E. and Heininga, Vera E.},
	urldate = {2020-12-31},
	date = {2020-08-08},
	pmid = {32701389},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09658211.2020.1797095},
	keywords = {independent probes, multiverse analysis, same probes, suppression effect, suppression-induced forgetting, {TNT} task},
	file = {Snapshot:/Users/tom/Zotero/storage/I2WE4CZP/09658211.2020.html:text/html;Snapshot:/Users/tom/Zotero/storage/8I2FZMCA/09658211.2020.html:text/html;Wessel_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wessel_etal_2020.pdf:application/pdf},
}

@article{howe_reconsolidation_2020,
	title = {Reconsolidation or interference? Aging effects and the reactivation of novel and familiar episodic memories},
	volume = {28},
	issn = {0965-8211},
	url = {https://doi.org/10.1080/09658211.2019.1705489},
	doi = {10.1080/09658211.2019.1705489},
	shorttitle = {Reconsolidation or interference?},
	abstract = {We examined aging effects in reconsolidation and interference in episodic memory by reactivating memories for well-learned items in young and healthy older adults while controlling memory strength and the degree semantic processes contributed to memory. In Experiment 1, young and old adults learned pairs of real words and images to a strict criterion. After 24-hours, half of the images were reactivated and new words were paired with the images and learned to criterion. Following a 1-week delay, recognition and source monitoring were measured for both sets of pairings. Experiment 2 was a replication of Experiment 1, but using previously unknown novel words and unusual images. As predicted, older adults needed more trials to learn both the A-B and A-C pairings. Older adults required more trials to learn the new associations for reactivated than the not reactivated pairs, although there was no main effect of reactivation and no Age x Reactivation interaction for measures of recognition one-week later. These results are inconsistent with previous findings concerning age differences in reactivation effects in episodic memory. Instead, they suggest that once memory strength and input from semantic memory are better controlled, young and old adults perform similarly on tests of long-term recognition memory.},
	pages = {839--849},
	number = {7},
	journaltitle = {Memory},
	author = {Howe, Mark L. and Akhtar, Shazia and Bland, Cassandra E. and Hellenthal, Maria V.},
	urldate = {2020-12-31},
	date = {2020-08-08},
	pmid = {31868120},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09658211.2019.1705489},
	keywords = {episodic memory, memory development, memory interference, memory reconsolidation, Reactivation},
	file = {Snapshot:/Users/tom/Zotero/storage/P8BHBXNA/09658211.2019.html:text/html},
}

@article{goodman_how_2018,
	title = {How sure are you of your result? Put a number on it},
	volume = {564},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/d41586-018-07589-2},
	doi = {10.1038/d41586-018-07589-2},
	shorttitle = {How sure are you of your result?},
	pages = {7--7},
	number = {7734},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Goodman, Steven N.},
	urldate = {2020-12-29},
	date = {2018-12},
	langid = {english},
	file = {Goodman - 2018 - How sure are you of your result Put a number on i.pdf:/Users/tom/Zotero/storage/L2TXWEYF/Goodman - 2018 - How sure are you of your result Put a number on i.pdf:application/pdf},
}

@article{suppe_structure_1998,
	title = {The Structure of a Scientific Paper},
	volume = {65},
	issn = {0031-8248},
	url = {https://www.journals.uchicago.edu/doi/10.1086/392651},
	doi = {10.1086/392651},
	abstract = {Scientific articles exemplify standard functional units constraining argumentative structures. Severe space limitations demand every paragraph and illustration contribute to establishing the paper's claims. Philosophical testing and confirmation models should take into account each paragraph, table, and illustration. Hypothetico-Deductive, Bayesian Inductive, and Inference-to-the-Best-Explanation models do not, garbling the logic of papers. Micro-analysis of the fundamental paper in plate tectonics reveals an argumentative structure commonplace in science but ignored by standard philosophical accounts that cannot be dismissed as mere rhetorical embellishment. Papers with illustrations often display a second argumentative structure differing from the text's. Constraints on adequate testing and confirmation analyses are adduced. "Experiments are about the assembly of persuasive arguments, ones that will stand up in court.... The task at hand is to capture the building-up of a persuasive argument about the world even in the absence of the logician's certainty." --Galison, How Experiments End, 277.},
	pages = {381--405},
	number = {3},
	journaltitle = {Philosophy of Science},
	shortjournal = {Philosophy of Science},
	author = {Suppe, Frederick},
	urldate = {2020-12-29},
	date = {1998-09-01},
	note = {Publisher: The University of Chicago Press},
	file = {Snapshot:/Users/tom/Zotero/storage/5UFQYSAA/392651.html:text/html},
}

@article{gardner_predicting_1982,
	title = {Predicting Novel Facts},
	volume = {33},
	issn = {0007-0882},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/33.1.1},
	doi = {10.1093/bjps/33.1.1},
	pages = {1--15},
	number = {1},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {The British Journal for the Philosophy of Science},
	author = {Gardner, Michael R.},
	urldate = {2020-12-29},
	date = {1982-03-01},
	note = {Publisher: The University of Chicago Press},
	file = {Gardner_1982.pdf:/Users/tom/pCloud Drive/Zotero_Library/Gardner_1982.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/V8CHHQ3E/33.1.html:text/html},
}

@article{hedges_statistical_2019,
	title = {Statistical analyses for studying replication: Meta-analytic perspectives.},
	volume = {24},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000189},
	doi = {10.1037/met0000189},
	shorttitle = {Statistical analyses for studying replication},
	abstract = {Formal empirical assessments of replication have recently become more prominent in several areas of science, including psychology. These assessments have used different statistical approaches to determine if a finding has been replicated. The purpose of this article is to provide several alternative conceptual frameworks that lead to different statistical analyses to test hypotheses about replication. All of these analyses are based on statistical methods used in meta-analysis. The differences among the methods described involve whether the burden of proof is placed on replication or nonreplication, whether replication is exact or allows for a small amount of “negligible heterogeneity,” and whether the studies observed are assumed to be fixed (constituting the entire body of relevant evidence) or are a sample from a universe of possibly relevant studies. The statistical power of each of these tests is computed and shown to be low in many cases, raising issues of the interpretability of tests for replication.},
	pages = {557--570},
	number = {5},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Hedges, Larry V. and Schauer, Jacob M.},
	urldate = {2020-12-28},
	date = {2019-10},
	langid = {english},
	file = {Hedges and Schauer - 2019 - Statistical analyses for studying replication Met.pdf:/Users/tom/Zotero/storage/7QBCN2RV/Hedges and Schauer - 2019 - Statistical analyses for studying replication Met.pdf:application/pdf},
}

@article{mathur_challenges_2019,
	title = {Challenges and suggestions for defining replication “success” when effects may be heterogeneous: Comment on Hedges and Schauer (2019).},
	volume = {24},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000223},
	doi = {10.1037/met0000223},
	shorttitle = {Challenges and suggestions for defining replication “success” when effects may be heterogeneous},
	pages = {571--575},
	number = {5},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Mathur, Maya B. and {VanderWeele}, Tyler J.},
	urldate = {2020-12-28},
	date = {2019-10},
	langid = {english},
	file = {Mathur_VanderWeele_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Mathur_VanderWeele_2019.pdf:application/pdf},
}

@article{hedges_consistency_2019,
	title = {Consistency of effects is important in replication: Rejoinder to Mathur and {VanderWeele} (2019).},
	volume = {24},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000237},
	doi = {10.1037/met0000237},
	shorttitle = {Consistency of effects is important in replication},
	pages = {576--577},
	number = {5},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Hedges, Larry V. and Schauer, Jacob M.},
	urldate = {2020-12-28},
	date = {2019-10},
	langid = {english},
	file = {Hedges_Schauer_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hedges_Schauer_2019.pdf:application/pdf},
}

@article{vallejo_propofol-induced_2019,
	title = {Propofol-induced deep sedation reduces emotional episodic memory reconsolidation in humans},
	volume = {5},
	rights = {Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution {NonCommercial} License 4.0 ({CC} {BY}-{NC}).. This is an open-access article distributed under the terms of the Creative Commons Attribution-{NonCommercial} license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
	issn = {2375-2548},
	url = {https://advances.sciencemag.org/content/5/3/eaav3801},
	doi = {10.1126/sciadv.aav3801},
	abstract = {The adjustment of maladaptive thoughts and behaviors associated with emotional memories is central to treating psychiatric disorders. Recent research, predominantly with laboratory animals, indicates that memories can become temporarily sensitive to modification following reactivation, before undergoing reconsolidation. A method to selectively impair reconsolidation of specific emotional or traumatic memories in humans could translate to an effective treatment for conditions such as posttraumatic stress disorder. We tested whether deep sedation could impair emotional memory reconsolidation in 50 human participants. Administering the intravenous anesthetic propofol following memory reactivation disrupted memory for the reactivated, but not for a non-reactivated, slideshow story. Propofol impaired memory for the reactivated story after 24 hours, but not immediately after propofol recovery. Critically, memory impairment occurred selectively for the emotionally negative phase of the reactivated story. One dose of propofol following memory reactivation selectively impaired subsequent emotional episodic memory retrieval in a time-dependent manner, consistent with reconsolidation impairment.
Administering the anesthetic propofol after a brief reminder reduces retrieval of established emotional memory 24 hours later.
Administering the anesthetic propofol after a brief reminder reduces retrieval of established emotional memory 24 hours later.},
	pages = {eaav3801},
	number = {3},
	journaltitle = {Science Advances},
	author = {Vallejo, Ana Galarza and Kroes, Marijn C. W. and Rey, Enrique and Acedo, Maria Victoria and Moratti, Stephan and Fernández, Guillén and Strange, Bryan A.},
	urldate = {2020-12-28},
	date = {2019-03-01},
	langid = {english},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
	file = {Snapshot:/Users/tom/Zotero/storage/B57RZE3M/eaav3801.html:text/html;Vallejo_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vallejo_etal_2019.pdf:application/pdf},
}

@article{douglas_state_2013,
	title = {State of the Field: Why novel prediction matters},
	volume = {44},
	issn = {0039-3681},
	url = {http://www.sciencedirect.com/science/article/pii/S0039368113000198},
	doi = {10.1016/j.shpsa.2013.04.001},
	shorttitle = {State of the Field},
	abstract = {There is considerable disagreement about the epistemic value of novel predictive success, i.e. when a scientist predicts an unexpected phenomenon, experiments are conducted, and the prediction proves to be accurate. We survey the field on this question, noting both fully articulated views such as weak and strong predictivism, and more nascent views, such as pluralist reasons for the instrumental value of prediction. By examining the various reasons offered for the value of prediction across a range of inferential contexts (including inferences from data to phenomena, from phenomena to theory, and from theory to framework), we can see that neither weak nor strong predictivism captures all of the reasons for valuing prediction available. A third path is presented, Pluralist Instrumental Predictivism; {PIP} for short.},
	pages = {580--589},
	number = {4},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Douglas, Heather and Magnus, P. D.},
	urldate = {2020-12-22},
	date = {2013-12-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/8WY3IKNA/Douglas and Magnus - 2013 - State of the Field Why novel prediction matters.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/HL6FX89N/S0039368113000198.html:text/html},
}

@article{schindler_novelty_2014,
	title = {Novelty, coherence, and Mendeleev’s periodic table},
	volume = {45},
	issn = {0039-3681},
	url = {http://www.sciencedirect.com/science/article/pii/S0039368113001039},
	doi = {10.1016/j.shpsa.2013.10.007},
	abstract = {Predictivism is the view that successful predictions of “novel” evidence carry more confirmational weight than accommodations of already known evidence. Novelty, in this context, has traditionally been conceived of as temporal novelty. However temporal predictivism has been criticized for lacking a rationale: why should the time order of theory and evidence matter? Instead, it has been proposed, novelty should be construed in terms of use-novelty, according to which evidence is novel if it was not used in the construction of a theory. Only if evidence is use-novel can it fully support the theory entailing it. As I point out in this paper, the writings of the most influential proponent of use-novelty contain a weaker and a stronger version of use-novelty. However both versions, I argue, are problematic. With regard to the appraisal of Mendeleev’ periodic table, the most contentious historical case in the predictivism debate, I argue that temporal predictivism is indeed supported, although in ways not previously appreciated. On the basis of this case, I argue for a form of so-called symptomatic predictivism according to which temporally novel predictions carry more confirmational weight only insofar as they reveal the theory’s presumed coherence of facts as real.},
	pages = {62--69},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Schindler, Samuel},
	urldate = {2020-12-22},
	date = {2014-03-01},
	langid = {english},
	keywords = {Coherence, Dmitri Mendeleev, Novel prediction, Periodic table, Use-novelty},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/RB39NM2Q/Schindler - 2014 - Novelty, coherence, and Mendeleev’s periodic table.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/WEEEDVRT/S0039368113001039.html:text/html},
}

@article{votsis_objectivity_2014,
	title = {Objectivity in confirmation: Post hoc monsters and novel predictions},
	volume = {45},
	issn = {0039-3681},
	url = {http://www.sciencedirect.com/science/article/pii/S0039368113001052},
	doi = {10.1016/j.shpsa.2013.10.009},
	shorttitle = {Objectivity in confirmation},
	abstract = {The aim of this paper is to put in place some cornerstones in the foundations for an objective theory of confirmation by considering lessons from the failures of predictivism. Discussion begins with a widely accepted challenge, to find out what is needed in addition to the right kind of inferential–semantical relations between hypothesis and evidence to have a complete account of confirmation, one that gives a definitive answer to the question whether hypotheses branded as “post hoc monsters” can be confirmed. The predictivist view is then presented as a way to meet this challenge. Particular attention is paid to Worrall’s version of predictivism, as it appears to be the most sophisticated of the lot. It is argued that, despite its faults, his view turns our heads in the right direction by attempting to remove contingent considerations from confirmational matters. The demand to remove such considerations becomes the first of four cornerstones. Each cornerstone is put in place with the aim to steer clear of the sort of failures that plague various kinds of predictivism. In the process, it becomes obvious that the original challenge is wrongheaded and in need of revision. The paper ends with just such a revision.},
	pages = {70--78},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Votsis, Ioannis},
	urldate = {2020-12-22},
	date = {2014-03-01},
	langid = {english},
	keywords = {Novel prediction, Ad hoc, Double counting, Objective confirmation, Post hoc, Predictivism},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/7GP28A5V/Votsis - 2014 - Objectivity in confirmation Post hoc monsters and.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/ASJS4LTM/S0039368113001052.html:text/html},
}

@incollection{maher_how_1990,
	location = {Dordrecht},
	title = {How prediction enhances confirmation},
	isbn = {978-94-009-0681-5},
	url = {https://doi.org/10.1007/978-94-009-0681-5_20},
	abstract = {Since the scientific revolution, there have been persistent endorsements, both by scientists and philosophers of science,2 of the view that predictions have special value in confirming hypotheses. According to this view, evidence which was used in formulating a hypothesis does not confirm the hypothesis as strongly as it otherwise would. I shall call this view the predictivist thesis.},
	pages = {327--343},
	booktitle = {Truth or Consequences: Essays in Honor of Nuel Belnap},
	publisher = {Springer Netherlands},
	author = {Maher, Patrick},
	editor = {Dunn, J. Michael and Gupta, Anil},
	urldate = {2020-12-22},
	date = {1990},
	langid = {english},
	doi = {10.1007/978-94-009-0681-5_20},
	keywords = {Coin Toss, Fair Coin, Scientific Revolution, Subjective Probability, Successful Prediction},
	file = {Maher - 1990 - How Prediction Enhances Confirmation.pdf:/Users/tom/Zotero/storage/KNTBFBZM/Maher - 1990 - How Prediction Enhances Confirmation.pdf:application/pdf},
}

@book{brush_making_2015,
	location = {Oxford ; New York},
	title = {Making 20th century science: how theories became knowledge},
	isbn = {978-0-19-997815-1},
	shorttitle = {Making 20th century science},
	pagetotal = {531},
	publisher = {Oxford University Press},
	author = {Brush, Stephen G. and Segal, Ariel},
	date = {2015},
	langid = {english},
	keywords = {Science, History, 19th century, 20th century, Methodology History},
	file = {Brush and Segal - 2015 - Making 20th century science how theories became k.pdf:/Users/tom/Zotero/storage/Q54YVEJH/Brush and Segal - 2015 - Making 20th century science how theories became k.pdf:application/pdf},
}

@article{scerri_prediction_2001,
	title = {Prediction and the periodic table},
	volume = {32},
	issn = {0039-3681},
	url = {http://www.sciencedirect.com/science/article/pii/S0039368101000231},
	doi = {10.1016/S0039-3681(01)00023-1},
	abstract = {The debate about the relative epistemic weights carried in favour of a theory by predictions of new phenomena as opposed to accommodations of already known phenomena has a long history. We readdress the issue through a detailed re-examination of a particular historical case that has often been discussed in connection with it—that of Mendeleev and the prediction by his periodic law of the three ‘new’ elements, gallium, scandium and germanium. We find little support for the standard story that these predictive successes were outstandingly important in the success of Mendeleev's scheme. Accommodations played an equal role—notably that of argon, the first of the ‘noble gases’ to be discovered; and the methodological situation in this chemical example turns out to be in interesting ways different from that in other cases—invariably from physics—that have been discussed in this connection. The historical episode when accurately analysed provides support for a different account of the relative weight of prediction and accommodation—one that is further articulated here.},
	pages = {407--452},
	number = {3},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Scerri, Eric R and Worrall, John},
	urldate = {2020-12-22},
	date = {2001-09-01},
	langid = {english},
	keywords = {Prediction, Accommodation, Mendeleev, Periodic Table, Weight of Evidence},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/C9MDUEXP/Scerri and Worrall - 2001 - Prediction and the periodic table.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/MKGVLHWR/S0039368101000231.html:text/html},
}

@article{prosperi_raiders_2019,
	title = {Raiders of the lost {HARK}: a reproducible inference framework for big data science},
	volume = {5},
	rights = {2019 The Author(s)},
	issn = {2055-1045},
	url = {https://www.nature.com/articles/s41599-019-0340-8},
	doi = {10.1057/s41599-019-0340-8},
	shorttitle = {Raiders of the lost {HARK}},
	abstract = {Hypothesizing after the results are known ({HARK}) has been disparaged as data dredging, and safeguards including hypothesis preregistration and statistically rigorous oversight have been recommended. Despite potential drawbacks, {HARK} has deepened thinking about complex causal processes. Some of the {HARK} precautions can conflict with the modern reality of researchers’ obligations to use big, ‘organic’ data sources—from high-throughput genomics to social media streams. We here propose a {HARK}-solid, reproducible inference framework suitable for big data, based on models that represent formalization of hypotheses. Reproducibility is attained by employing two levels of model validation: internal (relative to data collated around hypotheses) and external (independent to the hypotheses used to generate data or to the data used to generate hypotheses). With a model-centered paradigm, the reproducibility focus changes from the ability of others to reproduce both data and specific inferences from a study to the ability to evaluate models as representation of reality. Validation underpins ‘natural selection’ in a knowledge base maintained by the scientific community. The community itself is thereby supported to be more productive in generating and critically evaluating theories that integrate wider, complex systems.},
	pages = {1--12},
	number = {1},
	journaltitle = {Palgrave Communications},
	author = {Prosperi, Mattia and Bian, Jiang and Buchan, Iain E. and Koopman, James S. and Sperrin, Matthew and Wang, Mo},
	urldate = {2020-12-18},
	date = {2019-10-22},
	langid = {english},
	note = {Number: 1
Publisher: Palgrave},
	file = {Full Text PDF:/Users/tom/Zotero/storage/DLD2JKEU/Prosperi et al. - 2019 - Raiders of the lost HARK a reproducible inference.pdf:application/pdf},
}

@article{willis_trust_2020,
	title = {Trust but verify: how to leverage policies, workflows, and infrastructure to ensure computational reproducibility in publication},
	volume = {2},
	issn = {,},
	url = {https://hdsr.mitpress.mit.edu/pub/f0obb31j/release/1},
	doi = {10.1162/99608f92.25982dcf},
	shorttitle = {Trust but verify},
	abstract = {This article distills findings from a qualitative study of seven reproducibility initiatives to enumerate nine key decision points for journals seeking to address concerns about the quality and rigor of computational research by expanding the peer review and publication process. We evaluate our guidance in light of the recent National Academies of Science, Engineering, and Medicine ({NASEM}, 2019) report on Reproducibility and Replicability in Science recommendation for journal reproducibility audits. We present 10 findings that clarify how reproducibility initiatives contend with a variety of social and technical factors, including significant gaps in editorial infrastructure and a lack of uniformity in how research artifacts are packaged for dissemination. We propose and define a novel concept of assessable reproducible research artifacts and point the way to an improved understanding of how changes to author incentives and dissemination requirements impact the quality, rigor, and trustworthiness of published computational research.},
	number = {4},
	journaltitle = {Issue 2.4},
	author = {Willis, Craig and Stodden, Victoria},
	urldate = {2020-12-18},
	date = {2020-12-16},
	langid = {english},
	note = {Publisher: {PubPub}},
	file = {Full Text PDF:/Users/tom/Zotero/storage/KMMUSAMJ/Willis and Stodden - 2020 - Trust but Verify How to Leverage Policies, Workfl.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7DRBXTR8/1.html:text/html},
}

@article{christian_journal_2020,
	title = {Journal data policies: Exploring how the understanding of editors and authors corresponds to the policies themselves},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230281},
	doi = {10.1371/journal.pone.0230281},
	shorttitle = {Journal data policies},
	abstract = {Despite the increase in the number of journals issuing data policies requiring authors to make data underlying reporting findings publicly available, authors do not always do so, and when they do, the data do not always meet standards of quality that allow others to verify or extend published results. This phenomenon suggests the need to consider the effectiveness of journal data policies to present and articulate transparency requirements, and how well they facilitate (or hinder) authors’ ability to produce and provide access to data, code, and associated materials that meet quality standards for computational reproducibility. This article describes the results of a research study that examined the ability of journal-based data policies to: 1) effectively communicate transparency requirements to authors, and 2) enable authors to successfully meet policy requirements. To do this, we conducted a mixed-methods study that examined individual data policies alongside editors’ and authors’ interpretation of policy requirements to answer the following research questions. Survey responses from authors and editors along with results from a content analysis of data policies found discrepancies among editors’ assertion of data policy requirements, authors’ understanding of policy requirements, and the requirements stated in the policy language as written. We offer explanations for these discrepancies and offer recommendations for improving authors’ understanding of policies and increasing the likelihood of policy compliance.},
	pages = {e0230281},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Christian, Thu-Mai and Gooch, Amanda and Vision, Todd and Hull, Elizabeth},
	urldate = {2020-12-18},
	date = {2020-03-25},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Surveys, Open science, Science policy, Data management, Language, Medicine and health sciences, Reproducibility, Social policy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7UEE9XF3/Christian et al. - 2020 - Journal data policies Exploring how the understan.pdf:application/pdf},
}

@article{vuorre_sharing_2020,
	title = {Sharing and organizing research products as R packages},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-020-01436-x},
	doi = {10.3758/s13428-020-01436-x},
	abstract = {A consensus on the importance of open data and reproducible code is emerging. How should data and code be shared to maximize the key desiderata of reproducibility, permanence, and accessibility? Research assets should be stored persistently in formats that are not software restrictive, and documented so that others can reproduce and extend the required computations. The sharing method should be easy to adopt by already busy researchers. We suggest the R package standard as a solution for creating, curating, and communicating research assets. The R package standard, with extensions discussed herein, provides a format for assets and metadata that satisfies the above desiderata, facilitates reproducibility, open access, and sharing of materials through online platforms like {GitHub} and Open Science Framework. We discuss a stack of R resources that help users create reproducible collections of research assets, from experiments to manuscripts, in the {RStudio} interface. We created an R package, vertical, to help researchers incorporate these tools into their workflows, and discuss its functionality at length in an online supplement. Together, these tools may increase the reproducibility and openness of psychological science.},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Vuorre, Matti and Crump, Matthew J. C.},
	urldate = {2020-12-16},
	date = {2020-09-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/LT4BQ862/Vuorre and Crump - 2020 - Sharing and organizing research products as R pack.pdf:application/pdf},
}

@online{jacoby_should_2017,
	title = {Should Journals Be Responsible for Reproducibility?},
	url = {https://www.insidehighered.com/blogs/rethinking-research/should-journals-be-responsible-reproducibility},
	shorttitle = {Should Journals Be Responsible for Reproducibility?},
	abstract = {One of the top journals in political science makes data-sharing and replication part of the publication process.},
	titleaddon = {Inside Higher Ed},
	author = {Jacoby, W. G. and Lafferty-Hess, S. and Christian, T. M.},
	urldate = {2020-06-01},
	date = {2017},
	langid = {english},
	note = {Library Catalog: www.insidehighered.com},
	file = {Snapshot:/Users/tom/Zotero/storage/MVPCJLFC/should-journals-be-responsible-reproducibility.html:text/html},
}

@article{naudet_data_2018,
	title = {Data sharing and reanalysis of randomized controlled trials in leading biomedical journals with a full data sharing policy: survey of studies published in \textit{The {BMJ}} and \textit{{PLOS} Medicine}},
	volume = {360},
	issn = {0959-8138, 1756-1833},
	url = {http://www.bmj.com/lookup/doi/10.1136/bmj.k400},
	doi = {10.1136/bmj.k400},
	shorttitle = {Data sharing and reanalysis of randomized controlled trials in leading biomedical journals with a full data sharing policy},
	abstract = {{OBJECTIVES} To explore the effectiveness of data sharing by randomized controlled trials ({RCTs}) in journals with a full data sharing policy and to describe potential difficulties encountered in the process of performing reanalyses of the primary outcomes. {DESIGN} Survey of published {RCTs}. {SETTING} {PubMed}/Medline. {ELIGIBILITY} {CRITERIA} {RCTs} that had been submitted and published by The {BMJ} and {PLOS} Medicine subsequent to the adoption of data sharing policies by these journals.},
	pages = {k400},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Naudet, Florian and Sakarovitch, Charlotte and Janiaud, Perrine and Cristea, Ioana and Fanelli, Daniele and Moher, David and Ioannidis, John P A},
	urldate = {2020-06-12},
	date = {2018-02-13},
	langid = {english},
	file = {Naudet et al. - 2018 - Data sharing and reanalysis of randomized controll.pdf:/Users/tom/Zotero/storage/5KSA4BIL/Naudet et al. - 2018 - Data sharing and reanalysis of randomized controll.pdf:application/pdf},
}

@book{popper_conjectures_1963,
	location = {New York and Evanston},
	title = {Conjectures and Refutations: The Growth of Scientific Knowledge},
	publisher = {Harper \& Row},
	author = {Popper, Karl},
	date = {1963},
}

@article{white_epistemic_2003,
	title = {The epistemic advantage of prediction over accommodation},
	volume = {112},
	issn = {0026-4423, 1460-2113},
	url = {https://academic.oup.com/mind/article-lookup/doi/10.1093/mind/112.448.653},
	doi = {10.1093/mind/112.448.653},
	pages = {653--683},
	number = {448},
	journaltitle = {Mind},
	shortjournal = {Mind},
	author = {White, R.},
	urldate = {2020-12-14},
	date = {2003-10-01},
	langid = {english},
	file = {White - 2003 - The Epistemic Advantage of Prediction over Accommo.pdf:/Users/tom/Zotero/storage/XIS9LUQR/White - 2003 - The Epistemic Advantage of Prediction over Accommo.pdf:application/pdf},
}

@book{keynes_treatise_1921,
	location = {London},
	title = {Treatise on Probability},
	publisher = {Macmillan \& Co},
	author = {Keynes, John Maynard},
	date = {1921},
}

@article{maher_prediction_1988,
	title = {Prediction, accommodation, and the logic of discovery},
	volume = {1988},
	issn = {0270-8647},
	url = {https://www.journals.uchicago.edu/doi/10.1086/psaprocbienmeetp.1988.1.192994},
	doi = {10.1086/psaprocbienmeetp.1988.1.192994},
	pages = {273--285},
	number = {1},
	journaltitle = {{PSA}: Proceedings of the Biennial Meeting of the Philosophy of Science Association},
	shortjournal = {{PSA}: Proceedings of the Biennial Meeting of the Philosophy of Science Association},
	author = {Maher, Patrick},
	urldate = {2020-12-14},
	date = {1988-01},
	langid = {english},
	file = {Maher - 1988 - Prediction, Accommodation, and the Logic of Discov.pdf:/Users/tom/pCloud Drive/Zotero_Library/Maher - 1988 - Prediction, Accommodation, and the Logic of Discov.pdf:application/pdf},
}

@incollection{pierce_theory_1883,
	location = {Boston, {MA}},
	title = {A theory of probable inference},
	pages = {126--181},
	booktitle = {Studies in Logic},
	publisher = {Little, Brown},
	author = {Pierce, C. S.},
	date = {1883},
}

@book{whewell_philosophy_1847,
	location = {London},
	edition = {2},
	title = {The philosophy of the inductive sciences: founded upon their history.},
	volume = {2},
	shorttitle = {The philosophy of the inductive sciences},
	publisher = {John W. Parker},
	author = {Whewell, William},
	date = {1847},
}

@article{mertens_preregistration_2019,
	title = {Preregistration of Analyses of Preexisting Data},
	volume = {59},
	rights = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2054-670X},
	url = {http://www.psychologicabelgica.com/articles/10.5334/pb.493/},
	doi = {10.5334/pb.493},
	abstract = {The preregistration of a study’s hypotheses, methods, and data-analyses steps is becoming a popular psychological research practice. To date, most of the discussion on study preregistration has focused on the preregistration of studies that include the collection of original data. However, much of the research in psychology relies on the (re-)analysis of preexisting data. Importantly, this type of studies is different from original studies as researchers cannot change major aspects of the study (e.g., experimental manipulations, sample size). Here, we provide arguments as to why it is useful to preregister analyses of preexisting data, discuss practical considerations, consider potential concerns, and introduce a preregistration template tailored for studies focused on the analyses of preexisting data. We argue that the preregistration of hypotheses and data-analyses for analyses of preexisting data is an important step towards more transparent psychological research.},
	pages = {338--352},
	number = {1},
	journaltitle = {Psychologica Belgica},
	author = {Mertens, Gaëtan and Krypotos, Angelos-Miltiadis},
	urldate = {2020-12-11},
	date = {2019-08-22},
	langid = {english},
	note = {Number: 1
Publisher: Ubiquity Press},
	keywords = {Transparency, Archival Data, Open Science, Replicability},
	file = {Mertens_Krypotos_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Mertens_Krypotos_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/HP4KPENM/pb.493.html:text/html;Snapshot:/Users/tom/Zotero/storage/V3HMRD4X/pb.493.html:text/html},
}

@article{havron_preregistration_2020,
	title = {Preregistration in infant research—A primer},
	volume = {25},
	rights = {© 2020 International Congress of Infant Studies ({ICIS})},
	issn = {1532-7078},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/infa.12353},
	doi = {10.1111/infa.12353},
	abstract = {Preregistration, the act of specifying a research plan in advance, is becoming more common in scientific research. Infant researchers contend with unique problems that might make preregistration particularly challenging. Infants are a hard-to-reach population, usually yielding small sample sizes, they can only complete a limited number of trials, and they can be excluded based on hard-to-predict complications (e.g., parental interference, fussiness). In addition, as effects themselves potentially change with age and population, it is hard to calculate an a priori effect size. At the same time, these very factors make preregistration in infant studies a valuable tool. A priori examination of the planned study, including the hypotheses, sample size, and resulting statistical power, increases the credibility of single studies and adds value to the field. Preregistration might also improve explicit decision making to create better studies. We present an in-depth discussion of the issues uniquely relevant to infant researchers, and ways to contend with them in preregistration and study planning. We provide recommendations to researchers interested in following current best practices.},
	pages = {734--754},
	number = {5},
	journaltitle = {Infancy},
	author = {Havron, Naomi and Bergmann, Christina and Tsuji, Sho},
	urldate = {2020-12-11},
	date = {2020},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/infa.12353},
	file = {Havron_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Havron_etal_2020.pdf:application/pdf},
}

@article{krypotos_step-by-step_2019,
	title = {A step-by-step guide on preregistration and effective data sharing for psychopathology research},
	volume = {128},
	issn = {1939-1846(Electronic),0021-843X(Print)},
	doi = {10.1037/abn0000424},
	abstract = {Data analysis in psychopathology research typically entails multiple stages of data preprocessing (e.g., coding of physiological measures), statistical decisions (e.g., inclusion of covariates), and reporting (e.g., selecting which variables best answer the research questions). The complexity and lack of transparency of these procedures have resulted in two troubling trends: the central hypotheses and analytical approaches are often selected after observing the data, and the research data are often not properly indexed. These practices are particularly problematic for (experimental) psychopathology research because the data are often hard to gather due to the target populations (e.g., individuals with mental disorders), and because the standard methodological approaches are challenging and time consuming (e.g., longitudinal studies). Here, we present a workflow that covers study preregistration, data anonymization, and the easy sharing of data and experimental material with the rest of the research community. This workflow is tailored to both original studies and secondary statistical analyses of archival data sets. In order to facilitate the implementation of the described workflow, we have developed a free and open-source software program. We argue that this workflow will result in more transparent and easily shareable psychopathology research, eventually increasing and replicability reproducibility in our research field. ({PsycINFO} Database Record (c) 2019 {APA}, all rights reserved)},
	pages = {517--527},
	number = {6},
	journaltitle = {Journal of Abnormal Psychology},
	author = {Krypotos, Angelos-Miltiadis and Klugkist, Irene and Mertens, Gaëtan and Engelhard, Iris M.},
	date = {2019},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Statistical Analysis, Experimentation, Coding Scheme, Computer Software, Data Sharing, Mental Disorders, Psychopathology},
	file = {Krypotos_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Krypotos_etal_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/G7GMWDHD/2019-43757-006.html:text/html;Snapshot:/Users/tom/Zotero/storage/46L6Z8RL/2019-43757-006.html:text/html},
}

@article{proctor_cumulative_2009,
	title = {Cumulative Knowledge and Progress in Human Factors},
	volume = {61},
	issn = {0066-4308},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.psych.093008.100325},
	doi = {10.1146/annurev.psych.093008.100325},
	abstract = {This review provides a cumulative perspective on current human factors research by first briefly acknowledging previous Annual Review articles. We show that several recent conceptual advances are an outgrowth of the information-processing approach adopted by the field and present several areas of current research that are built directly on prior work. Topic areas that provide fundamental tools for human factors analyses are summarized, and several current application areas are reviewed. We end by considering alternatives to the information-processing approach that have been proposed and placing those alternatives in context. We argue that the information-processing language provides the foundation that has enabled much of the growth in human factors. This growth reflects a cumulative development of concepts and methods that continues today.},
	pages = {623--651},
	number = {1},
	journaltitle = {Annual Review of Psychology},
	shortjournal = {Annu. Rev. Psychol.},
	author = {Proctor, Robert W. and Vu, Kim-Phuong L.},
	urldate = {2020-12-10},
	date = {2009-12-03},
	note = {Publisher: Annual Reviews},
	file = {Snapshot:/Users/tom/Zotero/storage/D795GFV5/annurev.psych.093008.html:text/html},
}

@article{rindal_does_2016,
	title = {Does reactivating a witnessed memory increase its susceptibility to impairment by subsequent misinformation?},
	volume = {42},
	issn = {1939-1285, 0278-7393},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xlm0000265},
	doi = {10.1037/xlm0000265},
	abstract = {In a recent {PNAS} article, Chan and {LaPaglia} (2013) provided arguments and evidence to support the claim that reactivating a witnessed memory (by taking a test) renders the memory labile and susceptible to impairment by subsequent misinformation. In the current article, we argue that Chan and {LaPaglia}’s (2013) findings are open to alternative interpretations, and further test the hypothesis that reactivation increases a witnessed memory’s susceptibility to impairment. To this end, the current studies used a different set of materials and a different measure of memory impairment, the Modified Recognition Test ({McCloskey} \& Zaragoza, 1985). In Experiment 1a, we established that our reactivation manipulation was effective by showing that we could replicate the well-established retrieval enhanced suggestibility effect with our materials. However, when we assessed potential impairment of the witnessed memory with the Modified Recognition Test (Experiments 1a and 1b), we failed to find evidence that reactivating the witnessed memory prior to misinformation impaired memory for the originally witnessed event. In Experiment 2, we replicated Chan and {LaPaglia}’s (2013) findings when we used their memory impairment measure (misinformation-free True/False Recognition Test) and showed why that test does not permit clear inferences about memory impairment. Collectively, the results showed that, although the reactivation manipulation increased susceptibility to suggestion (i.e., as evidenced by increased reporting of suggested misinformation), there was no evidence that reactivation through testing increased the original memory’s susceptibility to impairment.},
	pages = {1544--1558},
	number = {10},
	journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	shortjournal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Rindal, Eric J. and {DeFranco}, Rachel M. and Rich, Patrick R. and Zaragoza, Maria S.},
	urldate = {2020-12-09},
	date = {2016},
	langid = {english},
	file = {Rindal et al. - 2016 - Does reactivating a witnessed memory increase its .pdf:/Users/tom/Zotero/storage/R8AIHMST/Rindal et al. - 2016 - Does reactivating a witnessed memory increase its .pdf:application/pdf},
}

@article{mccloskey_misleading_1985,
	title = {Misleading postevent information and memory for events: arguments and evidence against memory impairment hypotheses},
	volume = {114},
	doi = {10.1037/0096-3445.114.1.1},
	pages = {1--16},
	number = {1},
	journaltitle = {Journal of Experimental Psychology: General},
	author = {{McCloskey}, Michael and Zaragoza, Maria},
	date = {1985},
	langid = {english},
	file = {McCloskey and Zaragoza - Misleading Postevent Information and Memory for Ev.pdf:/Users/tom/Zotero/storage/G5BTS854/McCloskey and Zaragoza - Misleading Postevent Information and Memory for Ev.pdf:application/pdf},
}

@book{busemeyer_model_2015,
	title = {Model Comparison and the Principle of Parsimony},
	volume = {1},
	url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14},
	abstract = {According to the principle of parsimony, model selection methods should value both descriptive accuracy and simplicity. Here we focus primarily on Bayes factors and minimum description length, explaining how these procedures strike a balance between goodness-of-fit and parsimony. Throughout, we demonstrate the methods with an application on false memory, evaluating three competing multimonial proces tree models of interference in memory.},
	publisher = {Oxford University Press},
	author = {Vandekerckhove, Joachim and Matzke, Dora and Wagenmakers, Eric-Jan},
	editor = {Busemeyer, Jerome R. and Wang, Zheng and Townsend, James T. and Eidels, Ami},
	urldate = {2020-12-08},
	date = {2015-12-10},
	langid = {english},
	doi = {10.1093/oxfordhb/9780199957996.013.14},
	file = {Vandekerckhove et al. - 2015 - Model Comparison and the Principle of Parsimony.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vandekerckhove et al. - 2015 - Model Comparison and the Principle of Parsimony.pdf:application/pdf},
}

@article{hasher_i_1981,
	title = {I knew it all along: or, did I?},
	volume = {20},
	issn = {0022-5371},
	url = {http://www.sciencedirect.com/science/article/pii/S0022537181903236},
	doi = {10.1016/S0022-5371(81)90323-6},
	shorttitle = {I knew it all along},
	abstract = {Other studies have reported that when subjects are presented with outcome feedback they are unable to remember their original knowledge state (the “knew-it-all-along effect”). In the present studies, feedback was followed by manipulations which were intended to invalidate it. In the first experiment, we failed to discredit the feedback and so report a knew-it-all-along effect under circumstances different from those reported elsewhere. In the second experiment, the discrediting instructions were successful and the effect was disrupted. Contrary to previous interpretations, the latter results indicate that feedback information is not automatically assimilated and that people can access their prior knowledge state, if the circumstances require.},
	pages = {86--96},
	number = {1},
	journaltitle = {Journal of Verbal Learning and Verbal Behavior},
	shortjournal = {Journal of Verbal Learning and Verbal Behavior},
	author = {Hasher, Lynn and Attig, Mary S. and Alba, Joseph W.},
	urldate = {2020-12-08},
	date = {1981-02-01},
	langid = {english},
	file = {Hasher_etal_1981.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hasher_etal_1981.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/9VQYY5R3/S0022537181903236.html:text/html},
}

@article{batchelder_theoretical_1999,
	title = {Theoretical and empirical review of multinomial process tree modeling},
	volume = {6},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/BF03210812},
	doi = {10.3758/BF03210812},
	abstract = {We review a current and popular class of cognitive models calledmultinomial processing tree ({MPT}) models. {MPT} models are simple, substantively motivated statistical models that can be applied to categorical data. They are useful as data-analysis tools for measuring underlying or latent cognitive capacities and as simple models for representing and testing competing psychological theories. We formally describe the cognitive structure and parametric properties of the class of {MPT} models and provide an inferential statistical analysis for the entire class. Following this, we provide a comprehensive review of over 80 applications of {MPT} models to a variety of substantive areas in cognitive psychology, including various types of human memory, visual and auditory perception, and logical reasoning. We then address a number of theoretical issues relevant to the creation and evaluation of {MPT} models, including model development, model validity, discrete-state assumptions, statistical issues, and the relation between {MPT} models and other mathematical models. In the conclusion, we consider the current role of {MPT} models in psychological research and possible future directions.},
	pages = {57--86},
	number = {1},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychonomic Bulletin \& Review},
	author = {Batchelder, William H. and Riefer, David M.},
	urldate = {2020-12-08},
	date = {1999-03-01},
	langid = {english},
	file = {Batchelder_Riefer_1999.pdf:/Users/tom/pCloud Drive/Zotero_Library/Batchelder_Riefer_1999.pdf:application/pdf},
}

@article{wright_accurate_1996,
	title = {Accurate second guesses in misinformation studies},
	volume = {10},
	rights = {Copyright © 1996 John Wiley \& Sons, Ltd.},
	issn = {1099-0720},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-0720%28199602%2910%3A1%3C13%3A%3AAID-ACP356%3E3.0.CO%3B2-L},
	doi = {10.1002/(sici)1099-0720(199602)10:1<13::aid-acp356>3.0.co;2-l},
	abstract = {The reason why misleading post-event information affects later recollections is the subject of a heated debate within cognitive psychology. One series of studies that is often cited is when subjects are allowed a second guess. Loftus (1979) found that the second guesses of errant misled subjects were not above chance levels. This, she argued, suggests that the memory for the original information cannot be accessed at testing. Four studies are reported in which subjects were allowed second guesses. In these studies errant misled subjects second guesses were better than chance. We discuss how these findings inform the debate about why misinformation affects memory.},
	pages = {13--21},
	number = {1},
	journaltitle = {Applied Cognitive Psychology},
	author = {Wright, Daniel B. and Varley, Stella and Belton, Aine},
	urldate = {2020-12-08},
	date = {1996},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291099-0720\%28199602\%2910\%3A1\%3C13\%3A\%3AAID-{ACP}356\%3E3.0.{CO}\%3B2-L},
	file = {Snapshot:/Users/tom/Zotero/storage/9VRUYDWX/(SICI)1099-0720(199602)10113AID-ACP3563.0.html:text/html;Wright_etal_1996.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wright_etal_1996.pdf:application/pdf},
}

@article{gotzsche_reference_1987,
	title = {Reference bias in reports of drug trials.},
	volume = {295},
	issn = {0267-0623, 1468-5833},
	url = {https://www.bmj.com/content/295/6599/654},
	doi = {10.1136/bmj.295.6599.654},
	abstract = {Articles published before 1985 describing double blind trials of two or more non-steroidal anti-inflammatory drugs in rheumatoid arthritis were examined to see whether there was any bias in the references they cited. Althogether 244 articles meeting the criteria were found through a Medline search and through examining the reference lists of the articles retrieved. The drugs compared in the studies were classified as new or as control drugs and the outcome of the trial as positive or not positive. The reference lists of all papers with references to other trials on the new drug were then examined for reference bias. Positive bias was judged to have occurred if the reference list contained a higher proportion of references with a positive outcome for that drug than among all the articles assumed to have been available to the authors (those published more than two years earlier than the index article). Altogether 133 of the 244 articles were excluded for various reasons--for example, 44 because of multiple publication and 19 because they had no references. Among the 111 articles analysed bias was not possible in the references of 35 (because all the references gave the same outcome); 10 had a neutral selection of references, 22 a negative selection, and 44 a positive selection--a significant positive bias. This bias was not caused by better scientific standing of the cited articles over the uncited ones. Thus retrieving literature by scanning reference lists may produce a biased sample of articles, and reference bias may also render the conclusions of an article less reliable.},
	pages = {654--656},
	number = {6599},
	journaltitle = {Br Med J (Clin Res Ed)},
	shortjournal = {Br Med J (Clin Res Ed)},
	author = {Gøtzsche, P. C.},
	urldate = {2020-12-08},
	date = {1987-09-12},
	langid = {english},
	pmid = {3117277},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Article},
	file = {Gøtzsche_1987.pdf:/Users/tom/pCloud Drive/Zotero_Library/Gøtzsche_1987.pdf:application/pdf},
}

@article{mccloskey_postevent_nodate,
	title = {Postevent Information and Memory: Reply to Loftus, Schooler, and Wagenaar},
	doi = {10.1037/0096-3445.114.3.381},
	pages = {7},
	author = {{McCloskey}, Michael and Zaragoza, Maria},
	langid = {english},
	file = {McCloskey and Zaragoza - Postevent Information and Memory Reply to Loftus,.pdf:/Users/tom/Zotero/storage/N4Q2NSRB/McCloskey and Zaragoza - Postevent Information and Memory Reply to Loftus,.pdf:application/pdf},
}

@article{koriat_memory_1996,
	title = {Memory metaphors and the real-life/laboratory controversy: Correspondence versus storehouse conceptions of memory},
	volume = {19},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/memory-metaphors-and-the-reallifelaboratory-controversy-correspondence-versus-storehouse-conceptions-of-memory/5C2A5F74C9D820A4FEB516DAA1E2E358},
	doi = {10.1017/S0140525X00042114},
	shorttitle = {Memory metaphors and the real-life/laboratory controversy},
	abstract = {The study of memory is witnessing a spirited clash between proponents of traditional laboratory research and those advocating a more naturalistic approach to the study of “real-life” or “everyday” memory. The debate has generally centered on the “what” (content), “where” (context), and “how” (methods) of memory research. In this target article, we argue that the controversy discloses a further, more fundamental breach between two underlying memory metaphors, each having distinct implications for memory theory and assessment: Whereas traditional memory research has been dominated by the storehouse metaphor, leading to a focus on the number of items remaining in store and accessible to memory, the recent wave of everyday memory research has shifted toward a correspondence metaphor, focusing on the accuracy of memory in representing past events. The correspondence metaphor calls for a research approach that differs from the traditional one in important respects: in emphasizing the intentional –representational function of memory, in addressing the wholistic and graded aspects of memory correspondence, in taking an output-bound assessment perspective, and in allowing more room for the operation of subject-controlled metamemory processes and motivational factors. This analysis can help tie together soine of the what, where, and how aspects of the “real-life/laboratory” controversy. More important, however, by explicating the unique metatheoretical foundation of the accuracy-oriented approach to memory we aim to promote a more effective exploitation of the correspondence metaphor in both naturalistic and laboratory research contexts.},
	pages = {167--188},
	number = {2},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Koriat, Asher and Goldsmith, Morris},
	urldate = {2020-12-08},
	date = {1996-06},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {memory, accuracy, correspondence, everyday memory, intentionality, metamemory, monitoring, recall, recognition, response criterion, signal detection, storehouse},
	file = {Koriat_Goldsmith_1996.pdf:/Users/tom/pCloud Drive/Zotero_Library/Koriat_Goldsmith_1996.pdf:application/pdf},
}

@article{wright_misinformation_nodate,
	title = {Misinformation and warnings in eyewitness testimony: A new testing procedure to differentiate explan},
	doi = {10.1080/09658219308258229},
	pages = {15},
	author = {Wright, Daniel B},
	langid = {english},
	file = {Wright - Misinformation and warnings in eyewitness testimon.pdf:/Users/tom/Zotero/storage/KHXS6WZM/Wright - Misinformation and warnings in eyewitness testimon.pdf:application/pdf},
}

@incollection{howe_misinformation_1992,
	location = {New York, {NY}},
	title = {The Misinformation Effect: Transformations in Memory Induced by Postevent Information},
	isbn = {978-1-4612-7702-6 978-1-4612-2868-4},
	url = {http://link.springer.com/10.1007/978-1-4612-2868-4_5},
	shorttitle = {The Misinformation Effect},
	pages = {159--183},
	booktitle = {Development of Long-Term Retention},
	publisher = {Springer New York},
	author = {Loftus, Elizabeth F. and Hoffman, Hunter G. and Wagenaar, Willem A.},
	editor = {Howe, Mark L. and Brainerd, Charles J. and Reyna, Valerie F.},
	urldate = {2020-12-08},
	date = {1992},
	langid = {english},
	doi = {10.1007/978-1-4612-2868-4_5},
	file = {Loftus et al. - 1992 - The Misinformation Effect Transformations in Memo.pdf:/Users/tom/Zotero/storage/RJSDZLBK/Loftus et al. - 1992 - The Misinformation Effect Transformations in Memo.pdf:application/pdf},
}

@article{loftus_planting_2005,
	title = {Planting misinformation in the human mind: A 30-year investigation of the malleability of memory},
	volume = {12},
	issn = {1072-0502},
	url = {http://www.learnmem.org/cgi/doi/10.1101/lm.94705},
	doi = {10.1101/lm.94705},
	shorttitle = {Planting misinformation in the human mind},
	pages = {361--366},
	number = {4},
	journaltitle = {Learning \& Memory},
	shortjournal = {Learning \& Memory},
	author = {Loftus, E. F.},
	urldate = {2020-12-08},
	date = {2005-07-18},
	langid = {english},
	file = {Loftus - 2005 - Planting misinformation in the human mind A 30-ye.pdf:/Users/tom/Zotero/storage/BSMMY36U/Loftus - 2005 - Planting misinformation in the human mind A 30-ye.pdf:application/pdf},
}

@article{loftus_misinformation_1989,
	title = {Misinformation and Memory: The Creation of New Memories},
	volume = {118},
	doi = {10.1037/0096-3445.118.1.100},
	pages = {100--104},
	number = {1},
	journaltitle = {Journal of Experimental Psychology: General},
	author = {Loftus, Elizabeth F and Hoffman, Hunter G},
	date = {1989},
	langid = {english},
	file = {Loftus and Hoffman - Misinformation and Memory The Creation of New Mem.pdf:/Users/tom/Zotero/storage/GPEPH49C/Loftus and Hoffman - Misinformation and Memory The Creation of New Mem.pdf:application/pdf},
}

@article{zaragoza_misleading_1987,
	title = {Misleading postevent information and recall of the original event: further evidence against the memory impairment hypothesis},
	doi = {10.1037/0278-7393.13.1.36},
	pages = {9},
	author = {Zaragoza, Maria S and Jamis, Mary and {McCloskey}, Michael},
	date = {1987},
	langid = {english},
	file = {Zaragoza et al. - Misleading Postevent Information and Recall of the.pdf:/Users/tom/Zotero/storage/Y7L9PA3J/Zaragoza et al. - Misleading Postevent Information and Recall of the.pdf:application/pdf},
}

@article{decoster_opportunistic_2015,
	title = {Opportunistic biases: Their origins, effects, and an integrated solution.},
	volume = {70},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039191},
	doi = {10.1037/a0039191},
	shorttitle = {Opportunistic biases},
	abstract = {Researchers commonly explore their data in multiple ways before deciding which analyses they will include in the ﬁnal versions of their papers. While this improves the chances of researchers ﬁnding publishable results, it introduces an “opportunistic bias,” such that the reported relations are stronger or otherwise more supportive of the researcher’s theories than they would be without the exploratory process. The magnitudes of opportunistic biases can often be stronger than those of the effects being investigated, leading to invalid conclusions and a lack of clarity in research results. Authors typically do not report their exploratory procedures, so opportunistic biases are very difﬁcult to detect just by reading the ﬁnal version of a research report. In this article, we explain how a number of accepted research practices can lead to opportunistic biases, discuss the prevalence of these practices in psychology, consider the different effects that opportunistic biases have on psychological science, evaluate the strategies that methodologists have proposed to prevent or correct for the effects of these biases, and introduce an integrated solution to reduce the prevalence and inﬂuence of opportunistic biases. The recent prominence of articles discussing questionable research practices both in scientiﬁc journals and in the public media underscores the importance of understanding how opportunistic biases are created and how we might undo their effects.},
	pages = {499--514},
	number = {6},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {{DeCoster}, Jamie and Sparks, Erin A. and Sparks, Jordan C. and Sparks, Glenn G. and Sparks, Cheri W.},
	urldate = {2020-12-04},
	date = {2015-09},
	langid = {english},
	file = {DeCoster et al. - 2015 - Opportunistic biases Their origins, effects, and .pdf:/Users/tom/Zotero/storage/PR25SU36/DeCoster et al. - 2015 - Opportunistic biases Their origins, effects, and .pdf:application/pdf},
}

@article{towse_opening_2020,
	title = {Opening Pandora’s Box: Peeking inside psychology’s data sharing practices, and seven recommendations for change},
	volume = {53},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-020-01486-1},
	doi = {10.3758/s13428-020-01486-1},
	shorttitle = {Opening Pandora’s Box},
	abstract = {Open data-sharing is a valuable practice that ought to enhance the impact, reach, and transparency of a research project. While widely advocated by many researchers and mandated by some journals and funding agencies, little is known about detailed practices across psychological science. In a pre-registered study, we show that overall, few research papers directly link to available data in many, though not all, journals. Most importantly, even where open data can be identified, the majority of these lacked completeness and reusability—conclusions that closely mirror those reported outside of Psychology. Exploring the reasons behind these findings, we offer seven specific recommendations for engineering and incentivizing improved practices, so that the potential of open data can be better realized across psychology and social science more generally.},
	pages = {1455--1468},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Towse, John N. and Ellis, David A. and Towse, Andrea S.},
	urldate = {2020-12-04},
	date = {2020-11-11},
	langid = {english},
	file = {Towse et al. - 2020 - Opening Pandora’s Box Peeking inside psychology’s.pdf:/Users/tom/Zotero/storage/IYSNDXDJ/Towse et al. - 2020 - Opening Pandora’s Box Peeking inside psychology’s.pdf:application/pdf},
}

@article{wang_researcher_2018,
	title = {Researcher requests for inappropriate analysis and reporting: a U.S. survey of consulting biostatisticians},
	volume = {169},
	issn = {0003-4819},
	url = {https://www.acpjournals.org/doi/10.7326/M18-1230},
	doi = {10.7326/M18-1230},
	shorttitle = {Researcher requests for inappropriate analysis and reporting},
	pages = {554--558},
	number = {8},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Wang, Min Qi and Yan, Alice F. and Katz, Ralph V.},
	urldate = {2020-12-03},
	date = {2018-10-09},
	note = {Publisher: American College of Physicians},
	file = {Snapshot:/Users/tom/Zotero/storage/TNZBTRVE/M18-1230.html:text/html;Wang_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wang_etal_2018.pdf:application/pdf},
}

@book{mill_system_1843,
	location = {London},
	title = {A system of logic},
	shorttitle = {A system of logic ratio},
	publisher = {George Routledge \& Sons},
	author = {Mill, John Stuart},
	date = {1843},
}

@book{mayo_statistical_2018,
	edition = {1},
	title = {Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars},
	isbn = {978-1-107-28618-4 978-1-107-05413-4 978-1-107-66464-7},
	url = {https://www.cambridge.org/core/product/identifier/9781107286184/type/book},
	shorttitle = {Statistical Inference as Severe Testing},
	publisher = {Cambridge University Press},
	author = {Mayo, Deborah G.},
	urldate = {2020-12-03},
	date = {2018-09-20},
	langid = {english},
	file = {Mayo - 2018 - Statistical Inference as Severe Testing How to Ge.pdf:/Users/tom/Zotero/storage/JS7S9CQ8/Mayo - 2018 - Statistical Inference as Severe Testing How to Ge.pdf:application/pdf},
}

@article{vancouver_defense_2018,
	title = {In Defense of {HARKing}},
	volume = {11},
	issn = {1754-9426, 1754-9434},
	url = {https://www.cambridge.org/core/journals/industrial-and-organizational-psychology/article/abs/in-defense-of-harking/8DB9E0156C9D3A99BACA1F87D2DFB0E4},
	doi = {10.1017/iop.2017.89},
	abstract = {Science is a complex task. It involves the creation and dissemination of knowledge. The creation of knowledge requires identifying and abstracting patterns (i.e., identifying phenomena and theorizing about the processes that bring it about), as well as systematically observing to better see and quantify the patterns (e.g., effect size estimating) or assess the validity of the abstractions used to explain the patterns (i.e., theory testing). To help (a) hone in on what observations would be useful and (b) communicate what the patterns mean, we are encouraged to develop and report hypotheses. That is, strategically, hypotheses facilitate the planning of data collection by helping the researcher understand what patterns need to be observed to assess the merit of an explanation. Meanwhile, tactically, hypotheses help focus the audience on the crucial patterns needed to answer a question or test a theory. When the strategic hypotheses are not supported, it raises a question regarding what to do tactically. Depending on the result (i.e., different direction; null), one might construct a hypothesis to facilitate dissemination without reporting this post hoc construction or remove mention of a hypothesis altogether. This practice is called {HARKing} (i.e., hypothesizing after results are known). {HARKing} has been so disparaged as to be considered a “detrimental research practice” (Grand et al., 2018, p. 6). As such, the Society for Industrial and Organizational Psychology's ({SIOP}) Robust and Reliability Science task force appears to be recommending that {HARKing} not be taught by educators, encouraged by reviewers or editors, or practiced by authors. I do not agree with those recommendations, and I elaborate on my position below.},
	pages = {73--80},
	number = {1},
	journaltitle = {Industrial and Organizational Psychology},
	author = {Vancouver, Jeffrey B.},
	urldate = {2020-12-03},
	date = {2018-03},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Snapshot:/Users/tom/Zotero/storage/YDW97UF9/8DB9E0156C9D3A99BACA1F87D2DFB0E4.html:text/html;Vancouver_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vancouver_2018.pdf:application/pdf},
}

@article{worrall_prediction_2014,
	title = {Prediction and accommodation revisited},
	volume = {45},
	issn = {0039-3681},
	url = {http://www.sciencedirect.com/science/article/pii/S0039368113000976},
	doi = {10.1016/j.shpsa.2013.10.001},
	abstract = {The paper presents a further articulation and defence of the view on prediction and accommodation that I have proposed earlier. It operates by analysing two accounts of the issue—by Patrick Maher and by Marc Lange—that, at least at first sight, appear to be rivals to my own. Maher claims that the time-order of theory and evidence may be important in terms of degree of confirmation, while that claim is explicitly denied in my account. I argue, however, that when his account is analysed, Maher reveals no scientifically significant way in which the time-order counts, and that indeed his view is in the end best regarded as a less than optimally formulated version of my own. Lange has also responded to Maher by arguing that the apparent relevance of temporal considerations is merely apparent: what is really involved, according to Lange, is whether or not a hypothesis constitutes an “arbitrary conjunction.” I argue that Lange’s suggestion fails: the correct analysis of his and Maher’s examples is that provided by my account.},
	pages = {54--61},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Worrall, John},
	urldate = {2020-12-03},
	date = {2014-03-01},
	langid = {english},
	keywords = {Prediction, Accommodation, Confirmation, Theory-change},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/4EHGV4AZ/S0039368113000976.html:text/html;Worrall - 2014 - Prediction and accommodation revisited.pdf:/Users/tom/Zotero/storage/DGDVHTDB/Worrall - 2014 - Prediction and accommodation revisited.pdf:application/pdf},
}

@report{navarro_paths_2020,
	title = {Paths in strange spaces: A comment on preregistration},
	url = {https://psyarxiv.com/wxn58/},
	shorttitle = {Paths in strange spaces},
	abstract = {This is an archived version of a blog post on preregistration. The first half of the post argues that there is not a strong justification for preregistration as a tool to solve problems with statistical inference (p-hacking); the second half argues that preregistration has a stronger justification as one tool (among many) that can aid scientists in documenting our projects. [Note that this archival version exists only because the blog itself no longer does, and as the original has been cited multiple times there is value in ensuring that some version of the blog post remains accessible.]},
	institution = {{PsyArXiv}},
	author = {Navarro, Danielle},
	urldate = {2020-12-03},
	date = {2020-09-23},
	keywords = {Social and Behavioral Sciences},
	file = {Navarro_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Navarro_2020.pdf:application/pdf},
}

@article{oberauer_addressing_2019,
	title = {Addressing the theory crisis in psychology},
	volume = {26},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-019-01645-2},
	doi = {10.3758/s13423-019-01645-2},
	abstract = {A worrying number of psychological findings are not replicable. Diagnoses of the causes of this “replication crisis,” and recommendations to address it, have nearly exclusively focused on methods of data collection, analysis, and reporting. We argue that a further cause of poor replicability is the often weak logical link between theories and their empirical tests. We propose a distinction between discovery-oriented and theory-testing research. In discovery-oriented research, theories do not strongly imply hypotheses by which they can be tested, but rather define a search space for the discovery of effects that would support them. Failures to find these effects do not question the theory. This endeavor necessarily engenders a high risk of Type I errors—that is, publication of findings that will not replicate. Theory-testing research, by contrast, relies on theories that strongly imply hypotheses, such that disconfirmation of the hypothesis provides evidence against the theory. Theory-testing research engenders a smaller risk of Type I errors. A strong link between theories and hypotheses is best achieved by formalizing theories as computational models. We critically revisit recommendations for addressing the “replication crisis,” including the proposal to distinguish exploratory from confirmatory research, and the preregistration of hypotheses and analysis plans.},
	pages = {1596--1618},
	number = {5},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Oberauer, Klaus and Lewandowsky, Stephan},
	urldate = {2020-12-03},
	date = {2019-10-01},
	langid = {english},
	file = {Oberauer_Lewandowsky_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Oberauer_Lewandowsky_2019.pdf:application/pdf},
}

@book{hacking_introduction_2001,
	location = {Cambridge, U.K. ; New York},
	title = {An introduction to probability and inductive logic},
	isbn = {978-0-521-77287-7 978-0-521-77501-4},
	pagetotal = {302},
	publisher = {Cambridge University Press},
	author = {Hacking, Ian},
	date = {2001},
	keywords = {Induction (Logic), Probabilities},
	file = {Introduction to Probability and Inductive Logic.pdf:/Users/tom/Zotero/storage/M4HSMGPC/Introduction to Probability and Inductive Logic.pdf:application/pdf},
}

@book{earman_bayes_1992,
	location = {Cambridge, Mass},
	title = {Bayes or bust? a critical examination of Bayesian confirmation theory},
	isbn = {978-0-262-05046-3},
	shorttitle = {Bayes or bust?},
	pagetotal = {272},
	publisher = {{MIT} Press},
	author = {Earman, John},
	date = {1992},
	keywords = {Bayesian statistical decision theory, Philosophy, Science, Methodology},
	file = {Earman - 1992 - Bayes or bust a critical examination of Bayesian .pdf:/Users/tom/Zotero/storage/XMLIJJAU/Earman - 1992 - Bayes or bust a critical examination of Bayesian .pdf:application/pdf},
}

@book{dienes_understanding_2008,
	location = {New York},
	title = {Understanding psychology as a science: an introduction to scientific and statistical inference},
	isbn = {978-0-230-54230-3 978-0-230-54231-0},
	shorttitle = {Understanding psychology as a science},
	pagetotal = {170},
	publisher = {Palgrave Macmillan},
	author = {Dienes, Zoltan},
	date = {2008},
	keywords = {Psychology, Philosophy, Science},
	file = {Dienes - Understanding psychology as a science 978023054231.pdf:/Users/tom/Zotero/storage/NMAQ7LLH/Dienes - Understanding psychology as a science 978023054231.pdf:application/pdf},
}

@article{berkowitz_thoughts_1992,
	title = {Some Thoughts about Conservative Evaluations of Replications},
	volume = {18},
	issn = {0146-1672},
	url = {https://doi.org/10.1177/0146167292183007},
	doi = {10.1177/0146167292183007},
	abstract = {It is suggested that an unduly conservative research tradition operates in social psychology to heighten the perception of inconsistency in research replications. In large part, this tradition, producing a bias in favor of the null hypothesis, stems from a belief in "the law of small numbers" and a failure to appreciate the probabilistic nature of research results so that each replication or dependent measure is expected to be significant at better than the .05 level by itself: The legacy of R. A. Fisher's approach to inferential statistics, with its emphasis on avoiding Type I errors and neglect of the possibility of Type {II} errors, is also considered. It is also noted that social psychological tests of hypothesized relationships are typically low in power. he argument in favor of combining probabilities over the series of replications is supported. It may also be, however; that a good many social psychologists are also greatly risk averse, so that the possibility of a negative outcome (making a mistake) is given very much more weight than positive information (favoring the hypothesis being tested by the research).},
	pages = {319--324},
	number = {3},
	journaltitle = {Personality and Social Psychology Bulletin},
	shortjournal = {Pers Soc Psychol Bull},
	author = {Berkowitz, Leonard},
	urldate = {2020-12-02},
	date = {1992-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Berkowitz_1992.pdf:/Users/tom/pCloud Drive/Zotero_Library/Berkowitz_1992.pdf:application/pdf},
}

@article{diamond_truth_2020,
	title = {The Truth Is Out There: Accuracy in Recall of Verifiable Real-World Events},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797620954812},
	doi = {10.1177/0956797620954812},
	shorttitle = {The Truth Is Out There},
	abstract = {How accurate is memory? Although people implicitly assume that their memories faithfully represent past events, the prevailing view in research is that memories are error prone and constructive. Yet little is known about the frequency of errors, particularly in memories for naturalistic experiences. Here, younger and older adults underwent complex real-world experiences that were nonetheless controlled and verifiable, freely recalling these experiences after days to years. As expected, memory quantity and the richness of episodic detail declined with increasing age and retention interval. Details that participants did recall, however, were highly accurate (93\%–95\%) across age and time. This level of accuracy far exceeded comparatively low estimations among memory scientists and other academics in a survey. These findings suggest that details freely recalled from one-time real-world experiences can retain high correspondence to the ground truth despite significant forgetting, with higher accuracy than expected given the emphasis on fallibility in the field of memory research.},
	pages = {0956797620954812},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Diamond, Nicholas B. and Armson, Michael J. and Levine, Brian},
	urldate = {2020-12-01},
	date = {2020-11-23},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Diamond_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Diamond_etal_2020.pdf:application/pdf},
}

@article{jacob_experimenter_1968,
	title = {The experimenter bias effect: A failure to replicate},
	volume = {13},
	issn = {0033-3131(Print)},
	doi = {10.3758/BF03342502},
	shorttitle = {The experimenter bias effect},
	abstract = {Attempted to replicate the E-bias effect using Rosenthal's basic paradigm, examine the relationship between E-bias and Ss' perception of Es' expectancies, and evaluate the E-bias effect in relation to a no-expectancy control group. 15 undergraduates were Es and 6 undergraduates were Ss. Results failed to demonstrate the E-bias effect, although Ss' judgments of Es' expectancies were related to the 3 different treatment groups. The present findings are discussed briefly in terms of T. X. Barber and M. J. Silver's (see 43:5) review of this area. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {239--240},
	number = {4},
	journaltitle = {Psychonomic Science},
	author = {Jacob, Theodore},
	date = {1968},
	note = {Place: {US}
Publisher: Psychonomic Society},
	keywords = {Experimentation, Drawing, Expectations, Prejudice, Social Perception},
	file = {Jacob_1968.pdf:/Users/tom/pCloud Drive/Zotero_Library/Jacob_1968.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TXJWP55T/1969-06148-001.html:text/html},
}

@article{barber_fact_1968,
	title = {Fact, fiction, and the experimenter bias effect},
	volume = {70},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/h0026724},
	abstract = {Critically analyzes 31 studies which attempted to demonstrate that Es' expectancies and desires significantly affect the experimental outcome (E bias effect). The majority of studies do not clearly demonstrate the effect. Many of these studies are criticized for inadequacies in the analysis of results, e.g., failure to perform an overall statistical analysis to exclude the null hypothesis and failure to avoid probability pyramiding when postmortem tests are performed. 2 conclusions are drawn: (1) The E bias effect appears to be more difficult to demonstrate and less pervasive than was implied in previous reviews in this journal. (2) In those instances in which the effect was obtained, it was apparently due to one or more of the following: the student Es misjudged, misrecorded or misreported the results; they verbally reinforced their Ss for expected responses; or they intentionally or unintentionally transmitted their expectancies and desires by paralinguistic or kinesic cues. (2 p. ref.) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {1--29},
	number = {6},
	journaltitle = {Psychological Bulletin},
	author = {Barber, Theodore X. and Silver, Maurice J.},
	date = {1968},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Experimentation, Methodology, Expectations, Prejudice, Literature Review},
	file = {Barber_Silver_1968.pdf:/Users/tom/pCloud Drive/Zotero_Library/Barber_Silver_1968.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZJ5763HK/1969-06146-001.html:text/html},
}

@article{blank_effects_1991,
	title = {The effects of double-blind versus single-blind reviewing: experimental evidence from The American Economic Review},
	volume = {81},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/2006906},
	shorttitle = {The effects of double-blind versus single-blind reviewing},
	abstract = {The results from a randomized experiment conducted at The American Economic Review on the effects of double-blind versus single-blind peer reviewing on acceptance rates and referee ratings indicate that acceptance rates are lower and referees are more critical when the reviewer is unaware of the author's identity. These patterns are not significantly different between female and male authors. Authors at top-ranked universities and at colleges and low-ranked universities are largely unaffected by the different reviewing practices, but authors at near-top-ranked universities and at nonacademic institutions have lower acceptance rates under double-blind reviewing.},
	pages = {1041--1067},
	number = {5},
	journaltitle = {The American Economic Review},
	author = {Blank, Rebecca M.},
	urldate = {2020-11-27},
	date = {1991},
	note = {Publisher: American Economic Association},
	file = {Blank_1991.pdf:/Users/tom/pCloud Drive/Zotero_Library/Blank_1991.pdf:application/pdf},
}

@article{cicchetti_reliability_1991,
	title = {The reliability of peer review for manuscript and grant submissions: A cross-disciplinary investigation},
	volume = {14},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/reliability-of-peer-review-for-manuscript-and-grant-submissions-a-crossdisciplinary-investigation/D02B8D10E8F804E15B5FF882421B011E},
	doi = {10.1017/S0140525X00065675},
	shorttitle = {The reliability of peer review for manuscript and grant submissions},
	abstract = {The reliability of peer review of scientific documents and the evaluative criteria scientists use to judge the work of their peers are critically reexamined with special attention to the consistently low levels of reliability that have been reported. Referees of grant proposals agree much more about what is unworthy of support than about what does have scientific value. In the case of manuscript submissions this seems to depend on whether a discipline (or subfield) is general and diffuse (e.g., cross-disciplinary physics, general fields of medicine, cultural anthropology, social psychology) or specific and focused (e.g., nuclear physics, medical specialty areas, physical anthropology, and behavioral neuroscience). In the former there is also much more agreement on rejection than acceptance, but in the latter both the wide differential in manuscript rejection rates and the high correlation between referee recommendations and editorial decisions suggests that reviewers and editors agree more on acceptance than on rejection. Several suggestions are made for improving the reliability and quality of peer review. Further research is needed, especially in the physical sciences.},
	pages = {119--135},
	number = {1},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Cicchetti, Domenic V.},
	urldate = {2020-11-27},
	date = {1991-03},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {peer review, cross-disciplinary comparisons, evaluation, grant review, manuscript reviews, quality control, reliability},
	file = {Cicchetti_1991.pdf:/Users/tom/pCloud Drive/Zotero_Library/Cicchetti_1991.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8W7KUPQP/D02B8D10E8F804E15B5FF882421B011E.html:text/html},
}

@article{peters_peer-review_1982,
	title = {Peer-review practices of psychological journals: The fate of published articles, submitted again},
	volume = {5},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/peerreview-practices-of-psychological-journals-the-fate-of-published-articles-submitted-again/AFE650EB49A6B17992493DE5E49E4431},
	doi = {10.1017/S0140525X00011183},
	shorttitle = {Peer-review practices of psychological journals},
	abstract = {A growing interest in and concern about the adequacy and fairness of modern peer-review practices in publication and funding are apparent across a wide range of scientific disciplines. Although questions about reliability, accountability, reviewer bias, and competence have been raised, there has been very little direct research on these variables.The present investigation was an attempt to study the peer-review process directly, in the natural setting of actual journal referee evaluations of submitted manuscripts. As test materials we selected 12 already published research articles by investigators from prestigious and highly productive American psychology departments, one article from each of 12 highly regarded and widely read American psychology journals with high rejection rates (80\%) and nonblind refereeing practices.With fictitious names and institutions substituted for the original ones (e.g., Tri-Valley Center for Human Potential), the altered manuscripts were formally resubmitted to the journals that had originally refereed and published them 18 to 32 months earlier. Of the sample of 38 editors and reviewers, only three (8\%) detected the resubmissions. This result allowed nine of the 12 articles to continue through the review process to receive an actual evaluation: eight of the nine were rejected. Sixteen of the 18 referees (89\%) recommended against publication and the editors concurred. The grounds for rejection were in many cases described as “serious methodological flaws.” A number of possible interpretations of these data are reviewed and evaluated.},
	pages = {187--195},
	number = {2},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Peters, Douglas P. and Ceci, Stephen J.},
	urldate = {2020-11-27},
	date = {1982-06},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {bias, peer review, evaluation, reliability, journal review system, manuscript review, publication practices, ratings, refereeing, science management},
	file = {Peters and Ceci - 1982 - Peer-review practices of psychological journals T.pdf:/Users/tom/Zotero/storage/7GUM8S8P/Peters and Ceci - 1982 - Peer-review practices of psychological journals T.pdf:application/pdf},
}

@article{slovic_psychology_1977,
	title = {On the psychology of experimental surprises},
	volume = {3},
	issn = {1939-1277(Electronic),0096-1523(Print)},
	doi = {10.1037/0096-1523.3.4.544},
	abstract = {Studies of the psychology of hindsight have shown that reporting the outcome of a historical event increases the perceived likelihood of that outcome. Three experiments with a total of 463 paid volunteers show that similar hindsight effects occur when people evaluate the predictability of scientific results—they tend to believe they "knew all along" what the experiments would find. The hindsight effect was reduced, however, by forcing Ss to consider how the research could otherwise have turned out. Implications for the evaluation of scientific research by lay observers are discussed. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {544--551},
	number = {4},
	journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Slovic, Paul and Fischhoff, Baruch},
	date = {1977},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Information, Experimentation, Prediction, Probability Judgment},
	file = {Slovic_Fischhoff_1977.pdf:/Users/tom/pCloud Drive/Zotero_Library/Slovic_Fischhoff_1977.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VEYDQUYP/1978-26684-001.html:text/html},
}

@book{nisbett_human_1980,
	location = {Englewood Cliffs, N.J},
	title = {Human inference: strategies and shortcomings of social judgment},
	isbn = {978-0-13-445130-5},
	series = {The Century psychology series},
	shorttitle = {Human inference},
	pagetotal = {334},
	publisher = {Prentice-Hall},
	author = {Nisbett, Richard E. and Ross, Lee},
	date = {1980},
	langid = {english},
	keywords = {Cognition, Inference, Judgment},
	file = {Nisbett and Ross - 1980 - Human inference strategies and shortcomings of so.pdf:/Users/tom/Zotero/storage/DAU5N33Q/Nisbett and Ross - 1980 - Human inference strategies and shortcomings of so.pdf:application/pdf},
}

@book{barber_pitfalls_1976,
	location = {New York},
	title = {Pitfalls in Human Research: Ten Pivotal Points},
	isbn = {978-0-08-020935-7 978-0-08-020934-0},
	series = {Pergamon general psychology series ; v. 67},
	shorttitle = {Pitfalls in Human Research},
	pagetotal = {117},
	publisher = {Pergamon Press},
	author = {Barber, Theodore Xenophon},
	date = {1976},
	langid = {english},
	keywords = {Psychology, Behavioral sciences, Research, Research Effect of experimenters on},
	file = {Barber - 1976 - Pitfalls in human research ten pivotal points.pdf:/Users/tom/Zotero/storage/AZRRK2MG/Barber - 1976 - Pitfalls in human research ten pivotal points.pdf:application/pdf},
}

@article{klein_blind_2005,
	title = {Blind analysis in nuclear and particle physics},
	volume = {55},
	issn = {0163-8998},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.nucl.55.090704.151521},
	doi = {10.1146/annurev.nucl.55.090704.151521},
	abstract = {During the past decade, blind analysis has become a widely used tool in nuclear and particle physics measurements. A blind analysis avoids the possibility of experimenters biasing their result toward their own preconceptions by preventing them from knowing the answer until the analysis is complete. There is at least circumstantial evidence that such a bias has affected past measurements, and as experiments have become costlier and more difficult and hence harder to reproduce, the possibility of bias has become a more important issue than in the past. We describe here the motivations for performing a blind analysis, and give several modern examples of successful blind analysis strategies.},
	pages = {141--163},
	number = {1},
	journaltitle = {Annual Review of Nuclear and Particle Science},
	shortjournal = {Annu. Rev. Nucl. Part. Sci.},
	author = {Klein, Joshua R and Roodman, Aaron},
	urldate = {2020-11-26},
	date = {2005-11-03},
	note = {Publisher: Annual Reviews},
	file = {Klein_Roodman_2005.pdf:/Users/tom/pCloud Drive/Zotero_Library/Klein_Roodman_2005.pdf:application/pdf},
}

@article{chen_publication_2016,
	title = {Publication and reporting of clinical trial results: cross sectional analysis across academic medical centers},
	volume = {352},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/352/bmj.i637},
	doi = {10.1136/bmj.i637},
	shorttitle = {Publication and reporting of clinical trial results},
	abstract = {Objective To determine rates of publication and reporting of results within two years for all completed clinical trials registered in {ClinicalTrials}.gov across leading academic medical centers in the United States.
Design Cross sectional analysis.
Setting Academic medical centers in the United States.
Participants Academic medical centers with 40 or more completed interventional trials registered on {ClinicalTrials}.gov.
Methods Using the Aggregate Analysis of {ClinicalTrials}.gov database and manual review, we identified all interventional clinical trials registered on {ClinicalTrials}.gov with a primary completion date between October 2007 and September 2010 and with a lead investigator affiliated with an academic medical center.
Main outcome measures The proportion of trials that disseminated results, defined as publication or reporting of results on {ClinicalTrials}.gov, overall and within 24 months of study completion.
Results We identified 4347 interventional clinical trials across 51 academic medical centers. Among the trials, 1005 (23\%) enrolled more than 100 patients, 1216 (28\%) were double blind, and 2169 (50\%) were phase {II} through {IV}. Overall, academic medical centers disseminated results for 2892 (66\%) trials, with 1560 (35.9\%) achieving this within 24 months of study completion. The proportion of clinical trials with results disseminated within 24 months of study completion ranged from 16.2\% (6/37) to 55.3\% (57/103) across academic medical centers. The proportion of clinical trials published within 24 months of study completion ranged from 10.8\% (4/37) to 40.3\% (31/77) across academic medical centers, whereas results reporting on {ClinicalTrials}.gov ranged from 1.6\% (2/122) to 40.7\% (72/177).
Conclusions Despite the ethical mandate and expressed values and mission of academic institutions, there is poor performance and noticeable variation in the dissemination of clinical trial results across leading academic medical centers.},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Chen, Ruijun and Desai, Nihar R. and Ross, Joseph S. and Zhang, Weiwei and Chau, Katherine H. and Wayda, Brian and Murugiah, Karthik and Lu, Daniel Y. and Mittal, Amit and Krumholz, Harlan M.},
	urldate = {2020-11-26},
	date = {2016-02-17},
	langid = {english},
	pmid = {26888209},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Chen_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Chen_etal_2016.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KBDHJA5Q/bmj.html:text/html},
}

@article{howson_accommodation_1988,
	title = {Accommodation, Prediction and Bayesian Confirmation Theory},
	volume = {1988},
	issn = {0270-8647},
	url = {https://www.jstor.org/stable/192899},
	doi = {10.1086/psaprocbienmeetp.1988.2.192899},
	abstract = {This paper examines the famous doctrine that independent prediction garners more support than accommodation. The standard arguments for the doctrine are found to be invalid, and a more realistic position is put forward, that whether evidence supports or not a hypothesis depends on the prior probability of the hypothesis, and is independent of whether it was proposed before or after the evidence. This position is implicit in the subjective Bayesian theory of confirmation, and the paper ends with a brief account of this theory, and answer to the principal objections to it.},
	pages = {381--392},
	journaltitle = {{PSA}: Proceedings of the Biennial Meeting of the Philosophy of Science Association},
	author = {Howson, Colin},
	urldate = {2020-11-24},
	date = {1988},
	note = {Publisher: [University of Chicago Press, Springer, Philosophy of Science Association]},
	file = {Howson_1988.pdf:/Users/tom/pCloud Drive/Zotero_Library/Howson_1988.pdf:application/pdf},
}

@book{horwich_probability_1982,
	location = {Cambridge},
	title = {Probability and Evidence},
	isbn = {978-1-316-49421-9},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781316494219},
	publisher = {Cambridge University Press},
	author = {Horwich, Paul},
	urldate = {2020-11-23},
	date = {1982},
	langid = {english},
	doi = {10.1017/CBO9781316494219},
	file = {Horwich - 2016 - Probability and Evidence.pdf:/Users/tom/Zotero/storage/44KTU26R/Horwich - 2016 - Probability and Evidence.pdf:application/pdf},
}

@article{mcintyre_accommodation_2001,
	title = {Accommodation, Prediction, and Confirmation},
	volume = {9},
	issn = {1063-6145, 1530-9274},
	url = {http://www.mitpressjournals.org/doi/10.1162/10636140160176161},
	doi = {10.1162/10636140160176161},
	pages = {308--323},
	number = {3},
	journaltitle = {Perspectives on Science},
	shortjournal = {Perspectives on Science},
	author = {{McIntyre}, Lee},
	urldate = {2020-11-17},
	date = {2001-09},
	langid = {english},
	file = {McIntyre - 2001 - Accommodation, Prediction, and Confirmation.pdf:/Users/tom/Zotero/storage/FV7HYYDB/McIntyre - 2001 - Accommodation, Prediction, and Confirmation.pdf:application/pdf},
}

@article{artner_reproducibility_2020,
	title = {The reproducibility of statistical results in psychological research: An investigation using unpublished raw data.},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000365},
	doi = {10.1037/met0000365},
	shorttitle = {The reproducibility of statistical results in psychological research},
	abstract = {We investigated the reproducibility of the major statistical conclusions drawn in 46 articles published in 2012 in three {APA} journals. After having identified 232 key statistical claims, we tried to reproduce, for each claim, the test statistic, its degrees of freedom, and the corresponding p value, starting from the raw data that were provided by the authors and closely following the Method section in the article. Out of the 232 claims, we were able to successfully reproduce 163 (70\%), 18 of which only by deviating from the article’s analytical description. Thirteen (7\%) of the 185 claims deemed significant by the authors are no longer so. The reproduction successes were often the result of cumbersome and time-consuming trial-and-error work, suggesting that {APA} style reporting in conjunction with raw data makes numerical verification at least hard, if not impossible. This article discusses the types of mistakes we could identify and the tediousness of our reproduction efforts in the light of a newly developed taxonomy for reproducibility. We then link our findings with other findings of empirical research on this topic, give practical recommendations on how to achieve reproducibility, and discuss the challenges of large-scale reproducibility checks as well as promising ideas that could considerably increase the reproducibility of psychological research.},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Artner, Richard and Verliefde, Thomas and Steegen, Sara and Gomes, Sara and Traets, Frits and Tuerlinckx, Francis and Vanpaemel, Wolf},
	urldate = {2020-11-17},
	date = {2020-11-12},
	langid = {english},
	file = {Artner et al. - 2020 - The reproducibility of statistical results in psyc.pdf:/Users/tom/Zotero/storage/ZYQESKJC/Artner et al. - 2020 - The reproducibility of statistical results in psyc.pdf:application/pdf},
}

@article{simon_prediction_1955,
	title = {Prediction and Hindsight as Confirmatory Evidence},
	volume = {22},
	issn = {0031-8248},
	url = {https://www.jstor.org/stable/185318},
	doi = {10.1086/287427},
	pages = {227--230},
	number = {3},
	journaltitle = {Philosophy of Science},
	author = {Simon, Herbert A.},
	urldate = {2020-11-16},
	date = {1955},
	note = {Publisher: [The University of Chicago Press, Philosophy of Science Association]},
	file = {Simon - 1955 - Prediction and Hindsight as Confirmatory Evidence.pdf:/Users/tom/Zotero/storage/XI5AJKH3/Simon - 1955 - Prediction and Hindsight as Confirmatory Evidence.pdf:application/pdf},
}

@article{riet_all_2013,
	title = {All that glitters isn't gold: a survey on acknowledgment of limitations in biomedical studies},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073623},
	doi = {10.1371/journal.pone.0073623},
	shorttitle = {All that glitters isn't gold},
	abstract = {Background Acknowledgment of all serious limitations to research evidence is important for patient care and scientific progress. Formal research on how biomedical authors acknowledge limitations is scarce. Objectives To assess the extent to which limitations are acknowledged in biomedical publications explicitly, and implicitly by investigating the use of phrases that express uncertainty, so-called hedges; to assess the association between industry support and the extent of hedging. Design We analyzed reporting of limitations and use of hedges in 300 biomedical publications published in 30 high and medium -ranked journals in 2007. Hedges were assessed using linguistic software that assigned weights between 1 and 5 to each expression of uncertainty. Results Twenty-seven percent of publications (81/300) did not mention any limitations, while 73\% acknowledged a median of 3 (range 1–8) limitations. Five percent mentioned a limitation in the abstract. After controlling for confounders, publications on industry-supported studies used significantly fewer hedges than publications not so supported (p = 0.028). Limitations Detection and classification of limitations was – to some extent – subjective. The weighting scheme used by the hedging detection software has subjective elements. Conclusions Reporting of limitations in biomedical publications is probably very incomplete. Transparent reporting of limitations may protect clinicians and guideline committees against overly confident beliefs and decisions and support scientific progress through better design, conduct or analysis of new studies.},
	pages = {e73623},
	number = {11},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Riet, Gerben ter and Chesley, Paula and Gross, Alan G. and Siebeling, Lara and Muggensturm, Patrick and Heller, Nadine and Umbehr, Martin and Vollenweider, Daniela and Yu, Tsung and Akl, Elie A. and Brewster, Lizzy and Dekkers, Olaf M. and Mühlhauser, Ingrid and Richter, Bernd and Singh, Sonal and Goodman, Steven and Puhan, Milo A.},
	urldate = {2020-11-15},
	date = {2013-11-20},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Randomized controlled trials, Medical journals, Scientific publishing, Peer review, Computer software, Regression analysis, Bibliometrics, General medical journals},
	file = {Riet et al. - 2013 - All that glitters isn't gold a survey on acknowle.pdf:/Users/tom/Zotero/storage/E35I6FJC/Riet et al. - 2013 - All that glitters isn't gold a survey on acknowle.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DX3T8CF6/article.html:text/html},
}

@article{simons_constraints_2017,
	title = {Constraints on Generality ({COG}): A Proposed Addition to All Empirical Papers:},
	rights = {© The Author(s) 2017},
	url = {https://journals.sagepub.com/doi/10.1177/1745691617708630},
	doi = {10.1177/1745691617708630},
	shorttitle = {Constraints on Generality ({COG})},
	abstract = {Psychological scientists draw inferences about populations based on samples—of people, situations, and stimuli—from those populations. Yet, few papers identify ...},
	journaltitle = {Perspectives on Psychological Science},
	author = {Simons, Daniel J. and Shoda, Yuichi and Lindsay, D. Stephen},
	urldate = {2020-11-15},
	date = {2017-08-30},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Simons et al. - 2017 - Constraints on Generality (COG) A Proposed Additi.pdf:/Users/tom/Zotero/storage/U6JCIAUD/Simons et al. - 2017 - Constraints on Generality (COG) A Proposed Additi.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BWCXAA6A/1745691617708630.html:text/html},
}

@article{brutus_state_2010,
	title = {State of science in industrial and organisational psychology: A review of self-reported limitations},
	volume = {63},
	issn = {00315826},
	url = {http://doi.wiley.com/10.1111/j.1744-6570.2010.01192.x},
	doi = {10.1111/j.1744-6570.2010.01192.x},
	pages = {907--936},
	number = {4},
	journaltitle = {Personnel Psychology},
	author = {Brutus, Stéphane and Gill, Harjinder and Duniewicz, Kris},
	urldate = {2020-11-15},
	date = {2010-12},
	langid = {english},
	file = {Brutus et al. - 2010 - State of science in industrial and organisational .pdf:/Users/tom/Zotero/storage/D7ZMBG67/Brutus et al. - 2010 - State of science in industrial and organisational .pdf:application/pdf},
}

@article{yazici_there_2014,
	title = {There was less self-critique among basic than in clinical science articles in three rheumatology journals},
	volume = {67},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435613004459},
	doi = {10.1016/j.jclinepi.2013.10.014},
	abstract = {Objectives: There is concern that self-critique with authors acknowledging limitations of their work is not given due importance in scientiﬁc articles. We had the impression that this was more true for articles in basic compared with clinical science. We thus surveyed for the presence of self-critique in the discussion sections of the original articles in three rheumatology journals with attention to differences between the basic and the clinical science articles. Study Design and Setting: The discussion sections of the original articles in January, May, and September 2012 issues of Annals of the Rheumatic Diseases, Arthritis and Rheumatism, and Rheumatology (Oxford) were surveyed (n 5 223) after classifying each article as mainly related to clinical or basic science. The discussion sections were electronically scanned by two observers for the presence of the root word ‘‘limit’’ or its derivatives who also read each discussion section for the presence of any limitations otherwise voiced.
Results: A limitation discussion in any form was present in only 19 (20.2\%) or 29 (30.1\%) of 94 basic science vs. 95 (73.6\%) or 107 (82.3\%) of 129 clinical science articles (P ! 0.0001 for either observer).
Conclusion: Self-critique, especially lacking in basic science articles, should be given due attention. Ó 2014 Elsevier Inc. All rights reserved.},
	pages = {654--657},
	number = {6},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Yazici, Hasan and Gogus, Feride and Esen, Fehim and Yazici, Yusuf},
	urldate = {2020-11-15},
	date = {2014-06},
	langid = {english},
	file = {Yazici et al. - 2014 - There was less self-critique among basic than in c.pdf:/Users/tom/Zotero/storage/ECI9FW2Z/Yazici et al. - 2014 - There was less self-critique among basic than in c.pdf:application/pdf},
}

@article{brutus_many_2012,
	title = {The many heels of Achilles: An analysis of self-reported limitations in leadership research},
	volume = {23},
	issn = {10489843},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1048984311001780},
	doi = {10.1016/j.leaqua.2011.11.015},
	shorttitle = {The many heels of Achilles},
	abstract = {The aim of this study was to assess the research published in The Leadership Quarterly from its inception in 1990 to 2007. As the foundation for our study, we used self-reported limitations sections of empirical articles as an alternative, novel, and context-sensitive index of state-ofscience. Limitations reported in the one-hundred and seventy-four empirical articles published in The Leadership Quarterly to date were coded according to traditional threats to validity. Our results indicate that {LQ} articles mostly report limitations related to external validity issues. Also, a growing concern with internal validity was noted. These findings offer a unique perspective on leadership research, one that paints a considerably different picture than that offered from previous empirical reviews. We discuss the role of self-reported limitations in scientific communication and offer some prescriptions for increasing their value.},
	pages = {202--212},
	number = {1},
	journaltitle = {The Leadership Quarterly},
	shortjournal = {The Leadership Quarterly},
	author = {Brutus, Stéphane and Duniewicz, Kris},
	urldate = {2020-11-15},
	date = {2012-02},
	langid = {english},
	file = {Brutus and Duniewicz - 2012 - The many heels of Achilles An analysis of self-re.pdf:/Users/tom/Zotero/storage/XN7XNBZX/Brutus and Duniewicz - 2012 - The many heels of Achilles An analysis of self-re.pdf:application/pdf},
}

@article{puhan_discussing_2012,
	title = {Discussing study limitations in reports of biomedical studies- the need for more transparency},
	volume = {10},
	issn = {1477-7525},
	url = {https://doi.org/10.1186/1477-7525-10-23},
	doi = {10.1186/1477-7525-10-23},
	abstract = {Unbiased and frank discussion of study limitations by authors represents a crucial part of the scientific discourse and progress. In today's culture of publishing many authors or scientific teams probably balance 'utter honesty' when discussing limitations of their research with the risk of being unable to publish their work. Currently, too few papers in the medical literature frankly discuss how limitations could have affected the study findings and interpretations. The goals of this commentary are to review how limitations are currently acknowledged in the medical literature, to discuss the implications of limitations in biomedical studies, and to make suggestions as to how to openly discuss limitations for scientists submitting their papers to journals. This commentary was developed through discussion and logical arguments by the authors who are doing research in the area of hedging (use of language to express uncertainty) and who have extensive experience as authors and editors of biomedical papers. We strongly encourage authors to report on all potentially important limitations that may have affected the quality and interpretation of the evidence being presented. This will not only benefit science but also offers incentives for authors: If not all important limitations are acknowledged readers and reviewers of scientific articles may perceive that the authors were unaware of them. Authors should take advantage of their content knowledge and familiarity with the study to prevent misinterpretations of the limitations by reviewers and readers. Articles discussing limitations help shape the future research agenda and are likely to be cited because they have informed the design and conduct of future studies. Instead of perceiving acknowledgment of limitations negatively, authors, reviewers and editors should recognize the potential of a frank and unbiased discussion of study limitations that should not jeopardize acceptance of manuscripts.},
	pages = {23},
	number = {1},
	journaltitle = {Health and Quality of Life Outcomes},
	shortjournal = {Health and Quality of Life Outcomes},
	author = {Puhan, Milo A. and Akl, Elie A. and Bryant, Dianne and Xie, Feng and Apolone, Giovanni and Riet, Gerben ter},
	urldate = {2020-11-15},
	date = {2012-02-23},
	file = {Puhan_etal_2012.pdf:/Users/tom/pCloud Drive/Zotero_Library/Puhan_etal_2012.pdf:application/pdf},
}

@article{ioannidis_limitations_2007,
	title = {Limitations are not properly acknowledged in the scientific literature},
	volume = {60},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(06)00397-0/abstract},
	doi = {10.1016/j.jclinepi.2006.09.011},
	abstract = {{\textless}h2{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}p{\textgreater}Limitations are important to understand for placing research findings in context, interpreting the validity of the scientific work, and ascribing a credibility level to the conclusions of published research. This goes beyond listing the magnitude and direction of random and systematic errors and validity problems. Acknowledgment of limitations requires an interpretation of the meaning and influence of errors and validity problems on the published findings. An examination of the full-text files of the first 50 articles published in 2005 in the six most-cited research journals and in two recently launched leading open-access journals showed that only 67 articles (17\%) used at least one word denoting limitations in the context of the presented scientific work. Only four articles (1\%) used the word \textit{limitation} in their abstract; none referred to limitations of the present work that materially affected conclusions. Only five articles had a separate section on limitations. Conversely, 243 articles (61\%) used words detected by the roots \textit{error}, \textit{valid}, \textit{bias}, \textit{reproducib}, or \textit{false} and 289 articles (72\%) used words with the root \textit{importan}. Among the 25 top-cited journals' instructions to the authors and editorial policies, only one encourages discussion of limitations; importance, novelty, and lack of error are typically encouraged. Limitations should be better covered and discussed in research articles. To facilitate this, journals should give better guidance and promote the discussion of limitations. Otherwise, we are facing an important loss of context for the scientific literature.{\textless}/p{\textgreater}},
	pages = {324--329},
	number = {4},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Ioannidis, John P. A.},
	urldate = {2020-11-15},
	date = {2007-04-01},
	pmid = {17346604},
	note = {Publisher: Elsevier},
	file = {Ioannidis - 2007 - Limitations are not properly acknowledged in the s.pdf:/Users/tom/Zotero/storage/6NWEMKNG/Ioannidis - 2007 - Limitations are not properly acknowledged in the s.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7SNHQBWF/fulltext.html:text/html},
}

@article{kilicoglu_automatic_2018,
	title = {Automatic recognition of self-acknowledged limitations in clinical research literature},
	volume = {25},
	url = {https://academic.oup.com/jamia/article/25/7/855/4990607},
	doi = {10.1093/jamia/ocy038},
	abstract = {{AbstractObjective}. To automatically recognize self-acknowledged limitations in clinical research publications to support efforts in improving research transpare},
	pages = {855--861},
	number = {7},
	journaltitle = {Journal of the American Medical Informatics Association},
	shortjournal = {J Am Med Inform Assoc},
	author = {Kilicoglu, Halil and Rosemblat, Graciela and Malički, Mario and ter Riet, Gerben},
	urldate = {2020-11-15},
	date = {2018-07-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Kilicoglu_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kilicoglu_etal_2018.pdf:application/pdf},
}

@article{yavchitz_impact_2014,
	title = {Impact of adding a limitations section to abstracts of systematic reviews on readers’ interpretation: a randomized controlled trial},
	volume = {14},
	issn = {1471-2288},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4247631/},
	doi = {10.1186/1471-2288-14-123},
	shorttitle = {Impact of adding a limitations section to abstracts of systematic reviews on readers’ interpretation},
	abstract = {Background
To allow an accurate evaluation of abstracts of systematic reviews, the {PRISMA} Statement recommends that the limitations of the evidence (e.g., risk of bias, publication bias, inconsistency, imprecision) should be described in the abstract. We aimed to evaluate the impact of adding such limitations sections on reader’s interpretation.

Method
We performed a two-arm parallel group randomized controlled trial ({RCT}) using a sample of 30 abstracts of systematic reviews evaluating the effects of healthcare intervention with conclusions favoring the beneficial effect of the experimental treatments. Two formats of these abstracts were derived: one reported without and one with a standardized limitations section written according to the {PRISMA} statement for abstracts. The primary outcome was readers’ confidence in the results of the systematic review as stated in the abstract assessed by a Likert scale from 0, not at all confident, to 10, very confident. In total, 300 participants (corresponding authors of {RCT} reports indexed in {PubMed}) were randomized by a web-based randomization procedure to interpret one abstract with a limitations section (n = 150) or without a limitations section (n = 150). Participants were blinded to the study hypothesis.

Results
Adding a limitations section did not modify readers’ interpretation of findings in terms of confidence in the results (mean difference [95\% confidence interval] 0.19 [−0.37–0.74], p = 0.50), confidence in the validity of the conclusions (0.07 [−0.49–0.62], p = 0.80), or benefit of the experimental intervention (0.12 [−0.42–0.44], p = 0.65)., This study is limited because the participants were expert-readers and are not representative of all systematic review readers.

Conclusion
Adding a limitations section to abstracts of systematic reviews did not affect readers’ interpretation of the abstract results. Other studies are needed to confirm the results and explore the impact of a limitations section on a less expert panel of participants.

Trial registration
{ClinicalTrial}.gov ({NCT}01848782).

Electronic supplementary material
The online version of this article (doi:10.1186/1471-2288-14-123) contains supplementary material, which is available to authorized users.},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Med Res Methodol},
	author = {Yavchitz, Amélie and Ravaud, Philippe and Hopewell, Sally and Baron, Gabriel and Boutron, Isabelle},
	urldate = {2020-11-15},
	date = {2014-11-24},
	pmid = {25420433},
	pmcid = {PMC4247631},
	file = {Yavchitz_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Yavchitz_etal_2014.pdf:application/pdf},
}

@article{schauer_reconsidering_2020,
	title = {Reconsidering statistical methods for assessing replication.},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000302},
	doi = {10.1037/met0000302},
	abstract = {Recent empirical evaluations of replication in psychology have reported startlingly few successful replication attempts. At the same time, these programs have noted that the proper way to analyze replication studies is far from a settled matter and have analyzed their data in several different ways. This presents 2 challenges to interpreting the results of these programs. First, different analysis methods assess different operational definitions of replication. Second, the properties of these methods are not necessarily common knowledge; it is possible for a successful replication to be deemed a failure by nearly all of the metrics used, and it is not always immediately clear how likely such errors are to occur. In this article, we describe the methods commonly used in replication research and how they imply specific operational definitions of replication. We then compute the probability of false failure (i.e., a successful replication is concluded to have failed) and false success determinations. These are shown to be high (often over 50\%) and in many cases uncontrolled. We then demonstrate that errors are probable in the data to which these methods have been applied in the literature. We show that the probability that some reported conclusions about replication are incorrect can be as high as 75– 80\%.},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Schauer, J. M. and Hedges, L. V.},
	urldate = {2020-11-13},
	date = {2020-05-18},
	langid = {english},
	file = {Schauer and Hedges - 2020 - Reconsidering statistical methods for assessing re.pdf:/Users/tom/Zotero/storage/SKUWHWBE/Schauer and Hedges - 2020 - Reconsidering statistical methods for assessing re.pdf:application/pdf},
}

@article{hollenbeck_harking_2017,
	title = {{HARKing}, {SHARKing}, and {THARking}: making the case for post hoc analysis of scientific data},
	volume = {43},
	issn = {0149-2063},
	url = {https://doi.org/10.1177/0149206316679487},
	doi = {10.1177/0149206316679487},
	shorttitle = {Harking, sharking, and tharking},
	abstract = {In this editorial we discuss the problems associated with {HARKing} (Hypothesizing After Results Are Known) and draw a distinction between Sharking (Secretly {HARKing} in the Introduction section) and Tharking (Transparently {HARKing} in the Discussion section). Although there is never any justification for the process of Sharking, we argue that Tharking can promote the effectiveness and efficiency of both scientific inquiry and cumulative knowledge creation. We argue that the discussion sections of all empirical papers should include a subsection that reports post hoc exploratory data analysis. We explain how authors, reviewers, and editors can best leverage post hoc analyses in the spirit of scientific discovery in a way that does not bias parameter estimates and recognizes the lack of definitiveness associated with any single study or any single replication. We also discuss why the failure to Thark in high-stakes contexts where data is scarce and costly may also be unethical.},
	pages = {5--18},
	number = {1},
	journaltitle = {Journal of Management},
	shortjournal = {Journal of Management},
	author = {Hollenbeck, John R. and Wright, Patrick M.},
	urldate = {2020-11-13},
	date = {2017-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Hollenbeck_Wright_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hollenbeck_Wright_2017.pdf:application/pdf},
}

@article{klau_examining_2021,
	title = {Examining the robustness of observational associations to model, measurement and sampling uncertainty with the vibration of effects framework},
	volume = {50},
	url = {https://academic.oup.com/ije/advance-article/doi/10.1093/ije/dyaa164/5956264},
	doi = {10.1093/ije/dyaa164},
	abstract = {{AbstractBackground}.  The results of studies on observational associations may vary depending on the study design and analysis choices as well as due to measurem},
	pages = {266--278},
	number = {1},
	journaltitle = {International Journal of Epidemiology},
	shortjournal = {Int J Epidemiol},
	author = {Klau, Simon and Hoffmann, Sabine and Patel, Chirag J. and Ioannidis, John P. A. and Boulesteix, Anne-Laure},
	urldate = {2020-11-10},
	date = {2021},
	langid = {english},
	file = {Klau et al. - 2021 - Examining the robustness of observational associat.pdf:/Users/tom/pCloud Drive/Zotero_Library/Klau et al. - 2021 - Examining the robustness of observational associat.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TDMU4AWP/5956264.html:text/html},
}

@article{scott_bayes_2010,
	title = {Bayes and empirical-Bayes multiplicity adjustment in the variable-selection problem},
	volume = {38},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/29765241},
	doi = {10.1214/10-AOS792},
	abstract = {This paper studies the multiplicity-correction effect of standard Bayesian variable-selection priors in linear regression. Our first goal is to clarify when, and how, multiplicity correction happens automatically in Bayesian analysis, and to distinguish this correction from the Bayesian Ockham's-razor effect. Our second goal is to contrast empirical-Bayes and fully Bayesian approaches to variable selection through examples, theoretical results and simulations. Considerable differences between the two approaches are found. In particular, we prove a theorem that characterizes a surprising aymptotic discrepancy between fully Bayes and empirical Bayes. This discrepancy arises from a different source than the failure to account for hyperparameter uncertainty in the empirical-Bayes estimate. Indeed, even at the extreme, when the empirical-Bayes estimate converges asymptotically to the true variable-inclusion probability, the potential for a serious difference remains.},
	pages = {2587--2619},
	number = {5},
	journaltitle = {The Annals of Statistics},
	author = {Scott, James G. and Berger, James O.},
	urldate = {2020-11-03},
	date = {2010},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Scott_Berger_2010.pdf:/Users/tom/pCloud Drive/Zotero_Library/Scott_Berger_2010.pdf:application/pdf},
}

@article{chamberlin_method_1890,
	title = {The method of multiple working hypotheses: with this method the dangers of parental affection for a favorite theory can be circumvented},
	volume = {148},
	rights = {1965 by the American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/148/3671/754},
	doi = {10.1126/science.148.3671.754},
	shorttitle = {The method of multiple working hypotheses},
	pages = {754--759},
	number = {3671},
	journaltitle = {Science},
	author = {Chamberlin, T. C.},
	urldate = {2020-11-03},
	date = {1890},
	langid = {english},
	note = {original-date: 1890},
	file = {Chamberlin_1965.pdf:/Users/tom/pCloud Drive/Zotero_Library/Chamberlin_1965.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GYW4XW8J/754.html:text/html},
}

@article{bennett_neural_2009,
	title = {Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: an argument for multiple comparisons correction},
	volume = {47},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811909712029},
	doi = {10.1016/S1053-8119(09)71202-9},
	series = {Organization for Human Brain Mapping 2009 Annual Meeting},
	shorttitle = {Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon},
	pages = {S125},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Bennett, {CM} and Miller, {MB} and Wolford, {GL}},
	urldate = {2020-11-03},
	date = {2009-07-01},
	langid = {english},
	file = {Bennett_etal_2009.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bennett_etal_2009.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/FVSMWRPY/S1053811909712029.html:text/html},
}

@article{holmes_statistical_2017,
	title = {Statistical proof? The problem of irreproducibility},
	volume = {55},
	issn = {0273-0979, 1088-9485},
	url = {http://www.ams.org/bull/2018-55-01/S0273-0979-2017-01597-2/},
	doi = {10.1090/bull/1597},
	shorttitle = {Statistical proof?},
	abstract = {Data currently generated in the ﬁelds of ecology, medicine, climatology, and neuroscience often contain tens of thousands of measured variables. If special care is not taken, the complexity associated with statistical analysis of such data can lead to publication of results that prove to be irreproducible. The ﬁeld of modern statistics has had to revisit the classical hypothesis testing paradigm to accommodate modern high-throughput settings. A ﬁrst step is correction for multiplicity in the number of possible variables selected as signiﬁcant using multiple hypotheses correction to ensure false discovery rate ({FDR}) control (Benjamini, Hochberg, 1995). {FDR} adjustments do not solve the problem of double dipping the data, and recent work develops a ﬁeld known as post-selection inference that enables inference when the same data is used both to choose and to evaluate models.},
	pages = {31--55},
	number = {1},
	journaltitle = {Bulletin of the American Mathematical Society},
	shortjournal = {Bull. Amer. Math. Soc.},
	author = {Holmes, Susan},
	urldate = {2020-11-02},
	date = {2017-10-04},
	langid = {english},
	file = {Holmes - 2017 - Statistical proof The problem of irreproducibilit.pdf:/Users/tom/Zotero/storage/D9LG3L47/Holmes - 2017 - Statistical proof The problem of irreproducibilit.pdf:application/pdf},
}

@article{maccoun_biases_1998,
	title = {Biases in the interpretation and use of research results},
	volume = {49},
	url = {https://doi.org/10.1146/annurev.psych.49.1.259},
	doi = {10.1146/annurev.psych.49.1.259},
	abstract = {The latter half of this century has seen an erosion in the perceived legitimacy of science as an impartial means of finding truth. Many research topics are the subject of highly politicized dispute; indeed, the objectivity of the entire discipline of psychology has been called into question. This essay examines attempts to use science to study science: specifically, bias in the interpretation and use of empirical research findings. I examine theory and research on a range of cognitive and motivational mechanisms for bias. Interestingly, not all biases are normatively proscribed; biased interpretations are defensible under some conditions, so long as those conditions are made explicit. I consider a variety of potentially corrective mechanisms, evaluate prospects for collective rationality, and compare inquisitorial and adversarial models of science.},
	pages = {259--287},
	number = {1},
	journaltitle = {Annual Review of Psychology},
	author = {{MacCoun}, Robert J.},
	urldate = {2020-10-29},
	date = {1998},
	pmid = {15012470},
	note = {\_eprint: https://doi.org/10.1146/annurev.psych.49.1.259},
	file = {MacCoun_1998.pdf:/Users/tom/pCloud Drive/Zotero_Library/MacCoun_1998.pdf:application/pdf;maccoun1998.pdf:/Users/tom/Zotero/storage/GVT2D54N/maccoun1998.pdf:application/pdf},
}

@article{wallach_reproducible_2018,
	title = {Reproducible research practices, transparency, and open access data in the biomedical literature, 2015–2017},
	volume = {16},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2006930},
	doi = {10.1371/journal.pbio.2006930},
	abstract = {Currently, there is a growing interest in ensuring the transparency and reproducibility of the published scientific literature. According to a previous evaluation of 441 biomedical journals articles published in 2000–2014, the biomedical literature largely lacked transparency in important dimensions. Here, we surveyed a random sample of 149 biomedical articles published between 2015 and 2017 and determined the proportion reporting sources of public and/or private funding and conflicts of interests, sharing protocols and raw data, and undergoing rigorous independent replication and reproducibility checks. We also investigated what can be learned about reproducibility and transparency indicators from open access data provided on {PubMed}. The majority of the 149 studies disclosed some information regarding funding (103, 69.1\% [95\% confidence interval, 61.0\% to 76.3\%]) or conflicts of interest (97, 65.1\% [56.8\% to 72.6\%]). Among the 104 articles with empirical data in which protocols or data sharing would be pertinent, 19 (18.3\% [11.6\% to 27.3\%]) discussed publicly available data; only one (1.0\% [0.1\% to 6.0\%]) included a link to a full study protocol. Among the 97 articles in which replication in studies with different data would be pertinent, there were five replication efforts (5.2\% [1.9\% to 12.2\%]). Although clinical trial identification numbers and funding details were often provided on {PubMed}, only two of the articles without a full text article in {PubMed} Central that discussed publicly available data at the full text level also contained information related to data sharing on {PubMed}; none had a conflicts of interest statement on {PubMed}. Our evaluation suggests that although there have been improvements over the last few years in certain key indicators of reproducibility and transparency, opportunities exist to improve reproducible research practices across the biomedical literature and to make features related to reproducibility more readily visible in {PubMed}.},
	pages = {e2006930},
	number = {11},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Wallach, Joshua D. and Boyack, Kevin W. and Ioannidis, John P. A.},
	urldate = {2020-10-29},
	date = {2018-11-20},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open science, Reproducibility, Medical journals, Scientific publishing, Replication studies, Systematic reviews, Conflicts of interest, Government funding of science},
	file = {Snapshot:/Users/tom/Zotero/storage/UJ7TTDL4/article.html:text/html;Wallach_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wallach_etal_2018.pdf:application/pdf},
}

@incollection{bickel_wavelab_1995,
	location = {New York, {NY}},
	title = {{WaveLab} and Reproducible Research},
	volume = {103},
	isbn = {978-0-387-94564-4 978-1-4612-2544-7},
	url = {http://link.springer.com/10.1007/978-1-4612-2544-7_5},
	abstract = {{WAVELAB} is a library of {MATLAB} routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, {UNIX} and Windows machines.},
	pages = {55--81},
	booktitle = {Wavelets and Statistics},
	publisher = {Springer New York},
	author = {Buckheit, Jonathan B. and Donoho, David L.},
	editor = {Antoniadis, Anestis and Oppenheim, Georges},
	editorb = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
	editorbtype = {redactor},
	urldate = {2020-10-29},
	date = {1995},
	langid = {english},
	doi = {10.1007/978-1-4612-2544-7_5},
	note = {Series Title: Lecture Notes in Statistics},
	file = {Buckheit and Donoho - 1995 - WaveLab and Reproducible Research.pdf:/Users/tom/Zotero/storage/7UMLKIJ8/Buckheit and Donoho - 1995 - WaveLab and Reproducible Research.pdf:application/pdf},
}

@report{gollwitzer_data_2020,
	title = {Data management and data sharing in psychological science: revision of the dgps recommendations},
	url = {https://psyarxiv.com/24ncs/},
	shorttitle = {Data management and data sharing in psychological science},
	abstract = {Providing access to research data collected as part of scientific publications and publicly funded research projects is now regarded as a central aspect of an open and transparent scientific practice and is increasingly being called for by funding institutions and scientific journals. To this end, researchers should strive to comply with the so-called {FAIR} principles (of scientific data management), that is, research data should be findable, accessible, interoperable, and reusable. Systematic data management supports these goals and, at the same time, makes it possible to achieve them efficiently. With these revised recommendations on data management and data sharing, which also draw on feedback from a 2018 survey of its members, the German Psychological Society (Deutsche Gesellschaft für Psychologie; {DGPs}) specifies important basic principles of data management in psychology. 
Initially, based on discipline-specific definitions of raw data, primary data, secondary data, and metadata, we provide recommendations on the degree of data processing necessary when publishing data. We then discuss data protection as well as aspects of copyright and data usage before defining the qualitative requirements for trustworthy research data repositories. This is followed by a detailed discussion of pragmatic aspects of data sharing, such as the differences between Type 1 and Type 2 data publications, restrictions on use (embargo period), the definition of "scientific use" by secondary users of shared data, and recommendations on how to resolve potential disputes. 
Particularly noteworthy is the new recommendation of distinct "access categories" for data, each with different requirements in terms of data protection or research ethics. These range from completely open data without usage restrictions ("access category 0") to data shared under a set of standardized conditions (e.g., reuse restricted to scientific purposes; "access category 1"), individualized usage agreements ("access category 2"), and secure data access under strictly controlled conditions (e.g., in a research data center; “access category 3"). The practical implementation of this important innovation, however, will require data repositories to provide the necessary technical functionalities. 
In summary, the revised recommendations aim to present pragmatic guidelines for researchers to handle psychological research data in an open and transparent manner, while addressing structural challenges to data sharing solutions that are beneficial for all involved parties.},
	institution = {{PsyArXiv}},
	author = {Gollwitzer, Mario and Abele-Brehm, Andrea and Fiebach, Christian and Ramthun, Roland and Scheel, Anne M. and Schönbrodt, Felix and Steinberg, Ulf},
	urldate = {2020-10-28},
	date = {2020-09-10},
	doi = {10.31234/osf.io/24ncs},
	keywords = {Meta-science, Social and Behavioral Sciences, other, Psychology, Open Science, Data Sharing, Data Management},
	file = {Gollwitzer_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Gollwitzer_etal_2020.pdf:application/pdf},
}

@article{stanley_what_2018,
	title = {What meta-analyses reveal about the replicability of psychological research.},
	volume = {144},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000169},
	doi = {10.1037/bul0000169},
	abstract = {A survey of 12,065 estimated effects from nearly 8,000 research papers finds that the average statistical power in psychology is 36\% and only 8\% of studies have adequate power. Typical heterogeneity is nearly three times larger than reported sampling error variation. Heterogeneity this large easily explains recent highly publicized failures to replicate in psychology. In most cases, we find little evidence that publication bias is a major factor.},
	pages = {1325--1346},
	number = {12},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Stanley, T. D. and Carter, Evan C. and Doucouliagos, Hristos},
	urldate = {2020-10-27},
	date = {2018-12},
	langid = {english},
	file = {Stanley et al. - 2018 - What meta-analyses reveal about the replicability .pdf:/Users/tom/Zotero/storage/9AY8NMCA/Stanley et al. - 2018 - What meta-analyses reveal about the replicability .pdf:application/pdf},
}

@article{schauer_assessing_2020,
	title = {Assessing heterogeneity and power in replications of psychological experiments.},
	volume = {146},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000232},
	doi = {10.1037/bul0000232},
	pages = {701--719},
	number = {8},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Schauer, Jacob M. and Hedges, Larry V.},
	urldate = {2020-10-27},
	date = {2020-08},
	langid = {english},
	file = {Schauer_Hedges_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Schauer_Hedges_2020.pdf:application/pdf},
}

@article{vachon_changing_2021,
	title = {Changing research culture toward more use of replication research: a narrative review of barriers and strategies},
	volume = {129},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(20)31113-6/abstract},
	doi = {10.1016/j.jclinepi.2020.09.027},
	shorttitle = {Changing research culture toward more use of replication research},
	abstract = {{\textless}h2{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater}{\textless}p{\textgreater}The aim of this paper is to review the literature on barriers to conducting replication research and strategies to increase its use and promotion by researchers, editors, and funders.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Study Design and Setting{\textless}/h3{\textgreater}{\textless}p{\textgreater}This review was part of a larger meta-narrative review aimed at conducting a concept analysis of replication and developing a replication research framework. A combination of systematic and snowball search strategies was used to identify relevant literature in multiple research fields. Data were coded and analyzed using the Theoretical Domains Framework for barriers to replication and the behavior change wheel for solutions.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater}{\textless}p{\textgreater}In total, 153 papers were included in this narrative review. Multiple barriers limit the use of replication research by researchers, editors, and funders. Many of the barriers were related to knowledge and skills of all these actors. Social influences and the research environmental context were also described as not supportive. Multiple strategies were proposed to create positive outcomes expectations, reinforcement, and structural changes in the physical and social context of research.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusion{\textless}/h3{\textgreater}{\textless}p{\textgreater}A social change involving advisory groups, research organizations, and institutions is required to establish new norms that will value, promote, support, and reward replication research.{\textless}/p{\textgreater}},
	pages = {21--30},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Vachon, Brigitte and Curran, Janet A. and Karunananthan, Sathya and Brehaut, Jamie and Graham, Ian D. and Moher, David and Sales, Anne E. and Straus, Sharon E. and Fiander, Michele and Paprica, P. Alison and Grimshaw, Jeremy M.},
	urldate = {2020-10-25},
	date = {2021-01-01},
	pmid = {33007459},
	note = {Publisher: Elsevier},
	file = {Snapshot:/Users/tom/Zotero/storage/97JCM6YM/fulltext.html:text/html;Vachon_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Vachon_etal_2021.pdf:application/pdf},
}

@article{rothman_no_1990,
	title = {No adjustments are needed for multiple comparisons},
	volume = {1},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/Abstract/1990/01000/No_Adjustments_Are_Needed_for_Multiple_Comparisons.10.aspx},
	doi = {10.1097/00001648-199001000-00010},
	abstract = {Adjustments for making multiple comparisons in large bodies of data are recommended to avoid rejecting the null hypothesis too readily. Unfortunately, reducing the type I error for null associations increases the type {II} error for those associations that are not null. The theoretical basis for advocating a routine adjustment for multiple comparisons is the “universal null hypothesis” that “chance” serves as the first-order explanation for observed phenomena. This hypothesis undermines the basic premises of empirical research, which holds that nature hollows regular laws that may he studied through observations. A policy of not making adjustments for multiple comparisons is preferable because it will lead to fewer errors of interpretation when the data under evaluation are not random numbers but actual observations on nature. Furthermore, scientists should not he so reluctant to explore leads that may turn out to he wrong that they penalize themselves by missing possibly important findings.
        © Lippincott-Raven Publishers.},
	pages = {43--46},
	number = {1},
	journaltitle = {Epidemiology},
	author = {Rothman, Kenneth J.},
	urldate = {2020-10-23},
	date = {1990-01},
	langid = {american},
	file = {Rothman - 1990 - No Adjustments Are Needed for Multiple Comparisons.pdf:/Users/tom/Zotero/storage/NVNJ3CZI/Rothman - 1990 - No Adjustments Are Needed for Multiple Comparisons.pdf:application/pdf},
}

@article{chavalarias_evolution_2016,
	title = {Evolution of Reporting \textit{P} Values in the Biomedical Literature, 1990-2015},
	volume = {315},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2016.1952},
	doi = {10.1001/jama.2016.1952},
	abstract = {{OBJECTIVE} To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values. {DESIGN} Automated text-mining analysis was performed to extract data on P values reported in 12 821 790 {MEDLINE} abstracts and in 843 884 abstracts and full-text articles in {PubMed} Central ({PMC}) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by {PubMed} also was evaluated. A random sample of 1000 {MEDLINE} abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text. {MAIN} {OUTCOMES} {AND} {MEASURES} P values reported.
{RESULTS} Text mining identified 4 572 043 P values in 1 608 736 {MEDLINE} abstracts and 3 438 299 P values in 385 393 {PMC} full-text articles. Reporting of P values in abstracts increased from 7.3\% in 1990 to 15.6\% in 2014. In 2014, P values were reported in 33.0\% of abstracts from the 151 core clinical journals (n = 29 725 abstracts), 35.7\% of meta-analyses (n = 5620), 38.9\% of clinical trials (n = 4624), 54.8\% of randomized controlled trials (n = 13 544), and 2.4\% of reviews (n = 71 529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the “best” (most statistically significant) reported P values were modestly smaller and the “worst” (least statistically significant) reported P values became modestly less significant. Among the {MEDLINE} abstracts and {PMC} full-text articles with P values, 96\% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in {PMC} full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7\% (125/796 [95\% {CI}, 13.2\%-18.4\%]) of abstracts, confidence intervals in 2.3\% (18/796 [95\% {CI}, 1.3\%-3.6\%]), Bayes factors in 0\% (0/796 [95\% {CI}, 0\%-0.5\%]), effect sizes in 13.9\% (111/796 [95\% {CI}, 11.6\%-16.5\%]), other information that could lead to estimation of P values in 12.4\% (99/796 [95\% {CI}, 10.2\%-14.9\%]), and qualitative statements about significance in 18.1\% (181/1000 [95\% {CI}, 15.8\%-20.6\%]); only 1.8\% (14/796 [95\% {CI}, 1.0\%-2.9\%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome.
{CONCLUSIONS} {AND} {RELEVANCE} In this analysis of P values reported in {MEDLINE} abstracts and in {PMC} articles from 1990-2015, more {MEDLINE} abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.},
	pages = {1141--1148},
	number = {11},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Chavalarias, David and Wallach, Joshua David and Li, Alvin Ho Ting and Ioannidis, John P. A.},
	urldate = {2020-10-22},
	date = {2016-03-15},
	langid = {english},
	file = {Chavalarias et al. - 2016 - Evolution of Reporting P Values in the Biom.pdf:/Users/tom/Zotero/storage/3XE2TCFD/Chavalarias et al. - 2016 - Evolution of Reporting P Values in the Biom.pdf:application/pdf},
}

@article{petticrew_systematic_nodate,
	title = {Systematic Reviews in the Social Sciences},
	doi = {10.1002/9780470754887},
	pages = {354},
	author = {Petticrew, Mark and Roberts, Helen},
	langid = {english},
	file = {Petticrew and Roberts - Systematic Reviews in the Social Sciences.pdf:/Users/tom/Zotero/storage/7YUCRD96/Petticrew and Roberts - Systematic Reviews in the Social Sciences.pdf:application/pdf},
}

@article{young_model_2018,
	title = {Model uncertainty and the crisis in science},
	volume = {4},
	rights = {© The Author(s) 2018},
	url = {https://journals.sagepub.com/doi/10.1177/2378023117737206?icid=int.sj-full-text.similar-articles.1},
	doi = {10.1177/2378023117737206},
	shorttitle = {Model uncertainty and the crisis in science},
	abstract = {The “crisis in science” today is rooted in genuine problems of model uncertainty and lack of transparency. Researchers estimate a large number of models in the ...},
	pages = {1--7},
	journaltitle = {Socius},
	author = {Young, Cristobal},
	urldate = {2020-10-22},
	date = {2018-05-14},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/ZXKA4JL3/2378023117737206.html:text/html;Young_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Young_2018.pdf:application/pdf},
}

@article{young_model_2015,
	title = {Model uncertainty and robustness: a computational framework for multimodel analysis},
	rights = {© The Author(s) 2015},
	url = {https://journals.sagepub.com/doi/10.1177/0049124115610347},
	doi = {10.1177/0049124115610347},
	shorttitle = {Model uncertainty and robustness},
	abstract = {Model uncertainty is pervasive in social science. A key question is how robust empirical results are to sensible changes in model specification. We present a ne...},
	journaltitle = {Sociological Methods \& Research},
	author = {Young, Cristobal and Holsteen, Katherine},
	urldate = {2020-10-22},
	date = {2015-10-23},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/Q237CUXW/0049124115610347.html:text/html},
}

@article{simonsohn_specification_2020,
	title = {Specification curve analysis},
	rights = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-0912-z},
	doi = {10.1038/s41562-020-0912-z},
	abstract = {Empirical results hinge on analytical decisions that are defensible, arbitrary and motivated. These decisions probably introduce bias (towards the narrative put forward by the authors), and they certainly involve variability not reflected by standard errors. To address this source of noise and bias, we introduce specification curve analysis, which consists of three steps: (1) identifying the set of theoretically justified, statistically valid and non-redundant specifications; (2) displaying the results graphically, allowing readers to identify consequential specifications decisions; and (3) conducting joint inference across all specifications. We illustrate the use of this technique by applying it to three findings from two different papers, one investigating discrimination based on distinctively Black names, the other investigating the effect of assigning female versus male names to hurricanes. Specification curve analysis reveals that one finding is robust, one is weak and one is not robust at all.},
	pages = {1--7},
	journaltitle = {Nature Human Behaviour},
	author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
	urldate = {2020-10-22},
	date = {2020-07-27},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	file = {Simonsohn_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Simonsohn_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UAFWAY9J/s41562-020-0912-z.html:text/html},
}

@article{greenhalgh_how_1997,
	title = {How to read a paper: Papers that summarise other papers (systematic reviews and meta-analyses)},
	volume = {315},
	rights = {© 1997 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/315/7109/672},
	doi = {10.1136/bmj.315.7109.672},
	shorttitle = {How to read a paper},
	abstract = {Remember the essays you used to write as a student? You would browse through the indexes of books and journals until you came across a paragraph that looked relevant, and copied it out. If anything you found did not fit in with the theory you were proposing, you left it out. This, more or less, constitutes the methodology of the journalistic review—an overview of primary studies which have not been identified or analysed in a systematic (standardised and objective) way.

\#\#\#\# Summary points

A systematic review is an overview of primary studies that used explicit and reproducible methods

A meta-analysis is a mathematical synthesis of the results of two or more primary studies that addressed the same hypothesis in the same way

Although meta-analysis can increase the precision of a result, it is important to ensure that the methods used for the review were valid and reliable

In contrast, a systematic review is an overview of primary studies which contains an explicit statement of objectives, materials, and methods and has been conducted according to explicit and reproducible methodology (fig 1).



Fig 1 
Methodology for a systematic review of randomised controlled trials1



Some advantages of the systematic review are given in box. When a systematic review is undertaken, not only must the search for relevant articles be thorough and objective, but the criteria used to reject articles as “flawed” must be explicit and independent of the results of those trials. The most enduring and useful systematic reviews, notably those undertaken by the Cochrane Collaboration, are regularly updated to incorporate new evidence.2

\#\#\# Box 1: Advantages of systematic reviews3 {RETURN} {TO} {TEXT}},
	pages = {672--675},
	number = {7109},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Greenhalgh, Trisha},
	urldate = {2020-10-22},
	date = {1997-09-13},
	langid = {english},
	pmid = {9310574},
	note = {Publisher: British Medical Journal Publishing Group
Section: Education and debate},
	file = {Greenhalgh_1997.pdf:/Users/tom/pCloud Drive/Zotero_Library/Greenhalgh_1997.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6F5ILCK5/672.html:text/html},
}

@article{glass_meta-analysis_2015,
	title = {Meta-analysis at middle age: a personal history},
	volume = {6},
	rights = {Copyright © 2015 John Wiley \& Sons, Ltd.},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1133},
	doi = {10.1002/jrsm.1133},
	shorttitle = {Meta-analysis at middle age},
	abstract = {The 40-year history of meta-analysis is traced from the vantage point of one of its originators. Research syntheses leading to the first examples of meta-analysis are identified. Early meta-analyses of the literature on psychotherapy outcomes and school class size are recounted. The influence on the development of meta-analysis of several statisticians and psychologists is described. Finally, some directions for future development of research synthesis methods are suggested. Copyright © 2015 John Wiley \& Sons, Ltd.},
	pages = {221--231},
	number = {3},
	journaltitle = {Research Synthesis Methods},
	author = {Glass, Gene V.},
	urldate = {2020-10-22},
	date = {2015},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1133},
	keywords = {meta-analysis, psychotherapy, research synthesis},
	file = {Glass - 2015 - Meta-analysis at middle age a personal history.pdf:/Users/tom/Zotero/storage/RCJVATL2/Glass - 2015 - Meta-analysis at middle age a personal history.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/93NXXUWW/jrsm.html:text/html},
}

@incollection{busemeyer_model_2015-1,
	title = {Model Comparison and the Principle of Parsimony},
	volume = {1},
	url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14},
	abstract = {According to the principle of parsimony, model selection methods should value both descriptive accuracy and simplicity. Here we focus primarily on Bayes factors and minimum description length, explaining how these procedures strike a balance between goodness-of-fit and parsimony. Throughout, we demonstrate the methods with an application on false memory, evaluating three competing multimonial proces tree models of interference in memory.},
	booktitle = {The Oxford Handbook of Computational and Mathematical Psychology},
	publisher = {Oxford University Press},
	author = {Vandekerckhove, Joachim and Matzke, Dora and Wagenmakers, Eric-Jan},
	editor = {Busemeyer, Jerome R. and Wang, Zheng and Townsend, James T. and Eidels, Ami},
	urldate = {2020-10-22},
	date = {2015-12-10},
	langid = {english},
	doi = {10.1093/oxfordhb/9780199957996.013.14},
	file = {Vandekerckhove et al. - 2015 - Model Comparison and the Principle of Parsimony.pdf:/Users/tom/Zotero/storage/SUENU43U/Vandekerckhove et al. - 2015 - Model Comparison and the Principle of Parsimony.pdf:application/pdf},
}

@report{akker_inclusive_2020,
	title = {Inclusive systematic review registration form},
	url = {https://osf.io/preprints/metaarxiv/3nbea/},
	abstract = {This Systematic Review Registration Form is intended as a general-purpose registration form. The form is designed to be applicable to reviews across disciplines (i.e., psychology, economics, law, physics, or any other field) and across review types (i.e., scoping review, review of qualitative studies, meta-analysis, or any other type of review). That means that the reviewed records may include research reports as well as archive documents, case law, books, poems, etc. This form, therefore, is a fall-back for more specialized forms and can be used if no specialized form or registration platform is available.},
	institution = {{MetaArXiv}},
	author = {Akker, Olmo van den and Peters, Gjalt-Jorn and Bakker, Caitlin and Carlsson, Rickard and Coles, Nicholas and Corker, Katherine S. and Feldman, Gilad and Mellor, David Thomas and Moreau, David and Nordström, Thomas and Pfeiffer, Nicole and Pickering, Jade and Riegelman, Amy and Topor, Marta and Veggel, Nieky van and Yeung, Siu Kit},
	urldate = {2020-10-21},
	date = {2020-09-15},
	doi = {10.31222/osf.io/3nbea},
	keywords = {preregistration, Social and Behavioral Sciences, Medicine and Health Sciences, meta-analysis, systematic review},
	file = {Akker_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Akker_etal_2020.pdf:application/pdf},
}

@article{lin_graphical_2019,
	title = {Graphical augmentations to sample-size-based funnel plot in meta-analysis},
	volume = {10},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1340},
	doi = {10.1002/jrsm.1340},
	abstract = {Assessing publication bias is a critical procedure in meta-analyses for rating the synthesized overall evidence. Because statistical tests for publication bias are usually not powerful and only give P values that inform either the presence or absence of the bias, examining the asymmetry of funnel plots has been popular to investigate potentially missing studies and the direction of the bias. Most funnel plots present treatment effects against their standard errors, and the contours depicting studies' significance levels have been used in the plots to distinguish publication bias from other factors (such as heterogeneity and subgroup effects) that may cause the plots' asymmetry. However, treatment effects and their standard errors are frequently associated even if no publication bias exists (eg, both variables depend on the four data cells in a 2 × 2 table for the odds ratio), so standard-error-based funnel plots may lead to false positive conclusions when such association may not be negligible. In addition, the missingness of studies may relate to their sample sizes besides P values (which are partly determined by standard errors); studies with more samples are more likely published. Therefore, funnel plots based on sample sizes can be an alternative tool. However, the contours for standard-error-based funnel plots cannot be directly applied to sample-size-based ones. This article introduces contours for sample-size-based funnel plots of various effect sizes, which may help meta-analysts properly interpret such plots' asymmetry. We provide five examples to illustrate the use of the proposed contours.},
	pages = {376--388},
	number = {3},
	journaltitle = {Research Synthesis Methods},
	author = {Lin, Lifeng},
	urldate = {2020-10-21},
	date = {2019},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1340},
	keywords = {publication bias, sample size, meta-analysis, funnel plot, standard error},
	file = {Lin_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lin_2019.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/IKSNMSEN/jrsm.html:text/html},
}

@article{wagenaar_misleading_1987,
	title = {Misleading postevent information: Testing parameterized models of integration in memory},
	volume = {66},
	issn = {0001-6918},
	url = {http://www.sciencedirect.com/science/article/pii/0001691887900400},
	doi = {10.1016/0001-6918(87)90040-0},
	shorttitle = {Misleading postevent information},
	abstract = {A classic series of experiments by Loftus, Miller and Burns (1978) showed that a person's recollection of an event can be changed by misleading postevent information. Several hypotheses accounting for this effect have been proposed. Loftus' hypothesis of destructive updating claims that the original memory is destroyed by the postevent information. The coexistence hypothesis asserts that the older memory survives but is rendered inaccessible through a mechanism of inhibition or suppression. The non-conflict hypothesis simply accounts for the effect by claiming that subjects can only be misled if they did not encode or if they forgot the original event. These three hypotheses were modelled with the help of all-or-none probabilistic event trees. An experiment was conducted in order to test the three models and to assess parameter values. The experiment followed the classic Loftus paradigm. We suggested to some subjects that they had seen a stopsign, whereas in fact they had seen a traffic light. The misleading postevent information resulted in poorer reproduction of traffic light. Later, all subjects were asked whether they could remember the color of the traffic light, even if they believed they had seen a stopsign. The results showed that subjects who received the misleading post-event information were at least as good at recalling the color of the traffic light as subjects who did not receive misleading information. The no-conflict model accounts well for the obtained results, although the two other, less parsimonious, models cannot be entirely rejected.},
	pages = {291--306},
	number = {3},
	journaltitle = {Acta Psychologica},
	shortjournal = {Acta Psychologica},
	author = {Wagenaar, Willem A. and Boer, Johannes P. A.},
	urldate = {2020-10-21},
	date = {1987-12-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/R4RG432A/0001691887900400.html:text/html;Wagenaar_Boer_1987.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wagenaar_Boer_1987.pdf:application/pdf},
}

@incollection{doris_preschool_1991,
	location = {Washington},
	title = {Preschool children's susceptibility to memory impairment.},
	isbn = {978-1-55798-118-9},
	url = {http://content.apa.org/books/10097-003},
	pages = {27--39},
	booktitle = {The suggestibility of children's recollections.},
	publisher = {American Psychological Association},
	author = {Zaragoza, Maria S.},
	editor = {Doris, John},
	urldate = {2020-10-21},
	date = {1991},
	langid = {english},
	doi = {10.1037/10097-003},
	file = {Zaragoza - 1991 - Preschool children's susceptibility to memory impa.pdf:/Users/tom/Zotero/storage/VNW7B4F7/Zaragoza - 1991 - Preschool children's susceptibility to memory impa.pdf:application/pdf},
}

@incollection{ceci_memory_1987,
	location = {New York, {NY}},
	title = {Memory, Suggestibility, and Eyewitness Testimony in Children and Adults},
	isbn = {978-1-4684-6340-8 978-1-4684-6338-5},
	url = {http://link.springer.com/10.1007/978-1-4684-6338-5_4},
	pages = {53--78},
	booktitle = {Children’s Eyewitness Memory},
	publisher = {Springer {US}},
	author = {Zaragoza, Maria S.},
	editor = {Ceci, Stephen J. and Toglia, Michael P. and Ross, David F.},
	urldate = {2020-10-21},
	date = {1987},
	langid = {english},
	doi = {10.1007/978-1-4684-6338-5_4},
	file = {Zaragoza - 1987 - Memory, Suggestibility, and Eyewitness Testimony i.pdf:/Users/tom/Zotero/storage/NHSVVEEK/Zaragoza - 1987 - Memory, Suggestibility, and Eyewitness Testimony i.pdf:application/pdf},
}

@incollection{zaragoza_role_1992,
	location = {New York, {NY}},
	title = {The Role of Memory Impairment in Children’s Suggestibility},
	isbn = {978-1-4612-2868-4},
	url = {https://doi.org/10.1007/978-1-4612-2868-4_6},
	abstract = {A central issue in the study of children’s long-term retention is an understanding of children’s susceptibility to memory failures. It has long been recognized that an important cause of memory failures is interference caused by new learning. Recently, interest in memory failures caused by subsequent learning has been revived in the context of studies on suggestibility and eyewitness memory. These studies have shown that, for subjects of all ages, exposure to misinformation (i.e., false information presented as truth) after viewing an event can lead to profound decrements in performance on later tests of memory for the originally seen event.},
	pages = {184--216},
	booktitle = {Development of Long-Term Retention},
	publisher = {Springer},
	author = {Zaragoza, Maria S. and Dahlgren, Donna and Muench, Jean},
	editor = {Howe, Mark L. and Brainerd, Charles J. and Reyna, Valerie F.},
	urldate = {2020-10-21},
	date = {1992},
	langid = {english},
	doi = {10.1007/978-1-4612-2868-4_6},
	keywords = {Critical Item, Eyewitness Testimony, Memory Impairment, Misinformation Effect, Source Monitoring},
	file = {Zaragoza et al. - 1992 - The Role of Memory Impairment in Children’s Sugges.pdf:/Users/tom/pCloud Drive/Zotero_Library/Zaragoza et al. - 1992 - The Role of Memory Impairment in Children’s Sugges.pdf:application/pdf},
}

@article{loftus_creating_1989,
	title = {Creating new memories that are quickly accessed and confidently held},
	volume = {17},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03197083},
	doi = {10.3758/BF03197083},
	pages = {607--616},
	number = {5},
	journaltitle = {Memory \& Cognition},
	shortjournal = {Memory \& Cognition},
	author = {Loftus, Elizabeth F. and Donders, Karen and Hoffman, Hunter G. and Schooler, Jonathan W.},
	urldate = {2020-10-21},
	date = {1989-09},
	langid = {english},
	file = {Loftus et al. - 1989 - Creating new memories that are quickly accessed an.pdf:/Users/tom/Zotero/storage/4CUPAEDW/Loftus et al. - 1989 - Creating new memories that are quickly accessed an.pdf:application/pdf},
}

@article{bowman_similarity_1989,
	title = {Similarity of encoding context does not influence resistance to memory impairment following misinformation},
	volume = {102},
	issn = {00029556},
	url = {https://www.jstor.org/stable/1422956?origin=crossref},
	doi = {10.2307/1422956},
	pages = {249},
	number = {2},
	journaltitle = {The American Journal of Psychology},
	shortjournal = {The American Journal of Psychology},
	author = {Bowman, Laura L. and Zaragoza, Maria S.},
	urldate = {2020-10-21},
	date = {1989},
	langid = {english},
	file = {Bowman and Zaragoza - 1989 - Similarity of Encoding Context Does Not Influence .pdf:/Users/tom/Zotero/storage/Q3UQLAUM/Bowman and Zaragoza - 1989 - Similarity of Encoding Context Does Not Influence .pdf:application/pdf},
}

@article{lonsdorf_navigating_2019,
	title = {Navigating the garden of forking paths for data exclusions in fear conditioning research},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.52465},
	doi = {10.7554/eLife.52465},
	abstract = {In this report, we illustrate the considerable impact of researcher degrees of freedom with respect to exclusion of participants in paradigms with a learning element. We illustrate this empirically through case examples from human fear conditioning research, in which the exclusion of ‘non-learners’ and ‘non-responders’ is common – despite a lack of consensus on how to define these groups. We illustrate the substantial heterogeneity in exclusion criteria identified in a systematic literature search and highlight the potential problems and pitfalls of different definitions through case examples based on re-analyses of existing data sets. On the basis of these studies, we propose a consensus on evidence-based rather than idiosyncratic criteria, including clear guidelines on reporting details. Taken together, we illustrate how flexibility in data collection and analysis can be avoided, which will benefit the robustness and replicability of research findings and can be expected to be applicable to other fields of research that involve a learning element.},
	pages = {e52465},
	journaltitle = {{eLife}},
	author = {Lonsdorf, Tina B and Klingelhöfer-Jens, Maren and Andreatta, Marta and Beckers, Tom and Chalkia, Anastasia and Gerlicher, Anna and Jentsch, Valerie L and Meir Drexler, Shira and Mertens, Gaetan and Richter, Jan and Sjouwerman, Rachel and Wendt, Julia and Merz, Christian J},
	editor = {de Lange, Floris P and Shackman, Alexander and Bradford, Daniel and Larson, Christine and Balderston, Nick},
	urldate = {2020-10-20},
	date = {2019-12-16},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {memory, bias, exclusion, learning, non-learner, outlier},
	file = {Lonsdorf_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lonsdorf_etal_2019.pdf:application/pdf},
}

@article{bonto_role_1991,
	title = {Role of environmental context in eyewitness memory},
	volume = {104},
	issn = {00029556},
	url = {https://www.jstor.org/stable/1422854?origin=crossref},
	doi = {10.2307/1422854},
	pages = {117},
	number = {1},
	journaltitle = {The American Journal of Psychology},
	shortjournal = {The American Journal of Psychology},
	author = {Bonto, Marivic A. and Payne, David G.},
	urldate = {2020-10-20},
	date = {1991},
	langid = {english},
	file = {Bonto and Payne - 1991 - Role of Environmental Context in Eyewitness Memory.pdf:/Users/tom/Zotero/storage/8K3HEGFX/Bonto and Payne - 1991 - Role of Environmental Context in Eyewitness Memory.pdf:application/pdf},
}

@article{belli_failure_1993,
	title = {Failure of interpolated tests in inducing memory impairment with final modified tests: evidence unfavorable to the blocking hypothesis},
	volume = {106},
	issn = {00029556},
	url = {https://www.jstor.org/stable/1423184?origin=crossref},
	doi = {10.2307/1423184},
	shorttitle = {Failure of interpolated tests in inducing memory impairment with final modified tests},
	pages = {407},
	number = {3},
	journaltitle = {The American Journal of Psychology},
	shortjournal = {The American Journal of Psychology},
	author = {Belli, Robert F.},
	urldate = {2020-10-20},
	date = {1993},
	langid = {english},
	file = {Belli - 1993 - Failure of Interpolated Tests in Inducing Memory I.pdf:/Users/tom/Zotero/storage/P8TXGIBB/Belli - 1993 - Failure of Interpolated Tests in Inducing Memory I.pdf:application/pdf},
}

@incollection{toglia_suggestibility_1992,
	location = {New York, {NY}},
	title = {The Suggestibility of Children’s Memory: A Social-Psychological and Cognitive Interpretation},
	isbn = {978-1-4612-2868-4},
	url = {https://doi.org/10.1007/978-1-4612-2868-4_7},
	shorttitle = {The Suggestibility of Children’s Memory},
	abstract = {The suggestibility of the child witness has been a concern of the American judicial system since the turn of the century. At that time, a number of studies (e.g., Binet, 1900; Marple, 1933; Pear \& Wyatt, 1914; Stern, 1910; Varendonck, 1911; Whipple, 1909, 1911, 1912) were reported indicating that young children were quite susceptible to suggestive or leading questions. For example, Whipple (1909) wrote that “the one factor that more than any other is responsible for the poor reports of children is their suggestibility, especially in the years before puberty” (p. 162). Much of this early research, however, was methodologically flawed (Goodman, 1984a). Only recently has there been a resurgence of research on this topic. These newer studies have produced conflicting results as young children are not always more likely to succumb to suggestion than are older children or adults. Such complexities have raised questions about the nature and bases of the suggestibility of children’s memory (as well as adults’ suggestibility). Therefore, the major goals of this chapter are (a) to briefly review research on children’s suggestibility, (b) to describe social and cognitive factors that influence children’s susceptibility to suggestion, and (c) to address the theoretical significance of these factors.},
	pages = {217--241},
	booktitle = {Development of Long-Term Retention},
	publisher = {Springer},
	author = {Toglia, Michael P. and Ross, David F. and Ceci, Stephen J. and Hembrooke, Helene},
	editor = {Howe, Mark L. and Brainerd, Charles J. and Reyna, Valerie F.},
	urldate = {2020-10-20},
	date = {1992},
	langid = {english},
	doi = {10.1007/978-1-4612-2868-4_7},
	keywords = {Eyewitness Testimony, Misinformation Effect, Misleading Information, Original Event, Original Trace},
	file = {Toglia et al. - 1992 - The Suggestibility of Children’s Memory A Social-.pdf:/Users/tom/Zotero/storage/NZYI7SDM/Toglia et al. - 1992 - The Suggestibility of Children’s Memory A Social-.pdf:application/pdf},
}

@article{ceci_suggestibility_1987,
	title = {Suggestibility of Children's Memory: Psycholegal Implications},
	doi = {10.1037/0096-3445.116.1.38},
	abstract = {Historically, there has been a bias in the American judicial system against relying on eycwitness accounts of young children. Some of the apprehension about the veracity of children's recollections has arisen from a concem over the testimony provided by children during the Salera Witch Trials and been fueled further by research carried out around the turn of the century snggesting that children could not be trusted to accurately recount events. Today, ail states have corroboration rules mandating that the testimony of a child be confirmed by another person prior toits being accepted as evidence in a court oflaw. These rules are based, in part, on a beliefthat young children's testimony is especially vulnerable to suggestive or "leading" questions. To date, there have been few scientifically adequate data to confirm or disconfirm this belief. This article reports the results of four experiments that address this issue and some of the psychological mechanisms responsible for suggestibility. Experiment 1examined whether children are susceptible to misleading postevent information. The results of this experiment indicated that they are, with young children (3- and 4-year-olds)being particularly vulnerable to suggestion. Accordingly, the subsequent experiments focused on this age range and the basis for their susceptibility to misleading postevent information. Experiment 2 addressed the issue of demand characteristics. Specifically, it was found that children's susceptibility to misleading information was reduced when another child, as opposed to an adult, provided the misleading information. Therefore, suggestibility effects in children ar9 in part from a desire to conform to the wishes of an adult authority figure. Experiments 3 and 4 tested two competing hypotheses as to how postevent suggestions distort children's memories, in an effort to understand the mechanisms that underlie preschoolers' susceptibility to distortion as well as to resolve a contemporary controversy among researchers regarding the most appropriate method for assessing memory impairment. Using both the standard and modified recognition testing procedures recommended by {McCloskey} and Zaragoza (1985a), performance was found to be substanfially diminished in both of these conditions compared to the control condition. It was therefore concluded that postevent suggestions c,an in fact distort memory. The results from these four experiments are discussed within the context of children's eyewitness memory and the as{\textasciitilde}{\textasciitilde}iated psycholegal implieations.},
	pages = {12},
	author = {Ceci, Stephen J and Ross, David E and Toglia, Michael P},
	date = {1987},
	langid = {english},
	file = {Ceci et al. - Suggestibility of Children's Memory Psycholegal I.pdf:/Users/tom/Zotero/storage/TJ5SMDHS/Ceci et al. - Suggestibility of Children's Memory Psycholegal I.pdf:application/pdf},
}

@article{belli_detecting_1992,
	title = {Detecting memory impairment with a modified test procedure: Manipulating retention interval with centrally presented event items},
	volume = {18},
	issn = {1939-1285(Electronic),0278-7393(Print)},
	doi = {10.1037/0278-7393.18.2.356},
	shorttitle = {Detecting memory impairment with a modified test procedure},
	abstract = {Suggests that detecting memory impairment with the modified test relies on long retention intervals that provide the necessary forgetting of event information for impairing effects of postevent misinformation to occur. 288 Ss were tested in 4 experiments that presented event items centrally, introduced verbal postevent items to a misled condition, and used the modified test, but differed by using either short (15 min) or long (5–7 days) retention intervals. As evidenced by poorer misled than control test performances, memory impairment only occurred with long retention intervals. Retrieval- and storage-based versions of memory-impairment hypotheses are assessed. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {356--367},
	number = {2},
	journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Belli, Robert F. and Windschitl, Paul D. and {McCarthy}, Thomas T. and Winfrey, Steve E.},
	date = {1992},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Measurement, Memory, Recognition (Learning), Retention, Stimulus Duration},
	file = {Belli et al. - 1992 - Detecting memory impairment with a modified test p.pdf:/Users/tom/Zotero/storage/ENQ6XKHF/Belli et al. - 1992 - Detecting memory impairment with a modified test p.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/LLSH279B/1992-18624-001.html:text/html},
}

@article{chandler_specific_1989,
	title = {Specific retroactive interference in modified recognition tests: Evidence for an unknown cause of interference},
	volume = {15},
	pages = {256--265},
	journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Chandler, C C},
	date = {1989},
	file = {Chandler - 1989 - Specific retroactive interference in modified reco.pdf:/Users/tom/Zotero/storage/VXGWXXAP/Chandler - 1989 - Specific retroactive interference in modified reco.pdf:application/pdf},
}

@article{chandler_how_1991,
	title = {How Memory for an Event Is Influenced by Related Events: Interference in Modified Recognition Tests},
	volume = {17},
	doi = {10.1037/0278-7393.17.1.115},
	pages = {115--125},
	number = {1},
	journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Chandler, C C},
	date = {1991},
	langid = {english},
	file = {Chandler - How Memory for an Event Is Influenced by Related E.pdf:/Users/tom/Zotero/storage/PCWE7PEZ/Chandler - How Memory for an Event Is Influenced by Related E.pdf:application/pdf},
}

@article{chandler_witnessing_2001,
	title = {Witnessing postevents does not change memory traces, but can affect their retrieval},
	volume = {15},
	pages = {3--22},
	journaltitle = {Applied Cognitive Psychology},
	author = {Chandler, C C and Gargano, G J and Holt, B C},
	date = {2001},
	langid = {english},
	file = {Chandler et al. - 2001 - Witnessing postevents does not change memory trace.pdf:/Users/tom/Zotero/storage/DAI79FXJ/Chandler et al. - 2001 - Witnessing postevents does not change memory trace.pdf:application/pdf},
}

@incollection{chandler_retrieval_1996,
	title = {Retrieval Processes and Witness Memory},
	isbn = {978-0-12-102570-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780121025700500161},
	pages = {493--524},
	booktitle = {Memory},
	publisher = {Elsevier},
	author = {Chandler, Carla C. and Fisher, Ronald P.},
	urldate = {2020-10-20},
	date = {1996},
	langid = {english},
	doi = {10.1016/B978-012102570-0/50016-1},
	file = {Chandler and Fisher - 1996 - Retrieval Processes and Witness Memory.pdf:/Users/tom/Zotero/storage/2W3EB6UW/Chandler and Fisher - 1996 - Retrieval Processes and Witness Memory.pdf:application/pdf},
}

@article{payne_recognition_1994,
	title = {Recognition performance level and the magnitude of the misinformation effect in eyewitness memory},
	volume = {1},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/BF03213978},
	doi = {10.3758/BF03213978},
	pages = {376--382},
	number = {3},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychonomic Bulletin \& Review},
	author = {Payne, David G. and Toglia, Michael P. and Anastasi, Jeffrey S.},
	urldate = {2020-10-20},
	date = {1994-09},
	langid = {english},
	file = {Payne et al. - 1994 - Recognition performance level and the magnitude of.pdf:/Users/tom/Zotero/storage/TL3PN7VV/Payne et al. - 1994 - Recognition performance level and the magnitude of.pdf:application/pdf},
}

@article{tan_prevalence_2019,
	title = {Prevalence of trial registration varies by study characteristics and risk of bias},
	volume = {113},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435619300435},
	doi = {10.1016/j.jclinepi.2019.05.009},
	abstract = {Objectives: The objective of this study was to determine the prevalence of trial registration in health research, whether trial registration status and timing vary depending on trial characteristics, and the relationship between trial registration status and risk of bias. Study Design and Setting: We systematically reviewed all clinical trials published from January to June 2017 in 28 high- and lowimpact factor general and specialty medicine journals.
Results: We identiﬁed 370 trials and assessed risk of bias in 183 trials. Trial registration rates were high; 95\% of trials were registered prospectively or retrospectively before enrollment completion. Larger sample size, multiple recruitment countries, and primary industry funding were all predictors of earlier trial registration. Prospectively registered trials had a signiﬁcantly lower risk of bias compared to unregistered trials across all domains. Prospectively registered trials had a similar risk of bias compared to retrospectively registered trials across four out of six domains, and a lower risk of bias across the remaining two domains.
Conclusion: Trial registration is an imperfect proxy for risk of bias. Systematic reviewers should assess risk of bias on a case-by-case basis and conduct sensitivity analyses excluding high risk of bias studies. In the longer term, mechanisms should be implemented to facilitate prospective registration of all trials. Ó 2019 Elsevier Inc. All rights reserved.},
	pages = {64--74},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Tan, Aidan Christopher and Jiang, Ivy and Askie, Lisa and Hunter, Kylie and Simes, Robert John and Seidler, Anna Lene},
	urldate = {2020-10-20},
	date = {2019-09},
	langid = {english},
	file = {Tan et al. - 2019 - Prevalence of trial registration varies by study c.pdf:/Users/tom/Zotero/storage/762UK2MA/Tan et al. - 2019 - Prevalence of trial registration varies by study c.pdf:application/pdf},
}

@article{rosenbaum_replicating_2001,
	title = {Replicating Effects and Biases},
	volume = {55},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2685805},
	doi = {10.1198/000313001317098220},
	abstract = {One common view is that a well-designed empirical study will reach conclusions that can be found again and again if the study is replicated, whereas a poorly designed study is unlikely to replicate. In opposition to this view, it is argued that a well-designed empirical study reaches conclusions that tend to replicate when correct and are less likely to replicate when incorrect, whereas in a poorly designed study the conclusions tend to replicate even when incorrect. In observational studies of treatment effects, the same hidden bias may occur repeatedly in a series of studies, so the studies reproduce the same distorted estimates of treatment effects. The purpose of this article is to point to strategies in research design that make it less likely that biased estimates will replicate, and to illustrate these strategies with examples. To replicate effects without replicating biases, vary the treatment assignment mechanism, so that the reasons subjects are spared treatment are different in the original and replicated studies. Also, vary the treatment envelope-that is, the ostensibly irrelevant features in which the treatment is packaged.},
	pages = {223--227},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Rosenbaum, Paul R.},
	urldate = {2020-10-18},
	date = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	file = {Rosenbaum_2001.pdf:/Users/tom/pCloud Drive/Zotero_Library/Rosenbaum_2001.pdf:application/pdf},
}

@book{reichenbach_experience_1938,
	location = {Chicago},
	title = {Experience and Prediction. An Analysis of the Foundations and the Structure of Knowledge},
	publisher = {University of Chicago Press},
	author = {Reichenbach, H},
	date = {1938},
}

@article{haddaway_eight_2020,
	title = {Eight problems with literature reviews and how to fix them},
	rights = {2020 Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-020-01295-x},
	doi = {10.1038/s41559-020-01295-x},
	abstract = {Traditional approaches to reviewing literature may be susceptible to bias and result in incorrect decisions. This is of particular concern when reviews address policy- and practice-relevant questions. Systematic reviews have been introduced as a more rigorous approach to synthesizing evidence across studies; they rely on a suite of evidence-based methods aimed at maximizing rigour and minimizing susceptibility to bias. Despite the increasing popularity of systematic reviews in the environmental field, evidence synthesis methods continue to be poorly applied in practice, resulting in the publication of syntheses that are highly susceptible to bias. Recognizing the constraints that researchers can sometimes feel when attempting to plan, conduct and publish rigorous and comprehensive evidence syntheses, we aim here to identify major pitfalls in the conduct and reporting of systematic reviews, making use of recent examples from across the field. Adopting a ‘critical friend’ role in supporting would-be systematic reviews and avoiding individual responses to police use of the ‘systematic review’ label, we go on to identify methodological solutions to mitigate these pitfalls. We then highlight existing support available to avoid these issues and call on the entire community, including systematic review specialists, to work towards better evidence syntheses for better evidence and better decisions. Systematic reviews are a powerful tool to synthesize large volumes of the published literature, but are susceptible to a number of methodological biases. Here, the authors outline mitigation strategies for improving the quality of evidence syntheses.},
	pages = {1--8},
	journaltitle = {Nature Ecology \& Evolution},
	author = {Haddaway, Neal R. and Bethel, Alison and Dicks, Lynn V. and Koricheva, Julia and Macura, Biljana and Petrokofsky, Gillian and Pullin, Andrew S. and Savilaakso, Sini and Stewart, Gavin B.},
	urldate = {2020-10-17},
	date = {2020-10-12},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	file = {Haddaway_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Haddaway_etal_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/FXL7G8RD/s41559-020-01295-x.html:text/html},
}

@report{tunc_falsificationist_2020,
	title = {A Falsificationist Treatment of Auxiliary Hypotheses in Social and Behavioral Sciences: Systematic Replications Framework},
	url = {https://psyarxiv.com/pdm7y/},
	shorttitle = {A Falsificationist Treatment of Auxiliary Hypotheses in Social and Behavioral Sciences},
	abstract = {Single empirical tests are always ambiguous in their implications for the theory under investigation, because non-corroborative evidence leaves us underdetermined in our decision as to whether the main hypothesis or one or more auxiliary hypotheses should bear the burden of falsification. Methodological falsificationism tries to solve this problem by relegating auxiliary hypotheses that increase the testability of theories to unproblematic background knowledge and disallowing others. However, decisions to accept such auxiliaries as unproblematic are seldom conclusively justified in the social and behavioral sciences, where operationalizations play a central role, but are much less theory-driven and independently testable. Close and conceptual replications are crucial in tackling different aspects of underdetermination, but they fail to serve this purpose when conducted in isolation. To facilitate rational decision-making regarding falsifications, we propose Systematic Replications Framework ({SRF}) that organizes subsequent tests into a pre-planned series of logically interlinked close and conceptual replications. {SRF} reduces underdetermination by disentangling the implications of non-corroborative findings for the main hypothesis and the operationalization-related auxiliaries. It also serves as a severe-testing procedure through systematically organized self-replications. We further discuss how {SRF} will be particularly useful if applied to contested theoretical claims with mixed evidence and realized through adversarial collaboration. We argue that {SRF} will facilitate establishment of scientific consensus regarding established evidence and how exactly it supports or contradicts competing theoretical claims, which will in turn allow us to more objectively appraise scientific progress in the social and behavioral sciences.},
	institution = {{PsyArXiv}},
	author = {Tunç, Duygu Uygun and Tunç, Mehmet Necip},
	urldate = {2020-10-17},
	date = {2020-05-13},
	doi = {10.31234/osf.io/pdm7y},
	keywords = {Social and Behavioral Sciences, Quantitative Methods, replication, Theory and Philosophy of Science, falsification, adversarial collaboration, auxiliary hypotheses, Duhem-Quine Thesis, empirical underdetermination, systematic replications framework},
	file = {Tunç_Tunç_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Tunç_Tunç_2020.pdf:application/pdf},
}

@article{morris_choosing_2014,
	title = {Choosing sensitivity analyses for randomised trials: principles},
	volume = {14},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-14-11},
	doi = {10.1186/1471-2288-14-11},
	shorttitle = {Choosing sensitivity analyses for randomised trials},
	abstract = {Sensitivity analyses are an important tool for understanding the extent to which the results of randomised trials depend upon the assumptions of the analysis. There is currently no guidance governing the choice of sensitivity analyses.},
	pages = {11},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Morris, Tim P. and Kahan, Brennan C. and White, Ian R.},
	urldate = {2020-10-16},
	date = {2014-01-24},
	file = {Morris_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Morris_etal_2014.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5Q98SD3N/1471-2288-14-11.html:text/html},
}

@article{thabane_tutorial_2013,
	title = {A tutorial on sensitivity analyses in clinical trials: the what, why, when and how},
	volume = {13},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-13-92},
	doi = {10.1186/1471-2288-13-92},
	shorttitle = {A tutorial on sensitivity analyses in clinical trials},
	abstract = {Sensitivity analyses play a crucial role in assessing the robustness of the findings or conclusions based on primary analyses of data in clinical trials. They are a critical way to assess the impact, effect or influence of key assumptions or variations—such as different methods of analysis, definitions of outcomes, protocol deviations, missing data, and outliers—on the overall conclusions of a study.},
	pages = {92},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Thabane, Lehana and Mbuagbaw, Lawrence and Zhang, Shiyuan and Samaan, Zainab and Marcucci, Maura and Ye, Chenglin and Thabane, Marroon and Giangregorio, Lora and Dennis, Brittany and Kosa, Daisy and Debono, Victoria Borg and Dillenburg, Rejane and Fruci, Vincent and Bawor, Monica and Lee, Juneyoung and Wells, George and Goldsmith, Charles H.},
	urldate = {2020-10-16},
	date = {2013-07-16},
	file = {Snapshot:/Users/tom/Zotero/storage/FPE8VQ9P/1471-2288-13-92.html:text/html;Thabane_etal_2013.pdf:/Users/tom/pCloud Drive/Zotero_Library/Thabane_etal_2013.pdf:application/pdf},
}

@article{woodward_varieties_nodate,
	title = {Some varieties of robustness},
	doi = {10.1080/13501780600733376},
	abstract = {It is widely believed that robustness (of inferences, measurements, models, phenomena and relationships discovered in empirical investigation etc.) is a Good Thing. However, there are many different notions of robustness. These often differ both in their normative credentials and in the conditions that warrant their deployment. Failure to distinguish among these notions can result in the uncritical transfer of considerations which support one notion to contexts in which another notion is being deployed. This paper surveys several different notions of robustness and tries to identify why (and in what circumstances) each is valuable or appealing. I begin by discussing the notion of robustness addressed in Aldrich’s paper (robustness as insensitivity of the results of inference to alternative specifications) and then discuss how this relates to robustness of derivations, robustness of measurement results, and robustness as a mark of casual as opposed to (merely) correlational relationships.},
	pages = {23},
	author = {Woodward, Jim},
	langid = {english},
	file = {Woodward - Some varieties of robustness.pdf:/Users/tom/Zotero/storage/HAFGF9VV/Woodward - Some varieties of robustness.pdf:application/pdf},
}

@article{loftus_malleability_1979,
	title = {The Malleability of Human Memory: Information introduced after we view an incident can transform memory},
	volume = {67},
	url = {http://www.jstor.org/stable/27849223},
	pages = {312--320},
	number = {3},
	journaltitle = {American Scientist},
	author = {Loftus, Elizabeth F.},
	date = {1979},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Loftus - 1979 - The Malleability of Human Memory Information intr.pdf:/Users/tom/Zotero/storage/V6S6L8K4/Loftus - 1979 - The Malleability of Human Memory Information intr.pdf:application/pdf},
}

@article{makel_both_2021,
	title = {Both questionable and open research practices are prevalent in education research},
	issn = {0013-189X},
	url = {https://doi.org/10.3102/0013189X211001356},
	doi = {10.3102/0013189X211001356},
	abstract = {Concerns about the conduct of research are pervasive in many fields, including education. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and five open research practices. We asked them to estimate the prevalence of the practices in the field, to self-report their own use of such practices, and to estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are used by many education researchers. This baseline information will be useful as education researchers seek to understand existing social norms and grapple with whether and how to improve research practices.},
	pages = {0013189X211001356},
	journaltitle = {Educational Researcher},
	shortjournal = {Educational Researcher},
	author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan A.},
	urldate = {2021-03-16},
	date = {2021-03-10},
	langid = {english},
	note = {Publisher: American Educational Research Association},
	keywords = {open science, questionable research practices, replication, psychology, ethics, globalization, research methodology, survey research},
	file = {Makel et al. - 2021 - Both questionable and open research practices are .pdf:/Users/tom/Zotero/storage/PNPVWY63/Makel et al. - 2021 - Both questionable and open research practices are .pdf:application/pdf},
}

@article{mayo_surprising_2014,
	title = {Some surprising facts about (the problem of) surprising facts: (from the Dusseldorf Conference, February 2011)},
	volume = {45},
	issn = {0039-3681},
	url = {https://www.sciencedirect.com/science/article/pii/S0039368113001015},
	doi = {10.1016/j.shpsa.2013.10.005},
	shorttitle = {Some surprising facts about (the problem of) surprising facts},
	abstract = {A common intuition about evidence is that if data x have been used to construct a hypothesis H, then x should not be used again in support of H. It is no surprise that x fits H, if H was deliberately constructed to accord with x. The question of when and why we should avoid such “double-counting” continues to be debated in philosophy and statistics. It arises as a prohibition against data mining, hunting for significance, tuning on the signal, and ad hoc hypotheses, and as a preference for predesignated hypotheses and “surprising” predictions. I have argued that it is the severity or probativeness of the test—or lack of it—that should determine whether a double-use of data is admissible. I examine a number of surprising ambiguities and unexpected facts that continue to bedevil this debate.},
	pages = {79--86},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Mayo, Deborah G.},
	urldate = {2021-03-16},
	date = {2014-03-01},
	langid = {english},
	file = {Mayo_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Mayo_2014.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/XGPCRV4P/S0039368113001015.html:text/html},
}

@article{moher_assessing_1996,
	title = {Assessing the quality of randomized controlled trials: current issues and future directions},
	volume = {12},
	issn = {1471-6348, 0266-4623},
	url = {https://www.cambridge.org/core/journals/international-journal-of-technology-assessment-in-health-care/article/assessing-the-quality-of-randomized-controlled-trials-current-issues-and-future-directions/C89A8F33C95E179111256AF227C98541},
	doi = {10.1017/S0266462300009570},
	shorttitle = {Assessing the quality of randomized controlled trials},
	abstract = {Assessing the quality of randomized controlled trials is a relatively new and important development. Three approaches have been developed: component, checklist, and scale assessment. Component approaches evaluate selected aspects of trials, such as masking. Checklists and scales involve lists of items thought to be integral to study quality. Scales, unlike the other methods, provide a summary numeric score of quality, which can be formally incorporated into a systematic review. Most scales to date have not been developed with sufficient rigor, however. Empirical evidence indicates that differences in scale development can lead to important differences in quality assessment. Several methods for including quality scores in systematic reviews have been proposed, but since little empirical evidence supports any given method, results must be interpreted cautiously. Future efforts may be best focused on gathering more empirical evidence to identify trial characteristics directly related to bias in the estimates of intervention effects and on improving the way in which trials are reported.},
	pages = {195--208},
	number = {2},
	journaltitle = {International Journal of Technology Assessment in Health Care},
	author = {Moher, David and Jadad, Alejandro R. and Tugwell, Peter},
	urldate = {2020-09-21},
	date = {1996},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Moher et al. - 1996 - Assessing the quality of randomized controlled tri.pdf:/Users/tom/Zotero/storage/E9EKF3UQ/Moher et al. - 1996 - Assessing the quality of randomized controlled tri.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZI625EJY/C89A8F33C95E179111256AF227C98541.html:text/html},
}

@article{engzell_improving_2021,
	title = {Improving social science: lessons from the open science movement},
	volume = {54},
	issn = {1049-0965, 1537-5935},
	url = {https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/improving-social-science-lessons-from-the-open-science-movement/DF2C935045D37C0F8A2E453BC465B071},
	doi = {10.1017/S1049096520000967},
	shorttitle = {Improving social science},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000967/resource/name/{firstPage}-S1049096520000967a.jpg},
	pages = {297--300},
	number = {2},
	journaltitle = {{PS}: Political Science \& Politics},
	author = {Engzell, Per and Rohrer, Julia M.},
	urldate = {2021-03-15},
	date = {2021-04},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Engzell_Rohrer_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Engzell_Rohrer_2021.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/38PSCD48/DF2C935045D37C0F8A2E453BC465B071.html:text/html},
}

@book{laudan_science_1981,
	location = {Dordrecht},
	title = {Science and Hypothesis},
	isbn = {978-94-015-7290-3 978-94-015-7288-0},
	url = {http://link.springer.com/10.1007/978-94-015-7288-0},
	publisher = {Springer Netherlands},
	author = {Laudan, Larry},
	urldate = {2021-03-14},
	date = {1981},
	langid = {english},
	doi = {10.1007/978-94-015-7288-0},
	file = {Laudan - 1981 - Science and Hypothesis.pdf:/Users/tom/Zotero/storage/DGXEHXE9/Laudan - 1981 - Science and Hypothesis.pdf:application/pdf},
}

@article{collins_against_1994,
	title = {Against the epistemic value of prediction over accommodation},
	volume = {28},
	issn = {0029-4624},
	url = {https://www.jstor.org/stable/2216049},
	doi = {10.2307/2216049},
	pages = {210--224},
	number = {2},
	journaltitle = {Noûs},
	author = {Collins, Robin},
	urldate = {2021-03-14},
	date = {1994},
	note = {Publisher: Wiley},
	file = {Collins - 1994 - Against the epistemic value of prediction over acc.pdf:/Users/tom/pCloud Drive/Zotero_Library/Collins - 1994 - Against the epistemic value of prediction over acc.pdf:application/pdf},
}

@article{fidler_epistemic_2018,
	title = {The epistemic importance of establishing the absence of an effect},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918770407},
	doi = {10.1177/2515245918770407},
	pages = {237--244},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Fidler, Fiona and Singleton Thorn, Felix and Barnett, Ashley and Kambouris, Steven and Kruger, Ariel},
	urldate = {2020-10-26},
	date = {2018-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Fidler_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Fidler_etal_2018.pdf:application/pdf},
}

@article{simmons_pre-registration_2021,
	title = {Pre-registration: why and how},
	volume = {31},
	rights = {© 2021 Society for Consumer Psychology},
	issn = {1532-7663},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcpy.1208},
	doi = {10.1002/jcpy.1208},
	shorttitle = {Pre-registration},
	abstract = {In this article, we (1) discuss the reasons why pre-registration is a good idea, both for the field and individual researchers, (2) respond to arguments against pre-registration, (3) describe how to best write and review a pre-registration, and (4) comment on pre-registration’s rapidly accelerating popularity. Along the way, we describe the (big) problem that pre-registration can solve (i.e., false positives caused by p-hacking), while also offering viable solutions to the problems that pre-registration cannot solve (e.g., hidden confounds or fraud). Pre-registration does not guarantee that every published finding will be true, but without it you can safely bet that many more will be false. It is time for our field to embrace pre-registration, while taking steps to ensure that it is done right.},
	pages = {151--162},
	number = {1},
	journaltitle = {Journal of Consumer Psychology},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	urldate = {2021-03-09},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcpy.1208},
	keywords = {Open Science, P-Hacking., Research Integrity, Research Transparency},
	file = {Simmons_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Simmons_etal_2021.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/IW2BY5AT/jcpy.html:text/html},
}

@article{goldin-meadow_why_2016-1,
	title = {Why preregistration makes me nervous},
	volume = {29},
	url = {https://www.psychologicalscience.org/observer/why-preregistration-makes-me-nervous},
	abstract = {I must admit that when I first heard of the effort to get psychological scientists to preregister their studies (that is, to submit to a journal a study’s hypotheses and a plan for how the …},
	number = {7},
	journaltitle = {{APS} Observer},
	author = {Goldin-Meadow, Susan},
	urldate = {2020-10-05},
	date = {2016-08-31},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/GVMKQC3P/why-preregistration-makes-me-nervous.html:text/html},
}

@book{babbage_reflections_1830,
	location = {London, {UK}},
	title = {Reflections on the decline of science in England, and on some of its causes},
	publisher = {Franklin Classics},
	author = {Babbage, Charles},
	date = {1830},
}

@article{paul_making_2021,
	title = {Making {ERP} research more transparent: guidelines for preregistration},
	issn = {01678760},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016787602100074X},
	doi = {10.1016/j.ijpsycho.2021.02.016},
	shorttitle = {Making erp research more transparent},
	journaltitle = {International Journal of Psychophysiology},
	shortjournal = {International Journal of Psychophysiology},
	author = {Paul, Mariella and Govaart, Gisela H. and Schettino, Antonio},
	urldate = {2021-03-09},
	date = {2021-03},
	langid = {english},
	file = {Paul_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Paul_etal_2021.pdf:application/pdf},
}

@article{szollosi_arrested_2021,
	title = {Arrested theory development: the misguided distinction between exploratory and confirmatory research},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620966796},
	doi = {10.1177/1745691620966796},
	shorttitle = {Arrested theory development},
	abstract = {Science progresses by finding and correcting problems in theories. Good theories are those that help facilitate this process by being hard to vary: They explain what they are supposed to explain, they are consistent with other good theories, and they are not easily adaptable to explain anything. Here we argue that, rather than a lack of distinction between exploratory and confirmatory research, an abundance of flexible theories is a better explanation for the current replicability problems of psychology. We also explain why popular methods-oriented solutions fail to address the real problem of flexibility. Instead, we propose that a greater emphasis on theory criticism by argument might improve replicability.},
	pages = {1745691620966796},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Szollosi, Aba and Donkin, Chris},
	urldate = {2021-02-19},
	date = {2021-02-16},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {preregistration, philosophy of science, theory development, confirmatory research, direct replication, exploratory research},
	file = {Szollosi_Donkin_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Szollosi_Donkin_2021.pdf:application/pdf},
}

@article{zahar_why_1973,
	title = {Why did Einstein's programme supersede Lorentz's? (ii)},
	volume = {24},
	issn = {0007-0882},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/24.3.223},
	doi = {10.1093/bjps/24.3.223},
	shorttitle = {Why did einstein's programme supersede lorentz's?},
	pages = {223--262},
	number = {3},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {The British Journal for the Philosophy of Science},
	author = {Zahar, Elie},
	urldate = {2021-03-09},
	date = {1973-09-01},
	note = {Publisher: The University of Chicago Press},
	file = {Snapshot:/Users/tom/Zotero/storage/ZTMTPT73/24.3.html:text/html},
}

@article{haynes_more_nodate,
	title = {More informative abstracts revisited},
	pages = {8},
	author = {Haynes, R Brian and Mulrow, Cynthia D and Huth, Edward J and Altman, Douglas G and Gardner, Martin J},
	langid = {english},
	keywords = {❓ Multiple {DOI}},
	file = {Haynes et al. - More Informative Abstracts Revisited.pdf:/Users/tom/Zotero/storage/MBKGN3W7/Haynes et al. - More Informative Abstracts Revisited.pdf:application/pdf},
}

@article{kidwell_badges_2016,
	title = {Badges to acknowledge open practices: a simple, low-cost, effective method for increasing transparency},
	volume = {14},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456},
	doi = {10.1371/journal.pbio.1002456},
	shorttitle = {Badges to acknowledge open practices},
	abstract = {Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3\% of Psychological Science articles reported open data. After badges, 23\% reported open data, with an accelerating trend; 39\% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.},
	pages = {e1002456},
	number = {5},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Kidwell, Mallory C. and Lazarević, Ljiljana B. and Baranski, Erica and Hardwicke, Tom E. and Piechowski, Sarah and Falkenberg, Lina-Sophia and Kennett, Curtis and Slowik, Agnieszka and Sonnleitner, Carina and Hess-Holden, Chelsey and Errington, Timothy M. and Fiedler, Susann and Nosek, B. A.},
	urldate = {2020-05-11},
	date = {2016-05-12},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Open data, Open science, Scientific publishing, Behavior, Experimental psychology, Cognitive psychology, Research assessment},
	file = {Full Text PDF:/Users/tom/Zotero/storage/TQ88KP85/Kidwell et al. - 2016 - Badges to Acknowledge Open Practices A Simple, Lo.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/S28XCETC/article.html:text/html},
}

@article{nosek_scientific_2012,
	title = {Scientific Utopia: {II}. restructuring incentives and practices to promote truth over publishability},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612459058},
	doi = {10.1177/1745691612459058},
	shorttitle = {Scientific utopia},
	abstract = {An academic scientist’s professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive—getting it right—competitive with the more tangible and concrete incentive—getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.},
	pages = {615--631},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Nosek, B. A. and Spies, Jeffrey R. and Motyl, Matt},
	urldate = {2020-08-25},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/TRY7JJUV/Nosek et al. - 2012 - Scientific Utopia II. Restructuring Incentives an.pdf:application/pdf},
}

@article{nosek_preregistration_2018,
	title = {The preregistration revolution},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708274114},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
	pages = {2600--2606},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Nosek, B. A. and Ebersole, Charles R. and {DeHaven}, Alexander C. and Mellor, David T.},
	urldate = {2020-09-18},
	date = {2018-03-13},
	langid = {english},
	file = {Nosek et al. - 2018 - The preregistration revolution.pdf:/Users/tom/Zotero/storage/6DYWETWE/Nosek et al. - 2018 - The preregistration revolution.pdf:application/pdf},
}

@article{nosek_reply_2018,
	title = {Reply to Ledgerwood: Predictions without analysis plans are inert},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1816418115},
	doi = {10.1073/pnas.1816418115},
	shorttitle = {Reply to Ledgerwood},
	pages = {E10518--E10518},
	number = {45},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Nosek, B. A. and Ebersole, Charles R. and {DeHaven}, Alexander C. and Mellor, David T.},
	urldate = {2020-09-18},
	date = {2018-11-06},
	langid = {english},
	file = {Nosek et al. - 2018 - Reply to Ledgerwood Predictions without analysis .pdf:/Users/tom/Zotero/storage/INTVK8AC/Nosek et al. - 2018 - Reply to Ledgerwood Predictions without analysis .pdf:application/pdf},
}

@article{nosek_preregistration_2019,
	title = {Preregistration is hard, and worthwhile},
	volume = {23},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661319301846},
	doi = {10.1016/j.tics.2019.07.009},
	abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
	pages = {815--818},
	number = {10},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Nosek, B. A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and van ’t Veer, Anna E. and Vazire, Simine},
	urldate = {2020-09-18},
	date = {2019-10-01},
	langid = {english},
	keywords = {preregistration, transparency, reproducibility, confirmatory research, exploratory research},
	file = {Nosek et al. - 2019 - Preregistration is hard, and worthwhile.pdf:/Users/tom/pCloud Drive/Zotero_Library/Nosek et al. - 2019 - Preregistration is hard, and worthwhile.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/CLY64NHL/S1364661319301846.html:text/html},
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	rights = {2017 Macmillan Publishers Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	pages = {1--9},
	number = {1},
	journaltitle = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, B. A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	urldate = {2020-09-18},
	date = {2017-01-10},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	file = {Munafò et al. - 2017 - A manifesto for reproducible science.pdf:/Users/tom/Zotero/storage/8X52BFYH/Munafò et al. - 2017 - A manifesto for reproducible science.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/D3QPZL88/s41562-016-0021.html:text/html},
}

@article{ioannidis_publication_2014,
	title = {Publication and other reporting biases in cognitive sciences: detection, prevalence, and prevention},
	volume = {18},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661314000540},
	doi = {10.1016/j.tics.2014.02.010},
	shorttitle = {Publication and other reporting biases in cognitive sciences},
	abstract = {Recent systematic reviews and empirical evaluations of the cognitive sciences literature suggest that publication and other reporting biases are prevalent across diverse domains of cognitive science. In this review, we summarize the various forms of publication and reporting biases and other questionable research practices, and overview the available methods for probing into their existence. We discuss the available empirical evidence for the presence of such biases across the neuroimaging, animal, other preclinical, psychological, clinical trials, and genetics literature in the cognitive sciences. We also highlight emerging solutions (from study design to data analyses and reporting) to prevent bias and improve the fidelity in the field of cognitive science research.},
	pages = {235--241},
	number = {5},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Ioannidis, John P. A. and Munafò, Marcus R. and Fusar-Poli, Paolo and Nosek, B. A. and David, Sean P.},
	urldate = {2020-09-18},
	date = {2014-05-01},
	langid = {english},
	keywords = {publication bias, bias, reporting bias, cognitive sciences, neuroscience},
	file = {Ioannidis_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Ioannidis_etal_2014.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/DL65C8X4/S1364661314000540.html:text/html},
}

@article{silberzahn_many_2018,
	title = {Many analysts, one data set: making transparent how variations in analytic choices affect results:},
	rights = {© The Author(s) 2018},
	url = {https://journals.sagepub.com/doi/10.1177/2515245917747646},
	doi = {10.1177/2515245917747646},
	shorttitle = {Many analysts, one data set},
	abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards ...},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahník, Š and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Rosa, A. Dalla and Dam, L. and Evans, M. H. and Cervantes, I. Flores and Fong, N. and Gamez-Djokic, M. and Glenz, A. and Gordon-{McKeon}, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Mohr, A. J. Hofelich and Högden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schlüter, E. and Schönbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Spörlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
	urldate = {2020-09-22},
	date = {2018-08-23},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Silberzahn_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Silberzahn_etal_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KSPWIZKN/2515245917747646.html:text/html},
}

@article{klein_investigating_2014,
	title = {Investigating variation in replicability},
	volume = {45},
	issn = {1864-9335},
	url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178},
	doi = {10.1027/1864-9335/a000178},
	abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or {US} versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
	pages = {142--152},
	number = {3},
	journaltitle = {Social Psychology},
	author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and {IJzerman}, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and van ‘t Veer, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, B. A.},
	urldate = {2020-12-01},
	date = {2014-01-01},
	note = {Publisher: Hogrefe Publishing},
	file = {Klein_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Klein_etal_2014.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VT962RMN/a000178.html:text/html;Snapshot:/Users/tom/Zotero/storage/3UHKJYHY/a000178.html:text/html},
}

@article{nosek_what_2020,
	title = {What is replication?},
	volume = {18},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000691},
	doi = {10.1371/journal.pbio.3000691},
	abstract = {Credibility of scientific claims is established with evidence for their replicability using new data. According to common understanding, replication is repeating a study’s procedure and observing whether the prior finding recurs. This definition is intuitive, easy to apply, and incorrect. We propose that replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research. This definition reduces emphasis on operational characteristics of the study and increases emphasis on the interpretation of possible outcomes. The purpose of replication is to advance theory by confronting existing understanding with new evidence. Ironically, the value of replication may be strongest when existing understanding is weakest. Successful replication provides evidence of generalizability across the conditions that inevitably differ from the original study; Unsuccessful replication indicates that the reliability of the finding may be more constrained than recognized previously. Defining replication as a confrontation of current theoretical expectations clarifies its important, exciting, and generative role in scientific progress.},
	pages = {e3000691},
	number = {3},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Nosek, B. A. and Errington, Timothy M.},
	urldate = {2021-01-22},
	date = {2020-03-27},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open data, Social sciences, Reproducibility, Acetylcholine, Elections, Frogs, Obesity, Philippines},
	file = {Nosek and Errington - 2020 - What is replication.pdf:/Users/tom/Zotero/storage/2UVNLZAE/Nosek and Errington - 2020 - What is replication.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TKV6R9SR/article.html:text/html},
}

@article{bakker_ensuring_2020,
	title = {Ensuring the quality and specificity of preregistrations},
	volume = {18},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000937},
	doi = {10.1371/journal.pbio.3000937},
	abstract = {Researchers face many, often seemingly arbitrary, choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of “researcher degrees of freedom” aimed at obtaining statistical significance increases the likelihood of obtaining and publishing false-positive results and overestimated effect sizes. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes. The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans. In this preregistered study, we compared 2 formats of preregistration available on the {OSF}: Standard Pre-Data Collection Registration and Prereg Challenge Registration (now called “{OSF} Preregistration,” http://osf.io/prereg/). The Prereg Challenge format was a “structured” workflow with detailed instructions and an independent review to confirm completeness; the “Standard” format was “unstructured” with minimal direct guidance to give researchers flexibility for what to prespecify. Results of comparing random samples of 53 preregistrations from each format indicate that the “structured” format restricted the opportunistic use of researcher degrees of freedom better (Cliff’s Delta = 0.49) than the “unstructured” format, but neither eliminated all researcher degrees of freedom. We also observed very low concordance among coders about the number of hypotheses (14\%), indicating that they are often not clearly stated. We conclude that effective preregistration is challenging, and registration formats that provide effective guidance may improve the quality of research.},
	pages = {e3000937},
	number = {12},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Bakker, Marjan and Veldkamp, Coosje L. S. and Assen, Marcel A. L. M. van and Crompvoets, Elise A. V. and Ong, How Hwee and Nosek, B. A. and Soderberg, Courtney K. and Mellor, David and Wicherts, Jelte M.},
	urldate = {2021-02-17},
	date = {2020-12-09},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Research design, Social psychology, Reproducibility, Clinical trials, Peer review, Coding mechanisms, Preprocessing, Statistical models},
	file = {Bakker_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bakker_etal_3.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/F7L5L3EZ/article.html:text/html},
}

@report{bowman_osf_2020,
	title = {{OSF} Prereg Template},
	url = {https://osf.io/epgjd},
	abstract = {Preregistration is the act of submitting a study plan, ideally also with analytical plan, to a registry prior to conducting the work. Preregistration increases the discoverability of research even if it does not get published further. Adding specific analysis plans can clarify the distinction between planned, confirmatory tests and unplanned, exploratory research. This preprint contains a template for the “{OSF} Prereg” form available from the {OSF} Registry. An earlier version was originally developed for the Preregistration Challenge, an education campaign designed to initiate preregistration as a habit prior to data collection in basic research, funded by the Laura and John Arnold Foundation (now Arnold Ventures) and conducted by the Center for Open Science. More information is available at https://cos.io/prereg, and other templates are available at: https://osf.io/zab38/},
	institution = {{MetaArXiv}},
	type = {preprint},
	author = {Bowman, Sara D. and {DeHaven}, Alexander Carl and Errington, Timothy M. and Hardwicke, Tom Elis and Mellor, David Thomas and Nosek, B. A. and Soderberg, Courtney K.},
	urldate = {2021-02-25},
	date = {2020-01-22},
	doi = {10.31222/osf.io/epgjd},
}

@article{miguel_promoting_2014,
	title = {Promoting transparency in social science research},
	volume = {343},
	rights = {Copyright © 2014, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/343/6166/30},
	doi = {10.1126/science.1245317},
	abstract = {Social scientists should adopt higher transparency standards to improve the quality and credibility of research.
Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
	pages = {30--31},
	number = {6166},
	journaltitle = {Science},
	author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and Laan, M. Van der},
	urldate = {2021-03-08},
	date = {2014-01-03},
	langid = {english},
	pmid = {24385620},
	note = {Publisher: American Association for the Advancement of Science
Section: Policy Forum},
	file = {Miguel_etal_2014.pdf:/Users/tom/pCloud Drive/Zotero_Library/Miguel_etal_2014.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9RX2PWJE/30.html:text/html},
}

@article{nosek_promoting_2015,
	title = {Promoting an open research culture},
	volume = {348},
	rights = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/348/6242/1422},
	doi = {10.1126/science.aab2374},
	abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
	pages = {1422--1425},
	number = {6242},
	journaltitle = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, Christopher D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and {McNutt}, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and {VandenBos}, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	urldate = {2020-08-25},
	date = {2015-06-26},
	langid = {english},
	pmid = {26113702},
	note = {Publisher: American Association for the Advancement of Science
Section: Policy Forum},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ZEX2X9ES/Nosek et al. - 2015 - Promoting an open research culture.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Y8VSJ3QJ/1422.html:text/html},
}

@article{pashler_taking_2017,
	title = {Taking responsibility for our field’s reputation},
	volume = {30},
	url = {https://www.psychologicalscience.org/observer/taking-responsibility-for-our-fields-reputation},
	abstract = {To put it bluntly, academic psychology’s public reputation seems to be in free fall.

When the press coverage of the “replicability crisis” in psychological science first began a few years ago, reporters generally broached the topic …},
	number = {7},
	journaltitle = {{APS} Observer},
	author = {Pashler, Harold and Ruiter, J. P. de},
	urldate = {2021-03-07},
	date = {2017-08-31},
	langid = {american},
	keywords = {⛔ No {DOI} found},
	file = {Snapshot:/Users/tom/Zotero/storage/J9YSBD3E/taking-responsibility-for-our-fields-reputation.html:text/html},
}

@article{hardwicke_calibrating_2020,
	title = {Calibrating the scientific ecosystem through meta-research},
	volume = {7},
	issn = {2326-8298},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-031219-041104},
	doi = {10.1146/annurev-statistics-031219-041104},
	abstract = {While some scientists study insects, molecules, brains, or clouds, other scientists study science itself. Meta-research, or research-on-research, is a burgeoning discipline that investigates efficiency, quality, and bias in the scientific ecosystem, topics that have become especially relevant amid widespread concerns about the credibility of the scientific literature. Meta-research may help calibrate the scientific ecosystem toward higher standards by providing empirical evidence that informs the iterative generation and refinement of reform initiatives. We introduce a translational framework that involves (a) identifying problems, (b) investigating problems, (c) developing solutions, and (d) evaluating solutions. In each of these areas, we review key meta-research endeavors and discuss several examples of prior and ongoing work. The scientific ecosystem is perpetually evolving; the discipline of meta-research presents an opportunity to use empirical evidence to guide its development and maximize its potential.},
	pages = {11--37},
	number = {1},
	journaltitle = {Annual Review of Statistics and Its Application},
	shortjournal = {Annu. Rev. Stat. Appl.},
	author = {Hardwicke, Tom E. and Serghiou, Stylianos and Janiaud, Perrine and Danchev, Valentin and Crüwell, Sophia and Goodman, Steven N. and Ioannidis, John P.A.},
	urldate = {2020-05-15},
	date = {2020-03-09},
	note = {Publisher: Annual Reviews},
	file = {Hardwicke_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Hardwicke_etal_22.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/D46KIKI2/annurev-statistics-031219-041104.html:text/html},
}

@article{valkenburg_expanding_2021,
	title = {Expanding research integrity: a cultural-practice perspective},
	volume = {27},
	issn = {1471-5546},
	url = {https://doi.org/10.1007/s11948-021-00291-z},
	doi = {10.1007/s11948-021-00291-z},
	shorttitle = {Expanding research integrity},
	abstract = {Research integrity ({RI}) is usually discussed in terms of responsibilities that individual researchers bear towards the scientific work they conduct, as well as responsibilities that institutions have to enable those individual researchers to do so. In addition to these two bearers of responsibility, a third category often surfaces, which is variably referred to as culture and practice. These notions merit further development beyond a residual category that is to contain everything that is not covered by attributions to individuals and institutions. This paper discusses how thinking in {RI} can take benefit from more specific ideas on practice and culture. We start by articulating elements of practice and culture, and explore how values central to {RI} are related to these elements. These insights help identify additional points of intervention for fostering responsible conduct. This helps to build “cultures and practices of research integrity”, as it makes clear that specific times and places are connected to specific practices and cultures and should have a place in the debate on Research Integrity. With this conceptual framework, practitioners as well as theorists can avoid using the notions as residual categories that de facto amount to vague, additional burdens of responsibility for the individual.},
	pages = {10},
	number = {1},
	journaltitle = {Science and Engineering Ethics},
	shortjournal = {Sci Eng Ethics},
	author = {Valkenburg, Govert and Dix, Guus and Tijdink, Joeri and de Rijcke, Sarah},
	urldate = {2021-03-07},
	date = {2021-02-09},
	langid = {english},
	file = {Valkenburg_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Valkenburg_etal_2021.pdf:application/pdf},
}

@article{buchanan_getting_2021,
	title = {Getting started creating data dictionaries: how to create a shareable data set},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245920928007},
	doi = {10.1177/2515245920928007},
	shorttitle = {Getting started creating data dictionaries},
	abstract = {As researchers embrace open and transparent data sharing, they will need to provide information about their data that effectively helps others understand their data sets’ contents. Without proper documentation, data stored in online repositories such as {OSF} will often be rendered unfindable and unreadable by other researchers and indexing search engines. Data dictionaries and codebooks provide a wealth of information about variables, data collection, and other important facets of a data set. This information, called metadata, provides key insights into how the data might be further used in research and facilitates search-engine indexing to reach a broader audience of interested parties. This Tutorial first explains terminology and standards relevant to data dictionaries and codebooks. Accompanying information on {OSF} presents a guided workflow of the entire process from source data (e.g., survey answers on Qualtrics) to an openly shared data set accompanied by a data dictionary or codebook that follows an agreed-upon standard. Finally, we discuss freely available Web applications to assist this process of ensuring that psychology data are findable, accessible, interoperable, and reusable.},
	pages = {2515245920928007},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Buchanan, Erin M. and Crain, Sarah E. and Cunningham, Ari L. and Johnson, Hannah R. and Stash, Hannah and Papadatou-Pastou, Marietta and Isager, Peder M. and Carlsson, Rickard and Aczel, Balazs},
	urldate = {2021-03-07},
	date = {2021-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {reproducibility, codebook, data dictionary, metadata, open materials},
	file = {Buchanan_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Buchanan_etal_2021.pdf:application/pdf},
}

@article{rubin_costs_2020,
	title = {The costs of {HARKing}},
	rights = {© 2020 The Author. All rights reserved.},
	issn = {0007-0882},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1093/bjps/axz050},
	doi = {10.1093/bjps/axz050},
	abstract = {{AbstractKerr} ([1998]) coined the term ‘{HARKing}’ to refer to the practice of ‘hypothesizing after the results are known’. This questionable research practice has received increased attention in rece...},
	journaltitle = {The British Journal for the Philosophy of Science},
	author = {Rubin, Mark},
	urldate = {2021-03-07},
	date = {2020-12-18},
	langid = {english},
	note = {Publisher: The University of Chicago Press},
	file = {Rubin - 2020 - The Costs of HARKing.pdf:/Users/tom/pCloud Drive/Zotero_Library/Rubin - 2020 - The Costs of HARKing.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/NQTET477/axz050.html:text/html},
}

@article{poldrack_scanning_2017,
	title = {Scanning the horizon: towards transparent and reproducible neuroimaging research},
	volume = {18},
	rights = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn.2016.167},
	doi = {10.1038/nrn.2016.167},
	shorttitle = {Scanning the horizon},
	abstract = {There is growing concern about the reproducibility of scientific research, and neuroimaging research suffers from many features that are thought to lead to high levels of false results.Statistical power of neuroimaging studies has increased over time but remains relatively low, especially for group comparison studies. An analysis of effect sizes in the Human Connectome Project demonstrates that most functional {MRI} studies are not sufficiently powered to find reasonable effect sizes.Neuroimaging analysis has a high degree of flexibility in analysis methods, which can lead to inflated false-positive rates unless controlled for. Pre-registration of analysis plans and clear delineation of hypothesis-driven and exploratory research are potential solutions to this problem.The use of appropriate corrections for multiple tests has increased, but some common methods can have highly inflated false-positive rates. The use of non-parametric methods is encouraged to provide accurate correction for multiple tests.Software errors have the potential to lead to incorrect or irreproducible results. The adoption of improved software engineering methods and software testing strategies can help to reduce such problems.Reproducibility will be improved through greater transparency in methods reporting and through increased sharing of data and code.},
	pages = {115--126},
	number = {2},
	journaltitle = {Nature Reviews Neuroscience},
	author = {Poldrack, Russell A. and Baker, Chris I. and Durnez, Joke and Gorgolewski, Krzysztof J. and Matthews, Paul M. and Munafò, Marcus R. and Nichols, Thomas E. and Poline, Jean-Baptiste and Vul, Edward and Yarkoni, Tal},
	urldate = {2021-03-08},
	date = {2017-02},
	langid = {english},
	note = {Number: 2
Publisher: Nature Publishing Group},
	file = {Poldrack_etal_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Poldrack_etal_2017.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AXRJAPYN/nrn.2016.html:text/html},
}

@report{abrams_research_2021,
	title = {Research registries: taking stock and looking forward},
	url = {https://uploads.strikinglycdn.com/files/840dd740-4a8d-4f09-8dbd-e6498f5661c2/January2021Version.pdf},
	abstract = {We analyze one prominent policy solution to the credibility crisis in experimental research—research registries—with a focus on the {AEA} registry. We ﬁnd that a majority of economics ﬁeld experiments do not register, only half of registrations are prior to intervention, and most of these preregistrations lack sufﬁcient detail. We compare these facts to {ClinicalTrials}.gov and ﬁnd broad similarities. As such, we conclude by advancing an economic model to explore potential improvements to registries generally. The model shows that banning late registration and linking Institutional Review Board applications to registrations can signiﬁcantly increase registry effectiveness.},
	pages = {1--66},
	author = {Abrams, Eliot and Libgober, Jonathan and List, John A},
	date = {2021},
	langid = {english},
	file = {Abrams et al. - Research Registries Taking Stock and Looking Forw.pdf:/Users/tom/Zotero/storage/RI7K7L64/Abrams et al. - Research Registries Taking Stock and Looking Forw.pdf:application/pdf},
}

@article{kuorikoski_economic_2010,
	title = {Economic modelling as robustness analysis},
	volume = {61},
	issn = {0007-0882, 1464-3537},
	url = {https://academic.oup.com/bjps/article-lookup/doi/10.1093/bjps/axp049},
	doi = {10.1093/bjps/axp049},
	abstract = {We claim that the process of theoretical model reﬁnement in economics is best characterised as robustness analysis: the systematic examination of the robustness of modelling results with respect to particular modelling assumptions. We argue that this practise has epistemic value by extending William Wimsatt’s account of robustness analysis as triangulation via independent means of determination. For economists robustness analysis is a crucial methodological strategy because their models are often based on idealisations and abstractions, and it is usually difﬁcult to tell which idealisations are truly harmful.},
	pages = {541--567},
	number = {3},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {The British Journal for the Philosophy of Science},
	author = {Kuorikoski, J. and Lehtinen, A. and Marchionni, C.},
	urldate = {2020-10-16},
	date = {2010-09-01},
	langid = {english},
	file = {Kuorikoski et al. - 2010 - Economic Modelling as Robustness Analysis.pdf:/Users/tom/Zotero/storage/LDCWMHMY/Kuorikoski et al. - 2010 - Economic Modelling as Robustness Analysis.pdf:application/pdf},
}

@article{christensen_transparency_2018,
	title = {Transparency, reproducibility, and the credibility of economics research},
	volume = {56},
	issn = {0022-0515},
	url = {https://pubs.aeaweb.org/doi/10.1257/jel.20171350},
	doi = {10.1257/jel.20171350},
	abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future. ( {JEL} A11, C18, I23)},
	pages = {920--980},
	number = {3},
	journaltitle = {Journal of Economic Literature},
	shortjournal = {Journal of Economic Literature},
	author = {Christensen, Garret and Miguel, Edward},
	urldate = {2020-10-07},
	date = {2018-09-01},
	langid = {english},
	file = {Christensen and Miguel - 2018 - Transparency, Reproducibility, and the Credibility.pdf:/Users/tom/Zotero/storage/GE8MA62R/Christensen and Miguel - 2018 - Transparency, Reproducibility, and the Credibility.pdf:application/pdf},
}

@report{chang_is_2015,
	location = {Washington},
	title = {Is economics research replicable? Sixty published papers from thirteen journals say "usually not"},
	url = {http://www.federalreserve.gov/econresdata/feds/2015/files/2015083pap.pdf},
	shorttitle = {Is economics research replicable?},
	abstract = {We attempt to replicate 67 papers published in 13 well-regarded economics journals using author-provided replication ﬁles that include both data and code. Some journals in our sample require data and code replication ﬁles, and other journals do not require such ﬁles. Aside from 6 papers that use conﬁdential data, we obtain data and code replication ﬁles for 29 of 35 papers (83\%) that are required to provide such ﬁles as a condition of publication, compared to 11 of 26 papers (42\%) that are not required to provide data and code replication ﬁles. We successfully replicate the key qualitative result of 22 of 67 papers (33\%) without contacting the authors. Excluding the 6 papers that use conﬁdential data and the 2 papers that use software we do not possess, we replicate 29 of 59 papers (49\%) with assistance from the authors. Because we are able to replicate less than half of the papers in our sample even with help from the authors, we assert that economics research is usually not replicable. We conclude with recommendations on improving replication of economics research.},
	pages = {1--26},
	institution = {Board of Governors of the Federal Reserve System},
	author = {Chang, A. C. and Li, Phillip},
	urldate = {2020-05-11},
	date = {2015-10},
	langid = {english},
	file = {Board of Governors of the Federal Reserve System et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf:/Users/tom/Zotero/storage/WZ66RK48/Board of Governors of the Federal Reserve System et al. - 2015 - Is Economics Research Replicable Sixty Published .pdf:application/pdf},
}

@article{olken_promises_2015,
	title = {Promises and perils of pre-analysis plans},
	volume = {29},
	issn = {0895-3309},
	url = {https://pubs.aeaweb.org/doi/10.1257/jep.29.3.61},
	doi = {10.1257/jep.29.3.61},
	abstract = {The purpose of this paper is to help think through the advantages and costs of rigorous pre-specification of statistical analysis plans in economics. A pre-analysis plan pre-specifies in a precise way the analysis to be run before examining the data. A researcher can specify variables, data cleaning procedures, regression specifications, and so on. If the regressions are pre-specified in advance and researchers are required to report all the results they pre-specify, data-mining problems are greatly reduced. I begin by laying out the basics of what a statistical analysis plan actually contains so those researchers unfamiliar with it can better understand how it is done. In so doing, I have drawn both on standards used in clinical trials, which are clearly specified by the Food and Drug Administration, as well as my own practical experience from writing these plans in economics contexts. I then lay out some of the advantages of pre-specified analysis plans, both for the scientific community as a whole and also for the researcher. I also explore some of the limitations and costs of such plans. I then review a few pieces of evidence that suggest that, in many contexts, the benefits of using pre-specified analysis plans may not be as high as one might have expected initially. For the most part, I will focus on the relatively narrow issue of pre-analysis for randomized controlled trials.},
	pages = {61--80},
	number = {3},
	journaltitle = {Journal of Economic Perspectives},
	shortjournal = {Journal of Economic Perspectives},
	author = {Olken, Benjamin A.},
	urldate = {2020-09-22},
	date = {2015-08-01},
	langid = {english},
	file = {Olken - 2015 - Promises and perils of pre-analysis plans.pdf:/Users/tom/Zotero/storage/GRLWBXYI/Olken - 2015 - Promises and perils of pre-analysis plans.pdf:application/pdf},
}

@article{denton_data_1985,
	title = {Data mining as an industry},
	volume = {67},
	issn = {0034-6535},
	url = {https://www.jstor.org/stable/1928442},
	doi = {10.2307/1928442},
	abstract = {"Data mining" by an individual investigator can distort the probabilities in conventional significance tests. This paper argues that the same effect can occur when a given data set is used by more than one investigator, even if no individual investigator engages in data mining. A problem of publication selection bias is recalled and note is taken of its implications for the interpretation of published test results when there is collective data mining. Some illustrative calculations of probabilities associated with collective data mining are provided.},
	pages = {124--127},
	number = {1},
	journaltitle = {The Review of Economics and Statistics},
	author = {Denton, Frank T.},
	urldate = {2021-03-04},
	date = {1985},
	note = {Publisher: The {MIT} Press},
	file = {Denton_1985.pdf:/Users/tom/pCloud Drive/Zotero_Library/Denton_1985.pdf:application/pdf},
}

@report{christensen_open_2019,
	title = {Open science practices are on the rise: the state of social science (3s) survey},
	url = {https://osf.io/preprints/metaarxiv/5rksu/},
	shorttitle = {Open science practices are on the rise},
	abstract = {Has there been meaningful movement toward open science practices within the social sciences in recent years? Discussions about changes in practices such as posting data and pre-registering analyses have been marked by controversy—including controversy over the extent to which change has taken place. This study, based on the State of Social Science (3S) Survey, provides the first comprehensive assessment of awareness of, attitudes towards, perceived norms regarding, and adoption of open science practices within a broadly representative sample of scholars from four major social science disciplines: economics, political science, psychology, and sociology. We observe a steep increase in adoption: as of 2017, over 80\% of scholars had used at least one such practice, rising from one quarter a decade earlier. Attitudes toward research transparency are on average similar between older and younger scholars, but the pace
of change differs by field and methodology. According with theories of normal science and scientific change, the timing of increases in adoption coincides with technological innovations and institutional policies. Patterns are consistent with most scholars underestimating the trend toward open science in their discipline.},
	institution = {{MetaArXiv}},
	author = {Christensen, Garret and Wang, Zenan and Paluck, Elizabeth Levy and Swanson, Nicholas and Birke, David J. and Miguel, Edward and Littman, Rebecca},
	urldate = {2021-03-05},
	date = {2019-10-18},
	doi = {10.31222/osf.io/5rksu},
	note = {type: article},
	keywords = {open science, Social and Behavioral Sciences, transparency, reproducibility, open data, psychology, pre-registration, economics, open code, political science, pre-analysis plan, research practice, social science, sociology},
	file = {Christensen_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Christensen_etal_2019.pdf:application/pdf},
}

@article{pashler_editors_2012,
	title = {Editors’ introduction to the special section on replicability in psychological science: a crisis of confidence?},
	volume = {7},
	rights = {© The Author(s) 2012},
	url = {http://journals.sagepub.com/doi/10.1177/1745691612465253},
	doi = {10.1177/1745691612465253},
	shorttitle = {Editors’ introduction to the special section on replicability in psychological science},
	pages = {528--530},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	author = {Pashler, Harold and Wagenmakers, Eric-Jan},
	urldate = {2020-08-25},
	date = {2012-11-07},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Pashler and Wagenmakers - 2012 - Editors’ introduction to the special section on re.pdf:/Users/tom/pCloud Drive/Zotero_Library/Pashler and Wagenmakers - 2012 - Editors’ introduction to the special section on re.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3JZAC8RM/1745691612465253.html:text/html},
}

@book{bacon_novum_1620,
	title = {Novum organum},
	author = {Bacon, Francis},
	date = {1620},
}

@article{frankenhuis_open_2018,
	title = {Open science is liberating and can foster creativity:},
	rights = {© The Author(s) 2018},
	url = {https://journals.sagepub.com/doi/10.1177/1745691618767878},
	doi = {10.1177/1745691618767878},
	shorttitle = {Open science is liberating and can foster creativity},
	abstract = {Some scholars think that Open Science practices constrain researchers in ways that reduce their creativity, arguing, for instance, that preregistration discoura...},
	journaltitle = {Perspectives on Psychological Science},
	author = {Frankenhuis, Willem E. and Nettle, Daniel},
	urldate = {2020-09-21},
	date = {2018-07-02},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Frankenhuis_Nettle_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Frankenhuis_Nettle_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RK4S5XVX/1745691618767878.html:text/html},
}

@article{brainerd_replication_2018,
	title = {Replication, registration, and scientific creativity},
	rights = {© The Author(s) 2018},
	url = {https://journals.sagepub.com/doi/10.1177/1745691617739421},
	doi = {10.1177/1745691617739421},
	shorttitle = {Replication, registration, and scientific creativity},
	abstract = {The bureaucratization of psychological science exacts intellectual costs that go beyond the sheer amount of time that is drained away from creative scientific a...},
	journaltitle = {Perspectives on Psychological Science},
	author = {Brainerd, C. J. and Reyna, Valerie F.},
	urldate = {2020-09-21},
	date = {2018-07-02},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Brainerd_Reyna_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Brainerd_Reyna_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VT2XGSKH/1745691617739421.html:text/html},
}

@article{kaufman_road_2018,
	title = {The road to uncreative science is paved with good intentions: ideas, implementations, and uneasy balances:},
	rights = {© The Author(s) 2018},
	url = {https://journals.sagepub.com/doi/10.1177/1745691617753947},
	doi = {10.1177/1745691617753947},
	shorttitle = {The road to uncreative science is paved with good intentions},
	abstract = {How does the current replication crisis, along with other recent psychological trends, affect scientific creativity? To answer this question, we consider curren...},
	journaltitle = {Perspectives on Psychological Science},
	author = {Kaufman, James C. and Glǎveanu, Vlad P.},
	urldate = {2020-09-21},
	date = {2018-07-02},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Kaufman_Glǎveanu_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Kaufman_Glǎveanu_2018.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KXCKQMTM/1745691617753947.html:text/html},
}

@article{wagenmakers_creativity-verification_2018,
	title = {The creativity-verification cycle in psychological science: new methods to combat old idols:},
	rights = {© The Author(s) 2018},
	url = {https://journals.sagepub.com/doi/10.1177/1745691618771357},
	doi = {10.1177/1745691618771357},
	shorttitle = {The creativity-verification cycle in psychological science},
	abstract = {Over the years, researchers in psychological science have documented and investigated a host of powerful cognitive fallacies, including hindsight bias and confi...},
	journaltitle = {Perspectives on Psychological Science},
	author = {Wagenmakers, Eric-Jan and Dutilh, Gilles and Sarafoglou, Alexandra},
	urldate = {2020-09-21},
	date = {2018-07-02},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/TUZW8R3R/1745691618771357.html:text/html;Wagenmakers_etal_2018.pdf:/Users/tom/pCloud Drive/Zotero_Library/Wagenmakers_etal_2018.pdf:application/pdf},
}

@article{smaldino_natural_2016,
	title = {The natural selection of bad science},
	volume = {3},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.160384},
	doi = {10.1098/rsos.160384},
	abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
	pages = {160384},
	number = {9},
	journaltitle = {Royal Society Open Science},
	shortjournal = {Royal Society Open Science},
	author = {Smaldino, Paul E. and {McElreath}, Richard},
	urldate = {2021-03-08},
	date = {2016},
	note = {Publisher: Royal Society},
	file = {Smaldino_McElreath_.pdf:/Users/tom/pCloud Drive/Zotero_Library/Smaldino_McElreath_.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3H6SNFUF/rsos.html:text/html},
}

@article{edlund_saving_2021,
	title = {Saving science through replication studies},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620984385},
	doi = {10.1177/1745691620984385},
	abstract = {The scientific enterprise has long been based on the presumption of replication, although scientists have recently become aware of various corruptions of the enterprise that have hurt replicability. In this article, we begin by considering three illustrations of research paradigms that have all been subject to intense scrutiny through replications and theoretical concerns. The three paradigms are one for which the corpus of research points to a real finding, one for which the corpus of research points to a significantly attenuated effect, and one for which the debate is ongoing. We then discuss what scientists can learn—and how science can be improved—through replications more generally. From there, we discuss what we believe needs to be done to improve scientific inquiry with regard to replication moving forward. Finally, we conclude by providing readers with several different approaches to replication and how these approaches progress science. The approaches discussed include multilab replications of many effects, multilab replications of specific effects, adversarial collaborations, and stand-alone applications.},
	pages = {1745691620984385},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Edlund, John E. and Cuccolo, Kelly and Irgens, Megan S. and Wagge, Jordan R. and Zlokovich, Martha S.},
	urldate = {2021-03-08},
	date = {2021-03-08},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication, metascience, science, collaboration},
}

@unpublished{feynman_cargo_1974,
	title = {Cargo Cult Science},
	url = {http://calteches.library.caltech.edu/51/2/CargoCult.pdf},
	author = {Feynman, Richard P.},
	urldate = {2021-03-08},
	date = {1974},
	file = {CargoCult.pdf:/Users/tom/Zotero/storage/BKX5L8HP/CargoCult.pdf:application/pdf},
}

@article{bolles_difference_1962,
	title = {The difference between statistical hypotheses and scientific hypotheses},
	volume = {11},
	issn = {0033-2941},
	url = {https://doi.org/10.2466/pr0.1962.11.3.639},
	doi = {10.2466/pr0.1962.11.3.639},
	pages = {639--645},
	number = {3},
	journaltitle = {Psychological Reports},
	shortjournal = {Psychol Rep},
	author = {Bolles, Robert C.},
	urldate = {2021-03-07},
	date = {1962-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@incollection{newell_you_1973,
	title = {You can't play 20 questions with nature and win: projective comments on the papers of this symposium},
	isbn = {978-0-12-170150-5},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780121701505500123},
	shorttitle = {You can't play 20 questions with nature and win},
	pages = {283--308},
	booktitle = {Visual Information Processing},
	publisher = {Elsevier},
	author = {Newell, Allen},
	urldate = {2021-03-07},
	date = {1973},
	langid = {english},
	doi = {10.1016/B978-0-12-170150-5.50012-3},
	file = {Newell - 1973 - YOU CAN'T PLAY 20 QUESTIONS WITH NATURE AND WIN P.pdf:/Users/tom/Zotero/storage/CM9RZJ62/Newell - 1973 - YOU CAN'T PLAY 20 QUESTIONS WITH NATURE AND WIN P.pdf:application/pdf},
}

@article{shadish_campbell_2010,
	title = {Campbell and Rubin: A primer and comparison of their approaches to causal inference in field settings},
	volume = {15},
	issn = {1939-1463},
	doi = {10.1037/a0015916},
	shorttitle = {Campbell and Rubin},
	abstract = {This article compares Donald Campbell's and Donald Rubin's work on causal inference in field settings on issues of epistemology, theories of cause and effect, methodology, statistics, generalization, and terminology. The two approaches are quite different but compatible, differing mostly in matters of bandwidth versus fidelity. Campbell's work demonstrates broad narrative scope that covers a wide array of concepts related to causation, with a powerful appreciation for human fallibility in making causal judgments, with a more elaborate theory of cause and generalization, and with a preference for design over analysis. Rubin's approach is a more narrow and formal quantitative analysis of effect estimation, sharing a preference for design but best known for analysis, with compelling quantitative approaches to obtaining unbiased quantitative effect estimates from nonrandomized designs and with comparatively little to say about generalization. Much could be gained by joining the emphasis on design in Campbell with the emphasis on analysis in Rubin. However, the 2 approaches also speak modestly different languages that leave some questions about their total commensurability that only continued dialogue can fully clarify.},
	pages = {3--17},
	number = {1},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychol Methods},
	author = {Shadish, William R.},
	date = {2010-03},
	pmid = {20230099},
	keywords = {Psychology, Humans, Causality, Psychological Theory},
	file = {Shadish_2010.pdf:/Users/tom/pCloud Drive/Zotero_Library/Shadish_2010.pdf:application/pdf},
}

@article{inthout_obtaining_2016,
	title = {Obtaining evidence by a single well-powered trial or several modestly powered trials},
	volume = {25},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/0962280212461098},
	doi = {10.1177/0962280212461098},
	abstract = {There is debate whether clinical trials with suboptimal power are justified and whether results from large studies are more reliable than the (combined) results of smaller trials. We quantified the error rates for evaluations based on single conventionally powered trials (80\% or 90\% power) versus evaluations based on the random-effects meta-analysis of a series of smaller trials. When a treatment was assumed to have no effect but heterogeneity was present, the error rates for a single trial were increased more than 10-fold above the nominal rate, even for low heterogeneity. Conversely, for meta-analyses on a series of trials, the error rates were correct. When selective publication was present, the error rates were always increased, but they still tended to be lower for a series of trials than single trials. We conclude that evidence of efficacy based on a series of (smaller) trials, may lower the error rates compared with using a single well-powered trial. Only when both heterogeneity and selective publication can be excluded, a single trial is able to provide conclusive evidence.},
	pages = {538--552},
	number = {2},
	journaltitle = {Statistical Methods in Medical Research},
	shortjournal = {Stat Methods Med Res},
	author = {{IntHout}, Joanna and Ioannidis, John {PA} and Borm, George F},
	urldate = {2021-03-07},
	date = {2016-04-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd {STM}},
	keywords = {publication bias, meta-analysis, Clinical trial, heterogeneity, type I error},
}

@article{toth_study_2020,
	title = {Study preregistration: an evaluation of a method for transparent reporting},
	issn = {0889-3268, 1573-353X},
	url = {http://link.springer.com/10.1007/s10869-020-09695-3},
	doi = {10.1007/s10869-020-09695-3},
	shorttitle = {Study preregistration},
	abstract = {Study preregistration promotes transparency in scientific research by making a clear distinction between a priori and post hoc procedures or analyses. Management and applied psychology have not embraced preregistration in the way other closely related social science fields have. There may be concerns that preregistration does not add value and prevents exploratory data analyses. Using a mixed-method approach, in Study 1, we compared published preregistered samples against published non-preregistered samples. We found that preregistration effectively facilitated more transparent reporting based on criteria (i.e., confirmed hypotheses and a priori analysis plans). Moreover, consistent with concerns that the published literature contains elevated type I error rates, preregistered samples had fewer statistically significant results (48\%) than non-preregistered samples (66\%). To learn about the perceived advantages, disadvantages, and misconceptions of study preregistration, in Study 2, we surveyed authors of preregistered studies and authors who had never preregistered a study. Participants in both samples had positive inclinations towards preregistration yet expressed concerns about the process. We conclude with a review of best practices for management and applied psychology stakeholders.},
	journaltitle = {Journal of Business and Psychology},
	shortjournal = {J Bus Psychol},
	author = {Toth, Allison A. and Banks, George C. and Mellor, David and O’Boyle, Ernest H. and Dickson, Ashleigh and Davis, Daniel J. and {DeHaven}, Alex and Bochantin, Jaime and Borns, Jared},
	urldate = {2020-07-28},
	date = {2020-06-11},
	langid = {english},
	file = {Toth et al. - 2020 - Study Preregistration An Evaluation of a Method f.pdf:/Users/tom/Zotero/storage/YV464QKR/Toth et al. - 2020 - Study Preregistration An Evaluation of a Method f.pdf:application/pdf},
}

@article{rubin_when_2017,
	title = {When does {HARKing} hurt? Identifying when different types of undisclosed post hoc hypothesizing harm scientific progress},
	volume = {21},
	issn = {1089-2680},
	url = {https://doi.org/10.1037/gpr0000128},
	doi = {10.1037/gpr0000128},
	shorttitle = {When does harking hurt?},
	abstract = {Hypothesizing after the results are known, or {HARKing}, occurs when researchers check their research results and then add or remove hypotheses on the basis of those results without acknowledging this process in their research report (Kerr, 1998). In the present article, I discuss 3 forms of {HARKing}: (a) using current results to construct post hoc hypotheses that are then reported as if they were a priori hypotheses; (b) retrieving hypotheses from a post hoc literature search and reporting them as a priori hypotheses; and (c) failing to report a priori hypotheses that are unsupported by the current results. These 3 types of {HARKing} are often characterized as being bad for science and a potential cause of the current replication crisis. In the present article, I use insights from the philosophy of science to present a more nuanced view. Specifically, I identify the conditions under which each of these 3 types of {HARKing} is most and least likely to be bad for science. I conclude with a brief discussion about the ethics of each type of {HARKing}.},
	pages = {308--320},
	number = {4},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Rubin, Mark},
	urldate = {2020-12-03},
	date = {2017-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Rubin - 2017 - When Does HARKing Hurt Identifying When Different.pdf:/Users/tom/pCloud Drive/Zotero_Library/Rubin - 2017 - When Does HARKing Hurt Identifying When Different.pdf:application/pdf},
}

@report{topor_integrative_2020,
	title = {An integrative framework for planning and conducting Non-Interventional, Reproducible, and Open Systematic Reviews ({NIRO}-{SR})},
	url = {https://osf.io/preprints/metaarxiv/8gu5z/},
	abstract = {Mounting evidence indicates issues with low adherence to existing consensus-based guidelines for conducting systematic reviews ({SRs}), meaning that {SRs} can be subject to selective or misreporting practices. This problem arises in part from scarce guidance for reproducible reporting practices. This is compounded by the fact that existing guidelines are mainly applicable to interventional research designs, with systematic reviewers of non-interventional studies resorting to customised tools that deviate from best practice. Here, we present the development of the first comprehensive tool for conducting and reporting Non-Interventional, Reproducible, and Open Systematic Reviews ({NIRO}-{SR}). {NIRO}-{SR} is a 68-item checklist composed of two parts that provide itemised guidance on the preparation of a protocol for pre-registration (Part A) and reporting the review (Part B) in a reproducible and transparent manner. This paper, the tool, and an open repository (https://osf.io/f3brw/) provide a comprehensive resource for anyone who aims to conduct a high quality {SR} of non-interventional studies.},
	institution = {{MetaArXiv}},
	author = {Topor, Marta and Pickering, Jade and Mendes, Ana Barbosa and Bishop, Dorothy and Büttner, Fionn Cléirigh and Elsherif, Mahmoud and Evans, Thomas Rhys and Henderson, Emma L. and Kalandadze, Tamara and Nitschke, Faye and Staaks, Janneke and Akker, Olmo van den and Yeung, Siu Kit and Zaneva, Mirela and Lam, Alison and Madan, Christopher and Moreau, David and O'Mahony, Aoife and Parker, Adam J. and Riegelman, Amy and Testerman, Meghan and Westwood, Samuel},
	urldate = {2021-03-07},
	date = {2020-12-14},
	doi = {10.31222/osf.io/8gu5z},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, transparency, reproducibility, Medicine and Health Sciences, Physical Sciences and Mathematics, evidence synthesis, guidelines, non-interventional, non-interventional research, open research, systematic reviews},
	file = {Topor_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Topor_etal_2020.pdf:application/pdf},
}

@article{rubin_p_2017,
	title = {Do p values lose their meaning in exploratory analyses? It depends how you define the familywise error rate},
	volume = {21},
	issn = {1089-2680},
	url = {https://doi.org/10.1037/gpr0000123},
	doi = {10.1037/gpr0000123},
	shorttitle = {Do p values lose their meaning in exploratory analyses?},
	abstract = {Several researchers have recently argued that p values lose their meaning in exploratory analyses due to an unknown inflation of the alpha level (e.g., Nosek \& Lakens, 2014; Wagenmakers, 2016). For this argument to be tenable, the familywise error rate must be defined in relation to the number of hypotheses that are tested in the same study or article. Under this conceptualization, the familywise error rate is usually unknowable in exploratory analyses because it is usually unclear how many hypotheses have been tested on a spontaneous basis and then omitted from the final research report. In the present article, I argue that it is inappropriate to conceptualize the familywise error rate in relation to the number of hypotheses that are tested. Instead, it is more appropriate to conceptualize familywise error in relation to the number of different tests that are conducted on the same null hypothesis in the same study. Under this conceptualization, alpha-level adjustments in exploratory analyses are (a) less necessary and (b) objectively verifiable. As a result, p values do not lose their meaning in exploratory analyses.},
	pages = {269--275},
	number = {3},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Rubin, Mark},
	urldate = {2021-03-04},
	date = {2017-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {confirmatory research, exploratory research, familywise error rate, p values, Type I errors},
	file = {Rubin - 2017 - Do p values lose their meaning in exploratory anal.pdf:/Users/tom/pCloud Drive/Zotero_Library/Rubin - 2017 - Do p values lose their meaning in exploratory anal.pdf:application/pdf},
}

@article{nichols_opinion_2021,
	title = {Opinion: A better approach for dealing with reproducibility and replicability in science},
	volume = {118},
	rights = {© 2021 . https://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/118/7/e2100769118},
	doi = {10.1073/pnas.2100769118},
	shorttitle = {Opinion},
	abstract = {Science impacts our daily lives and guides national and international policies (1). Thus, results of scientific studies are of paramount importance; yet, there are concerns that many studies are not reproducible or replicable (2). To address these concerns, the National Research Council conducted a Consensus Study [{NASEM} 2019 (3)] that provides definitions of key concepts, discussions of problems, and recommendations for dealing with these problems. These recommendations are useful and well considered, but they do not go far enough in our opinion. The {NASEM} recommendations treat reproducibility and replicability as single-study issues, despite clear acknowledgement of the limitations of isolated studies and the need for research synthesis (3). We advocate a strategic approach to research, focusing on the accumulation of evidence via designed sequences of studies, as a means of dealing more effectively with reproducibility, replicability, and related problems. These sequences are designed to provide iterative tests based on comparison of data from empirical studies with predictions from competing hypotheses. Evidence is then formally accumulated based on the relative predictive abilities of the different hypotheses as the sequential studies proceed.



To deal more effectively with reproducibility, replicability, and related problems, scientists should pursue a strategic approach to research, focusing on the accumulation of evidence via designed sequences of studies. Image credit: Dave Cutler (artist).



In many disciplines, single studies are seldom adequate to substantially increase knowledge by themselves. Examples of Platt’s (4) “crucial experiments,” which are capable of definitively discriminating among competing hypotheses, can be found but are rare. Thus, we view individual study results as building blocks and the accumulation of evidence as requiring multiple studies of the same phenomena (5⇓–7). This view can be incorporated strategically into research planning by developing sequences of studies to investigate focal hypotheses.

Here we emphasize the comparison of study … 

[↵][1]1To whom correspondence may be addressed. Email: jamesdnichols2\{at\}gmail.com.

 [1]: \#xref-corresp-1-1},
	number = {7},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Nichols, James D. and Oli, Madan K. and Kendall, William L. and Boomer, G. Scott},
	urldate = {2021-03-07},
	date = {2021-02-16},
	langid = {english},
	pmid = {33568535},
	note = {Publisher: National Academy of Sciences
Section: Opinion},
	file = {Nichols_etal_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Nichols_etal_2021.pdf:application/pdf},
}

@article{brown_issues_2018,
	title = {Issues with data and analyses: Errors, underlying themes, and potential solutions},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708279115},
	doi = {10.1073/pnas.1708279115},
	shorttitle = {Issues with data and analyses},
	abstract = {Some aspects of science, taken at the broadest level, are universal in empirical research. These include collecting, analyzing, and reporting data. In each of these aspects, errors can and do occur. In this work, we first discuss the importance of focusing on statistical and data errors to continually improve the practice of science. We then describe underlying themes of the types of errors and postulate contributing factors. To do so, we describe a case series of relatively severe data and statistical errors coupled with surveys of some types of errors to better characterize the magnitude, frequency, and trends. Having examined these errors, we then discuss the consequences of specific errors or classes of errors. Finally, given the extracted themes, we discuss methodological, cultural, and system-level approaches to reducing the frequency of commonly observed errors. These approaches will plausibly contribute to the self-critical, self-correcting, ever-evolving practice of science, and ultimately to furthering knowledge.},
	pages = {2563--2570},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Brown, Andrew W. and Kaiser, Kathryn A. and Allison, David B.},
	urldate = {2021-03-07},
	date = {2018-03-13},
	langid = {english},
	file = {Brown et al. - 2018 - Issues with data and analyses Errors, underlying .pdf:/Users/tom/Zotero/storage/PTUH3ETL/Brown et al. - 2018 - Issues with data and analyses Errors, underlying .pdf:application/pdf},
}

@article{eronen_theory_2021,
	title = {The theory crisis in psychology: how to move forward},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620970586},
	doi = {10.1177/1745691620970586},
	shorttitle = {The theory crisis in psychology},
	abstract = {Meehl argued in 1978 that theories in psychology come and go, with little cumulative progress. We believe that this assessment still holds, as also evidenced by increasingly common claims that psychology is facing a “theory crisis” and that psychologists should invest more in theory building. In this article, we argue that the root cause of the theory crisis is that developing good psychological theories is extremely difficult and that understanding the reasons why it is so difficult is crucial for moving forward in the theory crisis. We discuss three key reasons based on philosophy of science for why developing good psychological theories is so hard: the relative lack of robust phenomena that impose constraints on possible theories, problems of validity of psychological constructs, and obstacles to discovering causal relationships between psychological variables. We conclude with recommendations on how to move past the theory crisis.},
	pages = {1745691620970586},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Eronen, Markus I. and Bringmann, Laura F.},
	urldate = {2021-03-07},
	date = {2021-01-29},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {causation, phenomena, robustness, theory, validity},
	file = {Eronen_Bringmann_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Eronen_Bringmann_2021.pdf:application/pdf},
}

@report{van_osselaer_recipe_2021,
	location = {Rochester, {NY}},
	title = {A recipe for honest consumer research},
	url = {https://papers.ssrn.com/abstract=3786989},
	abstract = {In the past decade, consumer research using experiments has experienced a crisis of confidence. Research in our field has rightfully been criticized for p-hacking, Hypothesizing After the Results are Known, and other practices that lead to overestimation of the reliability and replicability of published results. Remediation has centered on more closely approximating the ideal hypothetico-deductive method. There has been a push towards forming, and registering, one or few hypotheses before running experiments, testing only those hypotheses, and testing each hypothesis with a single, preplanned analysis. We argue that doing better hypothetico-deductive experiments is not the (whole) solution and that p-hacking and {HARKing} are not the problem per se. The problem is that we misrepresent exploratory research as hypothetico-deductive. Forcing exploratory research into a hypothetico-deductive straightjacket leads to bad hypothesis testing. The straightjacket also leads to bad exploration, crowding out essential, good exploration that deserves space in our journals. We propose a recipe for more honest consumer research, in which authors report exploratory studies meant to generate hypotheses followed by truly hypothetico-deductive studies that test those hypotheses.},
	number = {{ID} 3786989},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {van Osselaer, Stijn M. J. and Janiszewski, Chris},
	urldate = {2021-03-07},
	date = {2021-02-16},
	langid = {english},
	doi = {10.2139/ssrn.3786989},
	keywords = {preregistration, p-hacking, exploratory research, consumer psychology, consumer research, {HARKing}},
	file = {Snapshot:/Users/tom/Zotero/storage/73H6J5GI/papers.html:text/html},
}

@article{field_effect_2020,
	title = {The effect of preregistration on trust in empirical research findings: results of a registered report},
	volume = {7},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.181351},
	doi = {10.1098/rsos.181351},
	shorttitle = {The effect of preregistration on trust in empirical research findings},
	abstract = {The crisis of confidence has undermined the trust that researchers place in the findings of their peers. In order to increase trust in research, initiatives such as preregistration have been suggested, which aim to prevent various questionable research practices. As it stands, however, no empirical evidence exists that preregistration does increase perceptions of trust. The picture may be complicated by a researcher's familiarity with the author of the study, regardless of the preregistration status of the research. This registered report presents an empirical assessment of the extent to which preregistration increases the trust of 209 active academics in the reported outcomes, and how familiarity with another researcher influences that trust. Contrary to our expectations, we report ambiguous Bayes factors and conclude that we do not have strong evidence towards answering our research questions. Our findings are presented along with evidence that our manipulations were ineffective for many participants, leading to the exclusion of 68\% of complete datasets, and an underpowered design as a consequence. We discuss other limitations and confounds which may explain why the findings of the study deviate from a previously conducted pilot study. We reflect on the benefits of using the registered report submission format in light of our results. The {OSF} page for this registered report and its pilot can be found here: http://dx.doi.org/10.17605/{OSF}.{IO}/B3K75.},
	pages = {181351},
	number = {4},
	journaltitle = {Royal Society Open Science},
	shortjournal = {Royal Society Open Science},
	author = {Field, Sarahanne M. and Wagenmakers, E.-J. and Kiers, Henk A. L. and Hoekstra, Rink and Ernst, Anja F. and van Ravenzwaaij, Don},
	urldate = {2021-03-07},
	date = {2020},
	note = {Publisher: Royal Society},
	file = {Field_etal_.pdf:/Users/tom/pCloud Drive/Zotero_Library/Field_etal_.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BGKAU9J3/rsos.html:text/html},
}

@article{lee_robust_2019,
	title = {Robust modeling in cognitive science},
	volume = {2},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-019-00029-y},
	doi = {10.1007/s42113-019-00029-y},
	abstract = {In an attempt to increase the reliability of empirical findings, psychological scientists have recently proposed a number of changes in the practice of experimental psychology. Most current reform efforts have focused on the analysis of data and the reporting of findings for empirical studies. However, a large contingent of psychologists build models that explain psychological processes and test psychological theories using formal psychological models. Some, but not all, recommendations borne out of the broader reform movement bear upon the practice of behavioral or cognitive modeling. In this article, we consider which aspects of the current reform movement are relevant to psychological modelers, and we propose a number of techniques and practices aimed at making psychological modeling more transparent, trusted, and robust.},
	pages = {141--153},
	number = {3},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Lee, Michael D. and Criss, Amy H. and Devezer, Berna and Donkin, Christopher and Etz, Alexander and Leite, Fábio P. and Matzke, Dora and Rouder, Jeffrey N. and Trueblood, Jennifer S. and White, Corey N. and Vandekerckhove, Joachim},
	urldate = {2020-09-18},
	date = {2019-12-01},
	langid = {english},
	file = {Lee_etal_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lee_etal_2019.pdf:application/pdf},
}

@article{mohseni_harking_2020,
	title = {{HARKing}: From misdiagnosis to misprescription},
	abstract = {The practice of {HARKing}—hypothesizing after results are known—is commonly maligned as undermining the reliability of scientiﬁc ﬁndings. There are several accounts in the literature as to why {HARKing} undermines the reliability of ﬁndings. We argue that none of these is right and that the correct account is a Bayesian one. {HARKing} can indeed decrease the reliability of scientiﬁc ﬁndings, but it can also increase it. Which eﬀect {HARKing} produces depends on the diﬀerence of the prior odds of hypotheses characteristically selected ex ante and ex post to observing data. Further, we show how misdiagnosis of {HARKing} can lead to misprescription in the context of the replication crisis.},
	author = {Mohseni, Aydin},
	date = {2020},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Mohseni - HARKing From misdiagnosis to misprescription.pdf:/Users/tom/Zotero/storage/J8GSSCGN/Mohseni - HARKing From misdiagnosis to misprescription.pdf:application/pdf},
}

@article{murphy_harking_2019,
	title = {{HARKing}: how badly can cherry-picking and question trolling produce bias in published results?},
	volume = {34},
	issn = {1573-353X},
	url = {https://doi.org/10.1007/s10869-017-9524-7},
	doi = {10.1007/s10869-017-9524-7},
	shorttitle = {Harking},
	abstract = {The practice of hypothesizing after results are known ({HARKing}) has been identified as a potential threat to the credibility of research results. We conducted simulations using input values based on comprehensive meta-analyses and reviews in applied psychology and management (e.g., strategic management studies) to determine the extent to which two forms of {HARKing} behaviors might plausibly bias study outcomes and to examine the determinants of the size of this effect. When {HARKing} involves cherry-picking, which consists of searching through data involving alternative measures or samples to find the results that offer the strongest possible support for a particular hypothesis or research question, {HARKing} has only a small effect on estimates of the population effect size. When {HARKing} involves question trolling, which consists of searching through data involving several different constructs, measures of those constructs, interventions, or relationships to find seemingly notable results worth writing about, {HARKing} produces substantial upward bias particularly when it is prevalent and there are many effects from which to choose. Results identify the precise circumstances under which different forms of {HARKing} behaviors are more or less likely to have a substantial impact on a study’s substantive conclusions and the field’s cumulative knowledge. We offer suggestions for authors, consumers of research, and reviewers and editors on how to understand, minimize, detect, and deter detrimental forms of {HARKing} in future research.},
	pages = {1--17},
	number = {1},
	journaltitle = {Journal of Business and Psychology},
	shortjournal = {J Bus Psychol},
	author = {Murphy, Kevin R. and Aguinis, Herman},
	urldate = {2021-03-07},
	date = {2019-02-01},
	langid = {english},
	file = {Murphy_Aguinis_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Murphy_Aguinis_2019.pdf:application/pdf},
}

@article{bosco_harkings_2016,
	title = {{HARKing}'s threat to organizational research: evidence from primary and meta-analytic sources},
	volume = {69},
	rights = {© 2015 Wiley Periodicals, Inc.},
	issn = {1744-6570},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/peps.12111},
	doi = {10.1111/peps.12111},
	shorttitle = {Harking's threat to organizational research},
	abstract = {We assessed presumed consequences of hypothesizing after results are known ({HARKing}) by contrasting hypothesized versus nonhypothesized effect sizes among 10 common relations in organizational behavior, human resource management, and industrial and organizational psychology research. In Study 1, we analyzed 247 correlations representing 9 relations with individual performance in 136 articles published in Journal of Applied Psychology and Personnel Psychology and provide evidence that correlations are significantly larger when hypothesized compared to nonhypothesized. In Study 2, we analyzed 281 effect sizes from a meta-analysis on the job satisfaction–job performance relation and provide evidence that correlations are significantly larger when hypothesized compared to nonhypothesized. In addition, in Study 2, we documented that hypothesized variable pairs are more likely to be mentioned in article titles or abstracts. We also ruled out 13 alternative explanations to the presumed {HARKing} effect pertaining to methodological (e.g., unreliability, publication year, research setting, research design, measure contextualization, publication source) and substantive (e.g., predictor–performance pair, performance measure, satisfaction measure, occupation, job/task complexity) issues. Our results suggest that {HARKing} seems to pose a threat to research results, substantive conclusions, and practical applications. We offer recommended solutions to the {HARKing} threat.},
	pages = {709--750},
	number = {3},
	journaltitle = {Personnel Psychology},
	author = {Bosco, Frank A. and Aguinis, Herman and Field, James G. and Pierce, Charles A. and Dalton, Dan R.},
	urldate = {2021-02-17},
	date = {2016},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/peps.12111},
	file = {Bosco_etal_2016.pdf:/Users/tom/pCloud Drive/Zotero_Library/Bosco_etal_2016.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N45UH9TN/peps.html:text/html},
}

@article{brutus_self-reported_2013,
	title = {Self-reported limitations and future directions in scholarly reports: analysis and recommendations},
	volume = {39},
	issn = {0149-2063, 1557-1211},
	url = {http://journals.sagepub.com/doi/10.1177/0149206312455245},
	doi = {10.1177/0149206312455245},
	shorttitle = {Self-reported limitations and future directions in scholarly reports},
	pages = {48--75},
	number = {1},
	journaltitle = {Journal of Management},
	shortjournal = {Journal of Management},
	author = {Brutus, Stéphane and Aguinis, Herman and Wassmer, Ulrich},
	urldate = {2020-11-15},
	date = {2013-01},
	langid = {english},
	file = {Brutus et al. - 2013 - Self-Reported Limitations and Future Directions in.pdf:/Users/tom/Zotero/storage/KH5MIVM6/Brutus et al. - 2013 - Self-Reported Limitations and Future Directions in.pdf:application/pdf},
}

@incollection{mayo_theory_2009,
	location = {Cambridge},
	title = {Theory Confirmation and Novel Evidence},
	isbn = {978-0-511-65752-8},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511657528A012/type/book_part},
	pages = {125--169},
	booktitle = {Error and Inference},
	publisher = {Cambridge University Press},
	author = {Worrall, John and Mayo, Deborah G.},
	editor = {Mayo, Deborah G. and Spanos, Aris},
	urldate = {2021-03-07},
	date = {2009},
	langid = {english},
	doi = {10.1017/CBO9780511657528.006},
	file = {Worrall and Mayo - 2009 - Theory Confirmation and Novel Evidence.pdf:/Users/tom/Zotero/storage/NCKBLNWM/Worrall and Mayo - 2009 - Theory Confirmation and Novel Evidence.pdf:application/pdf},
}

@incollection{savage_fitting_1990,
	location = {Minneapolis},
	title = {Fitting your theory to the facts: probably not such a bad thing after all},
	isbn = {978-0-8166-1801-9},
	pages = {224--244},
	booktitle = {Scientific Theories},
	publisher = {University of Minnesota Press},
	author = {Howson, Colin},
	editor = {Savage, C. Wade},
	date = {1990},
	langid = {english},
	keywords = {Science / Philosophy \& Social Aspects},
	file = {Howson - 1990 - Fitting your theory to the facts probably not suc.pdf:/Users/tom/Zotero/storage/T298VD39/Howson - 1990 - Fitting your theory to the facts probably not suc.pdf:application/pdf},
}

@article{howson_bayesianism_1984,
	title = {Bayesianism and support by novel facts},
	volume = {35},
	issn = {0007-0882, 1464-3537},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/35.3.245},
	doi = {10.1093/bjps/35.3.245},
	pages = {245--251},
	number = {3},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {The British Journal for the Philosophy of Science},
	author = {Howson, Colin},
	urldate = {2021-03-07},
	date = {1984-09-01},
	langid = {english},
	file = {Howson - 1984 - Bayesianism and support by novel facts.pdf:/Users/tom/Zotero/storage/DK2A7AWR/Howson - 1984 - Bayesianism and support by novel facts.pdf:application/pdf},
}

@article{serghiou_assessment_2021,
	title = {Assessment of transparency indicators across the biomedical literature: How open is open?},
	volume = {19},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001107},
	doi = {10.1371/journal.pbio.3001107},
	shorttitle = {Assessment of transparency indicators across the biomedical literature},
	abstract = {Recent concerns about the reproducibility of science have led to several calls for more open and transparent research practices and for the monitoring of potential improvements over time. However, with tens of thousands of new biomedical articles published per week, manually mapping and monitoring changes in transparency is unrealistic. We present an open-source, automated approach to identify 5 indicators of transparency (data sharing, code sharing, conflicts of interest disclosures, funding disclosures, and protocol registration) and apply it across the entire open access biomedical literature of 2.75 million articles on {PubMed} Central ({PMC}). Our results indicate remarkable improvements in some (e.g., conflict of interest [{COI}] disclosures and funding disclosures), but not other (e.g., protocol registration and code sharing) areas of transparency over time, and map transparency across fields of science, countries, journals, and publishers. This work has enabled the creation of a large, integrated, and openly available database to expedite further efforts to monitor, understand, and promote transparency and reproducibility in science.},
	pages = {e3001107},
	number = {3},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Serghiou, Stylianos and Contopoulos-Ioannidis, Despina G. and Boyack, Kevin W. and Riedel, Nico and Wallach, Joshua D. and Ioannidis, John P. A.},
	urldate = {2021-03-07},
	date = {2021-03-01},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Citation analysis, Open science, Reproducibility, Scientific publishing, Conflicts of interest, Government funding of science, Algorithms, Metadata},
	file = {Serghiou et al. - 2021 - Assessment of transparency indicators across the b.pdf:/Users/tom/Zotero/storage/G2XRVDGG/Serghiou et al. - 2021 - Assessment of transparency indicators across the b.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7VVBGFSD/article.html:text/html},
}

@report{mcvay_transparent_2020,
	title = {Transparent manuscript reporting practices in behavioral medicine research: An audit of publications in 2018 and 2008},
	url = {https://psyarxiv.com/nx4fk/},
	shorttitle = {Transparent manuscript reporting practices in behavioral medicine research},
	abstract = {Objectives: Concerns about rigor and replicability have led to reforms to increase science transparency. We aimed to document the use of transparent reporting practices in behavioral medicine journals in 2018 in order to inform future efforts to improve reporting practices. We also aimed to compare 2018 reporting practices to 2008. Methods: We examined a randomly selected portion of articles published in 2018 and 2008 by the four behavioral medicine journals with the highest impact factor. We excluded manuscripts that were reviews, presented qualitative data, or were purely descriptive. We coded whether articles were clear in their presentation of analyses as being primary or secondary; whether studies were registered/pre-registered; whether they used “exploratory” or a related term to describe analyses/aims; and whether they reported power analyses. Results: We identified and coded 162 manuscripts published in 2018 (87\% observational and 12\% experimental). Among 2018 studies, 16\% were explicit in describing outcomes as primary or secondary, 51\% appeared to be reports of secondary outcomes but did not use the term “secondary,” and 33\% were unclear. Registration/pre-registration occurred in 14\% of studies; 77.3\% of registered/pre-registered studies did not report registration timing in relation to data collection, and 91\% did not report which analyses were pre-registered. “Exploratory” or a related term was used to describe an aim or analysis in 31\% of studies. Power analyses were reported in 8\% of studies. Compared to studies from 2008 (n=120), studies published in 2008 were less likely to clearly report whether outcomes presented were primary or secondary and less likely to have been registered/pre-registered. Conclusions: Behavioral medicine stakeholders should consider strategies to increase clarity of reporting of key analysis details.},
	institution = {{PsyArXiv}},
	author = {{McVay}, Megan and Cooper, Kellie and Seoane, Montserrat Carrera and Donahue, Marissa and Scherer, Laura},
	urldate = {2021-03-07},
	date = {2020-09-22},
	doi = {10.31234/osf.io/nx4fk},
	note = {type: article},
	keywords = {Meta-science, open science, Social and Behavioral Sciences, research methods, pre-registration, Health Psychology, scientific publication},
	file = {McVay_etal_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/McVay_etal_2020.pdf:application/pdf},
}

@incollection{barnes_prediction_2018,
	edition = {Fall 2018},
	title = {Prediction versus accommodation},
	url = {https://plato.stanford.edu/archives/fall2018/entries/prediction-accommodation/},
	abstract = {In early philosophical literature, a ‘prediction’ wasconsidered to be an empirical consequence of a theory that had not yetbeen verified at the time the theory was constructed—an‘accommodation’ was one that had.  The view thatpredictions are superior to accommodations in the assessment ofscientific theories is known as ‘predictivism’. Commonly,however, predictivism is understood more precisely as entailing thatevidence confirms theory more strongly when predicted than whenaccommodated.  Much ink has been spilled modifying the concept of‘prediction’ and explaining why predictivism is or is nottrue, and whether the history of science reveals that scientists arepredictivist in their assessment of theories. The debate overpredictivism also figures importantly in the debate about scientificrealism.},
	booktitle = {The Stanford Encyclopedia of Philosophy},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Barnes, Eric C.},
	editor = {Zalta, Edward N.},
	urldate = {2021-03-07},
	date = {2018},
	file = {SEP - Snapshot:/Users/tom/Zotero/storage/2R8FQJKS/prediction-accommodation.html:text/html},
}

@article{harker_predilections_2008,
	title = {On the predilections for predictions},
	volume = {59},
	issn = {0007-0882},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/axn017},
	doi = {10.1093/bjps/axn017},
	abstract = {Scientific theories are developed in response to a certain set of phenomena and subsequently evaluated, at least partially, in terms of the quality of fit between those same theories and appropriately distinctive phenomena. To differentiate between these two stages it is popular to describe the former as involving the accommodation of data and the latter as involving the prediction of data. Predictivism is the view that, ceteris paribus, correctly predicting data confers greater confirmation than successfully accommodating data. In this paper, I take issue with a variety of predictivist theses, argue that their role for issues of confirmation is extremely limited, and attempt to account for the appeal that predictivism has enjoyed. 1.  Introduction2.  Temporal Predictivism3.  Heuristic Predictivism4.  Weak Predictivism4.1.  Inference to better theories4.2.  Inference to better methods5.  Arguments for Strong Heuristic Predictivism5.1.  Best explanations argument5.2.  Conditional support5.3.  Unique explanations6.  Increased Explanatory Unification6.1.  Explaining what other theories can't6.2.  Contrived hypotheses6.2.  Strength and simplicity7.  Conclusions},
	pages = {429--453},
	number = {3},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {The British Journal for the Philosophy of Science},
	author = {Harker, David},
	urldate = {2021-03-07},
	date = {2008-09-01},
	note = {Publisher: The University of Chicago Press},
	file = {Snapshot:/Users/tom/Zotero/storage/B22ZIP23/axn017.html:text/html},
}

@article{harker_accommodation_2006,
	title = {Accommodation and prediction: the case of the persistent head},
	volume = {57},
	issn = {0007-0882},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/axl004},
	doi = {10.1093/bjps/axl004},
	shorttitle = {Accommodation and prediction},
	abstract = {A not unpopular thesis, when it comes to the confirmation of scientific theories, is that data which were used in the construction of a theory afford poorer support for that theory than data that played no role. Some compelling thought experiments have been offered in favour of this view, not as proof but rather to add some intuitive plausibility. In this paper I consider such thought experiments and argue that they do not support the thesis; the perceived importance of prediction over accommodation, at least in these cases, is illusory.  1.  Introduction2.  Background assumptions3.  Strong thesis4.  Weak thesis5.  Conclusions},
	pages = {309--321},
	number = {2},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {The British Journal for the Philosophy of Science},
	author = {Harker, David},
	urldate = {2021-03-07},
	date = {2006-06-01},
	note = {Publisher: The University of Chicago Press},
	file = {Snapshot:/Users/tom/Zotero/storage/5NIW857E/axl004.html:text/html},
}

@book{barnes_paradox_2008,
	location = {Cambridge ; New York},
	title = {The Paradox of Predictivism},
	isbn = {978-0-521-87962-0},
	pagetotal = {265},
	publisher = {Cambridge University Press},
	author = {Barnes, Eric C.},
	date = {2008},
	note = {{OCLC}: ocn174449717},
	keywords = {Philosophy, Science, Forecasting},
	file = {Eric Christian Barnes - The Paradox of Predictivism-Cambridge University Press (2008).pdf:/Users/tom/Zotero/storage/S843HQ8Z/Eric Christian Barnes - The Paradox of Predictivism-Cambridge University Press (2008).pdf:application/pdf},
}

@article{barnes_roots_2014,
	title = {The roots of predictivism},
	volume = {45},
	issn = {0039-3681},
	url = {http://www.sciencedirect.com/science/article/pii/S0039368113000988},
	doi = {10.1016/j.shpsa.2013.10.002},
	abstract = {In The Paradox of Predictivism (2008, Cambridge University Press) I tried to demonstrate that there is an intimate relationship between predictivism (the thesis that novel predictions sometimes carry more weight than accommodations) and epistemic pluralism (the thesis that one important form of evidence in science is the judgments of other scientists). Here I respond to various published criticisms of some of the key points from Paradox from David Harker, Jarret Leplin, and Clark Glymour. Foci include my account of predictive novelty (endorsement novelty), the claim that predictivism has two roots, the prediction per se and predictive success, and my account of why Mendeleev’s predictions carried special weight in confirming the Periodic Law of the Elements.},
	pages = {46--53},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Barnes, Eric C.},
	urldate = {2020-12-22},
	date = {2014-03-01},
	langid = {english},
	keywords = {Prediction, Predictivism, Accommodation, Mendeleev, Novel confirmation, Periodic Law of the Elements},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/6AQKTUVU/Barnes - 2014 - The roots of predictivism.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/XLM4CPRD/S0039368113000988.html:text/html},
}

@article{barnes_predictivism_2005,
	title = {Predictivism for pluralists},
	volume = {56},
	issn = {0007-0882},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/axi131},
	doi = {10.1093/bjps/axi131},
	abstract = {Predictivism asserts that novel confirmations carry special probative weight. Epistemic pluralism asserts that the judgments of agents (about, e.g., the probabilities of theories) carry epistemic import. In this paper, I propose a new theory of predictivism that is tailored to pluralistic evaluators of theories. I replace the orthodox notion of use-novelty with a notion of endorsement-novelty, and argue that the intuition that predictivism is true has two roots. I provide a detailed Bayesian rendering of this theory and argue that pluralistic theory evaluation pervades scientific practice. I compare my account of predictivism with those of Maher and Worrall.  1.  Introduction2.  Why construction is a red herring for pluralist evaluators3.  The unvirtuous accommodator4.  Virtuous endorsers and the two roots of predictivism5.  The two roots in Bayesian terms: the priors and background beliefs of endorsers6.  Who are the pluralist evaluators?7.  Two contemporary theories of predictivism7.1Maher: Reliable methods of theory construction7.2Worrall: The confirmation of core ideas8.  Conclusion},
	pages = {421--450},
	number = {3},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {The British Journal for the Philosophy of Science},
	author = {Barnes, Eric C.},
	urldate = {2020-12-29},
	date = {2005-09-01},
	note = {Publisher: The University of Chicago Press},
	file = {Barnes_2005.pdf:/Users/tom/pCloud Drive/Zotero_Library/Barnes_2005.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AU8NCMJN/axi131.html:text/html},
}

@article{lange_apparent_2001,
	title = {The apparent superiority of prediction to accommodation as a side effect: a reply to maher},
	volume = {52},
	issn = {0007-0882},
	url = {https://academic.oup.com/bjps/article/52/3/575/1496255},
	doi = {10.1093/bjps/52.3.575},
	shorttitle = {The apparent superiority of prediction to accommodation as a side effect},
	abstract = {Abstract. Maher ([1990], [1993]) has offered a lovely example to motivate the intuition that a successful prediction has a kind of confirmatory significance tha},
	pages = {575--588},
	number = {3},
	journaltitle = {The British Journal for the Philosophy of Science},
	shortjournal = {Br J Philos Sci},
	author = {Lange, Marc},
	urldate = {2020-12-03},
	date = {2001-09-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Lange_2001.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lange_2001.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GI33ED3Z/1496255.html:text/html},
}

@book{lipton_inference_2004,
	location = {London},
	edition = {2},
	title = {Inference to the Best Explanation},
	publisher = {Routledge},
	author = {Lipton, Peter},
	date = {2004},
	langid = {english},
	keywords = {❓ Multiple {DOI}},
	file = {Lipton - Inference to the Best Explanation.pdf:/Users/tom/Zotero/storage/Q76HKCXW/Lipton - Inference to the Best Explanation.pdf:application/pdf},
}

@article{lipton_prediction_1990,
	title = {Prediction and prejudice},
	volume = {4},
	issn = {0269-8595, 1469-9281},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02698599008573345},
	doi = {10.1080/02698599008573345},
	abstract = {Evidence that supports a theory may beavailable tothe scientist who constructs the theory andused as a guide to that construction, or it may only bediscovered in the course of testing the theory. The central claim of this essay is that information about whether the evidence was accommodated orpredicted affects the rational degreeofconfidence one ought tohave in the theory. Only when the evidence is accommodated is there some reason to believe that the theoretical system was 'fudged' to fit the evidence ina waythat weakens support. This weakening is an objective matter, but not one that canbeconclusively determined by examining the contents of the theory and its logical relationship to the evidence. Consequently, there is less reason to believe a theory on the basis of that evidence when it is known that the evidence was accommodated than there would be if it was known instead that the same evidence had been predicted.},
	pages = {51--65},
	number = {1},
	journaltitle = {International Studies in the Philosophy of Science},
	shortjournal = {International Studies in the Philosophy of Science},
	author = {Lipton, Peter},
	urldate = {2021-03-07},
	date = {1990-01},
	langid = {english},
	file = {Lipton - 1990 - Prediction and prejudice.pdf:/Users/tom/Zotero/storage/KUKHUWPL/Lipton - 1990 - Prediction and prejudice.pdf:application/pdf},
}

@article{lipton_testing_2005,
	title = {Testing hypotheses: prediction and prejudice},
	volume = {307},
	rights = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/307/5707/219},
	doi = {10.1126/science.1103024},
	shorttitle = {Testing hypotheses},
	abstract = {Observations that fit a hypothesis may be made before or after the hypothesis is formulated. Can that difference be relevant to the amount of support that the observations provide for the hypothesis? Philosophers of science and statisticians are both divided on this question, but there is an argument that predictions ought to count more than accommodations, because of the risk of “fudging” that accommodations run and predictions avoid.},
	pages = {219--221},
	number = {5707},
	journaltitle = {Science},
	author = {Lipton, Peter},
	urldate = {2020-12-03},
	date = {2005-01-14},
	langid = {english},
	pmid = {15653494},
	note = {Publisher: American Association for the Advancement of Science
Section: Review},
	file = {Lipton_2005.pdf:/Users/tom/pCloud Drive/Zotero_Library/Lipton_2005.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DDSPDFDU/219.html:text/html;Snapshot:/Users/tom/Zotero/storage/9CAY6QZ4/219.html:text/html},
}

@article{maher_howson_1993,
	title = {Howson and Franklin on prediction},
	volume = {60},
	url = {http://www.jstor.org/stable/188358},
	doi = {10.1086/289736},
	pages = {329--340},
	number = {2},
	journaltitle = {Philosophy of Science},
	author = {Maher, Patrick},
	date = {1993},
	langid = {english},
	file = {Maher - 1993 - Howson and Franklin on Prediction.pdf:/Users/tom/Zotero/storage/TVCLVZZJ/Maher - 1993 - Howson and Franklin on Prediction.pdf:application/pdf},
}

@article{depaoli_improving_2017,
	title = {Improving transparency and replication in Bayesian statistics: The {WAMBS}-Checklist.},
	volume = {22},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000065},
	doi = {10.1037/met0000065},
	shorttitle = {Improving transparency and replication in Bayesian statistics},
	pages = {240--261},
	number = {2},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Depaoli, Sarah and van de Schoot, Rens},
	urldate = {2021-03-05},
	date = {2017-06},
	langid = {english},
	file = {Depaoli_van de Schoot_2017.pdf:/Users/tom/pCloud Drive/Zotero_Library/Depaoli_van de Schoot_2017.pdf:application/pdf},
}

@article{berry_multiple_nodate,
	title = {Multiple comparisons, multiple tests, and data dredging: a bayesian perspective},
	pages = {23},
	author = {Berry, Donald A},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Berry - Multiple Comparisons, Multiple Tests, and Data Dre.pdf:/Users/tom/Zotero/storage/HYYSS5JC/Berry - Multiple Comparisons, Multiple Tests, and Data Dre.pdf:application/pdf},
}

@article{dutilh_seven_2016,
	title = {Seven selfish reasons for preregistration},
	volume = {29},
	url = {https://www.psychologicalscience.org/observer/seven-selfish-reasons-for-preregistration},
	abstract = {Psychological scientists Eric-Jan Wagenmakers and Gilles Dutilh present an illustrated guide to the career benefits of submitting your research plans before beginning your data collection.},
	number = {9},
	journaltitle = {{APS} Observer},
	author = {Dutilh, Gilles and Wagenmakers, Eric-Jan},
	urldate = {2021-03-04},
	date = {2016-10-31},
	langid = {american},
	keywords = {⛔ No {DOI} found},
	file = {Snapshot:/Users/tom/Zotero/storage/XGMYLKSF/seven-selfish-reasons-for-preregistration.html:text/html},
}

@article{allen_open_2019,
	title = {Open science challenges, benefits and tips in early career and beyond},
	volume = {17},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000246},
	doi = {10.1371/journal.pbio.3000246},
	abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers ({ECRs}). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect {ECRs} acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the {ECR} and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of {ECRs} for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
	pages = {e3000246},
	number = {5},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Allen, Christopher and Mehler, David M. A.},
	urldate = {2021-03-04},
	date = {2019-05-01},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open data, Statistical data, Open science, Reproducibility, Peer review, Experimental design, Careers, Neuroimaging},
	file = {Allen_Mehler_2019.pdf:/Users/tom/pCloud Drive/Zotero_Library/Allen_Mehler_2019.pdf:application/pdf},
}

@article{dirnagl_preregistration_2020,
	title = {Preregistration of exploratory research: Learning from the golden age of discovery},
	volume = {18},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000690},
	doi = {10.1371/journal.pbio.3000690},
	shorttitle = {Preregistration of exploratory research},
	abstract = {Preregistration of study protocols and, in particular, Registered Reports are novel publishing formats that are currently gaining substantial traction. Besides rating the research question and soundness of methodology over outstanding significance of the results, they can help with antagonizing inadequate statistical power, selective reporting of results, undisclosed analytic flexibility, as well as publication bias. Preregistration works well when a clear hypothesis, primary outcome, and mode of analysis can be formulated. But is it also applicable and useful in discovery research, which develops theories and hypotheses, measurement techniques, and generates evidence that justifies further research? I will argue that only slight modifications are needed to harness the potential of preregistration and make exploratory research more trustworthy and useful.},
	pages = {e3000690},
	number = {3},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Dirnagl, Ulrich},
	urldate = {2021-03-04},
	date = {2020-03-26},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Measurement, Scientific publishing, Clinical trials, Peer review, Drug discovery, Scientists, Publication ethics, Safety studies},
	file = {Dirnagl_2020.pdf:/Users/tom/pCloud Drive/Zotero_Library/Dirnagl_2020.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Z5HBPPG8/article.html:text/html},
}

@article{irvine_role_2021,
	title = {The role of replication studies in theory building},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620970558},
	doi = {10.1177/1745691620970558},
	abstract = {At least since Meehl’s (in)famous 1978 article, the state of theorizing in psychology has often been lamented. Replication studies have been presented as a way of directly supporting theory development by enabling researchers to more confidently and precisely test and update theoretical claims. In this article I use contemporary work from philosophy of science to make explicit and emphasize just how much theory development is required before “good” replication studies can be carried out and show just how little theoretical payoff even good conceptual replications offer. I suggest that in many areas of psychology aiming at replication is misplaced and that instead replication attempts are better seen as exploratory studies that can be used in the cumulative development of theory and measurement procedures.},
	pages = {1745691620970558},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Irvine, Elizabeth},
	urldate = {2021-01-18},
	date = {2021-01-13},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication, robustness, measurement, progress},
	file = {Irvine_2021.pdf:/Users/tom/pCloud Drive/Zotero_Library/Irvine_2021.pdf:application/pdf},
}

@article{meehl_theory-testing_1967,
	title = {Theory-testing in psychology and physics: a methodological paradox},
	volume = {34},
	issn = {0031-8248},
	url = {https://www.journals.uchicago.edu/doi/10.1086/288135},
	doi = {10.1086/288135},
	shorttitle = {Theory-testing in psychology and physics},
	abstract = {Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by "success" is very weak, and becomes weaker with increased precision. "Statistical significance" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental "cuteness" and a free reliance upon ad hoc explanations to avoid refutation.},
	pages = {103--115},
	number = {2},
	journaltitle = {Philosophy of Science},
	shortjournal = {Philosophy of Science},
	author = {Meehl, Paul E.},
	urldate = {2020-10-05},
	date = {1967-06-01},
	note = {Publisher: The University of Chicago Press},
	file = {Meehl - 1967 - Theory-Testing in Psychology and Physics A Method.pdf:/Users/tom/Zotero/storage/K83WQ7AZ/Meehl - 1967 - Theory-Testing in Psychology and Physics A Method.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KSJV2AZJ/288135.html:text/html},
}

@article{starns_blinded_2019,
	title = {Blinded inference: an opportunity for mathematical modelers to lead the way in research reform},
	volume = {2},
	rights = {2019 Society for Mathematical Psychology},
	issn = {2522-087X},
	url = {https://link.springer.com/article/10.1007/s42113-019-00040-3},
	doi = {10.1007/s42113-019-00040-3},
	shorttitle = {Blinded inference},
	abstract = {In a blinded inference study, researchers are asked to analyze condition-blinded datasets and make inferences about various aspects of the data generation process, such as whether or not a variable that manipulates some target cognitive process varied across conditions. This procedure directly tests researchers’ ability to make valid conclusions about underlying processes based on data patterns and assesses the extent to which they accurately report the level of uncertainty associated with their research conclusions. As such, blinded inference studies are a valuable tool in the effort to improve research practices. In this comment, we review three recent studies in the cognitive modeling literature to highlight the benefits of blinded inference, and we make recommendations for future blinded inference studies. We conclude by encouraging modelers to champion the blinded inference method as a fundamental component of effective psychological research.},
	pages = {223--228},
	number = {3},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Starns, Jeffrey J. and Cataldo, Andrea M. and Rotello, Caren M.},
	urldate = {2021-03-17},
	date = {2019-12-01},
	langid = {english},
	note = {Company: Springer
Distributor: Springer
Institution: Springer
Label: Springer
Number: 3
Publisher: Springer International Publishing},
}

@article{maceachern_preregistration_2019,
	title = {Preregistration of modeling exercises may not be useful},
	volume = {2},
	rights = {2019 Society for Mathematical Psychology},
	issn = {2522-087X},
	url = {https://link.springer.com/article/10.1007/s42113-019-00038-x},
	doi = {10.1007/s42113-019-00038-x},
	abstract = {This is a commentary on Lee et al.’s (2019) article encouraging preregistration of model development, fitting, and evaluation. While we are in general agreement with Lee et al.’s characterization of the modeling process, we disagree on whether preregistration of this process will move the scientific enterprise forward. We emphasize the subjective and exploratory nature of model development, and point out that “under-modeling” of data (relying on black-box approaches applied to data without data exploration) is as big a problem as “over-modeling” (fitting noise, resulting in models that generalize poorly). We also note the potential long-run negative impact of preregistration on future generations of cognitive scientists. It is our opinion that preregistration of model development will lead to less, and to less creative, exploratory analysis (i.e., to more under-modeling), and that Lee at al.’s primary goals can be achieved by requiring publication of raw data and code. We conclude our commentary with suggestions on how to move forward.},
	pages = {179--182},
	number = {3},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {{MacEachern}, Steven N. and Zandt, Trisha Van},
	urldate = {2021-03-17},
	date = {2019-12-01},
	langid = {english},
	note = {Company: Springer
Distributor: Springer
Institution: Springer
Label: Springer
Number: 3
Publisher: Springer International Publishing},
	file = {MacEachern and Zandt - 2019 - Preregistration of Modeling Exercises May Not Be U.pdf:/Users/tom/Zotero/storage/TKMI8AGJ/MacEachern and Zandt - 2019 - Preregistration of Modeling Exercises May Not Be U.pdf:application/pdf},
}

@article{shiffrin_scientific_2018,
	title = {Scientific progress despite irreproducibility: A seeming paradox},
	volume = {115},
	rights = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/11/2632},
	doi = {10.1073/pnas.1711786114},
	shorttitle = {Scientific progress despite irreproducibility},
	abstract = {It appears paradoxical that science is producing outstanding new results and theories at a rapid rate at the same time that researchers are identifying serious problems in the practice of science that cause many reports to be irreproducible and invalid. Certainly, the practice of science needs to be improved, and scientists are now pursuing this goal. However, in this perspective, we argue that this seeming paradox is not new, has always been part of the way science works, and likely will remain so. We first introduce the paradox. We then review a wide range of challenges that appear to make scientific success difficult. Next, we describe the factors that make science work—in the past, present, and presumably also in the future. We then suggest that remedies for the present practice of science need to be applied selectively so as not to slow progress and illustrate with a few examples. We conclude with arguments that communication of science needs to emphasize not just problems but the enormous successes and benefits that science has brought and is now bringing to all elements of modern society.},
	pages = {2632--2639},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Shiffrin, Richard M. and Börner, Katy and Stigler, Stephen M.},
	urldate = {2021-03-17},
	date = {2018-03-13},
	langid = {english},
	pmid = {29531095},
	note = {Publisher: National Academy of Sciences
Section: Colloquium Paper},
	keywords = {reproducibility, science communication, science history, scientific progress, scientometrics},
	file = {Full Text PDF:/Users/tom/Zotero/storage/4YFGQT7L/Shiffrin et al. - 2018 - Scientific progress despite irreproducibility A s.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BMYRTWD4/2632.html:text/html},
}

@article{shiffrin_commentary_2019,
	title = {Commentary on “robust modeling in cognitive science: misunderstanding the goal of modeling”},
	volume = {2},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-019-00043-0},
	doi = {10.1007/s42113-019-00043-0},
	shorttitle = {Commentary on “robust modeling in cognitive science},
	abstract = {The article “Robust Modeling in Cognitive Science” (2019) by Lee et al. makes several recommendations about best practices for cognitive science modelers. Many of these are reasonable and will not be discussed in this commentary. I believe several other critically important recommendations either put too much emphasis on less important components of good practice, or are somewhat misguided, and suggest that these are distorted in part because they are based on a misunderstanding of, or a failure to take into account, the goals of modeling. This commentary will highlight those areas where I believe the recommendations for good modeling practice deserve refinement and change.},
	pages = {176--178},
	number = {3},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Shiffrin, Richard M.},
	urldate = {2021-03-17},
	date = {2019-12-01},
	langid = {english},
	file = {Shiffrin - 2019 - Commentary on “Robust Modeling in Cognitive Scienc.pdf:/Users/tom/Zotero/storage/XS622SEE/Shiffrin - 2019 - Commentary on “Robust Modeling in Cognitive Scienc.pdf:application/pdf},
}

@article{humphreys_philosophical_2009,
	title = {The philosophical novelty of computer simulation methods},
	volume = {169},
	issn = {0039-7857, 1573-0964},
	url = {http://link.springer.com/10.1007/s11229-008-9435-2},
	doi = {10.1007/s11229-008-9435-2},
	abstract = {Reasons are given to justify the claim that computer simulations and computational science constitute a distinctively new set of scientiﬁc methods and that these methods introduce new issues in the philosophy of science. These issues are both epistemological and methodological in kind.},
	pages = {615--626},
	number = {3},
	journaltitle = {Synthese},
	shortjournal = {Synthese},
	author = {Humphreys, Paul},
	urldate = {2021-03-18},
	date = {2009-08},
	langid = {english},
	file = {Humphreys - 2009 - The philosophical novelty of computer simulation m.pdf:/Users/tom/Zotero/storage/Q8PH4UHZ/Humphreys - 2009 - The philosophical novelty of computer simulation m.pdf:application/pdf},
}

@article{schlesinger_accommodation_1987,
	title = {Accommodation and prediction},
	volume = {65},
	issn = {0004-8402},
	url = {https://aap.tandfonline.com/doi/abs/10.1080/00048408712342751},
	doi = {10.1080/00048408712342751},
	pages = {33--42},
	number = {1},
	journaltitle = {Australasian Journal of Philosophy},
	shortjournal = {null},
	author = {Schlesinger, George N.},
	urldate = {2021-03-17},
	date = {1987-03-01},
	note = {Publisher: Routledge},
	file = {Snapshot:/Users/tom/Zotero/storage/XXYLQWFS/00048408712342751.html:text/html},
}

@incollection{simon_judging_1968,
	title = {On Judging the Plausibility of Theories},
	volume = {52},
	url = {https://www.sciencedirect.com/science/article/pii/S0049237X08712114},
	series = {Logic, Methodology and Philosophy of Science {III}},
	abstract = {This chapter focuses on the close interaction between hypotheses and data in the building and testing of theories. In most formal theories of induction—particularly those that belong to the genus “hypothetico-deductive” (H-D)—hypotheses spring full-blown from the head of Zeus, then are tested with data that exist, timelessly and quite independently of the hypotheses. Theories as otherwise divergent as Popper's and Carnap's share this common framework. It was one of Norwood Hanson's important contributions to challenge this separation of hypothesis from data and to demonstrate that in the history of science the retroduction of generalizations and explanations from data has been one of the central and crucial processes. Also several aspects of the problem of testing theories have been examined in the chapter and particularly those important theories that take the form of extreme hypotheses.},
	pages = {439--459},
	booktitle = {Studies in Logic and the Foundations of Mathematics},
	publisher = {Elsevier},
	author = {Simon, H. A.},
	editor = {Van Rootselaar, B. and Staal, J. F.},
	urldate = {2021-03-17},
	date = {1968-01-01},
	langid = {english},
	doi = {10.1016/S0049-237X(08)71211-4},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/GCBCTRH3/S0049237X08712114.html:text/html},
}

@article{wilson_blowing_2006,
	title = {On blowing trumpets to the tulips: To prove or not to prove the null hypothesis--Comment on Bösch, Steinkamp, and Boller (2006).},
	volume = {132},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.132.4.524},
	doi = {10.1037/0033-2909.132.4.524},
	shorttitle = {On blowing trumpets to the tulips},
	abstract = {The H. Bo¨sch, F. Steinkamp, and E. Boller (2006) meta-analysis reaches mixed and cautious conclusions about the possibility of psychokinesis. The authors argue that, for both methodological and philosophical reasons, it is nearly impossible to draw any conclusions from this body of research. The authors do not agree that any significant effect at all, no matter how small, is fundamentally important (Bo¨sch et al., 2006, p. 517), and they suggest that psychokinesis researchers focus either on producing larger effects or on specifying the conditions under which they would be willing to accept the null hypothesis.},
	pages = {524--528},
	number = {4},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Wilson, David B. and Shadish, William R.},
	urldate = {2021-03-17},
	date = {2006},
	langid = {english},
	file = {Wilson and Shadish - 2006 - On blowing trumpets to the tulips To prove or not.pdf:/Users/tom/Zotero/storage/ZTASQ32H/Wilson and Shadish - 2006 - On blowing trumpets to the tulips To prove or not.pdf:application/pdf},
}

@article{bosch_eye_2006,
	title = {In the eye of the beholder: Reply to Wilson and Shadish (2006) and Radin, Nelson, Dobyns, and Houtkooper (2006).},
	volume = {132},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.132.4.533},
	doi = {10.1037/0033-2909.132.4.533},
	shorttitle = {In the eye of the beholder},
	pages = {533--537},
	number = {4},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Bösch, Holger and Steinkamp, Fiona and Boller, Emil},
	urldate = {2021-03-17},
	date = {2006-07},
	langid = {english},
	file = {Bösch et al. - 2006 - In the eye of the beholder Reply to Wilson and Sh.pdf:/Users/tom/Zotero/storage/9REGTZSY/Bösch et al. - 2006 - In the eye of the beholder Reply to Wilson and Sh.pdf:application/pdf},
}

@article{bosch_examining_2006,
	title = {Examining psychokinesis: The interaction of human intention with random number generators--A meta-analysis.},
	volume = {132},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.132.4.497},
	doi = {10.1037/0033-2909.132.4.497},
	shorttitle = {Examining psychokinesis},
	pages = {497--523},
	number = {4},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Bösch, Holger and Steinkamp, Fiona and Boller, Emil},
	urldate = {2021-03-17},
	date = {2006},
	langid = {english},
	file = {Bösch et al. - 2006 - Examining psychokinesis The interaction of human .pdf:/Users/tom/Zotero/storage/YHSEYZVX/Bösch et al. - 2006 - Examining psychokinesis The interaction of human .pdf:application/pdf},
}

@article{daston_scientific_2005-1,
	title = {Scientific error and the ethos of belief},
	pages = {1--28},
	journaltitle = {Social Research},
	author = {Daston, Lorraine},
	date = {2005},
	langid = {english},
	file = {2020 - Scientific Error and the Ethos of Belief.pdf:/Users/tom/Zotero/storage/SF7BEU8J/2020 - Scientific Error and the Ethos of Belief.pdf:application/pdf},
}

@article{austin_testing_2006,
	title = {Testing multiple statistical hypotheses resulted in spurious associations: a study of astrological signs and health},
	volume = {59},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435606001247},
	doi = {10.1016/j.jclinepi.2006.01.012},
	shorttitle = {Testing multiple statistical hypotheses resulted in spurious associations},
	abstract = {Objectives: To illustrate how multiple hypotheses testing can produce associations with no clinical plausibility. Study Design and Setting: We conducted a study of all 10,674,945 residents of Ontario aged between 18 and 100 years in 2000. Residents were randomly assigned to equally sized derivation and validation cohorts and classiﬁed according to their astrological sign. Using the derivation cohort, we searched through 223 of the most common diagnoses for hospitalization until we identiﬁed two for which subjects born under one astrological sign had a signiﬁcantly higher probability of hospitalization compared to subjects born under the remaining signs combined (P ! 0.05).
Results: We tested these 24 associations in the independent validation cohort. Residents born under Leo had a higher probability of gastrointestinal hemorrhage (P 5 0.0447), while Sagittarians had a higher probability of humerus fracture (P 5 0.0123) compared to all other signs combined. After adjusting the signiﬁcance level to account for multiple comparisons, none of the identiﬁed associations remained signiﬁcant in either the derivation or validation cohort.
Conclusions: Our analyses illustrate how the testing of multiple, non-prespeciﬁed hypotheses increases the likelihood of detecting implausible associations. Our ﬁndings have important implications for the analysis and interpretation of clinical studies. Ó 2006 Elsevier Inc. All rights reserved.},
	pages = {964--969},
	number = {9},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Austin, Peter C. and Mamdani, Muhammad M. and Juurlink, David N. and Hux, Janet E.},
	urldate = {2021-03-17},
	date = {2006-09},
	langid = {english},
	file = {Austin et al. - 2006 - Testing multiple statistical hypotheses resulted i.pdf:/Users/tom/Zotero/storage/RD7KJVT4/Austin et al. - 2006 - Testing multiple statistical hypotheses resulted i.pdf:application/pdf},
}

@article{savitz_scientific_1990,
	title = {Scientific Standards of Criticism: A Reaction to "Scientific Standards in Epidemiologic Studies of the Menace of Daily Life," by A. R. Feinstein},
	volume = {1},
	url = {http://www.jstor.org/stable/20065629},
	doi = {10.1097/00001648-199001000-00017},
	pages = {78--83},
	number = {1},
	journaltitle = {Epidemiology},
	author = {Savitz, David A. and Greenland, Sander and Stolley, Paul D. and Kelsey, Jennifer L.},
	date = {1990},
	langid = {english},
	file = {Savitz et al. - 1990 - Scientific Standards of Criticism A Reaction to .pdf:/Users/tom/Zotero/storage/43GQJVT2/Savitz et al. - 1990 - Scientific Standards of Criticism A Reaction to .pdf:application/pdf},
}

@article{moreau_conducting_2020,
	title = {Conducting a meta-analysis in the age of open science: Tools, tips, and practical recommendations.},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000351},
	doi = {10.1037/met0000351},
	shorttitle = {Conducting a meta-analysis in the age of open science},
	abstract = {Psychology researchers are rapidly adopting open science practices, yet clear guidelines on how to apply these practices to meta-analysis remain lacking. In this tutorial, we describe why open science is important in the context of meta-analysis in psychology, and suggest how to adopt the 3 main components of open science: preregistration, open materials, and open data. We first describe how to make the preregistration as thorough as possible—and how to handle deviations from the plan. We then focus on creating easy-to-read materials (e.g., search syntax, R scripts) to facilitate reproducibility and bolster the impact of a meta-analysis. Finally, we suggest how to organize data (e.g., literature search results, data extracted from studies) that are easy to share, interpret, and update as new studies emerge. For each step of the meta-analysis, we provide example templates, accompanied by brief video tutorials, and show how to integrate these practices into the Open Science Framework (https://osf.io/q8stz/).},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Moreau, David and Gamble, Beau},
	urldate = {2021-03-17},
	date = {2020-09-10},
	langid = {english},
	file = {Moreau and Gamble - 2020 - Conducting a meta-analysis in the age of open scie.pdf:/Users/tom/Zotero/storage/DAYTQ5VI/Moreau and Gamble - 2020 - Conducting a meta-analysis in the age of open scie.pdf:application/pdf},
}

@article{palfi_why_2020-1,
	title = {Why Bayesian “Evidence for \textit{H} $_{\textrm{1}}$ ” in One Condition and Bayesian “Evidence for \textit{H} $_{\textrm{0}}$ ” in Another Condition Does Not Mean Good-Enough Bayesian Evidence for a Difference Between the Conditions},
	volume = {3},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/2515245920913019},
	doi = {10.1177/2515245920913019},
	abstract = {Psychologists are often interested in whether an independent variable has a different effect in condition A than in condition B. To test such a question, one needs to directly compare the effect of that variable in the two conditions (i.e., test the interaction). Yet many researchers tend to stop when they find a significant test in one condition and a nonsignificant test in the other condition, deeming this as sufficient evidence for a difference between the two conditions. In this Tutorial, we aim to raise awareness of this inferential mistake when Bayes factors are used with conventional cutoffs to draw conclusions. For instance, some researchers might falsely conclude that there must be good-enough evidence for the interaction if they find good-enough Bayesian evidence for the alternative hypothesis, H1, in condition A and good-enough Bayesian evidence for the null hypothesis, H0, in condition B. The case study we introduce highlights that ignoring the test of the interaction can lead to unjustified conclusions and demonstrates that the principle that any assertion about the existence of an interaction necessitates the direct comparison of the conditions is as true for Bayesian as it is for frequentist statistics. We provide an R script of the analyses of the case study and a Shiny app that can be used with a 2 × 2 design to develop intuitions on this issue, and we introduce a rule of thumb with which one can estimate the sample size one might need to have a well-powered design.},
	pages = {300--308},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Palfi, Bence and Dienes, Zoltan},
	urldate = {2021-03-17},
	date = {2020-09},
	langid = {english},
	file = {Palfi and Dienes - 2020 - Why Bayesian “Evidence for H 1 ”.pdf:/Users/tom/Zotero/storage/6KNJTGR3/Palfi and Dienes - 2020 - Why Bayesian “Evidence for H 1 ”.pdf:application/pdf},
}

@article{schmidt_what_1992,
	title = {What do data really mean? Research findings, meta-analysis, and cumulative knowledge in psychology.},
	volume = {47},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0003-066X.47.10.1173},
	doi = {10.1037/0003-066X.47.10.1173},
	shorttitle = {What do data really mean?},
	pages = {1173--1181},
	number = {10},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Schmidt, Frank L.},
	urldate = {2021-03-17},
	date = {1992-10},
	langid = {english},
	file = {Schmidt - 1992 - What do data really mean Research findings, meta-.pdf:/Users/tom/Zotero/storage/AYNCGP8C/Schmidt - 1992 - What do data really mean Research findings, meta-.pdf:application/pdf},
}

@article{wagenmakers_bayesian_2018,
	title = {Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications},
	volume = {25},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-017-1343-3},
	doi = {10.3758/s13423-017-1343-3},
	shorttitle = {Bayesian inference for psychology. Part I},
	abstract = {Bayesian parameter estimation and Bayesian hypothesis testing present attractive alternatives to classical inference using confidence intervals and p values. In part I of this series we outline ten prominent advantages of the Bayesian approach. Many of these advantages translate to concrete opportunities for pragmatic researchers. For instance, Bayesian hypothesis testing allows researchers to quantify evidence and monitor its progression as data come in, without needing to know the intention with which the data were collected. We end by countering several objections to Bayesian hypothesis testing. Part {II} of this series discusses {JASP}, a free and open source software program that makes it easy to conduct Bayesian estimation and testing for a range of popular statistical scenarios (Wagenmakers et al., this issue).},
	pages = {35--57},
	number = {1},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Wagenmakers, Eric-Jan and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Love, Jonathon and Selker, Ravi and Gronau, Quentin F. and Šmíra, Martin and Epskamp, Sacha and Matzke, Dora and Rouder, Jeffrey N. and Morey, Richard D.},
	urldate = {2021-03-17},
	date = {2018-02},
	langid = {english},
	file = {Wagenmakers et al. - 2018 - Bayesian inference for psychology. Part I Theoret.pdf:/Users/tom/Zotero/storage/PDM4ADXX/Wagenmakers et al. - 2018 - Bayesian inference for psychology. Part I Theoret.pdf:application/pdf},
}

@article{miller_laplaces_nodate,
	title = {Laplace’s theories of cognitive illusions, heuristics, and biases},
	doi = {10.2139/ssrn.3149224},
	abstract = {In his book from the early 1800s, Essai Philosophique sur les Probabilit´es, the mathematician Pierre-Simon de Laplace anticipated many ideas developed within the past 50 years in cognitive psychology and behavioral economics, explaining human tendencies to deviate from norms of rationality in the presence of probability and uncertainty. A look at Laplace’s theories and reasoning is striking, both in how modern they seem, how much progress he made without the beneﬁt of systematic experimentation, and the novelty of a few of his unexplored conjectures. We argue that this work points to these theories being more fundamental and less contingent on recent experimental ﬁndings than we might have thought.},
	pages = {18},
	author = {Miller, Joshua B and Gelman, Andrew},
	langid = {english},
	file = {Miller and Gelman - Laplace’s theories of cognitive illusions, heurist.pdf:/Users/tom/Zotero/storage/IZ63JJPS/Miller and Gelman - Laplace’s theories of cognitive illusions, heurist.pdf:application/pdf},
}

@book{laplace_essai_1825,
	location = {New York},
	edition = {5},
	title = {Essai Philosophique sur les Probabilités},
	publisher = {Springer},
	author = {Laplace, P. S.},
	date = {1825},
}

@article{frisch_predictivism_2015,
	title = {Predictivism and old evidence: a critical look at climate model tuning},
	volume = {5},
	issn = {1879-4920},
	url = {https://doi.org/10.1007/s13194-015-0110-4},
	doi = {10.1007/s13194-015-0110-4},
	shorttitle = {Predictivism and old evidence},
	abstract = {Many climate scientists have made claims that may suggest that evidence used in tuning or calibrating a climate model cannot be used to evaluate the model. By contrast, the philosophers Katie Steele and Charlotte Werndl have argued that, at least within the context of Bayesian confirmation theory, tuning is simply an instance of hypothesis testing. In this paper I argue for a weak predictivism and in support of a nuanced reading of climate scientists’ concerns about tuning: there are cases, model-tuning among them, in which predictive successes are more highly confirmatory of a model than accommodation of evidence.},
	pages = {171--190},
	number = {2},
	journaltitle = {European Journal for Philosophy of Science},
	shortjournal = {Euro Jnl Phil Sci},
	author = {Frisch, Mathias},
	urldate = {2021-03-18},
	date = {2015-05-01},
	langid = {english},
	file = {Frisch - 2015 - Predictivism and old evidence a critical look at .pdf:/Users/tom/Zotero/storage/F9GJECKI/Frisch - 2015 - Predictivism and old evidence a critical look at .pdf:application/pdf},
}

@article{leamer_lets_1983,
	title = {Let's take the con out of econometrics},
	volume = {73},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/1803924},
	pages = {31--43},
	number = {1},
	journaltitle = {The American Economic Review},
	author = {Leamer, Edward E.},
	urldate = {2021-03-18},
	date = {1983},
	note = {Publisher: American Economic Association},
	keywords = {⛔ No {DOI} found},
	file = {Leamer - 1983 - Let's Take the Con Out of Econometrics.pdf:/Users/tom/Zotero/storage/CGTLE7PE/Leamer - 1983 - Let's Take the Con Out of Econometrics.pdf:application/pdf},
}

@article{bruner_self-correction_2019,
	title = {Self-correction in science: Meta-analysis, bias and social structure},
	volume = {78},
	issn = {00393681},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S003936811830164X},
	doi = {10.1016/j.shpsa.2019.02.001},
	shorttitle = {Self-correction in science},
	pages = {93--97},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Bruner, Justin P. and Holman, Bennett},
	urldate = {2021-03-18},
	date = {2019-12},
	langid = {english},
	file = {Bruner and Holman - 2019 - Self-correction in science Meta-analysis, bias an.pdf:/Users/tom/Zotero/storage/6FZH9AGI/Bruner and Holman - 2019 - Self-correction in science Meta-analysis, bias an.pdf:application/pdf},
}

@article{korn_likelihood_2006,
	title = {The likelihood as statistical evidence in multiple comparisons in clinical trials: no free lunch},
	volume = {48},
	issn = {0323-3847, 1521-4036},
	url = {http://doi.wiley.com/10.1002/bimj.200510216},
	doi = {10.1002/bimj.200510216},
	shorttitle = {The likelihood as statistical evidence in multiple comparisons in clinical trials},
	abstract = {The likelihood ratio summarizes the strength of statistical evidence for one simple pre-determined hypothesis versus another. However, it does not directly address the multiple comparisons problem. In this paper we discuss some concerns related to the application of likelihood ratio methods to several multiple comparisons issues in clinical trials, in particular, subgroup analysis, multiple variables, interim monitoring, and data driven choice of hypotheses.},
	pages = {346--355},
	number = {3},
	journaltitle = {Biometrical Journal},
	shortjournal = {Biom. J.},
	author = {Korn, Edward L. and Freidlin, Boris},
	urldate = {2021-03-18},
	date = {2006-06},
	langid = {english},
	file = {Korn and Freidlin - 2006 - The Likelihood as Statistical Evidence in Multiple.pdf:/Users/tom/Zotero/storage/T8LR3FHL/Korn and Freidlin - 2006 - The Likelihood as Statistical Evidence in Multiple.pdf:application/pdf},
}

@article{cook_history_1979,
	title = {History of the sleeper effect: Some logical pitfalls in accepting the null hypothesis},
	volume = {86},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/0033-2909.86.4.662},
	shorttitle = {History of the sleeper effect},
	abstract = {The history of research on the sleeper effect prior to 1978 can be divided into 5 stages: (a) initial discovery of the effect, (b) development of the underlying theory, (c) widespread acceptance of the effect and of the discounting cue explanation of it, (d) realization that past operational definitions of the effect were not isomorphic with the conceptual definition, and (e) repeated failure to demonstrate the effect once operational definitions were employed that corresponded to the conceptual definition (P. M. Gillig and A. G. Greenwald, 1974). These failures resulted in an invitation to accept the null hypothesis and to "lay sleeper effect to rest." This article illustrates why it is not justifiable to accept the null hypothesis about the sleeper effect. It is suggested that provisional acceptance of the null hypothesis depends on assuming that all the necessary theoretical, countervailing, statistical, and procedural conditions for an adequate test of the effect have been demonstrably met. It is further suggested that none of the empirical studies prior to 1978 demonstrably succeeded in meeting these conditions. However, adequate tests following the guidelines described by the authors for provisionally accepting the null hypothesis have recently been conducted, and the effect has been repeatedly found. A deductive model of the logical factors that should guide provisional acceptance of the null hypothesis is contrasted with a current model that stresses induction and statistical power analyses. (29 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {662--679},
	number = {4},
	journaltitle = {Psychological Bulletin},
	author = {Cook, Thomas D. and Gruder, Charles L. and Hennigan, Karen M. and Flay, Brian R.},
	date = {1979},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Experimentation, Null Hypothesis Testing, Literature Review, Dependent Variables},
	file = {Cook et al. - 1979 - History of the sleeper effect Some logical pitfal.pdf:/Users/tom/Zotero/storage/XJXNU5FH/Cook et al. - 1979 - History of the sleeper effect Some logical pitfal.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ALQDX2L3/1979-30169-001.html:text/html},
}

@book{mcelreath_statistical_2020,
	location = {Boca Raton},
	edition = {2},
	title = {Statistical rethinking: a Bayesian course with examples in R and Stan},
	isbn = {978-0-367-13991-9},
	series = {{CRC} texts in statistical science},
	shorttitle = {Statistical rethinking},
	abstract = {"Statistical Rethinking: A Bayesian Course with Examples in R and Stan, Second Edition builds knowledge/confidence in statistical modeling. Pushes readers to perform step-by-step calculations (usually automated.) Unique, computational approach ensures readers understand details to make reasonable choices and interpretations in their modeling work"--},
	publisher = {Taylor and Francis, {CRC} Press},
	author = {{McElreath}, Richard},
	date = {2020},
	file = {McElreath - 2020 - Statistical rethinking a Bayesian course with exa.pdf:/Users/tom/Zotero/storage/UB36NWAB/McElreath - 2020 - Statistical rethinking a Bayesian course with exa.pdf:application/pdf},
}

@report{huntington-klein_influence_2020,
	location = {Rochester, {NY}},
	title = {The influence of hidden researcher decisions in applied microeconomics},
	url = {https://papers.ssrn.com/abstract=3602409},
	abstract = {Researchers make hundreds of decisions about data collection, preparation, and analysis in their research. We use a many-analysts approach to measure the extent and impact of these decisions. Two published causal empirical results are replicated by seven replicators each. We find large differences in data preparation and analysis decisions, many of which would not likely be reported in a publication. No two replicators reported the same sample size. Statistical significance varied across replications, and for one of the studies the effect's sign varied as well. The standard deviation of estimates across replications was 3-4 times the typical reported standard error.},
	number = {{ID} 3602409},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Huntington-Klein, Nick and Arenas, Andreu and Beam, Emily and Bertoni, Marco and Bloem, Jeffrey and Burli, Pralhad H. and Chen, Naibin and Grieco, Paul L. E. and Ekpe, Godwin and Pugatch, Todd and Saavedra, Martin Hugo and Stopnitzky, Yaniv},
	urldate = {2021-03-22},
	date = {2020-05-19},
	langid = {english},
	keywords = {replication, metascience, research},
	file = {Huntington-Klein et al. - 2020 - The influence of hidden researcher decisions in ap.pdf:/Users/tom/Zotero/storage/MIRLI59H/Huntington-Klein et al. - 2020 - The influence of hidden researcher decisions in ap.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DWEGGH7D/papers.html:text/html},
}

@incollection{egger_problems_2001,
	location = {London, {UK}},
	title = {Problems and limitations in conducting systematic reviews},
	isbn = {978-0-470-69392-6 978-0-7279-1488-0},
	url = {http://doi.wiley.com/10.1002/9780470693926.ch3},
	pages = {43--68},
	booktitle = {Systematic Reviews in Health Care},
	publisher = {{BMJ} Publishing Group},
	author = {Egger, Matthias and Dickersin, Kay and Smith, George Davey},
	editor = {Egger, Matthias and Smith, George Davey and Altman, Douglas G},
	urldate = {2021-03-22},
	date = {2001-01-01},
	langid = {english},
	doi = {10.1002/9780470693926.ch3},
	file = {Egger et al. - 2001 - Problems and Limitations in Conducting Systematic .pdf:/Users/tom/Zotero/storage/WWMDDBZK/Egger et al. - 2001 - Problems and Limitations in Conducting Systematic .pdf:application/pdf},
}

@article{albright_experimental_2000,
	title = {Experimental validity: Brunswik, Campbell, Cronbach, and enduring Issues},
	volume = {4},
	issn = {1089-2680},
	url = {https://doi.org/10.1037/1089-2680.4.4.337},
	doi = {10.1037/1089-2680.4.4.337},
	shorttitle = {Experimental Validity},
	abstract = {Donald Campbell and Lee Cronbach had a long history of mutual respect for and fundamental disagreement with each other's ideas about experimental validity. Issues that Campbell labeled as external validity, Cronbach labeled internal validity. Issues that Campbell labeled internal validity, Cronbach suggested are trivial. Nevertheless, these methodological pioneers share much common ground, in part because of their alliance with Egon Brunswik. As science moved from a deterministic to a probabilistic paradigm, all 3 endeavored to protect behavioral science from validity-threatening practices that could result from naive use of the Fisherian approach to scientific investigation. This review shows that issues concerning the prioritization of types of validity still need to be resolved and that most social scientists do not understand internal validity. Several empirical practices for enhancing validity are suggested.},
	pages = {337--353},
	number = {4},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Albright, Linda and Malloy, Thomas E.},
	urldate = {2021-03-22},
	date = {2000-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/GLSHG8QS/Albright and Malloy - 2000 - Experimental Validity Brunswik, Campbell, Cronbac.pdf:application/pdf},
}

@report{yarkoni_implicit_2020,
	title = {Implicit realism impedes progress in psychology: Comment on Fried (2020)},
	url = {https://psyarxiv.com/xj5uq/},
	shorttitle = {Implicit realism impedes progress in psychology},
	abstract = {Fried (in press) argues that progress in the factor and network modeling literatures is currently impeded by inadequate theory development. Here I take issue with this conclusion, focusing on two broad concerns. First, I argue that much of Fried's criticism of previous work (e.g., of general factor models) reflects a particular set of aesthetic preferences and priorities that other researchers are under no obligation to share. Second, I argue that Fried’s central argument tacitly assumes a strong realism about psychological constructs that is difficult to defend, and has deleterious practical consequences. When stripped of its realist commitments, Fried’s paper provides the reader with little reason to suppose that improved theory development would do much to facilitate progress in psychology. I suggest that applied psychologists may want to consider an alternative possibility---namely, that models constructed at a psychological level of description are simply not very conducive to the production of effective real-world predictions or interventions.},
	institution = {{PsyArXiv}},
	author = {Yarkoni, Tal},
	urldate = {2021-03-22},
	date = {2020-09-24},
	doi = {10.31234/osf.io/xj5uq},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, Quantitative Methods, psychology, Theory and Philosophy of Science, theory, causality, instrumentalism, realism, turtles},
	file = {Full Text PDF:/Users/tom/Zotero/storage/BVV3ANUY/Yarkoni - 2020 - Implicit realism impedes progress in psychology C.pdf:application/pdf},
}

@article{polanin_review_2017,
	title = {A Review of Meta-Analysis Packages in R},
	volume = {42},
	issn = {1076-9986},
	url = {https://doi.org/10.3102/1076998616674315},
	doi = {10.3102/1076998616674315},
	abstract = {Meta-analysis is a statistical technique that allows an analyst to synthesize effect sizes from multiple primary studies. To estimate meta-analysis models, the open-source statistical environment R is quickly becoming a popular choice. The meta-analytic community has contributed to this growth by developing numerous packages specific to meta-analysis. The purpose of this study is to locate all publicly available meta-analytic R packages. We located 63 packages via a comprehensive online search. To help elucidate these functionalities to the field, we describe each of the packages, recommend applications for researchers interested in using R for meta-analyses, provide a brief tutorial of two meta-analysis packages, and make suggestions for future meta-analytic R package creators.},
	pages = {206--242},
	number = {2},
	journaltitle = {Journal of Educational and Behavioral Statistics},
	shortjournal = {Journal of Educational and Behavioral Statistics},
	author = {Polanin, Joshua R. and Hennessy, Emily A. and Tanner-Smith, Emily E.},
	urldate = {2021-03-22},
	date = {2017-04-01},
	langid = {english},
	note = {Publisher: American Educational Research Association},
	keywords = {meta-analysis, R, effect size, statistical software},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/T6Q5RYZJ/Polanin et al. - 2017 - A Review of Meta-Analysis Packages in R.pdf:application/pdf},
}

@online{noauthor_synthetic_nodate,
	title = {A synthetic dataset primer for the biobehavioural sciences to promote reproducibility and hypothesis generation {\textbar} {eLife}},
	url = {https://elifesciences.org/articles/53275},
	urldate = {2021-03-22},
	file = {A synthetic dataset primer for the biobehavioural sciences to promote reproducibility and hypothesis generation | eLife:/Users/tom/Zotero/storage/6SWL63AN/53275.html:text/html},
}

@article{bradley_reducing_2020,
	title = {Reducing bias and improving transparency in medical research: a critical overview of the problems, progress and suggested next steps},
	volume = {113},
	issn = {0141-0768},
	url = {https://doi.org/10.1177/0141076820956799},
	doi = {10.1177/0141076820956799},
	shorttitle = {Reducing bias and improving transparency in medical research},
	abstract = {In recent years there has been increasing awareness of problems that have undermined trust in medical research. This review outlines some of the most important issues including research culture, reporting biases, and statistical and methodological issues. It examines measures that have been instituted to address these problems and explores the success and limitations of these measures. The paper concludes by proposing three achievable actions which could be implemented to deliver significantly improved transparency and mitigation of bias. These measures are as follows: (1) mandatory registration of interests by those involved in research; (2) that journals support the ?registered reports? publication format; and (3) that comprehensive study documentation for all publicly funded research be made available on a World Health Organization research repository. We suggest that achieving such measures requires a broad-based campaign which mobilises public opinion. We invite readers to feedback on the proposed actions and to join us in calling for their implementation.},
	pages = {433--443},
	number = {11},
	journaltitle = {Journal of the Royal Society of Medicine},
	shortjournal = {J R Soc Med},
	author = {Bradley, Stephen H and {DeVito}, Nicholas J and Lloyd, Kelly E and Richards, Georgia C and Rombey, Tanja and Wayant, Cole and Gill, Peter J},
	urldate = {2021-03-22},
	date = {2020-11-01},
	note = {Publisher: {SAGE} Publications},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/PSXS2QLM/Bradley et al. - 2020 - Reducing bias and improving transparency in medica.pdf:application/pdf},
}

@report{simon_comparing_2020,
	title = {Comparing the vibration of effects due to model, data pre-processing and sampling uncertainty on a large data set in personality psychology},
	url = {https://psyarxiv.com/c7v8b/},
	abstract = {Researchers have great flexibility in the analysis of observational data. If combined with selective reporting and pressure to publish, this flexibility can have devastating consequences on the validity of research findings. We extend the recently proposed vibration of effects approach to provide a framework comparing three main sources of  uncertainty which lead to instability in observational associations, namely data pre-processing, model and sampling uncertainty. We analyze their behavior for varying sample sizes for two associations in personality psychology. While all types of vibration show a decrease for increasing sample sizes, data pre-processing and model vibration remain non-negligible, even for a sample of over 80000 participants. The increasing availability of large data sets that are not initially recorded for research purposes can make data pre-processing and model choices very influential. We therefore recommend the framework as a tool for the transparent reporting of the stability of research findings.},
	institution = {{PsyArXiv}},
	author = {Simon and Schönbrodt, Felix and Patel, Chirag J. and Ioannidis, John and Boulesteix, Anne-Laure and Hoffmann, Sabine},
	urldate = {2021-03-22},
	date = {2020-06-05},
	doi = {10.31234/osf.io/c7v8b},
	note = {type: article},
	keywords = {Meta-science, replicability, metascience, Big Five, researcher degrees of freedom, stability},
	file = {Full Text PDF:/Users/tom/Zotero/storage/342IP5BS/Simon et al. - 2020 - Comparing the vibration of effects due to model, d.pdf:application/pdf},
}

@article{penders_rinse_2019,
	title = {Rinse and repeat: understanding the value of replication across different ways of knowing},
	volume = {7},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2304-6775/7/3/52},
	doi = {10.3390/publications7030052},
	shorttitle = {Rinse and repeat},
	abstract = {The increasing pursuit of replicable research and actual replication of research is a political project that articulates a very specific technology of accountability for science. This project was initiated in response to concerns about the openness and trustworthiness of science. Though applicable and valuable in many fields, here we argue that this value cannot be extended everywhere, since the epistemic content of fields, as well as their accountability infrastructures, differ. Furthermore, we argue that there are limits to replicability across all fields; but in some fields, including parts of the humanities, these limits severely undermine the value of replication to account for the value of research.},
	pages = {52},
	number = {3},
	journaltitle = {Publications},
	author = {Penders, Bart and Holbrook, J. Britt and de Rijcke, Sarah},
	urldate = {2021-03-22},
	date = {2019-09},
	langid = {english},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {reproducibility, replication, Replicability, accountability, epistemic pluralism, humanities, reproduction},
	file = {Full Text PDF:/Users/tom/Zotero/storage/8QQFHJ7I/Penders et al. - 2019 - Rinse and Repeat Understanding the Value of Repli.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VWMZPDTS/52.html:text/html},
}

@article{barwich_value_2019,
	title = {The value of failure in science: the story of grandmother cells in neuroscience},
	volume = {13},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2019.01121/full},
	doi = {10.3389/fnins.2019.01121},
	shorttitle = {The value of failure in science},
	abstract = {The annals of science are filled with successes. In footnotes do we hear about the failures, the cul-de-sacs, and the forgotten ideas. Failure is how research advances. Yet it hardly features in theoretical perspectives on science. That is a mistake. Failures, whether clear-cut or ambiguous, are heuristically fruitful in their own right. Thinking about failure questions our measures of success, including the conceptual foundations of current practice, that can only be transient in an experimental context. This article advances the heuristics of failure analysis, meaning the explicit treatment of certain ideas or models as failures. The value of failures qua being a failure is illustrated with the example of grandmother cells; the contested idea of a hypothetical neuron that encodes a highly specific but complex stimulus, such as the image of one’s grandmother. Repeatedly evoked in popular science and maintained in textbooks, there is sufficient reason to critically review the theoretical and empirical background of this idea.},
	journaltitle = {Frontiers in Neuroscience},
	shortjournal = {Front. Neurosci.},
	author = {Barwich, Ann-Sophie},
	urldate = {2021-03-22},
	date = {2019},
	note = {Publisher: Frontiers},
	keywords = {Gnostic units, grandmother cells, History of Science, Localist theory, localization, Model pluralism, object recogntion, Philosphy of science, Sparse Coding},
	file = {Full Text PDF:/Users/tom/Zotero/storage/98WFLUCM/Barwich - 2019 - The Value of Failure in Science The Story of Gran.pdf:application/pdf},
}

@article{barendregt_meta-analysis_2013,
	title = {Meta-analysis of prevalence},
	volume = {67},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {0143-005X, 1470-2738},
	url = {https://jech.bmj.com/content/67/11/974},
	doi = {10.1136/jech-2013-203104},
	abstract = {Meta-analysis is a method to obtain a weighted average of results from various studies. In addition to pooling effect sizes, meta-analysis can also be used to estimate disease frequencies, such as incidence and prevalence. In this article we present methods for the meta-analysis of prevalence. We discuss the logit and double arcsine transformations to stabilise the variance. We note the special situation of multiple category prevalence, and propose solutions to the problems that arise. We describe the implementation of these methods in the {MetaXL} software, and present a simulation study and the example of multiple sclerosis from the Global Burden of Disease 2010 project. We conclude that the double arcsine transformation is preferred over the logit, and that the {MetaXL} implementation of multiple category prevalence is an improvement in the methodology of the meta-analysis of prevalence.},
	pages = {974--978},
	number = {11},
	journaltitle = {J Epidemiol Community Health},
	shortjournal = {J Epidemiol Community Health},
	author = {Barendregt, Jan J. and Doi, Suhail A. and Lee, Yong Yi and Norman, Rosana E. and Vos, Theo},
	urldate = {2021-03-22},
	date = {2013-11-01},
	langid = {english},
	pmid = {23963506},
	note = {Publisher: {BMJ} Publishing Group Ltd
Section: Theory and methods},
	keywords = {Statistics, Methodology, Meta Analysis},
	file = {Full Text PDF:/Users/tom/Zotero/storage/IQJAH522/Barendregt et al. - 2013 - Meta-analysis of prevalence.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/M7UN88C4/974.html:text/html},
}

@book{wang_how_2018,
	title = {How to Conduct a Meta-Analysis of Proportions in R: A Comprehensive Tutorial},
	shorttitle = {How to Conduct a Meta-Analysis of Proportions in R},
	abstract = {Meta-analysis of proportions is observational and non-comparative in nature. Rarely have we seen a study or tutorial demonstrate how a meta-analysis of proportions should be performed using the R programming language. This tutorial intends to fill this gap. The tutorial consists of two major components: (1) a comprehensive, critical review of the process of conducting a meta-analysis of proportions, in which a number of common practices that possibly lead to biased estimates and misleading inferences are highlighted (e.g., not taking study size and within-group estimates of between-study variance into consideration when calculating mean proportions in the presence of subgroups), and (2) a step-by-step guide to conducting the analysis using R. The process is described in six stages: (1) setting up the R environment and getting a sense of the data being analyzed; (2) calculating effect sizes; (3) identifying and quantifying heterogeneity; (4) constructing forest plots; (5) explaining heterogeneity with moderator analysis; and (6) assessing publication bias. In the last section (assessing publication bias), we argued that funnel plot analyses developed for investigating publication bias in randomized controlled trials may not be suitable for use with meta-analyses of proportions. Three computational options are incorporated in the code for users to choose from to transform proportional data. The presentation of the tutorial is conceptually oriented, the use of formulas is kept to a minimum, and a published meta-analysis of proportions is used as an example to illustrate how to implement the R code and interpret the results of the analysis. Generic R code is provided for readers to use for their analyses.},
	author = {Wang, Naike},
	date = {2018-06-01},
	doi = {10.13140/RG.2.2.27199.00161},
}

@article{lindsay_design_1993,
	title = {The Design of Replicated Studies},
	volume = {47},
	issn = {0003-1305},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1993.10475983},
	doi = {10.1080/00031305.1993.10475983},
	abstract = {Replication is little discussed in the statistical literature nor practiced widely by statistically minded researchers. It is needed not merely to validate one's findings, but more importantly, to establish the increasing range of radically different conditions under which the findings hold, and the predictable exceptions. This article describes how to design highly differentiated replications. The irrelevance and/or impossibility of identical replications are also discussed. Practical illustrations of the success and failure of replicated studies are given.},
	pages = {217--228},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Lindsay, R. Murray and Ehrenberg, A. S. C.},
	urldate = {2021-03-22},
	date = {1993-08-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1993.10475983},
	keywords = {Close replications, Differential replications, One-off studies, Varying more than one condition},
	file = {Snapshot:/Users/tom/Zotero/storage/XSB5E9MM/00031305.1993.html:text/html},
}

@article{altman_statistics_1980-1,
	title = {Statistics and ethics in medical research: {III} How large a sample?},
	volume = {281},
	issn = {0007-1447},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1714734/},
	shorttitle = {Statistics and ethics in medical research},
	pages = {1336--1338},
	number = {6251},
	journaltitle = {British Medical Journal},
	shortjournal = {Br Med J},
	author = {Altman, D G},
	urldate = {2021-03-22},
	date = {1980-11-15},
	pmid = {7437789},
	pmcid = {PMC1714734},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/HUH5YFNP/Altman - 1980 - Statistics and ethics in medical research III How.pdf:application/pdf},
}

@article{altman_statistics_1981,
	title = {Statistics and ethics in medical research. {VIII}-Improving the quality of statistics in medical journals.},
	volume = {282},
	issn = {0267-0623},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1503760/},
	pages = {44--46},
	number = {6257},
	journaltitle = {British Medical Journal (Clinical research ed.)},
	shortjournal = {Br Med J (Clin Res Ed)},
	author = {Altman, D G},
	urldate = {2021-03-22},
	date = {1981-01-03},
	pmid = {6778564},
	pmcid = {PMC1503760},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/8F8C7BUM/Altman - 1981 - Statistics and ethics in medical research. VIII-Im.pdf:application/pdf},
}

@article{altman_statistics_1980-2,
	title = {Statistics and ethics in medical research: study design.},
	volume = {281},
	issn = {0007-1447},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1714604/},
	shorttitle = {Statistics and ethics in medical research},
	pages = {1267--1269},
	number = {6250},
	journaltitle = {British Medical Journal},
	shortjournal = {Br Med J},
	author = {Altman, D G},
	urldate = {2021-03-22},
	date = {1980-11-08},
	pmid = {7000298},
	pmcid = {PMC1714604},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/IJY2Y5CW/Altman - 1980 - Statistics and ethics in medical research study d.pdf:application/pdf},
}

@article{romero_novelty_2017,
	title = {Novelty versus Replicability: Virtues and Vices in the Reward System of Science},
	volume = {84},
	issn = {0031-8248},
	url = {https://www.journals.uchicago.edu/doi/full/10.1086/694005},
	doi = {10.1086/694005},
	shorttitle = {Novelty versus Replicability},
	abstract = {The reward system of science is the priority rule. The first scientist making a new discovery is rewarded with prestige, while second runners get little or nothing. Michael Strevens, following Philip Kitcher, defends this reward system, arguing that it incentivizes an efficient division of cognitive labor. I argue that this assessment depends on strong implicit assumptions about the replicability of findings. I question these assumptions on the basis of metascientific evidence and argue that the priority rule systematically discourages replication. My analysis leads us to qualify Kitcher and Strevens’s contention that a priority-based reward system is normatively desirable for science.},
	pages = {1031--1043},
	number = {5},
	journaltitle = {Philosophy of Science},
	author = {Romero, Felipe},
	urldate = {2021-03-22},
	date = {2017-12-01},
	note = {Publisher: The University of Chicago Press},
	file = {Full Text PDF:/Users/tom/Zotero/storage/2AMKXL5D/Romero - 2017 - Novelty versus Replicability Virtues and Vices in.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/FU575HC5/694005.html:text/html},
}

@online{noauthor_why_nodate,
	title = {Why researchers should share their analytic code {\textbar} The {BMJ}},
	url = {https://www.bmj.com/content/367/bmj.l6365},
	urldate = {2021-03-22},
	file = {Why researchers should share their analytic code | The BMJ:/Users/tom/Zotero/storage/PGGG4PZ3/bmj.html:text/html},
}

@article{thompson_dataset_2020,
	title = {Dataset decay and the problem of sequential analyses on open datasets},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.53498},
	doi = {10.7554/eLife.53498},
	abstract = {Open data allows researchers to explore pre-existing datasets in new ways. However, if many researchers reuse the same dataset, multiple statistical testing may increase false positives. Here we demonstrate that sequential hypothesis testing on the same dataset by multiple researchers can inflate error rates. We go on to discuss a number of correction procedures that can reduce the number of false positives, and the challenges associated with these correction procedures.},
	pages = {e53498},
	journaltitle = {{eLife}},
	author = {Thompson, William Hedley and Wright, Jessey and Bissett, Patrick G and Poldrack, Russell A},
	editor = {Rodgers, Peter and Baker, Chris I and Holmes, Nick and Baker, Chris I and Rousselet, Guillaume A},
	urldate = {2021-03-22},
	date = {2020-05-19},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {multiple comparisons, open data, meta-research, multiple comparison correction, sequential testing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NZEITIRG/Thompson et al. - 2020 - Dataset decay and the problem of sequential analys.pdf:application/pdf},
}

@online{lavelle_when_2020,
	title = {When a crisis becomes an opportunity: the role of replications in making better theories},
	url = {http://philsci-archive.pitt.edu/17122/},
	shorttitle = {When a crisis becomes an opportunity},
	titleaddon = {British Journal for the Philosophy of Science},
	type = {Published Article or Volume},
	author = {Lavelle, Jane Suilin},
	urldate = {2021-03-22},
	date = {2020},
	langid = {english},
	note = {{ISSN}: 1464-3537},
	file = {Full Text PDF:/Users/tom/Zotero/storage/SCUL69V6/Lavelle - 2020 - When a crisis becomes an opportunity the role of .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/U44B2KBI/17122.html:text/html},
}

@article{adda_p-hacking_2020,
	title = {P-hacking in clinical trials and how incentives shape the distribution of results across phases},
	volume = {117},
	rights = {Copyright © 2020 the Author(s). Published by {PNAS}.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-{NonCommercial}-{NoDerivatives} License 4.0 ({CC} {BY}-{NC}-{ND}).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/24/13386},
	doi = {10.1073/pnas.1919906117},
	abstract = {Clinical research should conform to high standards of ethical and scientific integrity, given that human lives are at stake. However, economic incentives can generate conflicts of interest for investigators, who may be inclined to withhold unfavorable results or even tamper with data in order to achieve desired outcomes. To shed light on the integrity of clinical trial results, this paper systematically analyzes the distribution of P values of primary outcomes for phase {II} and phase {III} drug trials reported to the {ClinicalTrials}.gov registry. First, we detect no bunching of results just above the classical 5\% threshold for statistical significance. Second, a density-discontinuity test reveals an upward jump at the 5\% threshold for phase {III} results by small industry sponsors. Third, we document a larger fraction of significant results in phase {III} compared to phase {II}. Linking trials across phases, we find that early favorable results increase the likelihood of continuing into the next phase. Once we take into account this selective continuation, we can explain almost completely the excess of significant results in phase {III} for trials conducted by large industry sponsors. For small industry sponsors, instead, part of the excess remains unexplained.},
	pages = {13386--13392},
	number = {24},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Adda, Jérôme and Decker, Christian and Ottaviani, Marco},
	urldate = {2021-03-22},
	date = {2020-06-16},
	langid = {english},
	pmid = {32487730},
	note = {Publisher: National Academy of Sciences
Section: Social Sciences},
	keywords = {p-hacking, clinical trials, drug development, economic incentives in research, selective reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/UAJD5AJS/Adda et al. - 2020 - P-hacking in clinical trials and how incentives sh.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/2MEEMW28/13386.html:text/html},
}

@software{ortiz-bobea_arielortizbobeaspec_chart_2021,
	title = {{ArielOrtizBobea}/spec\_chart},
	url = {https://github.com/ArielOrtizBobea/spec_chart},
	abstract = {Generate a specification chart in base R (with reproducible example)},
	author = {Ortiz-Bobea, Ariel},
	urldate = {2021-03-23},
	date = {2021-03-22},
	note = {original-date: 2020-03-10T23:06:42Z},
}

@book{howson_scientific_2006,
	location = {Chicago},
	title = {Scientific Reasoning: The Bayesian Approach},
	publisher = {Open Court},
	author = {Howson, Colin and Urbach, Peter},
	date = {2006},
	langid = {english},
	file = {Howson and Urbach - Scientific Reasoning The Bayesian Approach.pdf:/Users/tom/Zotero/storage/G8WQUU46/Howson and Urbach - Scientific Reasoning The Bayesian Approach.pdf:application/pdf},
}

@article{goodman_multiple_1998,
	title = {Multiple comparisons, explained},
	volume = {147},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/oxfordjournals.aje.a009531},
	doi = {10.1093/oxfordjournals.aje.a009531},
	pages = {807--812},
	number = {9},
	journaltitle = {American Journal of Epidemiology},
	shortjournal = {American Journal of Epidemiology},
	author = {Goodman, Steven N.},
	urldate = {2021-03-24},
	date = {1998-05-01},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MF7U5CGL/Goodman - 1998 - Multiple Comparisons, Explained.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/NXSRTYCX/67969.html:text/html},
}

@article{gelman_philosophy_2013,
	title = {Philosophy and the practice of Bayesian statistics},
	volume = {66},
	issn = {2044-8317},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.2011.02037.x},
	doi = {https://doi.org/10.1111/j.2044-8317.2011.02037.x},
	abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
	pages = {8--38},
	number = {1},
	journaltitle = {British Journal of Mathematical and Statistical Psychology},
	author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
	urldate = {2021-03-25},
	date = {2013},
	langid = {english},
	note = {\_eprint: https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.2011.02037.x},
	file = {Accepted Version:/Users/tom/Zotero/storage/7V3EZNBQ/Gelman and Shalizi - 2013 - Philosophy and the practice of Bayesian statistics.pdf:application/pdf;gelman2012.pdf:/Users/tom/Zotero/storage/FKWMSD7L/gelman2012.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UXEYI37I/j.2044-8317.2011.02037.html:text/html},
}

@book{salmon_reality_2005,
	location = {Oxford ; New York},
	title = {Reality and rationality},
	isbn = {978-0-19-517784-8 978-0-19-518195-1},
	pagetotal = {285},
	publisher = {Oxford University Press},
	author = {Salmon, Wesley C. and Dowe, Phil and Salmon, Merrilee H.},
	date = {2005},
	langid = {english},
	keywords = {Philosophy, Science},
	file = {Salmon et al. - 2005 - Reality and rationality.pdf:/Users/tom/Zotero/storage/LUXYYH9N/Salmon et al. - 2005 - Reality and rationality.pdf:application/pdf},
}

@article{altman_scandal_1994,
	title = {The scandal of poor medical research},
	volume = {308},
	rights = {© 1994 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/308/6924/283},
	doi = {10.1136/bmj.308.6924.283},
	abstract = {We need less research, better research, and research done for the right reasons

What should we think about a doctor who uses the wrong treatment, either wilfully or through ignorance, or who uses the right treatment wrongly (such as by giving the wrong dose of a drug)? Most people would agree that such behaviour was unprofessional, arguably unethical, and certainly unacceptable.

What, then, should we think about researchers who use the wrong techniques (either wilfully or in ignorance), use the right techniques wrongly, misinterpret their results, report their results selectively, cite the literature selectively, and draw unjustified conclusions? We should be appalled. Yet numerous studies of the medical literature, in both general and specialist journals, have shown that all of the above phenomena are common.1 2 3 4 5 6 7 This is surely a scandal.

When I tell friends outside medicine that many papers published in medical journals are misleading because of methodological weaknesses they are rightly shocked. Huge sums of money are spent annually on research that is seriously flawed through the use of inappropriate designs, unrepresentative samples, small samples, incorrect methods …},
	pages = {283--284},
	number = {6924},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Altman, D. G.},
	urldate = {2021-03-26},
	date = {1994-01-29},
	langid = {english},
	pmid = {8124111},
	note = {Publisher: British Medical Journal Publishing Group
Section: Editorial},
	file = {Full Text:/Users/tom/Zotero/storage/PHATKVE6/Altman - 1994 - The scandal of poor medical research.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VLE2JBHE/283.html:text/html},
}

@article{horton_postpublication_2002,
	title = {Postpublication criticism and the shaping of clinical knowledge},
	volume = {287},
	issn = {0098-7484},
	doi = {10.1001/jama.287.21.2843},
	abstract = {{CONTEXT}: Letters to the editor are an important means for ensuring accountability of authors and editors. They form a part of the postpublication peer review process. I studied the critical footprint made in the medical literature by 3 randomized trials (Hypertension Optimal Treatment [{HOT}], Captopril Prevention Project [{CAPPP}], and Swedish Trial in Old Patients with Hypertension 2 [{STOP}-2]) published in The Lancet and investigated the extent to which that footprint was preserved in shaping clinical knowledge.
{METHODS}: Qualitative appraisal of the criticism of each trial, taken from published letters. Agreed weaknesses and unanswered criticisms were identified from the authors' reply. I searched {MEDLINE} for practice guidelines published after the trial report and sought evidence for incorporation of criticism into these guidelines.
{RESULTS}: From the time of publication to October 2000, {HOT} was cited in 9 of 36 practice guidelines; {CAPPP}, in 6 of 36; and {STOP}-2, not at all. {HOT} received 14 published criticisms, 5 comments, and 3 questions, of which 15 were responded to. Only 1 criticism, lack of power, was referred to in 1 guideline. {CAPPP} received 14 criticisms, 9 comments, and 3 questions, of which 8 were responded to. Only 1 criticism, imbalances between groups, was referred to in 1 guideline. {STOP}-2 received 12 criticisms, 9 comments, and 3 questions, of which only 6 were responded to.
{CONCLUSIONS}: More than half of all criticism made in correspondence went unanswered by authors. Important weaknesses in trials were ignored in subsequently published practice guidelines. Failure to recognize the critical footprint of primary research weakens the validity of guidelines and distorts clinical knowledge.},
	pages = {2843--2847},
	number = {21},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Horton, Richard},
	date = {2002-06-05},
	pmid = {12038929},
	keywords = {Reproducibility of Results, Randomized Controlled Trials as Topic, Clinical Competence, Peer Review, Research, Practice Guidelines as Topic, Publishing},
	file = {Full Text:/Users/tom/Zotero/storage/BZT2V2E6/Horton - 2002 - Postpublication criticism and the shaping of clini.pdf:application/pdf},
}

@article{von_elm_role_2009,
	title = {The role of correspondence sections in post-publication peer review: A bibliometric study of general and internal medicine journals},
	volume = {81},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-009-2236-0},
	doi = {10.1007/s11192-009-2236-0},
	shorttitle = {The role of correspondence sections in post-publication peer review},
	abstract = {Scientific journals claim that correspondence sections are for post-publication peer review. We compared the conditions for submission and the bibliometrics of letters-to-editors published in leading medical journals in 2002 and 2007 using journal-derived information and data from {PubMed} and Journal Citation Reports. The median time limit for letter submissions decreased from 6 to 3.5 weeks, the median word limit from 400 to 350. The median number of letters per published article was near one in both years. Only about half of the letters were followed by an author reply in either year. Electronic response systems were available for four journals in 2007.},
	pages = {747},
	number = {3},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Von Elm, Erik and Wandel, Simon and Jüni, Peter},
	urldate = {2021-03-26},
	date = {2009-04-17},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/AR54CBFH/Von Elm et al. - 2009 - The role of correspondence sections in post-public.pdf:application/pdf},
}

@article{athey_measure_2015,
	title = {A measure of robustness to misspecification},
	volume = {105},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20151020},
	doi = {10.1257/aer.p20151020},
	abstract = {Researchers often report estimates and standard errors for the object of interest (such as a treatment effect) based on a single specification of a statistical model. We propose a systematic approach to assessing sensitivity to specification. We construct estimates of the object of interest for each of a large set of models. Our proposed robustness measure is the standard deviation of the point estimates over the set of models. Each member of the set is generated by splitting the sample into two subsamples based on covariate values, constructing separate parameter estimates for each subsample, and then combining the results.},
	pages = {476--480},
	number = {5},
	journaltitle = {American Economic Review},
	author = {Athey, Susan and Imbens, Guido},
	urldate = {2021-03-27},
	date = {2015-05},
	langid = {english},
	keywords = {and Selection, Model Evaluation, Validation},
	file = {Snapshot:/Users/tom/Zotero/storage/QJZNH986/articles.html:text/html},
}

@book{feynman_surely_1985,
	location = {New York},
	title = {"Surely you're joking, Mr. Feynman!": adventures of a curious character},
	shorttitle = {"Surely you're joking, Mr. Feynman!"},
	publisher = {W.W. Norton},
	author = {Feynman, Richard Phillips},
	date = {1985},
	langid = {english},
	file = {Feynman et al. - 1988 - Surely you're joking, Mr. Feynman! adventures o.pdf:/Users/tom/Zotero/storage/GH9M5FJR/Feynman et al. - 1988 - Surely you're joking, Mr. Feynman! adventures o.pdf:application/pdf},
}

@article{olive_review_2014,
	title = {Review of particle physics},
	volume = {38},
	issn = {1674-1137},
	url = {https://iopscience.iop.org/article/10.1088/1674-1137/38/9/090001},
	doi = {10.1088/1674-1137/38/9/090001},
	abstract = {The Review summarizes much of particle physics and cosmology. Using data from previous editions, plus 3,283 new measurements from 899 papers, we list, evaluate, and average measured properties of gauge bosons and the recently discovered Higgs boson, leptons, quarks, mesons, and baryons. We summarize searches for hypothetical particles such as heavy neutrinos, supersymmetric and technicolor particles, axions, dark photons, etc. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, ﬁgures, formulae, and reviews of topics such as Supersymmetry, Extra Dimensions, Particle Detectors, Probability, and Statistics. Among the 112 reviews are many that are new or heavily revised including those on: Dark Energy, Higgs Boson Physics, Electroweak Model, Neutrino Cross Section Measurements, Monte Carlo Neutrino Generators, Top Quark, Dark Matter, Dynamical Electroweak Symmetry Breaking, Accelerator Physics of Colliders, High-Energy Collider Parameters, Big Bang Nucleosynthesis, Astrophysical Constants and Cosmological Parameters. A booklet is available containing the Summary Tables and abbreviated versions of some of the other sections of this full Review. All tables, listings, and reviews (and errata) are also available on the Particle Data Group website: http://pdg.lbl.gov.},
	pages = {090001},
	number = {9},
	journaltitle = {Chinese Physics C},
	shortjournal = {Chinese Phys. C},
	author = {Olive, K.A.},
	urldate = {2021-03-27},
	date = {2014-08},
	langid = {english},
	file = {Olive - 2014 - Review of Particle Physics.pdf:/Users/tom/Zotero/storage/Q3KVKD4L/Olive - 2014 - Review of Particle Physics.pdf:application/pdf},
}

@article{lyons_open_2008,
	title = {Open statistical issues in Particle Physics},
	volume = {2},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Open-statistical-issues-in-Particle/10.1214/08-AOAS163.full},
	doi = {10.1214/08-AOAS163},
	abstract = {Many statistical issues arise in the analysis of Particle Physics experiments. We give a brief introduction to Particle Physics, before describing the techniques used by Particle Physicists for dealing with statistical problems, and also some of the open statistical questions.},
	pages = {887--915},
	number = {3},
	journaltitle = {The Annals of Applied Statistics},
	author = {Lyons, Louis},
	urldate = {2021-03-27},
	date = {2008-09},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {blind analysis, combining results, goodness of fit, Hypothesis testing, nuisance parameters, P-values, parameter determination, Particle Physics, signal-background separation, upper limits},
	file = {Full Text PDF:/Users/tom/Zotero/storage/IYD59AK8/Lyons - 2008 - Open statistical issues in Particle Physics.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/M3UL5CTG/08-AOAS163.html:text/html},
}

@article{klayman_confirmation_1987,
	title = {Confirmation, disconfirmation, and information in hypothesis testing},
	volume = {94},
	pages = {211--228},
	number = {2},
	journaltitle = {Psychological Review},
	author = {Klayman, Joshua and Ha, Young-Won},
	date = {1987},
	langid = {english},
	file = {Klayman and Ha - Confirmation, Disconfirmation, and Information in .pdf:/Users/tom/Zotero/storage/TITFQUJP/Klayman and Ha - Confirmation, Disconfirmation, and Information in .pdf:application/pdf},
}

@article{henrion_assessing_2014,
	title = {Assessing uncertainty in physical constants},
	volume = {54},
	pages = {791--798},
	number = {9},
	journaltitle = {American Journal of Physics},
	author = {Henrion, Max and Fischhoff, Baruch},
	date = {2014},
	langid = {english},
	file = {Henrion and Fischhoff - 2014 - Assessing uncertainty in physical constants.pdf:/Users/tom/Zotero/storage/RLAV2AQZ/Henrion and Fischhoff - 2014 - Assessing uncertainty in physical constants.pdf:application/pdf},
}

@article{wilson_mental_1994,
	title = {Mental contamination and mental correction: Unwanted influences on judgments and evaluations.},
	volume = {116},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.116.1.117},
	doi = {10.1037/0033-2909.116.1.117},
	shorttitle = {Mental contamination and mental correction},
	pages = {117--142},
	number = {1},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Wilson, Timothy D. and Brekke, Nancy},
	urldate = {2021-03-27},
	date = {1994},
	langid = {english},
	file = {Wilson and Brekke - 1994 - Mental contamination and mental correction Unwant.pdf:/Users/tom/Zotero/storage/ZAEYAP39/Wilson and Brekke - 1994 - Mental contamination and mental correction Unwant.pdf:application/pdf},
}

@article{lash_heuristic_2007,
	title = {Heuristic thinking and inference from observational epidemiology},
	volume = {18},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/Fulltext/2007/01000/Heuristic_Thinking_and_Inference_From.13.aspx?casa_token=t40QCwpvPOkAAAAA:8dCFv27xd8VxoSgU-dpkE75--kRhXHYdNr-Hsnl1mYDu3MWm8dGzWqHEQ4SNw3_7Itdx6NF3o1RyV_NGHQAesrJAH7A8},
	doi = {10.1097/01.ede.0000249522.75868.16},
	abstract = {Epidemiologic research is an exercise in measurement. Observational epidemiologic results usually include a point estimate, a measure of random error such as a frequentist confidence interval, and a qualitative discussion of study limitations. Without randomization of study subjects to exposure groups, inference from study results requires an educated guess about the strength of the systematic errors compared with the strength of the exposure effects. Although quantitative methods to make these educated guesses exist, the conventional approach is qualitative, which reduces the educated guessing to a problem of reasoning under uncertainty. In circumstances such as these, humans predictably reason poorly. Heuristics and resulting biases that simplify the judgmental tasks tend to underestimate the systematic error, underestimate the uncertainty, and focus the inference on the study’s specific evidence while excluding countervailing external information. Common warnings to interpret results with trepidation are an ineffective solution. The methods that quantify systematic error and uncertainty challenge the analyst to specify the alternative explanations for associations that are otherwise too readily judged causal.},
	pages = {67--72},
	number = {1},
	journaltitle = {Epidemiology},
	author = {Lash, Timothy L.},
	urldate = {2021-03-27},
	date = {2007-01},
	langid = {american},
	file = {Lash - 2007 - Heuristic thinking and inference from observationa.pdf:/Users/tom/Zotero/storage/I7IPGC4R/Lash - 2007 - Heuristic thinking and inference from observationa.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PZMMJP7Q/Heuristic_Thinking_and_Inference_From.13.html:text/html},
}

@article{jeng_selected_2006,
	title = {A selected history of expectation bias in physics},
	volume = {74},
	issn = {0002-9505},
	url = {https://aapt.scitation.org/doi/full/10.1119/1.2186333},
	doi = {10.1119/1.2186333},
	abstract = {The beliefs of physicists can bias their results toward their expectations in a number of ways. We survey a variety of historical cases of expectation bias in observations, experiments, and calculations.},
	pages = {578--583},
	number = {7},
	journaltitle = {American Journal of Physics},
	shortjournal = {American Journal of Physics},
	author = {Jeng, Monwhea},
	urldate = {2021-03-27},
	date = {2006-07-01},
	note = {Publisher: American Association of Physics Teachers},
	file = {Jeng - 2006 - A selected history of expectation bias in physics.pdf:/Users/tom/Zotero/storage/JN9E8J63/Jeng - 2006 - A selected history of expectation bias in physics.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/EY77UZDU/1.html:text/html},
}

@article{rosenthal_how_1978,
	title = {How often are our numbers wrong?},
	volume = {33},
	issn = {1935-990X(Electronic),0003-066X(Print)},
	doi = {10.1037/0003-066X.33.11.1005},
	abstract = {Located 21 studies that permitted an estimate of the frequency of recording errors and/or the degree to which errors were nonrandom. The studies, summarizing data from over 300 observers making about 140,000 observations, suggest that about 1\% of all observations may be in error and that about two thirds of all errors favor the hypothesis of the observer. (19 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {1005--1008},
	number = {11},
	journaltitle = {American Psychologist},
	author = {Rosenthal, Robert},
	date = {1978},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Errors, Empirical Methods},
	file = {Rosenthal - 1978 - How often are our numbers wrong.pdf:/Users/tom/Zotero/storage/FBSZQHME/Rosenthal - 1978 - How often are our numbers wrong.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8C5LAM6L/1979-25044-001.html:text/html},
}

@article{worrall_pressure_1982,
	title = {The pressure of light: The strange case of the vacillating ‘crucial experiment’},
	volume = {13},
	issn = {0039-3681},
	url = {https://www.sciencedirect.com/science/article/pii/0039368182900231},
	doi = {10.1016/0039-3681(82)90023-1},
	shorttitle = {The pressure of light},
	pages = {133--171},
	number = {2},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Worrall, John},
	urldate = {2021-03-27},
	date = {1982-06-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/ULY6X2WI/Worrall - 1982 - The pressure of light The strange case of the vac.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/V4TL78NK/0039368182900231.html:text/html},
}

@book{franklin_selectivity_2002,
	location = {Pittsburgh, Pa},
	title = {Selectivity and discord: two problems of experiment},
	isbn = {978-0-8229-4191-0},
	shorttitle = {Selectivity and discord},
	pagetotal = {290},
	publisher = {University of Pittsburgh Press},
	author = {Franklin, Allan},
	date = {2002},
	keywords = {Experiments, Philosophy, Science, History, Physics},
}

@article{jeng_bandwagon_2007,
	title = {Bandwagon effects and error bars in particle physics},
	volume = {571},
	issn = {0168-9002},
	url = {https://www.sciencedirect.com/science/article/pii/S0168900206022753},
	doi = {10.1016/j.nima.2006.11.024},
	abstract = {We study historical records of experiments on particle masses, lifetimes, and widths, both for signs of expectation bias, and to compare actual errors with reported error bars. We show that significant numbers of particle properties exhibit “bandwagon effects”: reported values show trends and clustering as a function of the year of publication, rather than random scatter about the mean. While the total amount of clustering is significant, it is also fairly small; most individual particle properties do not display obvious clustering. When differences between experiments are compared with the reported error bars, the deviations do not follow a normal distribution, but instead follow an exponential distribution for up to ten standard deviations.},
	pages = {704--708},
	number = {3},
	journaltitle = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
	shortjournal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
	author = {Jeng, Monwhea},
	urldate = {2021-03-27},
	date = {2007-02-11},
	langid = {english},
	keywords = {Bias, Measurement, Bandwagon, Error bars},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/SDWCSSHN/Jeng - 2007 - Bandwagon effects and error bars in particle physi.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/UT53Z5ID/S0168900206022753.html:text/html},
}

@article{westfall_newton_1973,
	title = {Newton and the Fudge Factor},
	volume = {179},
	rights = {1973 by the American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/179/4075/751},
	doi = {10.1126/science.179.4075.751},
	pages = {751--758},
	number = {4075},
	journaltitle = {Science},
	author = {Westfall, Richard S.},
	urldate = {2021-03-27},
	date = {1973-02-23},
	langid = {english},
	pmid = {17806284},
	note = {Publisher: American Association for the Advancement of Science
Section: Articles},
	file = {Snapshot:/Users/tom/Zotero/storage/IKEPILBT/751.html:text/html},
}

@article{hon_towards_1989,
	title = {Towards a typology of experimental errors: An epistemological view},
	volume = {20},
	issn = {0039-3681},
	url = {https://www.sciencedirect.com/science/article/pii/0039368189900204},
	doi = {10.1016/0039-3681(89)90020-4},
	shorttitle = {Towards a typology of experimental errors},
	abstract = {This paper is concerned with the problem of experimental error. The prevalent view that experimental errors can be dismissed as a tiresome but trivial blemish on the method of experimentation is criticized. It is stressed that the occurrence of errors in experiments constitutes a permanent feature of the attempt to test theories in the physical world, and this feature deserves proper attention. It is suggested that a classification of types of experimental error may be useful as a heuristic device in studying the nature of these errors. However, the standard classification of systematic and random errors is mathematically based does not focus on the causes of the errors, their origins, or the contexts in which they arise. A new typology of experimental errors is therefore proposed whose criterion is epistemological. This typology reflects the various stages that can be discerned in the execution of an experiment, each stage constituting a category of a certain type of experimental error. The proposed classification consists of four categories which are illustrated by historical cases.},
	pages = {469--504},
	number = {4},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Hon, Giora},
	urldate = {2021-03-27},
	date = {1989-12-01},
	langid = {english},
	keywords = {toread},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/ZVH82DEV/Hon - 1989 - Towards a typology of experimental errors An epis.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/B2GWLQ72/0039368189900204.html:text/html},
}

@article{machery_mistaken_2021,
	title = {A mistaken confidence in data},
	volume = {11},
	issn = {1879-4920},
	url = {https://doi.org/10.1007/s13194-021-00354-9},
	doi = {10.1007/s13194-021-00354-9},
	abstract = {In this paper I explore an underdiscussed factor contributing to the replication crisis: Scientists, and following them policy makers, often neglect sources of errors in the production and interpretation of data and thus overestimate what can be learnt from them. This neglect leads scientists to conduct experiments that are insufficiently informative and science consumers, including other scientists, to put too much weight on experimental results. The former leads to fragile empirical literatures, the latter to surprise and disappointment when the fragility of the empirical basis of some disciplines is revealed.},
	pages = {34},
	number = {2},
	journaltitle = {European Journal for Philosophy of Science},
	shortjournal = {Euro Jnl Phil Sci},
	author = {Machery, Edouard},
	urldate = {2021-03-27},
	date = {2021-03-20},
	langid = {english},
	keywords = {toread},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/MMAK8UFV/Machery - 2021 - A mistaken confidence in data.pdf:application/pdf},
}

@article{allchin_error_2001,
	title = {Error Types},
	volume = {9},
	issn = {1530-9274},
	url = {https://muse.jhu.edu/article/28083},
	pages = {38--58},
	number = {1},
	journaltitle = {Perspectives on Science},
	author = {Allchin, Douglas},
	urldate = {2021-03-27},
	date = {2001},
	note = {Publisher: The {MIT} Press},
	keywords = {toread},
	file = {Allchin - 2001 - Error Types.pdf:/Users/tom/Zotero/storage/RQXD86FC/Allchin - 2001 - Error Types.pdf:application/pdf},
}

@article{allchin_epistemology_nodate,
	title = {The Epistemology of Error},
	pages = {17},
	author = {Allchin, Douglas},
	langid = {english},
	keywords = {toread},
	file = {Allchin - The Epistemology of Error.pdf:/Users/tom/Zotero/storage/KAG6C3D3/Allchin - The Epistemology of Error.pdf:application/pdf},
}

@online{noauthor_err_nodate,
	title = {To Err is Science},
	url = {http://douglasallchin.net/papers/2-err.htm},
	urldate = {2021-03-27},
	keywords = {toread},
	file = {To Err is Science:/Users/tom/Zotero/storage/WBHELU92/2-err.html:text/html},
}

@article{allchin_credibility_2020,
	title = {The Credibility Game},
	volume = {82},
	issn = {0002-7685, 1938-4211},
	url = {https://online.ucpress.edu/abt/article/82/8/535/113781/The-Credibility-Game},
	doi = {10.1525/abt.2020.82.8.535},
	abstract = {Science denial, misinformation, and science con-artists are on the rise. We are plagued by anti-vaxxers, climate change naysayers, and promoters of ineffective fad diets and medical cures. The scientifically literate citizen or consumer needs skills in differentiating good science and trustworthy sources from impostors. Here, I present a series of student-centered activities that help students inquire into the nature of credibility and the problems of expertise, mediated knowledge, and science communication. I open with a playful guessing game about “fantastic beasts” reported in the 16th century, then follow with more modern examples. I then describe a science version of “To Tell the Truth,” a reflective exercise on “Finding the Expert,” and then a student opportunity to explore deceptive strategies by trying to bluff their classmates with false news stories about science. These all develop basic concepts in science media literacy and prepare students for more serious investigation into a contemporary scientific controversy.},
	pages = {535--541},
	number = {8},
	journaltitle = {The American Biology Teacher},
	author = {Allchin, Douglas},
	urldate = {2021-03-27},
	date = {2020-11-17},
	langid = {english},
	keywords = {toread},
	file = {Allchin - 2020 - The Credibility Game.pdf:/Users/tom/Zotero/storage/ZHU6BR72/Allchin - 2020 - The Credibility Game.pdf:application/pdf},
}

@article{hensel_double_2020,
	title = {Double trouble? The communication dimension of the reproducibility crisis in experimental psychology and neuroscience},
	volume = {10},
	issn = {1879-4920},
	url = {https://doi.org/10.1007/s13194-020-00317-6},
	doi = {10.1007/s13194-020-00317-6},
	shorttitle = {Double trouble?},
	abstract = {Most discussions of the reproducibility crisis focus on its epistemic aspect: the fact that the scientific community fails to follow some norms of scientific investigation, which leads to high rates of irreproducibility via a high rate of false positive findings. The purpose of this paper is to argue that there is a heretofore underappreciated and understudied dimension to the reproducibility crisis in experimental psychology and neuroscience that may prove to be at least as important as the epistemic dimension. This is the communication dimension. The link between communication and reproducibility is immediate: independent investigators would not be able to recreate an experiment whose design or implementation were inadequately described. I exploit evidence of a replicability and reproducibility crisis in computational science, as well as research into quality of reporting to support the claim that a widespread failure to adhere to reporting standards, especially the norm of descriptive completeness, is an important contributing factor in the current reproducibility crisis in experimental psychology and neuroscience.},
	pages = {44},
	number = {3},
	journaltitle = {European Journal for Philosophy of Science},
	shortjournal = {Euro Jnl Phil Sci},
	author = {Hensel, Witold M.},
	urldate = {2021-03-27},
	date = {2020-10-01},
	langid = {english},
	keywords = {toread},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/DLJY2MYF/Hensel - 2020 - Double trouble The communication dimension of the.pdf:application/pdf},
}

@article{tukey_philosophy_1991,
	title = {The philosophy of multiple comparisons},
	volume = {6},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/2245714},
	abstract = {This paper is based on the 1989 Miller Memorial Lecture at Stanford University. The topic was chosen because of Rupert Miller's long involvement and significant contributions to multiple comparison procedures and theory. Our emphasis will be on the major questions that have received relatively little attention--on what one wants multiple comparisons to do, on why one wants to do that, and on how one can communicate the results. Very little attention will be given to how the results can be calculated--after all, there are books about that (e.g., Miller, 1966, 1981; Hochberg and Tamhane, 1987).},
	pages = {100--116},
	number = {1},
	journaltitle = {Statistical Science},
	author = {Tukey, John W.},
	urldate = {2021-03-27},
	date = {1991},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Tukey - 1991 - The philosophy of multiple comparisons.pdf:/Users/tom/Zotero/storage/MSQAH4ZA/Tukey - 1991 - The philosophy of multiple comparisons.pdf:application/pdf},
}

@article{tukey_analyzing_1969,
	title = {Analyzing data: Sanctification or detective work?},
	volume = {24},
	issn = {0003-066X},
	shorttitle = {Analyzing data},
	abstract = {Examines exploratory and confirmatory data analysis, emphasizing the need for flexibility in technique supported by empirical trial. It is suggested that the restrictions of Campbellian measurement should be cast off. The need for using conjoint measurement in working with simultaneous changes is stressed, along with the need for more diverse bodies of data that can be measured in common ways. The use of correlation coefficients is regarded as an enemy of generalization. Exploratory and confirmatory data analysis are both considered to be essential, especially in detective work and guidance counselling. ({PsycINFO} Database Record (c) 2006 {APA}, all rights reserved), (C) 1969 by the American Psychological Association},
	pages = {83--91},
	number = {2},
	journaltitle = {American Psychologist},
	author = {Tukey, John W.},
	date = {1969-02},
	note = {Institution: Princeton U},
	keywords = {exploratory \& confirmatory data analysis, technique flexibility \& empirical trial support},
	file = {Tukey - 1969 - Analyzing data Sanctification or detective work.pdf:/Users/tom/Zotero/storage/GXMTA8MN/Tukey - 1969 - Analyzing data Sanctification or detective work.pdf:application/pdf},
}

@article{payne_fishing_1974,
	title = {Fishing expedition probability: the statistics of post hoc hypothesizing},
	volume = {7},
	issn = {0032-3497},
	url = {https://www.jstor.org/stable/3234273},
	doi = {10.2307/3234273},
	shorttitle = {Fishing expedition probability},
	pages = {130--138},
	number = {1},
	journaltitle = {Polity},
	author = {Payne, James L.},
	urldate = {2021-03-27},
	date = {1974},
	note = {Publisher: Palgrave Macmillan Journals},
	file = {JSTOR Full Text PDF:/Users/tom/Zotero/storage/LAPAHUDV/Payne - 1974 - Fishing Expedition Probability The Statistics of .pdf:application/pdf},
}

@article{odonohue_are_2021,
	title = {Are psychologists appraising research properly? Some Popperian notes regarding replication failures in psychology},
	issn = {2151-3341(Electronic),1068-8471(Print)},
	doi = {10.1037/teo0000179},
	shorttitle = {Are psychologists appraising research properly?},
	abstract = {Describes the implications of Popper’s philosophy of science for appraising research and thus the replication crisis in psychology. A core problem may be a fundamental misunderstanding of an epistemology of scientific methods rather than more narrow, technical issues like statistical analyses. Popper argued against Platonic criteria of knowledge such as justification and truth and instead made the case for severe testing of theories of high empirical content that are designed to solve scientific problems. Popper also argued for an evolutionary epistemology in which surviving a (hopefully severe) falsification attempt is not seen as producing truth but as verisimilitude in which the theory is seen as having survived one among a very large number of possible tests. Puzzles regarding replication failures can then be seen as mistaken epistemic appraisals of research that do not properly take into account five interrelated dimensions: (a) the researcher’s motivations (craving to be right vs. finding error as efficiently as possible), (b) neglecting a proper appraisal of the empirical content of the hypothesis under test, (c) neglecting an appraisal of the severity of the test, (d) neglecting to consider the ratio of tests survived versus all possible tests, and (e) neglecting an appraisal of the problem-solving status of the hypothesis. ({PsycInfo} Database Record (c) 2021 {APA}, all rights reserved)},
	pages = {No Pagination Specified--No Pagination Specified},
	journaltitle = {Journal of Theoretical and Philosophical Psychology},
	author = {O'Donohue, William},
	date = {2021},
	note = {Place: {US}
Publisher: Educational Publishing Foundation},
	keywords = {Measurement, Psychologists, Methodology, Sciences, Epistemology, Experimental Replication, Philosophies, Problem Solving},
	file = {Snapshot:/Users/tom/Zotero/storage/2SC2HGBF/2021-28939-001.html:text/html},
}

@article{francis_psychology_2012,
	title = {The psychology of replication and replication in psychology},
	volume = {7},
	issn = {1745-6924(Electronic),1745-6916(Print)},
	doi = {10.1177/1745691612459520},
	abstract = {Like other scientists, psychologists believe experimental replication to be the final arbiter for determining the validity of an empirical finding. Reports in psychology journals often attempt to prove the validity of a hypothesis or theory with multiple experiments that replicate a finding. Unfortunately, these efforts are sometimes misguided because in a field like experimental psychology, ever more successful replication does not necessarily ensure the validity of an empirical finding. When psychological experiments are analyzed with statistics, the rules of probability dictate that random samples should sometimes be selected that do not reject the null hypothesis, even if an effect is real. As a result, it is possible for a set of experiments to have too many successful replications. When there are too many successful replications for a given set of experiments, a skeptical scientist should be suspicious that null or negative findings have been suppressed, the experiments were run improperly, or the experiments were analyzed improperly. This article describes the implications of this observation and demonstrates how to test for too much successful replication by using a set of experiments from a recent research paper. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {585--594},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	author = {Francis, Gregory},
	date = {2012},
	note = {Place: {US}
Publisher: Sage Publications},
	keywords = {Psychology, Scientists, Psychologists, Experimental Replication, Random Sampling},
	file = {Snapshot:/Users/tom/Zotero/storage/RLCG584U/2012-30402-008.html:text/html},
}

@book{gilovich_how_1991,
	location = {New York [Great Britain]},
	title = {How We Know What Isn't So: The Fallibility of Human Reason in Everyday Life},
	isbn = {978-0-02-911706-4},
	shorttitle = {How We Know What Isn't so},
	pagetotal = {216},
	publisher = {Free Press},
	author = {Gilovich, Thomas},
	date = {1991},
	note = {{OCLC}: ocm28511375},
}

@article{schonbrodt_bayes_2018,
	title = {Bayes factor design analysis: Planning for compelling evidence},
	volume = {25},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-017-1230-y},
	doi = {10.3758/s13423-017-1230-y},
	shorttitle = {Bayes factor design analysis},
	abstract = {A sizeable literature exists on the use of frequentist power analysis in the null-hypothesis significance testing ({NHST}) paradigm to facilitate the design of informative experiments. In contrast, there is almost no literature that discusses the design of experiments when Bayes factors ({BFs}) are used as a measure of evidence. Here we explore Bayes Factor Design Analysis ({BFDA}) as a useful tool to design studies for maximum efficiency and informativeness. We elaborate on three possible {BF} designs, (a) a fixed-n design, (b) an open-ended Sequential Bayes Factor ({SBF}) design, where researchers can test after each participant and can stop data collection whenever there is strong evidence for either \${\textbackslash}mathcal \{H\}\_\{1\}\$or \${\textbackslash}mathcal \{H\}\_\{0\}\$, and (c) a modified {SBF} design that defines a maximal sample size where data collection is stopped regardless of the current state of evidence. We demonstrate how the properties of each design (i.e., expected strength of evidence, expected sample size, expected probability of misleading evidence, expected probability of weak evidence) can be evaluated using Monte Carlo simulations and equip researchers with the necessary information to compute their own Bayesian design analyses.},
	pages = {128--142},
	number = {1},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Schönbrodt, Felix D. and Wagenmakers, Eric-Jan},
	urldate = {2021-03-28},
	date = {2018-02-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/APNKU47C/Schönbrodt and Wagenmakers - 2018 - Bayes factor design analysis Planning for compell.pdf:application/pdf},
}

@article{stefan_tutorial_2019-1,
	title = {A tutorial on Bayes Factor Design Analysis using an informed prior},
	volume = {51},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-018-01189-8},
	doi = {10.3758/s13428-018-01189-8},
	abstract = {Well-designed experiments are likely to yield compelling evidence with efficient sample sizes. Bayes Factor Design Analysis ({BFDA}) is a recently developed methodology that allows researchers to balance the informativeness and efficiency of their experiment (Schönbrodt \& Wagenmakers, Psychonomic Bulletin \& Review, 25(1), 128–142 2018). With {BFDA}, researchers can control the rate of misleading evidence but, in addition, they can plan for a target strength of evidence. {BFDA} can be applied to fixed-N and sequential designs. In this tutorial paper, we provide an introduction to {BFDA} and analyze how the use of informed prior distributions affects the results of the {BFDA}. We also present a user-friendly web-based {BFDA} application that allows researchers to conduct {BFDAs} with ease. Two practical examples highlight how researchers can use a {BFDA} to plan for informative and efficient research designs.},
	pages = {1042--1058},
	number = {3},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Stefan, Angelika M. and Gronau, Quentin F. and Schönbrodt, Felix D. and Wagenmakers, Eric-Jan},
	urldate = {2021-03-28},
	date = {2019-06-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/7CNZRQR7/Stefan et al. - 2019 - A tutorial on Bayes Factor Design Analysis using a.pdf:application/pdf},
}

@article{trisovic_large-scale_2021,
	title = {A large-scale study on research code quality and execution},
	url = {http://arxiv.org/abs/2103.12793},
	abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74{\textbackslash}\% of R files crashed in the initial execution, while 56{\textbackslash}\% crashed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals' collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
	journaltitle = {{arXiv}:2103.12793 [cs]},
	author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Mercè},
	urldate = {2021-03-28},
	date = {2021-03-23},
	eprinttype = {arxiv},
	eprint = {2103.12793},
	keywords = {Computer Science - Digital Libraries, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/tom/Zotero/storage/2ZB5N28M/Trisovic et al. - 2021 - A large-scale study on research code quality and e.pdf:application/pdf;arXiv.org Snapshot:/Users/tom/Zotero/storage/QM5M7XNS/2103.html:text/html},
}

@article{kerr_bias_1996,
	title = {Bias in judgment: Comparing individuals and groups},
	volume = {103},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/0033-295X.103.4.687},
	shorttitle = {Bias in judgment},
	abstract = {The relative susceptibility of individuals and groups to systematic judgmental biases is considered. An overview of the relevant empirical literature reveals no clear or general pattern. However, a theoretical analysis employing J. H. Davis's (1973) social decision scheme ({SDS}) model reveals that the relative magnitude of individual and group bias depends upon several factors, including group size, initial individual judgment, the magnitude of bias among individuals, the type of bias, and most of all, the group-judgment process. It is concluded that there can be no simple answer to the question, "Which are more biased, individuals or groups?," but the {SDS} model offers a framework for specifying some of the conditions under which individuals are both more and less biased than groups. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {687--719},
	number = {4},
	journaltitle = {Psychological Review},
	author = {Kerr, Norbert L. and {MacCoun}, Robert J. and Kramer, Geoffrey P.},
	date = {1996},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Judgment, Decision Making, Group Decision Making},
	file = {Accepted Version:/Users/tom/Zotero/storage/2FIIGL4E/Kerr et al. - 1996 - Bias in judgment Comparing individuals and groups.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JMUYSZ55/1996-06397-004.html:text/html},
}

@article{goodman_stopping_2007,
	title = {Stopping at nothing? Some dilemmas of data monitoring in clinical trials},
	volume = {146},
	issn = {0003-4819},
	url = {https://www.acpjournals.org/doi/10.7326/0003-4819-146-12-200706190-00010},
	doi = {10.7326/0003-4819-146-12-200706190-00010},
	shorttitle = {Stopping at nothing?},
	pages = {882--887},
	number = {12},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Goodman, Steven N.},
	urldate = {2021-03-28},
	date = {2007-06-19},
	note = {Publisher: American College of Physicians},
	file = {Full Text PDF:/Users/tom/Zotero/storage/DXYQEBXU/Goodman - 2007 - Stopping at Nothing Some Dilemmas of Data Monitor.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ERRFDYB2/0003-4819-146-12-200706190-00010.html:text/html},
}

@article{bailey_not_nodate,
	title = {Not Normal: the uncertainties of scientific measurements},
	volume = {4},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.160600},
	doi = {10.1098/rsos.160600},
	shorttitle = {Not Normal},
	abstract = {Judging the significance and reproducibility of quantitative research requires a good understanding of relevant uncertainties, but it is often unclear how well these have been evaluated and what they imply. Reported scientific uncertainties were studied by analysing 41 000 measurements of 3200 quantities from medicine, nuclear and particle physics, and interlaboratory comparisons ranging from chemistry to toxicology. Outliers are common, with 5σ disagreements up to five orders of magnitude more frequent than naively expected. Uncertainty-normalized differences between multiple measurements of the same quantity are consistent with heavy-tailed Student’s t-distributions that are often almost Cauchy, far from a Gaussian Normal bell curve. Medical research uncertainties are generally as well evaluated as those in physics, but physics uncertainty improves more rapidly, making feasible simple significance criteria such as the 5σ discovery convention in particle physics. Contributions to measurement uncertainty from mistakes and unknown problems are not completely unpredictable. Such errors appear to have power-law distributions consistent with how designed complex systems fail, and how unknown systematic errors are constrained by researchers. This better understanding may help improve analysis and meta-analysis of data, and help scientists and the public have more realistic expectations of what scientific results imply.},
	pages = {160600},
	number = {1},
	journaltitle = {Royal Society Open Science},
	shortjournal = {Royal Society Open Science},
	author = {Bailey, David C.},
	urldate = {2021-03-28},
	note = {Publisher: Royal Society},
	file = {Full Text PDF:/Users/tom/Zotero/storage/88SGA9FE/Bailey - Not Normal the uncertainties of scientific measur.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B9ZVZVZU/rsos.html:text/html},
}

@article{trpin_jeffrey_2020,
	title = {Jeffrey conditionalization: proceed with caution},
	volume = {177},
	issn = {1573-0883},
	url = {https://doi.org/10.1007/s11098-019-01356-3},
	doi = {10.1007/s11098-019-01356-3},
	shorttitle = {Jeffrey conditionalization},
	abstract = {It has been argued that if the rigidity condition is satisfied, a rational agent operating with uncertain evidence should update her subjective probabilities by Jeffrey conditionalization ({JC}) or else a series of bets resulting in a sure loss could be made against her (the Dynamic Dutch Book Argument). We show, however, that even if the rigidity condition is satisfied, it is not always safe to update probability distributions by {JC} because there exist such sequences of non-misleading uncertain observations where it may be foreseen that an agent who updates her subjective probabilities by {JC} will end up nearly certain that a false hypothesis is true. We analyze the features of {JC} that lead to this problem, specify the conditions in which it arises and respond to potential objections.},
	pages = {2985--3012},
	number = {10},
	journaltitle = {Philosophical Studies},
	shortjournal = {Philos Stud},
	author = {Trpin, Borut},
	urldate = {2021-03-28},
	date = {2020-10-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/6GR3TVNM/Trpin - 2020 - Jeffrey conditionalization proceed with caution.pdf:application/pdf},
}

@report{dienes_testing_2021,
	title = {Testing theories with Bayes factors},
	url = {https://psyarxiv.com/pxhd2/},
	abstract = {Bayes factors are a useful tool for researchers in the behavioural and social sciences, partly because they can provide evidence for no effect relative to the sort of effect expected. By contrast, a non-significant result does not provide evidence for the H0 tested. So, if non-significance does not in itself count against any theory predicting an effect, how could a theory fail a test?  Bayes factors provide a measure of evidence from first principles. A severe test is one that is likely to obtain evidence against a theory if it were false; that is, to obtain an extreme Bayes factor against the theory. Bayes factors show why hacking and cherry picking degrade evidence; how to deal with multiple testing situations; and how optional stopping is consistent with severe testing. Further, informed Bayes factors can be used to link theory tightly to how that theory is tested, so that the measured evidence does relate to the theory.},
	institution = {{PsyArXiv}},
	author = {Dienes, Zoltan},
	urldate = {2021-03-28},
	date = {2021-03-26},
	doi = {10.31234/osf.io/pxhd2},
	note = {type: article},
	keywords = {Meta-science, open science, Social and Behavioral Sciences, Quantitative Methods, Statistical Methods, statistics, hypothesis testing, bayes factors, severe tests},
	file = {Full Text PDF:/Users/tom/Zotero/storage/EX7AEY6D/Dienes - 2021 - Testing theories with Bayes factors.pdf:application/pdf},
}

@report{dienes_inner_2020,
	title = {The inner workings of Registered Reports},
	url = {https://osf.io/yhp2a},
	abstract = {Registered Reports provide one way to address shortcomings in the current way we manage research, right from the design of studies to their publication. The format requires pre-specifying why a design may crucially test a theory, what auxiliary assumptions are required for the experiment to be such a test, what outcome neutral tests are required in turn to test those assumptions, what specific crucial tests will therefore be used test the theory (of the many tests that could be used), and why those tests could provide evidence for no effect of interest given the proposed numbers of trials and participants. Reviewers and authors are then constructively involved in optimizing the experiment before it is run. The agreement between reviewers and authors, as adjudicated by the editors, then defines in advance the proposed method and analytic protocol, virtually guaranteeing acceptance of the paper, no matter what position, if any, the results support. Here we go through what problems the format solves, and why it must therefore be approached in a way that is little understood. Common errors are discussed. The paper thus provides an argument for how to approach Registered Reports for readers, authors, and editors of the format.},
	institution = {{PsyArXiv}},
	type = {preprint},
	author = {Dienes, Zoltan},
	urldate = {2021-03-28},
	date = {2020-10-28},
	langid = {english},
	doi = {10.31234/osf.io/yhp2a},
	file = {Dienes - 2020 - The inner workings of Registered Reports.pdf:/Users/tom/Zotero/storage/CUC6LH5Y/Dienes - 2020 - The inner workings of Registered Reports.pdf:application/pdf},
}

@article{chuard_evidence_2019,
	title = {Evidence that nonsignificant results are sometimes preferred: Reverse P-hacking or selective reporting?},
	volume = {17},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000127},
	doi = {10.1371/journal.pbio.3000127},
	shorttitle = {Evidence that nonsignificant results are sometimes preferred},
	abstract = {There is increased concern about poor scientific practices arising from an excessive focus on P-values. Two particularly worrisome practices are selective reporting of significant results and ‘P-hacking’. The latter is the manipulation of data collection, usage, or analyses to obtain statistically significant outcomes. Here, we introduce the novel, to our knowledge, concepts of selective reporting of nonsignificant results and ‘reverse P-hacking’ whereby researchers ensure that tests produce a nonsignificant result. We test whether these practices occur in experiments in which researchers randomly assign subjects to treatment and control groups to minimise differences in confounding variables that might affect the focal outcome. By chance alone, 5\% of tests for a group difference in confounding variables should yield a significant result (P {\textless} 0.05). If researchers less often report significant findings and/or reverse P-hack to avoid significant outcomes that undermine the ethos that experimental and control groups only differ with respect to actively manipulated variables, we expect significant results from tests for group differences to be under-represented in the literature. We surveyed the behavioural ecology literature and found significantly more nonsignificant P-values reported for tests of group differences in potentially confounding variables than the expected 95\% (P = 0.005; N = 250 studies). This novel, to our knowledge, publication bias could result from selective reporting of nonsignificant results and/or from reverse P-hacking. We encourage others to test for a bias toward publishing nonsignificant results in the equivalent context in their own research discipline.},
	pages = {e3000127},
	number = {1},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Chuard, Pierre J. C. and Vrtílek, Milan and Head, Megan L. and Jennions, Michael D.},
	urldate = {2021-03-29},
	date = {2019-01-25},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Statistical data, Experimental design, Metaanalysis, Behavioral ecology, Publication ethics, Statistical models, Careers, Physiological parameters},
	file = {Full Text PDF:/Users/tom/Zotero/storage/RMDPXH5D/Chuard et al. - 2019 - Evidence that nonsignificant results are sometimes.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KF4WFRUQ/article.html:text/html},
}

@article{gelman_why_2012,
	title = {Why we (usually) don't have to worry about multiple comparisons},
	volume = {5},
	issn = {1934-5747},
	url = {https://doi.org/10.1080/19345747.2011.618213},
	doi = {10.1080/19345747.2011.618213},
	abstract = {Applied researchers often find themselves making statistical inferences in settings that would seem to require multiple comparisons adjustments. We challenge the Type I error paradigm that underlies these corrections. Moreover we posit that the problem of multiple comparisons can disappear entirely when viewed from a hierarchical Bayesian perspective. We propose building multilevel models in the settings where multiple comparisons arise. Multilevel models perform partial pooling (shifting estimates toward each other), whereas classical procedures typically keep the centers of intervals stationary, adjusting for multiple comparisons by making the intervals wider (or, equivalently, adjusting the p values corresponding to intervals of fixed width). Thus, multilevel models address the multiple comparisons problem and also yield more efficient estimates, especially in settings with low group-level variation, which is where multiple comparisons are a particular concern.},
	pages = {189--211},
	number = {2},
	journaltitle = {Journal of Research on Educational Effectiveness},
	author = {Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
	urldate = {2021-03-29},
	date = {2012-04-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/19345747.2011.618213},
	keywords = {multiple comparisons, statistical significance, Type S error, Bayesian inference, hierarchical modeling},
	file = {Full Text PDF:/Users/tom/Zotero/storage/QW7TTN66/Gelman et al. - 2012 - Why We (Usually) Don't Have to Worry About Multipl.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TU8JPM89/19345747.2011.html:text/html},
}

@article{cox_foundations_1978,
	title = {Foundations of statistical inference: the case for eclecticism},
	volume = {20},
	issn = {1467-842X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-842X.1978.tb01094.x},
	doi = {https://doi.org/10.1111/j.1467-842X.1978.tb01094.x},
	shorttitle = {Statistical society of australia the knibbs lecture for 1977},
	abstract = {First there is some general discussion of the place in statistics of work on the foundations of statistical inference. Three broad approaches, sampling theory, pure likelihood and Bayesian are distinguished and some general comments made on the advantages and limitations of each approach. It is concluded that all three have some role.},
	pages = {43--59},
	number = {1},
	journaltitle = {Australian Journal of Statistics},
	author = {Cox, D. R.},
	urldate = {2021-03-29},
	date = {1978},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-842X.1978.tb01094.x},
	file = {Snapshot:/Users/tom/Zotero/storage/EVJ5CTSB/j.1467-842X.1978.tb01094.html:text/html},
}

@article{greenland_analysis_2021,
	title = {Analysis goals, error‐cost sensitivity, and analysis hacking: Essential considerations in hypothesis testing and multiple comparisons},
	volume = {35},
	issn = {0269-5022, 1365-3016},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ppe.12711},
	doi = {10.1111/ppe.12711},
	shorttitle = {Analysis goals, error‐cost sensitivity, and analysis hacking},
	pages = {8--23},
	number = {1},
	journaltitle = {Paediatric and Perinatal Epidemiology},
	shortjournal = {Paediatr Perinat Epidemiol},
	author = {Greenland, Sander},
	urldate = {2021-03-29},
	date = {2021-01},
	langid = {english},
	file = {Greenland - 2021 - Analysis goals, error‐cost sensitivity, and analys.pdf:/Users/tom/Zotero/storage/GZBNUAGN/Greenland - 2021 - Analysis goals, error‐cost sensitivity, and analys.pdf:application/pdf},
}

@article{kruschke_bayesian_2018,
	title = {Bayesian data analysis for newcomers},
	volume = {25},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-017-1272-1},
	doi = {10.3758/s13423-017-1272-1},
	abstract = {This article explains the foundational concepts of Bayesian data analysis using virtually no mathematical notation. Bayesian ideas already match your intuitions from everyday reasoning and from traditional data analysis. Simple examples of Bayesian data analysis are presented that illustrate how the information delivered by a Bayesian analysis can be directly interpreted. Bayesian approaches to null-value assessment are discussed. The article clarifies misconceptions about Bayesian methods that newcomers might have acquired elsewhere. We discuss prior distributions and explain how they are not a liability but an important asset. We discuss the relation of Bayesian data analysis to Bayesian models of mind, and we briefly discuss what methodological problems Bayesian data analysis is not meant to solve. After you have read this article, you should have a clear sense of how Bayesian data analysis works and the sort of information it delivers, and why that information is so intuitive and useful for drawing conclusions from data.},
	pages = {155--177},
	number = {1},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Kruschke, John K. and Liddell, Torrin M.},
	urldate = {2021-03-30},
	date = {2018-02-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/BIN2IK58/Kruschke and Liddell - 2018 - Bayesian data analysis for newcomers.pdf:application/pdf},
}

@article{miller_failures_2021,
	title = {Failures of memory and the fate of forgotten memories},
	issn = {1074-7427},
	url = {https://www.sciencedirect.com/science/article/pii/S1074742721000484},
	doi = {10.1016/j.nlm.2021.107426},
	abstract = {This review is intended primarily to provide cognitive benchmarks and perhaps a new mindset for behavioral neuroscientists who study memory. Forgetting, defined here broadly as all types of decreases in acquired responding to stimulus-specific eliciting cues, is commonly attributed to one or more of the following families of mechanisms: (1) (4) associative interference by information similar to, but different from the target information, (2) spontaneous decay of memory with increasing retention intervals, (3) displacement from short-term memory by irrelevant information, and (4) inadequate retrieval cues at test. I briefly review each of these families and discuss data suggesting that many apparent instances of spontaneous forgetting and displacement from short-term memory can be viewed as variants of inadequate retrieval cues and associative interference. The potential for recovery of target information from each of these families of forgetting without further relevant training is then reviewed, with a conclusion that most forgetting is due to retrieval failure as opposed to irreversible erasure of memory. The more general point is made that there are logical problems with ever talking about attenuating or erasing a memory as a consequence of conventional forgetting or disrupted consolidation/reconsolidation. Consideration is then given to the frequently overlooked but highly beneficial consequences of most forgetting. Lastly, the major variables that moderate forgetting are summarized, including (a) the similarities of the target information including training context to the explicit retrieval cues and context present at test, (b) the similarities of potentially interfering acquired information to the retrieval cues and context present at test, and (c) the retention interval for the target information relative to that for the potentially interfering information. Appropriate manipulation of these variables can reduce forgetting, and increase forgetting when desired.},
	pages = {107426},
	journaltitle = {Neurobiology of Learning and Memory},
	shortjournal = {Neurobiology of Learning and Memory},
	author = {Miller, Ralph R.},
	urldate = {2021-03-30},
	date = {2021-03-29},
	langid = {english},
	keywords = {memory, consolidation, associative interference, forgetting, retrieval failure, spontaneous forgetting},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/P937CK8Z/S1074742721000484.html:text/html},
}

@article{sanborn_reply_2014,
	title = {Reply to Rouder (2014): Good frequentist properties raise confidence},
	volume = {21},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-014-0607-4},
	doi = {10.3758/s13423-014-0607-4},
	shorttitle = {Reply to Rouder (2014)},
	abstract = {Established psychological results have been called into question by demonstrations that statistical significance is easy to achieve, even in the absence of an effect. One oftenwarned-against practice, choosing when to stop the experiment on the basis of the results, is guaranteed to produce significant results. In response to these demonstrations, Bayes factors have been proposed as an antidote to this practice, because they are invariant with respect to how an experiment was stopped. Should researchers only care about the resulting Bayes factor, without concern for how it was produced? Yu, Sprenger, Thomas, and Dougherty (2014) and Sanborn and Hills (2014) demonstrated that Bayes factors are sometimes strongly influenced by the stopping rules used. However, Rouder (2014) has provided a compelling demonstration that despite this influence, the evidence supplied by Bayes factors remains correct. Here we address why the ability to influence Bayes factors should still matter to researchers, despite the correctness of the evidence. We argue that good frequentist properties mean that results will more often agree with researchers’ statistical intuitions, and good frequentist properties control the number of studies that will later be refuted. Both help raise confidence in psychological results.},
	pages = {309--311},
	number = {2},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Sanborn, Adam N. and Hills, Thomas T. and Dougherty, Michael R. and Thomas, Rick P. and Yu, Erica C. and Sprenger, Amber M.},
	urldate = {2021-03-31},
	date = {2014-04},
	langid = {english},
	file = {Sanborn et al. - 2014 - Reply to Rouder (2014) Good frequentist propertie.pdf:/Users/tom/Zotero/storage/ACCBIPPB/Sanborn et al. - 2014 - Reply to Rouder (2014) Good frequentist propertie.pdf:application/pdf},
}

@article{rosenbaum_sensitivity_2021,
	title = {Sensitivity of bayes inference with data-dependent stopping rules},
	pages = {5},
	author = {Rosenbaum, Paul R and Rubin, Donald B},
	date = {2021},
	langid = {english},
	file = {Rosenbaum and Rubin - 2021 - Sensitivity of Bayes Inference with Data-Dependent.pdf:/Users/tom/Zotero/storage/DEU3FTML/Rosenbaum and Rubin - 2021 - Sensitivity of Bayes Inference with Data-Dependent.pdf:application/pdf},
}

@article{yu_when_2014,
	title = {When decision heuristics and science collide},
	volume = {21},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-013-0495-z},
	doi = {10.3758/s13423-013-0495-z},
	abstract = {The ongoing discussion among scientists about null-hypothesis significance testing and Bayesian data analysis has led to speculation about the practices and consequences of “researcher degrees of freedom.” This article advances this debate by asking the broader questions that we, as scientists, should be asking: How do scientists make decisions in the course of doing research, and what is the impact of these decisions on scientific conclusions? We asked practicing scientists to collect data in a simulated research environment, and our findings show that some scientists use data collection heuristics that deviate from prescribed methodology. Monte Carlo simulations show that data collection heuristics based on p values lead to biases in estimated effect sizes and Bayes factors and to increases in both false-positive and false-negative rates, depending on the specific heuristic. We also show that using Bayesian data collection methods does not eliminate these biases. Thus, our study highlights the little appreciated fact that the process of doing science is a behavioral endeavor that can bias statistical description and inference in a manner that transcends adherence to any particular statistical framework.},
	pages = {268--282},
	number = {2},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Yu, Erica C. and Sprenger, Amber M. and Thomas, Rick P. and Dougherty, Michael R.},
	urldate = {2021-03-31},
	date = {2014-04-01},
	langid = {english},
}

@article{fischhoff_hindsight_1975,
	title = {Hindsight does not equal foresight: the effect of outcome knowledge on judgment under uncertainty},
	volume = {1},
	pages = {288--299},
	number = {3},
	journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
	author = {Fischhoff, Baruch},
	date = {1975},
	langid = {english},
	file = {Fischhoff - Hindsight ^Foresight The Effect of Outcome Knowle.pdf:/Users/tom/Zotero/storage/5AUH4WPF/Fischhoff - Hindsight ^Foresight The Effect of Outcome Knowle.pdf:application/pdf},
}

@article{forstmeier_detecting_2017,
	title = {Detecting and avoiding likely false-positive findings – a practical guide},
	volume = {92},
	rights = {© 2016 The Authors. Biological Reviews published by John Wiley \& Sons Ltd on behalf of Cambridge Philosophical Society.},
	issn = {1469-185X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12315},
	doi = {https://doi.org/10.1111/brv.12315},
	abstract = {Recently there has been a growing concern that many published research findings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of ‘you can publish if you found a significant effect’. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difficult to falsify. In order to pinpoint the sources of error and possible solutions, we review current scientific practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive findings is expected to increase with (i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher flexibility, and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points (clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of ‘you can publish if your study is rigorous’. To this end, we highlight promising strategies towards making science more objective. Specifically, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis, and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research findings of one's own and of others for the benefit of the scientific community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of ‘impact’ almost exclusively and towards a system which explicitly values indices of scientific rigour.},
	pages = {1941--1968},
	number = {4},
	journaltitle = {Biological Reviews},
	author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, Timothy H.},
	urldate = {2021-04-01},
	date = {2017},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12315},
	keywords = {preregistration, power, replication, P-hacking, {HARKing}, researcher degrees of freedom, confirmation bias, hindsight bias, overfitting, Type I error},
	file = {Full Text PDF:/Users/tom/Zotero/storage/KGGPUL62/Forstmeier et al. - 2017 - Detecting and avoiding likely false-positive findi.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TFUEG2PA/brv.html:text/html},
}

@book{jaynes_probability_2003,
	location = {Cambridge},
	title = {Probability theory: the logic of science},
	isbn = {978-0-511-06589-7},
	url = {http://www5.unitn.it/Biblioteca/it/Web/LibriElettroniciDettaglio/50847},
	shorttitle = {Probability theory},
	abstract = {The standard rules of probability can be interpreted as uniquely valid principles in logic. In this book, E.T. Jaynes dispels the imaginary distinction between'probability theory'and'statistical inference', leaving a logical unity and simplicity, which provides greater technical power and flexibility in applications. This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. New results are discussed, along with applications of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book will be of interest to scientists working in any area where inference from incomplete information is necessary. (A cura dell'editore).},
	publisher = {Cambridge University Press},
	author = {Jaynes, Edwin T and Bretthorst},
	urldate = {2021-04-01},
	date = {2003},
	langid = {english},
	note = {{OCLC}: 982265136},
	file = {Jaynes et al. - 2003 - Probability theory the logic of science.pdf:/Users/tom/Zotero/storage/ZYSZYP35/Jaynes et al. - 2003 - Probability theory the logic of science.pdf:application/pdf},
}

@article{royall_probability_2000,
	title = {On the probability of observing misleading statistical evidence},
	volume = {95},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2669456},
	doi = {10.2307/2669456},
	abstract = {The law of likelihood explains how to interpret statistical data as evidence. Specifically, it gives to the discipline of statistics a precise and objective measure of the strength of statistical evidence supporting one probability distribution vis-a-vis another. That measure is the likelihood ratio. But evidence, even when properly interpreted, can be misleading-observations can truly constitute strong evidence supporting one distribution when the other is true. What makes statistical evidence valuable to science is that this cannot occur very often. Here we examine two bounds on the probability of observing strong misleading evidence. One is a universal bound, applicable to every pair of probability distributions. The other bound, much smaller, applies to all pairs of distributions within fixed-dimensional parametric models in large samples. The second bound comes from examining how the probability of strong misleading evidence varies as a function of the alternative value of the parameter. We show that in large samples one curve describes how this probability first rises and then falls as the alternative moves away from the true parameter value for a very wide class of models. We also show that this large-sample curve, and the bound that its maximum value represents, applies to profile likelihood ratios for one-dimensional parameters in fixed-dimensional parametric models, but does not apply to the estimated likelihood ratios that result from replacing the nuisance parameters by their global maximum likelihood estimates.},
	pages = {760--768},
	number = {451},
	journaltitle = {Journal of the American Statistical Association},
	author = {Royall, Richard},
	urldate = {2021-04-01},
	date = {2000},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	file = {Royall - 2000 - On the probability of observing misleading statist.pdf:/Users/tom/Zotero/storage/4JXY9T7D/Royall - 2000 - On the probability of observing misleading statist.pdf:application/pdf},
}

@article{kerr_harking_1998,
	title = {{HARKing}: Hypothesizing After the Results are Known},
	volume = {2},
	issn = {10888683},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=7460175&site=ehost-live&scope=site},
	doi = {10.1207/s15327957pspr0203_4},
	shorttitle = {{HARKing}},
	abstract = {Examines the practice of hypothesizing after the results are known ({HARKing}) in scientific communication.  Several forms of {HARKing}; Reasons why scientists ought not to {HARK}; Recommendations for deterring the practice of {HARKing}.},
	pages = {196--217},
	number = {3},
	journaltitle = {Personality \& Social Psychology Review (Lawrence Erlbaum Associates)},
	shortjournal = {Personality \& Social Psychology Review (Lawrence Erlbaum Associates)},
	author = {Kerr, Norbert L.},
	urldate = {2021-04-02},
	date = {1998-07},
	note = {Publisher: Taylor \& Francis Ltd},
	keywords = {{HYPOTHESIS}, {SCIENTIFIC} communication, {SCIENTIFIC} method},
	file = {EBSCO Full Text:/Users/tom/Zotero/storage/SG8BJYIY/Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf:application/pdf},
}

@online{noauthor_explorable_nodate,
	title = {Explorable Multiverse Data Analysis and Reports in R},
	url = {https://mucollective.github.io/multiverse/},
	abstract = {Create Explorable Multiverse Analysis in R. Multiverse analysis is a philosophy of statistical reporting where paper authors report the outcomes of many different statistical analyses in order to show how fragile or robust their findings are.},
	urldate = {2021-04-02},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/BZN5SKWW/multiverse.html:text/html},
}

@article{carp_plurality_2012,
	title = {On the plurality of (methodological) worlds: estimating the analytic flexibility of {fMRI} experiments},
	volume = {6},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2012.00149/full},
	doi = {10.3389/fnins.2012.00149},
	shorttitle = {On the plurality of (methodological) worlds},
	abstract = {How likely are published findings in the functional neuroimaging literature to be false? According to a recent mathematical model, the potential for false positives increases with the flexibility of analysis methods. Functional {MRI} ({fMRI}) experiments can be analyzed using a large number of commonly used tools, with little consensus on how, when, or whether to apply each one. This situation may lead to substantial variability in analysis outcomes. Thus, the present study sought to estimate the flexibility of neuroimaging analysis by submitting a single event-related {fMRI} experiment to a large number of unique analysis procedures. Ten analysis steps for which multiple strategies appear in the literature were identified, and two to four strategies were enumerated for each step. Considering all possible combinations of these strategies yielded 6,912 unique analysis pipelines. Activation maps from each pipeline were corrected for multiple comparisons using five thresholding approaches, yielding 34,560 significance maps. While some outcomes were relatively consistent across pipelines, others showed substantial methods-related variability in activation strength, location, and extent. Some analysis decisions contributed to this variability more than others, and different decisions were associated with distinct patterns of variability across the brain. Qualitative outcomes also varied with analysis parameters: many contrasts yielded significant activation under some pipelines but not others. Altogether, these results reveal considerable flexibility in the analysis of {fMRI} experiments. This observation, when combined with mathematical simulations linking analytic flexibility with elevated false positive rates, suggests that false positive results may be more prevalent than expected in the literature. This risk of inflated false positive rates may be mitigated by constraining the flexibility of analytic choices or by abstaining from selective analysis reporting.},
	journaltitle = {Frontiers in Neuroscience},
	shortjournal = {Front. Neurosci.},
	author = {Carp, Joshua},
	urldate = {2021-04-02},
	date = {2012},
	note = {Publisher: Frontiers},
	keywords = {data analysis, {fMRI}, selective reporting, analysis flexibility, false positive results},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MVLW63FH/Carp - 2012 - On the Plurality of (Methodological) Worlds Estim.pdf:application/pdf},
}

@report{moreau_preregistration_2019,
	title = {Preregistration in the context of expertise research: benefits, challenges, and recommendations},
	url = {https://psyarxiv.com/v7xrb/},
	shorttitle = {Preregistration in the context of expertise research},
	abstract = {A number of recent reforms in psychological science have centered around following best practices to improve the robustness and reliability of empirical findings. Among these, preregistration has become a fundamental component, on the rise in the last few years, yet it remains relatively uncommon in expertise research. In this paper, I point out the numerous benefits of preregistration, drawing on specific examples from the field of expertise. I then examine some of the challenges the field of psychology is currently facing to implement systematic preregistration, including many that are particularly exacerbated in expertise research. Specifically, I discuss widespread design characteristics such as small sample sizes, the lack of consistent definitions regarding what constitutes expert performance, and inherent difficulties in conducting replication studies with rare, elite populations. Finally, I make a number of recommendations to facilitate preregistration in expertise research, including tips to handle and report deviations from original plans, and discuss future directions toward more prevalent open science practices.},
	institution = {{PsyArXiv}},
	author = {Moreau, David},
	urldate = {2021-04-02},
	date = {2019-12-30},
	doi = {10.31234/osf.io/v7xrb},
	note = {type: article},
	keywords = {Meta-science, preregistration, Social and Behavioral Sciences, statistics, Cognitive Psychology, inference, Learning, meta-science, expertise},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MP6PQIY3/Moreau - 2019 - Preregistration in the Context of Expertise Resear.pdf:application/pdf},
}

@article{young_why_2008,
	title = {Why current publication practices may distort science},
	volume = {5},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0050201},
	doi = {10.1371/journal.pmed.0050201},
	abstract = {John Ioannidis and colleagues argue that the current system of publication in biomedical research provides a distorted view of the reality of scientific data.},
	pages = {e201},
	number = {10},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Young, Neal S. and Ioannidis, John P. A. and Al-Ubaydli, Omar},
	urldate = {2021-04-02},
	date = {2008-10-07},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Citation analysis, Reproducibility, Medical journals, Scientific publishing, Peer review, Scientists, Careers, Publication practices},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PQXQ57CL/Young et al. - 2008 - Why Current Publication Practices May Distort Scie.pdf:application/pdf},
}

@misc{higgins_cochrane_2011,
	title = {Cochrane Handbook for Systematic Reviews of Interventions Version 5.1.0},
	url = {www.handbook.cochrane.org},
	publisher = {The Cochrane Collaboration},
	editor = {Higgins, Julian P. T. and Green, S},
	date = {2011},
}

@article{lord_biased_1979,
	title = {Biased assimilation and attitude polarization: The effects of prior theories on subsequently considered evidence},
	volume = {37},
	issn = {0022-3514},
	shorttitle = {Biased assimilation and attitude polarization},
	abstract = {People who hold strong opinions on complex social issues are likely to examine relevant empirical evidence in a biased manner. They are apt to accept "confirming" evidence at face value while subjecting "disconfirming" evidence to critical evaluation, and, as a result, draw undue support for their initial positions from mixed or random empirical findings. Thus, the result of exposing contending factions in a social dispute to an identical body of relevant empirical evidence may be not a narrowing of disagreement but rather an increase in polarization. To test these assumptions, 48 undergraduates supporting and opposing capital punishment were exposed to 2 purported studies, one seemingly confirming and one seemingly disconfirming their existing beliefs about the deterrent efficacy of the death penalty. As predicted, both proponents and opponents of capital punishment rated those results and procedures that confirmed their own beliefs to be the more convincing and probative ones, and they reported corresponding shifts in their beliefs as the various results and procedures were presented. The net effect of such evaluations and opinion shifts was the postulated increase in attitude polarization. (28 ref) ({PsycINFO} Database Record (c) 2006 {APA}, all rights reserved), (C) 1979 by the American Psychological Association},
	pages = {2098--2109},
	number = {11},
	journaltitle = {Journal of Personality},
	author = {Lord, Charles G. and Ross, Lee and Lepper, Mark R.},
	date = {1979-11},
	note = {Institution: Stanford U},
	keywords = {attitude polarization, college students, confirmation vs disconfirmation of opinion toward capital punishment},
}

@article{oneill_peirce_1993,
	title = {Peirce and the nature of evidence},
	volume = {29},
	issn = {0009-1774},
	url = {https://www.jstor.org/stable/40320412},
	pages = {211--224},
	number = {2},
	journaltitle = {Transactions of the Charles S. Peirce Society},
	author = {O'Neill, Len},
	urldate = {2021-04-02},
	date = {1993},
	note = {Publisher: Indiana University Press},
	file = {O'Neill - 1993 - Peirce and the Nature of Evidence.pdf:/Users/tom/Zotero/storage/Y4F2G7HR/O'Neill - 1993 - Peirce and the Nature of Evidence.pdf:application/pdf},
}

@article{pavlov_eegmanylabs_2021,
	title = {\#{EEGManyLabs}: Investigating the Replicability of Influential {EEG} Experiments},
	issn = {0010-9452},
	url = {https://www.sciencedirect.com/science/article/pii/S0010945221001106},
	doi = {10.1016/j.cortex.2021.03.013},
	shorttitle = {\#{EEGManyLabs}},
	abstract = {There is growing awareness across the neuroscience community that the replicability of findings about the relationship between brain activity and cognitive phenomena can be improved by conducting studies with high statistical power that adhere to well-defined and standardised analysis pipelines. Inspired by recent efforts from the psychological sciences, and with the desire to examine some of the foundational findings using electroencephalography ({EEG}), we have launched \#{EEGManyLabs}, a large-scale international collaborative replication effort. Since its discovery in the early 20th century, {EEG} has had a profound influence on our understanding of human cognition, but there is limited evidence on the replicability of some of the most highly cited discoveries. After a systematic search and selection process, we have identified 27 of the most influential and continually cited studies in the field. We plan to directly test the replicability of key findings from 20 of these studies in teams of at least three independent laboratories. The design and protocol of each replication effort will be submitted as a Registered Report and peer-reviewed prior to data collection. Prediction markets, open to all {EEG} researchers, will be used as a forecasting tool to examine which findings the community expects to replicate. This project will update our confidence in some of the most influential {EEG} findings and generate a large open access database that can be used to inform future research practices. Finally, through this international effort, we hope to create a cultural shift towards inclusive, high-powered multi-laboratory collaborations.},
	journaltitle = {Cortex},
	shortjournal = {Cortex},
	author = {Pavlov, Yuri G. and Adamian, Nika and Appelhoff, Stefan and Arvaneh, Mahnaz and Benwell, Christopher S. Y. and Beste, Christian and Bland, Amy R. and Bradford, Daniel E. and Bublatzky, Florian and Busch, Niko A. and Clayson, Peter E. and Cruse, Damian and Czeszumski, Artur and Dreber, Anna and Dumas, Guillaume and Ehinger, Benedikt and Giorgio, Ganis and He, Xun and Hinojosa, José A. and Huber-Huber, Christoph and Inzlicht, Michael and Jack, Bradley N. and Johannesson, Magnus and Jones, Rhiannon and Kalenkovich, Evgenii and Kaltwasser, Laura and Karimi-Rouzbahani, Hamid and Keil, Andreas and König, Peter and Kouara, Layla and Kulke, Louisa and Ladouceur, Cecile D. and Langer, Nicolas and Liesefeld, Heinrich R. and Luque, David and {MacNamara}, Annmarie and Mudrik, Liad and Muthuraman, Muthuraman and Neal, Lauren B. and Nilsonne, Gustav and Niso, Guiomar and Ocklenburg, Sebastian and Oostenveld, Robert and Pernet, Cyril R. and Pourtois, Gilles and Ruzzoli, Manuela and Sass, Sarah M. and Schaefer, Alexandre and Senderecka, Magdalena and Snyder, Joel S. and Tamnes, Christian K. and Tognoli, Emmanuelle and van Vugt, Marieke K. and Verona, Edelyn and Vloeberghs, Robin and Welke, Dominik and Wessel, Jan R. and Zakharov, Ilya and Mushtaq, Faisal},
	urldate = {2021-04-03},
	date = {2021-04-02},
	langid = {english},
	keywords = {{EEG}, {ERP}, Replication, Open Science, Cognitive Neuroscience, {ManyLabs}},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/G6V2WQGR/S0010945221001106.html:text/html},
}

@article{simera_catalogue_2010,
	title = {A catalogue of reporting guidelines for health research},
	volume = {40},
	rights = {© 2009 The Authors. Journal Compilation © 2009 Stichting European Society for Clinical Investigation Journal Foundation},
	issn = {1365-2362},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2362.2009.02234.x},
	doi = {https://doi.org/10.1111/j.1365-2362.2009.02234.x},
	abstract = {Eur J Clin Invest 2010; 40 (1): 35–53 Abstract Growing evidence demonstrates widespread deficiencies in the reporting of health research studies. The {EQUATOR} Network is an international initiative that aims to enhance the reliability and value of the published health research literature. {EQUATOR} provides resources, education and training to facilitate good research reporting and assists in the development, dissemination and implementation of robust reporting guidelines. This paper presents a collection of tools and guidelines available on the {EQUATOR} website (http://www.equator-network.org) that have been developed to increase the accuracy and transparency of health research reporting.},
	pages = {35--53},
	number = {1},
	journaltitle = {European Journal of Clinical Investigation},
	author = {Simera, I. and Moher, D. and Hoey, J. and Schulz, K. F. and Altman, D. G.},
	urldate = {2021-04-03},
	date = {2010},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2362.2009.02234.x},
	keywords = {{EQUATOR} Network, reporting guidelines, research reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XUMJ438S/Simera et al. - 2010 - A catalogue of reporting guidelines for health res.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RENSJWV2/j.1365-2362.2009.02234.html:text/html},
}

@article{saquib_practices_2013,
	title = {Practices and impact of primary outcome adjustment in randomized controlled trials: meta-epidemiologic study},
	volume = {347},
	rights = {© Saquib et al 2013. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See:  http://creativecommons.org/licenses/by-nc/3.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/347/bmj.f4313},
	doi = {10.1136/bmj.f4313},
	shorttitle = {Practices and impact of primary outcome adjustment in randomized controlled trials},
	abstract = {Objective To assess adjustment practices for primary outcomes of randomized controlled trials and their impact on the results.
Design Meta-epidemiologic study.
Data sources 25 biomedical journals with the highest impact factor according to Journal Citation Reports 2009.
Study selection Randomized controlled trials published in print in 2009 that reported primary outcomes. The search yielded 684 eligible papers of randomized controlled trials, of which 200 were randomly selected.
Data extraction Two researchers independently extracted data on study population, intervention, primary outcome, and the adjustment plan for primary outcomes. They also recorded the magnitude and statistical significance of the intervention effect with and without adjustments, and estimated whether adjustment made a difference in the level of nominal significance. They also compared the analysis plan for model adjustment in the published trial versus the trial protocol with information on the protocol collected from registries, design papers, and communication with all corresponding authors.
Results 54\% of the trials used stratified randomization, 96\% presented baseline characteristics in the compared arms, and 46\% also evaluated differences in baseline factors with statistical testing. Half of the trials performed adjusted analyses for the main outcome, as the sole analysis (29\%) or along with unadjusted analyses (21\%). Adjustment for stratification variables and for baseline variables was performed in 39\% (42/108) and 42\% (84/199) of the trials, respectively. Among 40 comparisons with both adjusted and unadjusted analyses, 43\% had statistically significant effects, 40\% had non-significant effects, and 18\% had significant effects with only one of the two analyses, but not with the other. Information on analysis plan regarding model adjustment was available in 6\% (9/162) of trial registry entries, 78\% (21/27) of design papers, and 74\% (40/54) of protocols obtained from authors. The analysis plan disagreed between the published trial and the registry, protocol, or design paper in 47\% (28/60) of the studies.
Conclusions There is large diversity on whether and how analyses of primary outcomes are adjusted in randomized controlled trials and these choices can sometimes change the nominal significance of the results. Registered protocols should explicitly specify adjustments plans for main outcomes and analysis should follow these plans.},
	pages = {f4313},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Saquib, Nazmus and Saquib, Juliann and Ioannidis, John P. A.},
	urldate = {2021-04-03},
	date = {2013-07-12},
	langid = {english},
	pmid = {23851720},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/X8NXBAYP/Saquib et al. - 2013 - Practices and impact of primary outcome adjustment.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/IJZ2S4MC/bmj.html:text/html},
}

@article{goldacre_compare_2019,
	title = {{COMPare}: a prospective cohort study correcting and monitoring 58 misreported trials in real time},
	volume = {20},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/s13063-019-3173-2},
	doi = {10.1186/s13063-019-3173-2},
	shorttitle = {{COMPare}},
	abstract = {Discrepancies between pre-specified and reported outcomes are an important source of bias in trials. Despite legislation, guidelines and public commitments on correct reporting from journals, outcome misreporting continues to be prevalent. We aimed to document the extent of misreporting, establish whether it was possible to publish correction letters on all misreported trials as they were published, and monitor responses from editors and trialists to understand why outcome misreporting persists despite public commitments to address it.},
	pages = {118},
	number = {1},
	journaltitle = {Trials},
	shortjournal = {Trials},
	author = {Goldacre, Ben and Drysdale, Henry and Dale, Aaron and Milosevic, Ioan and Slade, Eirion and Hartley, Philip and Marston, Cicely and Powell-Smith, Anna and Heneghan, Carl and Mahtani, Kamal R.},
	urldate = {2021-04-03},
	date = {2019-02-14},
	keywords = {Audit, {CONSORT}, Editorial conduct, {ICMJE}, Misreporting, Outcomes, Trials},
	file = {Full Text PDF:/Users/tom/Zotero/storage/UJ3GWRSX/Goldacre et al. - 2019 - COMPare a prospective cohort study correcting and.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XBUFJ49L/s13063-019-3173-2.html:text/html},
}

@article{szollosi_neglected_2019,
	title = {Neglected sources of flexibility in psychological theories: from replicability to good explanations},
	volume = {2},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-019-00045-y},
	doi = {10.1007/s42113-019-00045-y},
	shorttitle = {Neglected sources of flexibility in psychological theories},
	abstract = {How useful are methods aiming to make research findings more replicable—particularly preregistration—for developing good psychological theories? We distinguish between two kinds of flexibility—the flexibility of a theory and the flexibility of a model—and argue that even when attempts are made to reduce model flexibility, the lack of attention to theoretical flexibility renders the utility of such methods questionable. We speculate that psychology’s current issues with replicability and model flexibility would grow increasingly irrelevant as the underlying theories become less flexible. The path towards better theory development requires us to place more attention on the assessment and evaluation of theoretical flexibility.},
	pages = {190--192},
	number = {3},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Szollosi, Aba and Donkin, Chris},
	urldate = {2021-04-03},
	date = {2019-12-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/PB2GCEAW/Szollosi and Donkin - 2019 - Neglected Sources of Flexibility in Psychological .pdf:application/pdf},
}

@article{buxton_avoiding_2021,
	title = {Avoiding wasted research resources in conservation science},
	volume = {3},
	rights = {© 2021 The Authors. Conservation Science and Practice published by Wiley Periodicals {LLC}. on behalf of Society for Conservation Biology},
	issn = {2578-4854},
	url = {https://conbio.onlinelibrary.wiley.com/doi/abs/10.1111/csp2.329},
	doi = {https://doi.org/10.1111/csp2.329},
	abstract = {Scientific evidence is fundamental for guiding effective conservation action to curb biodiversity loss. Yet, research resources in conservation are often wasted due to biased allocation of research effort, irrelevant or low-priority questions, flawed studies, inaccessible research outputs, and biased or poor-quality reporting. We outline a striking example of wasted research resources, highlight a powerful case of data rescue/reuse, and discuss an exemplary model of evidence-informed conservation. We suggest that funding agencies, research institutions, {NGOs}, publishers, and researchers are part of the problem and solutions, and outline recommendations to curb the waste of research resources, including knowledge co-creation and open science practices.},
	pages = {e329},
	number = {2},
	journaltitle = {Conservation Science and Practice},
	author = {Buxton, Rachel T. and Nyboer, Elizabeth A. and Pigeon, Karine E. and Raby, Graham D. and Rytwinski, Trina and Gallagher, Austin J. and Schuster, Richard and Lin, Hsien-Yung and Fahrig, Lenore and Bennett, Joseph R. and Cooke, Steven J. and Roche, Dominique G.},
	urldate = {2021-04-04},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://conbio.onlinelibrary.wiley.com/doi/pdf/10.1111/csp2.329},
	keywords = {open science, research data management, co-production, data rescue and reuse, evidence-informed decision making, {FAIR} data},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3RDGMW3S/Buxton et al. - 2021 - Avoiding wasted research resources in conservation.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/LHN4Y9L6/csp2.html:text/html},
}

@article{ioannidis_why_2005,
	title = {Why most published research findings are false},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	pages = {e124},
	number = {8},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Ioannidis, John P. A.},
	urldate = {2021-04-04},
	date = {2005-08-30},
	langid = {english},
	keywords = {Research design, Randomized controlled trials, Metaanalysis, Genetics of disease, Cancer risk factors, Finance, Genetic epidemiology, Schizophrenia},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ZWMS8GN9/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/22RTY2I3/article.html:text/html},
}

@article{rasmussen_walking_2011,
	title = {Walking the talk: the need for a trial registry for development interventions},
	volume = {3},
	issn = {1943-9342},
	url = {https://doi.org/10.1080/19439342.2011.605160},
	doi = {10.1080/19439342.2011.605160},
	shorttitle = {Walking the talk},
	abstract = {Recent advances in the use of randomised control trials to evaluate the effect of development interventions promise to enhance our knowledge of what works and why. A core argument supporting randomised studies is the claim that they have high internal validity. The authors argue that this claim is weak as long as a trial registry of development interventions is not in place. Without a trial registry, the possibilities for data mining, created by analyses of multiple outcomes and subgroups, undermine internal validity. Drawing on experience from evidence-based medicine and recent examples from microfinance, they argue that a trial registry would also enhance external validity and foster innovative research.},
	pages = {502--519},
	number = {4},
	journaltitle = {Journal of Development Effectiveness},
	author = {Rasmussen, Ole Dahl and Malchow-Møller, Nikolaj and Andersen, Thomas Barnebeck},
	urldate = {2021-04-04},
	date = {2011-12-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/19439342.2011.605160},
	keywords = {impact assessment, randomised control trials, trial registry},
	file = {Rasmussen et al. - 2011 - Walking the talk the need for a trial registry fo.pdf:/Users/tom/Zotero/storage/LKBNTXIH/Rasmussen et al. - 2011 - Walking the talk the need for a trial registry fo.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3Y9MIT8C/19439342.2011.html:text/html},
}

@article{page_prisma_2021,
	title = {{PRISMA} 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews},
	volume = {372},
	rights = {© Author(s) (or their employer(s)) 2019. Re-use permitted under {CC}                 {BY}. No commercial re-use. See rights and permissions. Published by                 {BMJ}.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution ({CC} {BY} 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/372/bmj.n160},
	doi = {10.1136/bmj.n160},
	shorttitle = {{PRISMA} 2020 explanation and elaboration},
	abstract = {{\textless}p{\textgreater}The methods and results of systematic reviews should be reported in sufficient detail to allow users to assess the trustworthiness and applicability of the review findings. The Preferred Reporting Items for Systematic reviews and Meta-Analyses ({PRISMA}) statement was developed to facilitate transparent and complete reporting of systematic reviews and has been updated (to {PRISMA} 2020) to reflect recent advances in systematic review methodology and terminology. Here, we present the explanation and elaboration paper for {PRISMA} 2020, where we explain why reporting of each item is recommended, present bullet points that detail the reporting recommendations, and present examples from published reviews. We hope that changes to the content and structure of {PRISMA} 2020 will facilitate uptake of the guideline and lead to more transparent, complete, and accurate reporting of systematic reviews.{\textless}/p{\textgreater}},
	pages = {n160},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Page, Matthew J. and Moher, David and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hróbjartsson, Asbjørn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and {McDonald}, Steve and {McGuinness}, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and {McKenzie}, Joanne E.},
	urldate = {2021-04-05},
	date = {2021-03-29},
	langid = {english},
	pmid = {33781993},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/WFX8HD94/Page et al. - 2021 - PRISMA 2020 explanation and elaboration updated g.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XS7843GJ/bmj.html:text/html},
}

@article{stewart_why_2012,
	title = {Why prospective registration of systematic reviews makes sense},
	volume = {1},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/2046-4053-1-7},
	doi = {10.1186/2046-4053-1-7},
	abstract = {Prospective registration of systematic reviews promotes transparency, helps reduce potential for bias and serves to avoid unintended duplication of reviews. Registration offers advantages to many stakeholders in return for modest additional effort from the researchers registering their reviews.},
	pages = {7},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Systematic Reviews},
	author = {Stewart, Lesley and Moher, David and Shekelle, Paul},
	urldate = {2021-04-05},
	date = {2012-02-09},
	file = {Full Text PDF:/Users/tom/Zotero/storage/VCZEKD4D/Stewart et al. - 2012 - Why prospective registration of systematic reviews.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UIYRDKG2/2046-4053-1-7.html:text/html},
}

@article{sterne_rob_2019,
	title = {{RoB} 2: a revised tool for assessing risk of bias in randomised trials},
	volume = {366},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/content/366/bmj.l4898},
	doi = {10.1136/bmj.l4898},
	shorttitle = {{RoB} 2},
	abstract = {{\textless}p{\textgreater}Assessment of risk of bias is regarded as an essential component of a systematic review on the effects of an intervention. The most commonly used tool for randomised trials is the Cochrane risk-of-bias tool. We updated the tool to respond to developments in understanding how bias arises in randomised trials, and to address user feedback on and limitations of the original tool.{\textless}/p{\textgreater}},
	pages = {l4898},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Sterne, Jonathan A. C. and Savović, Jelena and Page, Matthew J. and Elbers, Roy G. and Blencowe, Natalie S. and Boutron, Isabelle and Cates, Christopher J. and Cheng, Hung-Yuan and Corbett, Mark S. and Eldridge, Sandra M. and Emberson, Jonathan R. and Hernán, Miguel A. and Hopewell, Sally and Hróbjartsson, Asbjørn and Junqueira, Daniela R. and Jüni, Peter and Kirkham, Jamie J. and Lasserson, Toby and Li, Tianjing and {McAleenan}, Alexandra and Reeves, Barnaby C. and Shepperd, Sasha and Shrier, Ian and Stewart, Lesley A. and Tilling, Kate and White, Ian R. and Whiting, Penny F. and Higgins, Julian P. T.},
	urldate = {2021-04-05},
	date = {2019-08-28},
	langid = {english},
	pmid = {31462531},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/UQ5HVVBC/Sterne et al. - 2019 - RoB 2 a revised tool for assessing risk of bias i.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/LKW6VHRX/bmj.html:text/html},
}

@incollection{higgins_considering_2019,
	edition = {1},
	title = {Considering bias and conflicts of interest among the included studies},
	isbn = {978-1-119-53662-8 978-1-119-53660-4},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119536604.ch7},
	pages = {177--204},
	booktitle = {Cochrane Handbook for Systematic Reviews of Interventions},
	publisher = {Wiley},
	author = {Boutron, Isabelle and Page, Matthew J and Higgins, Julian PT and Altman, Douglas G and Lundh, Andreas and Hróbjartsson, Asbjørn and {on behalf of the Cochrane Bias Methods Group}},
	editor = {Higgins, Julian P.T. and Thomas, James and Chandler, Jacqueline and Cumpston, Miranda and Li, Tianjing and Page, Matthew J. and Welch, Vivian A.},
	urldate = {2021-04-05},
	date = {2019-09-23},
	langid = {english},
	doi = {10.1002/9781119536604.ch7},
	file = {Boutron et al. - 2019 - Considering bias and conflicts of interest among t.pdf:/Users/tom/Zotero/storage/SQ29UYUJ/Boutron et al. - 2019 - Considering bias and conflicts of interest among t.pdf:application/pdf},
}

@book{senn_statistical_2007,
	location = {Chichester, England ; Hoboken, {NJ}},
	edition = {2nd ed},
	title = {Statistical issues in drug development},
	isbn = {978-0-470-01877-4},
	series = {Statistics in practice},
	pagetotal = {498},
	publisher = {John Wiley \& Sons},
	author = {Senn, Stephen},
	date = {2007},
	note = {{OCLC}: ocn180907943},
	keywords = {Clinical Trials as Topic, Drug Design, Drug development, methods, Statistical methods, Statistics as Topic},
}

@article{ijzerman_use_2020,
	title = {Use caution when applying behavioural science to policy},
	volume = {4},
	rights = {2020 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-00990-w},
	doi = {10.1038/s41562-020-00990-w},
	abstract = {Social and behavioural scientists have attempted to speak to the {COVID}-19 crisis. But is behavioural research on {COVID}-19 suitable for making policy decisions? We offer a taxonomy that lets our science advance in ‘evidence readiness levels’ to be suitable for policy. We caution practitioners to take extreme care translating our findings to applications.},
	pages = {1092--1094},
	number = {11},
	journaltitle = {Nature Human Behaviour},
	author = {{IJzerman}, Hans and Lewis, Neil A. and Przybylski, Andrew K. and Weinstein, Netta and {DeBruine}, Lisa and Ritchie, Stuart J. and Vazire, Simine and Forscher, Patrick S. and Morey, Richard D. and Ivory, James D. and Anvari, Farid},
	urldate = {2021-04-05},
	date = {2020-11},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	file = {Full Text PDF:/Users/tom/Zotero/storage/QRMPSZDI/IJzerman et al. - 2020 - Use caution when applying behavioural science to p.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/IR4T9QVA/s41562-020-00990-w.html:text/html},
}

@book{prasad_ending_2019,
	location = {Baltimore},
	title = {Ending Medical Reversal: Improving Outcomes, Saving Lives},
	isbn = {978-1-4214-2904-5},
	shorttitle = {Ending medical reversal},
	abstract = {Medical reversal happens when doctors start using a medication, procedure, or diagnostic tool without a robust evidence base - and then stop using it when it is found not to help, or even to harm, patients. The authors narrate stories from every corner of medicine to explore why medical reversals occur, how they are harmful, and what can be done to avoid them. They explore the difference between medical innovations that improve care and those that only appear to be promising. They also outline a comprehensive plan to reform medical education, research funding and protocols, and the process for approving new drugs that will ensure that more of what gets done in doctors' offices and hospitals is truly effective.},
	publisher = {John Hopkins University Press},
	author = {Prasad, Vinayak K and Cifu, Adam S},
	date = {2019},
	note = {{OCLC}: 1057762570},
}

@incollection{peirce_theory_1883,
	location = {Boston, {MA}},
	title = {A theory of probable inference},
	pages = {126--281},
	booktitle = {Studies in logic},
	publisher = {Little, Brown},
	author = {Peirce, C. S.},
	editor = {Peirce, C. S.},
	date = {1883},
}

@article{spellman_short_2015,
	title = {A short (personal) future history of Revolution 2.0},
	volume = {10},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691615609918},
	doi = {10.1177/1745691615609918},
	abstract = {Crisis of replicability is one term that psychological scientists use for the current introspective phase we are in—I argue instead that we are going through a revolution analogous to a political revolution. Revolution 2.0 is an uprising focused on how we should be doing science now (i.e., in a 2.0 world). The precipitating events of the revolution have already been well-documented: failures to replicate, questionable research practices, fraud, etc. And the fact that none of these events is new to our field has also been well-documented. I suggest four interconnected reasons as to why this time is different: changing technology, changing demographics of researchers, limited resources, and misaligned incentives. I then describe two reasons why the revolution is more likely to catch on this time: technology (as part of the solution) and the fact that these concerns cut across social and life sciences—that is, we are not alone. Neither side in the revolution has behaved well, and each has characterized the other in extreme terms (although, of course, each has had a few extreme actors). Some suggested reforms are already taking hold (e.g., journals asking for more transparency in methods and analysis decisions; journals publishing replications) but the feared tyrannical requirements have, of course, not taken root (e.g., few journals require open data; there is no ban on exploratory analyses). Still, we have not yet made needed advances in the ways in which we accumulate, connect, and extract conclusions from our aggregated research. However, we are now ready to move forward by adopting incremental changes and by acknowledging the multiplicity of goals within psychological science.},
	pages = {886--899},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Spellman, Barbara A.},
	urldate = {2021-04-05},
	date = {2015-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication, methodology, journal practices, scientific practices},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/W9AMEHBE/Spellman - 2015 - A Short (Personal) Future History of Revolution 2..pdf:application/pdf},
}

@article{noauthor_world_2013,
	title = {World Medical Association Declaration of Helsinki: Ethical principles for medical research involving human subjects},
	volume = {310},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2013.281053},
	doi = {10.1001/jama.2013.281053},
	shorttitle = {World Medical Association Declaration of Helsinki},
	pages = {2191},
	number = {20},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	urldate = {2021-04-05},
	date = {2013-11-27},
	langid = {english},
	file = {2013 - World Medical Association Declaration of Helsinki.pdf:/Users/tom/Zotero/storage/7ZUIIIP7/2013 - World Medical Association Declaration of Helsinki.pdf:application/pdf},
}

@article{gelman_preregistration_2013,
	title = {Preregistration of studies and mock reports},
	volume = {21},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/preregistration-of-studies-and-mock-reports/54F37C44329781273C63339B930369C2},
	doi = {10.1093/pan/mps032},
	abstract = {The traditional system of scientific and scholarly publishing is breaking down in two different directions.},
	pages = {40--41},
	number = {1},
	journaltitle = {Political Analysis},
	author = {Gelman, Andrew},
	urldate = {2021-04-05},
	date = {2013},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Full Text PDF:/Users/tom/Zotero/storage/J32DKFAD/Gelman - 2013 - Preregistration of Studies and Mock Reports.pdf:application/pdf},
}

@article{cramer_hidden_2016,
	title = {Hidden multiplicity in exploratory multiway {ANOVA}: Prevalence and remedies},
	volume = {23},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-015-0913-5},
	doi = {10.3758/s13423-015-0913-5},
	shorttitle = {Hidden multiplicity in exploratory multiway {ANOVA}},
	abstract = {Many psychologists do not realize that exploratory use of the popular multiway analysis of variance harbors a multiple-comparison problem. In the case of two factors, three separate null hypotheses are subject to test (i.e., two main effects and one interaction). Consequently, the probability of at least one Type I error (if all null hypotheses are true) is 14 \% rather than 5 \%, if the three tests are independent. We explain the multiple-comparison problem and demonstrate that researchers almost never correct for it. To mitigate the problem, we describe four remedies: the omnibus F test, control of the familywise error rate, control of the false discovery rate, and preregistration of the hypotheses.},
	pages = {640--647},
	number = {2},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Cramer, Angélique O. J. and van Ravenzwaaij, Don and Matzke, Dora and Steingroever, Helen and Wetzels, Ruud and Grasman, Raoul P. P. P. and Waldorp, Lourens J. and Wagenmakers, Eric-Jan},
	urldate = {2021-04-05},
	date = {2016-04-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/KFTWDEIE/Cramer et al. - 2016 - Hidden multiplicity in exploratory multiway ANOVA.pdf:application/pdf},
}

@article{maccallum_practice_2002,
	title = {On the practice of dichotomization of quantitative variables.},
	volume = {7},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.7.1.19},
	doi = {10.1037/1082-989X.7.1.19},
	pages = {19--40},
	number = {1},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {{MacCallum}, Robert C. and Zhang, Shaobo and Preacher, Kristopher J. and Rucker, Derek D.},
	urldate = {2021-04-05},
	date = {2002},
	langid = {english},
	file = {MacCallum et al. - 2002 - On the practice of dichotomization of quantitative.pdf:/Users/tom/Zotero/storage/H94ZZ2CJ/MacCallum et al. - 2002 - On the practice of dichotomization of quantitative.pdf:application/pdf},
}

@report{coenen_distorting_2021,
	title = {The distorting effects of deciding to stop sampling information},
	url = {https://psyarxiv.com/tbrea/},
	abstract = {This paper asks how strategies of information sampling are affected by a learner’s goal. Based on a theoretical analysis and two behavioral experiments, we show that learning goals have a crucial impact on the decision of when to stop sampling. This decision, in turn, affects the statistical properties (e.g. average values, or standard deviations) of the data collected under different goals. Specifically, we find that sampling with the goal of making a binary choice can introduce a correlation between the average value of a sample and its size (the number of values sampled). Across multiple rounds of sampling, this has the potential of biasing learn- ers’ inferences about the underlying process that generated the samples, specifically if learners ignore sample size when making these inferences. We find that people are indeed biased in this way and make different inferences about the same data-generating process when sampling with different learning goals. These findings highlight yet another danger of inferring general patterns from samples of evidence the learner had a hand in collecting.},
	institution = {{PsyArXiv}},
	author = {Coenen, Anna and Gureckis, Todd},
	urldate = {2021-04-05},
	date = {2021-03-16},
	doi = {10.31234/osf.io/tbrea},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Cognitive Psychology, Judgment and Decision Making, toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/36SN3RA9/Coenen and Gureckis - 2021 - The distorting effects of deciding to stop samplin.pdf:application/pdf},
}

@article{page_rethinking_2016,
	title = {Rethinking the assessment of risk of bias due to selective reporting: a cross-sectional study},
	volume = {5},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-016-0289-2},
	doi = {10.1186/s13643-016-0289-2},
	shorttitle = {Rethinking the assessment of risk of bias due to selective reporting},
	abstract = {Selective reporting is included as a core domain of Cochrane’s tool for assessing risk of bias in randomised trials. There has been no evaluation of review authors’ use of this domain. We aimed to evaluate assessments of selective reporting in a cross-section of Cochrane reviews and to outline areas for improvement.},
	pages = {108},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {Page, Matthew J. and Higgins, Julian P. T.},
	urldate = {2021-04-05},
	date = {2016-07-08},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/NAUCF6YD/Page and Higgins - 2016 - Rethinking the assessment of risk of bias due to s.pdf:application/pdf},
}

@article{quine_main_1951,
	title = {Main Trends in Recent Philosophy: Two Dogmas of Empiricism},
	volume = {60},
	issn = {00318108},
	url = {https://www.jstor.org/stable/2181906?origin=crossref},
	doi = {10.2307/2181906},
	shorttitle = {Main Trends in Recent Philosophy},
	pages = {20},
	number = {1},
	journaltitle = {The Philosophical Review},
	shortjournal = {The Philosophical Review},
	author = {Quine, W. V.},
	urldate = {2021-04-05},
	date = {1951-01},
	langid = {english},
	file = {Quine - 1951 - Main Trends in Recent Philosophy Two Dogmas of Em.pdf:/Users/tom/Zotero/storage/LUY7IGLV/Quine - 1951 - Main Trends in Recent Philosophy Two Dogmas of Em.pdf:application/pdf},
}

@book{duhem_aim_1954,
	location = {Princeton, {NJ}},
	edition = {2},
	title = {The Aim and Structure of Physical Theory},
	publisher = {Princeton University Press},
	author = {Duhem, P.},
	translator = {Wiener, P. W.},
	date = {1954},
	note = {original-title: La Théorie Physique: Son Objet et sa Structure
original-publisher: Marcel Riviera \& Cie.
original-publisher-place: Paris
original-date: 1914},
}

@article{simon_problems_1994,
	title = {Problems of multiplicity in clinical trials},
	volume = {42},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/037837589490197X},
	doi = {10.1016/0378-3758(94)90197-X},
	abstract = {Most of the uncertainties in the conclusions of randomized clinical trials results either from inadequate sample size or from problems of multiplicity. Problems of multiplicity result from the testing of multiple hypotheses or the testing of a hypothesis at multiple points in time or in multiple ways. I review several common problems of this type: multiple analyses of accumulating data, analyses of multiple endpoints, multiple subsets of patients, multiple treatment group contrasts and interpreting the results of multiple clinical trials. Multiplicity issues are of importance both within the frequentist and Bayesian frameworks of inference. Within the Neyman-Pearson theory hypotheses must be pre-specified. With Bayesian methods the prior distributions must be pre-specified. I review some recently developed methods for dealing with difficult problems of multiplicity in the above mentioned categories. Multiplicity problems sometimes reflect inadequately specified objectives of the clinical trial and proper solutions involve clarification at the design stage of hypotheses to be tested and analysis strategies to be used.},
	pages = {209--221},
	number = {1},
	journaltitle = {Journal of Statistical Planning and Inference},
	shortjournal = {Journal of Statistical Planning and Inference},
	author = {Simon, Richard},
	urldate = {2021-04-06},
	date = {1994-11-01},
	langid = {english},
	keywords = {multiple comparisons, Bayesian methods, meta-analysis, Clinical trials, multivariate, sequential analysis},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/DXJJ3C8P/Simon - 1994 - Problems of multiplicity in clinical trials.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/FPZYGI4Y/037837589490197X.html:text/html},
}

@article{hardwicke_should_2019,
	title = {Should psychology journals adopt specialized statistical review?},
	volume = {2},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245919858428},
	doi = {10.1177/2515245919858428},
	abstract = {Readers of peer-reviewed research may assume that the reported statistical analyses supporting scientific claims have been closely scrutinized and surpass a high-quality threshold. However, widespread misunderstanding and misuse of statistical concepts and methods suggests that suboptimal or erroneous statistical practice is routinely overlooked during peer review in psychology. Here, we explore whether psychology journals could ameliorate some of the field’s statistical ailments by adopting specialized statistical review: a focused technical assessment, performed by statistical experts, that addresses the analysis and presentation of quantitative information and supplements regular peer review. We discuss evidence from a recent survey of journal editors suggesting that specialized statistical review may be unusual in psychology journals and is regarded by many editors as unnecessary. We contrast these views with those in the biomedical domain, where statistical review has been considered a partial preventive measure against the improper use of statistics since the late 1970s. We suggest that the current “credibility revolution” presents an opportune occasion for psychology journals to consider adopting specialized statistical review.},
	pages = {240--249},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Hardwicke, Tom E. and Frank, Michael C. and Vazire, Simine and Goodman, Steven N.},
	urldate = {2021-04-06},
	date = {2019-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {open data, peer review, metaresearch, open materials, journal policy, statistical methods, statistical review},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/VUPYWFHV/Hardwicke et al. - 2019 - Should Psychology Journals Adopt Specialized Stati.pdf:application/pdf},
}

@article{hardwicke_how_2020,
	title = {How often do leading biomedical journals use statistical experts to evaluate statistical methods? The results of a survey},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0239598},
	doi = {10.1371/journal.pone.0239598},
	shorttitle = {How often do leading biomedical journals use statistical experts to evaluate statistical methods?},
	abstract = {Scientific claims in biomedical research are typically derived from statistical analyses. However, misuse or misunderstanding of statistical procedures and results permeate the biomedical literature, affecting the validity of those claims. One approach journals have taken to address this issue is to enlist expert statistical reviewers. How many journals do this, how statistical review is incorporated, and how its value is perceived by editors is of interest. Here we report an expanded version of a survey conducted more than 20 years ago by Goodman and colleagues (1998) with the intention of characterizing contemporary statistical review policies at leading biomedical journals. We received eligible responses from 107 of 364 (28\%) journals surveyed, across 57 fields, mostly from editors in chief. 34\% (36/107) rarely or never use specialized statistical review, 34\% (36/107) used it for 10–50\% of their articles and 23\% used it for all articles. These numbers have changed little since 1998 in spite of dramatically increased concern about research validity. The vast majority of editors regarded statistical review as having substantial incremental value beyond regular peer review and expressed comparatively little concern about the potential increase in reviewing time, cost, and difficulty identifying suitable statistical reviewers. Improved statistical education of researchers and different ways of employing statistical expertise are needed. Several proposals are discussed.},
	pages = {e0239598},
	number = {10},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Hardwicke, Tom E. and Goodman, Steven N.},
	urldate = {2021-04-06},
	date = {2020-10-01},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Research design, Surveys, Survey research, Scientific publishing, Peer review, Computer software, Statistical methods, Disease informatics},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PB6ZY5PL/Hardwicke and Goodman - 2020 - How often do leading biomedical journals use stati.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XVINN8DL/article.html:text/html},
}

@book{jeffreys_theory_1998,
	location = {Oxford [Oxfordshire] : New York},
	edition = {3rd ed},
	title = {Theory of probability},
	isbn = {978-0-19-850368-2},
	series = {Oxford classic texts in the physical sciences},
	pagetotal = {459},
	publisher = {Clarendon Press ; Oxford University Press},
	author = {Jeffreys, Harold},
	date = {1998},
	langid = {english},
	keywords = {Probabilities},
	file = {Jeffreys - 1998 - Theory of probability.pdf:/Users/tom/Zotero/storage/4MWU8Y44/Jeffreys - 1998 - Theory of probability.pdf:application/pdf},
}

@article{zou_registration_2018,
	title = {Registration, results reporting, and publication bias of clinical trials supporting {FDA} approval of neuropsychiatric drugs before and after {FDAAA}: a retrospective cohort study},
	volume = {19},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/s13063-018-2957-0},
	doi = {10.1186/s13063-018-2957-0},
	shorttitle = {Registration, results reporting, and publication bias of clinical trials supporting {FDA} approval of neuropsychiatric drugs before and after {FDAAA}},
	abstract = {Mandatory trial registration, and later results reporting, were proposed to mitigate selective clinical trial publication and outcome reporting. The Food and Drug Administration ({FDA}) Amendments Act ({FDAAA}) was enacted by Congress on September 27, 2007, requiring the registration of all non-phase I clinical trials involving {FDA}-regulated medical interventions and results reporting for approved drugs. The association between {FDAAA} enactment and the registration, results reporting, and publication bias of neuropsychiatric trials has not been studied.},
	pages = {581},
	number = {1},
	journaltitle = {Trials},
	shortjournal = {Trials},
	author = {Zou, Constance X. and Becker, Jessica E. and Phillips, Adam T. and Garritano, James M. and Krumholz, Harlan M. and Miller, Jennifer E. and Ross, Joseph S.},
	urldate = {2021-04-06},
	date = {2018-10-23},
	keywords = {{ClinicalTrials}.gov, Publication bias, Clinical trial registration, Clinical trial results, {FDAAA}, Selective publication},
	file = {Full Text PDF:/Users/tom/Zotero/storage/WRJ3LCNP/Zou et al. - 2018 - Registration, results reporting, and publication b.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/L5HZA6E3/s13063-018-2957-0.html:text/html},
}

@article{vries_cumulative_2018-1,
	title = {The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression},
	volume = {48},
	issn = {0033-2917, 1469-8978},
	url = {https://www.cambridge.org/core/journals/psychological-medicine/article/cumulative-effect-of-reporting-and-citation-biases-on-the-apparent-efficacy-of-treatments-the-case-of-depression/71D73CADE32C0D3D996DABEA3FCDBF57},
	doi = {10.1017/S0033291718001873},
	shorttitle = {The cumulative effect of reporting and citation biases on the apparent efficacy of treatments},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0033291718001873/resource/name/{firstPage}-S0033291718001873a.jpg},
	pages = {2453--2455},
	number = {15},
	journaltitle = {Psychological Medicine},
	author = {Vries, Y. A. de and Roest, A. M. and Jonge, P. de and Cuijpers, P. and Munafò, M. R. and Bastiaansen, J. A.},
	urldate = {2021-04-06},
	date = {2018-11},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {bias, Antidepressants, citation bias, depression, psychotherapy, reporting bias},
	file = {Full Text PDF:/Users/tom/Zotero/storage/JNZ5SK57/Vries et al. - 2018 - The cumulative effect of reporting and citation bi.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/HY9RSNCB/71D73CADE32C0D3D996DABEA3FCDBF57.html:text/html},
}

@article{turner_selective_2008,
	title = {Selective publication of antidepressant trials and its influence on apparent efficacy},
	volume = {358},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMsa065779},
	doi = {10.1056/NEJMsa065779},
	abstract = {This study compares the published data on a dozen antidepressant drugs with analyses of the same drugs by the Food and Drug Administration. The results of studies in the entire database were less likely to be favorable to the drug than those in the published literature.},
	pages = {252--260},
	number = {3},
	journaltitle = {New England Journal of Medicine},
	author = {Turner, Erick H. and Matthews, Annette M. and Linardatos, Eftihia and Tell, Robert A. and Rosenthal, Robert},
	urldate = {2021-04-06},
	date = {2008-01-17},
	pmid = {18199864},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJMsa}065779},
	file = {Full Text PDF:/Users/tom/Zotero/storage/255KDQ36/Turner et al. - 2008 - Selective Publication of Antidepressant Trials and.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DFKUVXX7/NEJMsa065779.html:text/html},
}

@article{phillips_association_2017,
	title = {Association of the {FDA} Amendment Act with trial registration, publication, and outcome reporting},
	volume = {18},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/s13063-017-2068-3},
	doi = {10.1186/s13063-017-2068-3},
	abstract = {Selective clinical trial publication and outcome reporting has the potential to bias the medical literature. The 2007 Food and Drug Administration ({FDA}) Amendment Act ({FDAAA}) mandated clinical trial registration and outcome reporting on {ClinicalTrials}.gov, a publicly accessible trial registry.},
	pages = {333},
	number = {1},
	journaltitle = {Trials},
	shortjournal = {Trials},
	author = {Phillips, Adam T. and Desai, Nihar R. and Krumholz, Harlan M. and Zou, Constance X. and Miller, Jennifer E. and Ross, Joseph S.},
	urldate = {2021-04-06},
	date = {2017-07-18},
	keywords = {Clinical trials, Drug approval, Publications, United States Food and Drug Administration},
	file = {Full Text PDF:/Users/tom/Zotero/storage/YGJ5GWY7/Phillips et al. - 2017 - Association of the FDA Amendment Act with trial re.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/G74NPH6V/s13063-017-2068-3.html:text/html},
}

@report{hofler_call_2021,
	title = {A call to accompany pre-registration with means to valuable and effective exploration},
	url = {https://psyarxiv.com/psfqw/},
	abstract = {Harking and p-hacking are considered as main causes of the replication crisis, and pre-registration of hypotheses and analyses is regarded as the best countermeasure so far. However, the pressure to produce purportedly confirming results through hidden exploration largely resists. While some pre-registration advocates encourage the alternative of honest and comprehensive explorative research, others accuse pre-registration of harming exploration. We argue that researchers must be equipped with competencies on valuable exploration if pre-registration is to become mainstream and exploration is to be freed from its narrowed and flawed purpose. We discuss what valuable exploration should be: honest, as full as necessary (especially in new research domains) and as efficient as possible (resulting in only few and presumably true new hypotheses). After discussions on methods for filtering explorative results and pre-registration we end with a short research agenda on exploration and proposals for implementation to stakeholders (peer-reviewers, journal editors and funding agencies) who have the means both to enforce pre-registration and to provide room for valuable exploration. For example, we propose that yet a study abstract must contain separated lists of confirmatory results and new hypotheses obtained from exploration.},
	institution = {{PsyArXiv}},
	author = {Höfler, Michael and Scherbaum, Stefan and Kanske, Philipp and Kanske, Philipp},
	urldate = {2021-04-07},
	date = {2021-04-04},
	doi = {10.31234/osf.io/psfqw},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, replication, Theory and Philosophy of Science, pre-registration, confirmation, editors, Exploration, filtering, reviewers, smoothing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/YY6G9VQJ/Höfler et al. - 2021 - A call to accompany pre-registration with means to.pdf:application/pdf},
}

@article{ferrero_is_2021,
	title = {Is project-based learning effective among kindergarten and elementary students? A systematic review},
	volume = {16},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0249627},
	doi = {10.1371/journal.pone.0249627},
	shorttitle = {Is project-based learning effective among kindergarten and elementary students?},
	abstract = {Project-based learning ({PjBL}) is becoming widespread in many schools. However, the evidence of its effectiveness in the classroom is still limited, especially in basic education. The aim of the present study was to perform a systematic review of the empirical evidence assessing the impact of {PjBL} on academic achievement of kindergarten and elementary students. We also examined the quality of studies, their compliance with basic prerequisites for a successful result, and their fidelity towards the key elements of {PBL} intervention. For this objective, we conducted a literature search in January 2020. The inclusion criteria for the review required that studies followed a pre-post design with control group and measured quantitatively the impact of {PBL} on content knowledge of students. The final sample included eleven articles comprising data from 722 students. The studies yielded inconclusive results, had important methodological flaws, and reported insufficient or no information about important aspects of the materials, procedure and key requirements from students and instructors to guarantee the success of {PjBL}. Educational implications of these results are discussed.},
	pages = {e0249627},
	number = {4},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Ferrero, Marta and Vadillo, Miguel A. and León, Samuel P.},
	urldate = {2021-04-07},
	date = {2021-04-02},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Metaanalysis, Systematic reviews, Human learning, toread, Education, Instructors, Schools, Teachers, Turkey (country)},
	file = {Full Text PDF:/Users/tom/Zotero/storage/88T3NNXW/Ferrero et al. - 2021 - Is project-based learning effective among kinderga.pdf:application/pdf},
}

@article{devito_compliance_2020,
	title = {Compliance with legal requirement to report clinical trial results on {ClinicalTrials}.gov: a cohort study},
	volume = {395},
	issn = {0140-6736},
	url = {https://www.sciencedirect.com/science/article/pii/S0140673619332209},
	doi = {10.1016/S0140-6736(19)33220-9},
	shorttitle = {Compliance with legal requirement to report clinical trial results on {ClinicalTrials}.gov},
	abstract = {Background
Failure to report the results of a clinical trial can distort the evidence base for clinical practice, breaches researchers' ethical obligations to participants, and represents an important source of research waste. The Food and Drug Administration Amendments Act ({FDAAA}) of 2007 now requires sponsors of applicable trials to report their results directly onto {ClinicalTrials}.gov within 1 year of completion. The first trials covered by the Final Rule of this act became due to report results in January, 2018. In this cohort study, we set out to assess compliance.
Methods
We downloaded data for all registered trials on {ClinicalTrials}.gov each month from March, 2018, to September, 2019. All cross-sectional analyses in this manuscript were performed on data extracted from {ClinicalTrials}.gov on Sept 16, 2019; monthly trends analysis used archived data closest to the 15th day of each month from March, 2018, to September, 2019. Our study cohort included all applicable trials due to report results under {FDAAA}. We excluded all non-applicable trials, those not yet due to report, and those given a certificate allowing for delayed reporting. A trial was considered reported if results had been submitted and were either publicly available, or undergoing quality control review at {ClinicalTrials}.gov. A trial was considered compliant if these results were submitted within 1 year of the primary completion date, as required by the legislation. We described compliance with the {FDAAA} 2007 Final Rule, assessed trial characteristics associated with results reporting using logistic regression models, described sponsor-level reporting, examined trends in reporting, and described time-to-report using the Kaplan-Meier method.
Findings
4209 trials were due to report results; 1722 (40·9\%; 95\% {CI} 39·4–42·2) did so within the 1-year deadline. 2686 (63·8\%; 62·4–65·3) trials had results submitted at any time. Compliance has not improved since July, 2018. Industry sponsors were significantly more likely to be compliant than non-industry, non-{US} Government sponsors (odds ratio [{OR}] 3·08 [95\% {CI} 2·52–3·77]), and sponsors running large numbers of trials were significantly more likely to be compliant than smaller sponsors ({OR} 11·84 [9·36–14·99]). The median delay from primary completion date to submission date was 424 days (95\% {CI} 412–435), 59 days higher than the legal reporting requirement of 1 year.
Interpretation
Compliance with the {FDAAA} 2007 is poor, and not improving. To our knowledge, this is the first study to fully assess compliance with the Final Rule of the {FDAAA} 2007. Poor compliance is likely to reflect lack of enforcement by regulators. Effective enforcement and action from sponsors is needed; until then, open public audit of compliance for each individual sponsor may help. We will maintain updated compliance data for each individual sponsor and trial at fdaaa.trialstracker.net.
Funding
Laura and John Arnold Foundation.},
	pages = {361--369},
	number = {10221},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {{DeVito}, Nicholas J and Bacon, Seb and Goldacre, Ben},
	urldate = {2021-04-07},
	date = {2020-02-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/3LU4M42R/DeVito et al. - 2020 - Compliance with legal requirement to report clinic.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/DJZCNB6K/S0140673619332209.html:text/html},
}

@article{becker_reporting_2014,
	title = {Reporting of results in {ClinicalTrials}.gov and high-impact journals},
	volume = {311},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2013.285634},
	doi = {10.1001/jama.2013.285634},
	pages = {1063--1065},
	number = {10},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Becker, Jessica E. and Krumholz, Harlan M. and Ben-Josef, Gal and Ross, Joseph S.},
	urldate = {2021-04-07},
	date = {2014-03-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/V2HFXM2L/Becker et al. - 2014 - Reporting of Results in ClinicalTrials.gov and Hig.pdf:application/pdf},
}

@article{hartung_reporting_2014,
	title = {Reporting discrepancies between the clinicaltrials.gov results database and peer-reviewed publications},
	volume = {160},
	issn = {0003-4819},
	url = {https://www.acpjournals.org/doi/10.7326/M13-0480},
	doi = {10.7326/M13-0480},
	pages = {477--483},
	number = {7},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Hartung, Daniel M. and Zarin, Deborah A. and Guise, Jeanne-Marie and {McDonagh}, Marian and Paynter, Robin and Helfand, Mark},
	urldate = {2021-04-07},
	date = {2014-04-01},
	note = {Publisher: American College of Physicians},
	file = {Full Text PDF:/Users/tom/Zotero/storage/RGUFL6M9/Hartung et al. - 2014 - Reporting Discrepancies Between the ClinicalTrials.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RT9U23JL/M13-0480.html:text/html},
}

@article{ofosu_pre-analysis_2020,
	title = {Pre-analysis plans: an early stocktaking},
	issn = {1537-5927, 1541-0986},
	url = {https://www.cambridge.org/core/journals/perspectives-on-politics/article/preanalysis-plans-an-early-stocktaking/94E7FAE76001C45A04E8F5E272C773CE},
	doi = {10.1017/S1537592721000931},
	shorttitle = {Pre-analysis plans},
	abstract = {Pre-analysis plans ({PAPs}) have been championed as a solution to the problem of research credibility, but without any evidence that {PAPs} actually bolster the credibility of research. We analyze a representative sample of 195 {PAPs} registered on the Evidence in Governance and Politics ({EGAP}) and American Economic Association ({AEA}) registration platforms to assess whether {PAPs} registered in the early days of pre-registration (2011–2016) were sufficiently clear, precise, and comprehensive to achieve their objective of preventing “fishing” and reducing the scope for post-hoc adjustment of research hypotheses. We also analyze a subset of ninety-three {PAPs} from projects that resulted in publicly available papers to ascertain how faithfully they adhere to their pre-registered specifications and hypotheses. We find significant variation in the extent to which {PAPs} registered during this period accomplished the goals they were designed to achieve. We discuss these findings in light of both the costs and benefits of pre-registration, showing how our results speak to the various arguments that have been made in support of and against {PAPs}. We also highlight the norms and institutions that will need to be strengthened to augment the power of {PAPs} to improve research credibility and to create incentives for researchers to invest in both producing and policing them.},
	pages = {1--17},
	journaltitle = {Perspectives on Politics},
	author = {Ofosu, George K. and Posner, Daniel N.},
	urldate = {2021-04-07},
	date = {2020},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Full Text PDF:/Users/tom/Zotero/storage/LWPQT2CE/Ofosu and Posner - Pre-Analysis Plans An Early Stocktaking.pdf:application/pdf},
}

@article{baker_1500_2016,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	url = {http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the ‘crisis’ rocking research.},
	pages = {452},
	number = {7604},
	journaltitle = {Nature News},
	author = {Baker, Monya},
	urldate = {2021-04-07},
	date = {2016-05-26},
	langid = {english},
	note = {Section: News Feature},
	file = {Full Text:/Users/tom/Zotero/storage/CNETYEKP/Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PKY7GF2I/1-500-scientists-lift-the-lid-on-reproducibility-1.html:text/html},
}

@article{orben_association_2019,
	title = {The association between adolescent well-being and digital technology use},
	volume = {3},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0506-1},
	doi = {10.1038/s41562-018-0506-1},
	abstract = {The widespread use of digital technologies by young people has spurred speculation that their regular use negatively impacts psychological well-being. Current empirical evidence supporting this idea is largely based on secondary analyses of large-scale social datasets. Though these datasets provide a valuable resource for highly powered investigations, their many variables and observations are often explored with an analytical flexibility that marks small effects as statistically significant, thereby leading to potential false positives and conflicting results. Here we address these methodological challenges by applying specification curve analysis ({SCA}) across three large-scale social datasets (total n = 355,358) to rigorously examine correlational evidence for the effects of digital technology on adolescents. The association we find between digital technology use and adolescent well-being is negative but small, explaining at most 0.4\% of the variation in well-being. Taking the broader context of the data into account suggests that these effects are too small to warrant policy change.},
	pages = {173--182},
	number = {2},
	journaltitle = {Nature Human Behaviour},
	author = {Orben, Amy and Przybylski, Andrew K.},
	urldate = {2021-04-08},
	date = {2019-02},
	langid = {english},
	note = {Number: 2
Publisher: Nature Publishing Group},
	file = {Snapshot:/Users/tom/Zotero/storage/MUKB6NTA/s41562-018-0506-1.html:text/html;Submitted Version:/Users/tom/Zotero/storage/35DKS5JL/Orben and Przybylski - 2019 - The association between adolescent well-being and .pdf:application/pdf},
}

@article{fanelli_how_2009,
	title = {How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data},
	volume = {4},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005738},
	doi = {10.1371/journal.pone.0005738},
	shorttitle = {How many scientists fabricate and falsify research?},
	abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, “cooking” of data, etc… Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%{CI}: 0.86–4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once –a serious form of misconduct by any standard– and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% {CI}: 9.91–19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words “falsification” or “fabrication”, and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
	pages = {e5738},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Fanelli, Daniele},
	urldate = {2021-04-08},
	date = {2009-05-29},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Surveys, Scientific misconduct, Medicine and health sciences, Medical journals, Metaanalysis, Scientists, Deception, Social research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/5H8SUI2J/Fanelli - 2009 - How Many Scientists Fabricate and Falsify Research.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N9ZVRRV9/article.html:text/html},
}

@article{veldkamp_who_2017,
	title = {Who believes in the storybook image of the scientist?},
	volume = {24},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2016.1268922},
	doi = {10.1080/08989621.2016.1268922},
	abstract = {Do lay people and scientists themselves recognize that scientists are human and therefore prone to human fallibilities such as error, bias, and even dishonesty? In a series of three experimental studies and one correlational study (total N = 3,278) we found that the “storybook image of the scientist” is pervasive: American lay people and scientists from over 60 countries attributed considerably more objectivity, rationality, open-mindedness, intelligence, integrity, and communality to scientists than to other highly-educated people. Moreover, scientists perceived even larger differences than lay people did. Some groups of scientists also differentiated between different categories of scientists: established scientists attributed higher levels of the scientific traits to established scientists than to early-career scientists and Ph.D. students, and higher levels to Ph.D. students than to early-career scientists. Female scientists attributed considerably higher levels of the scientific traits to female scientists than to male scientists. A strong belief in the storybook image and the (human) tendency to attribute higher levels of desirable traits to people in one’s own group than to people in other groups may decrease scientists’ willingness to adopt recently proposed practices to reduce error, bias and dishonesty in science.},
	pages = {127--151},
	number = {3},
	journaltitle = {Accountability in Research},
	author = {Veldkamp, Coosje L. S. and Hartgerink, Chris H. J. and Assen, Marcel A. L. M. van and Wicherts, Jelte M.},
	urldate = {2021-04-08},
	date = {2017-04-03},
	pmid = {28001440},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2016.1268922},
	keywords = {Bias, integrity, fallibility, {RCR}, scientists},
	file = {Full Text PDF:/Users/tom/Zotero/storage/FTR85ZDJ/Veldkamp et al. - 2017 - Who Believes in the Storybook Image of the Scienti.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5HR8H5HP/08989621.2016.html:text/html},
}

@article{walster_proposal_1970,
	title = {A proposal for a new editorial policy in the social sciences},
	volume = {24},
	abstract = {Kurtosis is best described not as a measure of peakedness versus flatness, as in most texts, but as a measure of unimodality versus bimodality.},
	pages = {16--19},
	number = {2},
	journaltitle = {The American Statistician},
	author = {Walster, G William and Cleary, T Anne},
	date = {1970},
	langid = {english},
	file = {Walster and Cleary - 2021 - A Proposal for a New Editorial Policy in the Socia.pdf:/Users/tom/Zotero/storage/PQENIEH2/Walster and Cleary - 2021 - A Proposal for a New Editorial Policy in the Socia.pdf:application/pdf},
}

@article{fanelli_negative_2012,
	title = {Negative results are disappearing from most disciplines and countries},
	volume = {90},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-011-0494-7},
	doi = {10.1007/s11192-011-0494-7},
	abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have “tested” a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
	pages = {891--904},
	number = {3},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Fanelli, Daniele},
	urldate = {2021-04-08},
	date = {2012-03-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/NHF8K44E/Fanelli - 2012 - Negative results are disappearing from most discip.pdf:application/pdf},
}

@article{sterling_publication_1959,
	title = {Publication decisions and their possible effects on inferences drawn from tests of significance--or vice versa},
	volume = {54},
	pages = {30--34},
	number = {285},
	journaltitle = {Journal of the American Statistical Association},
	author = {Sterling, Theodore D},
	date = {1959},
	langid = {english},
	file = {Sterling - 2021 - Publication Decisions and Their Possible Effects o.pdf:/Users/tom/Zotero/storage/9N8DSEC3/Sterling - 2021 - Publication Decisions and Their Possible Effects o.pdf:application/pdf},
}

@article{cooper_finding_1997,
	title = {Finding the missing science: the fate of studies submitted for review by a human subjects committee},
	volume = {2},
	pages = {447--452},
	number = {4},
	journaltitle = {Psychological Methods},
	author = {Cooper, Harris and {DeNeve}, Kristina and Charlton, Kelly},
	date = {1997},
	langid = {english},
	file = {Cooper et al. - Finding the Missing Science The Fate of Studies S.pdf:/Users/tom/Zotero/storage/88SMN5FB/Cooper et al. - Finding the Missing Science The Fate of Studies S.pdf:application/pdf},
}

@article{sterne_recommendations_2011,
	title = {Recommendations for examining and interpreting funnel plot asymmetry in meta-analyses of randomised controlled trials},
	volume = {343},
	rights = {© {BMJ} Publishing Group Ltd 2011},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/343/bmj.d4002},
	doi = {10.1136/bmj.d4002},
	abstract = {{\textless}p{\textgreater}Funnel plots, and tests for funnel plot asymmetry, have been widely used to examine bias in the results of meta-analyses. Funnel plot asymmetry should not be equated with publication bias, because it has a number of other possible causes. This article describes how to interpret funnel plot asymmetry, recommends appropriate tests, and explains the implications for choice of meta-analysis model{\textless}/p{\textgreater}},
	pages = {d4002},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Sterne, Jonathan A. C. and Sutton, Alex J. and Ioannidis, John P. A. and Terrin, Norma and Jones, David R. and Lau, Joseph and Carpenter, James and Rücker, Gerta and Harbord, Roger M. and Schmid, Christopher H. and Tetzlaff, Jennifer and Deeks, Jonathan J. and Peters, Jaime and Macaskill, Petra and Schwarzer, Guido and Duval, Sue and Altman, Douglas G. and Moher, David and Higgins, Julian P. T.},
	urldate = {2021-04-08},
	date = {2011-07-22},
	langid = {english},
	pmid = {21784880},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/R78YZL5B/Sterne et al. - 2011 - Recommendations for examining and interpreting fun.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/EP22RIN3/bmj.html:text/html},
}

@article{camerer_evaluating_2018,
	title = {Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015},
	volume = {2},
	rights = {2018 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0399-z},
	doi = {10.1038/s41562-018-0399-z},
	abstract = {Being able to replicate scientific findings is crucial for scientific progress1–15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516–36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
	pages = {637--644},
	number = {9},
	journaltitle = {Nature Human Behaviour},
	author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
	urldate = {2021-04-08},
	date = {2018-09},
	langid = {english},
	note = {Number: 9
Publisher: Nature Publishing Group},
	file = {Accepted Version:/Users/tom/Zotero/storage/5GB7CNKN/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/YRED35DN/s41562-018-0399-z.html:text/html},
}

@article{camerer_evaluating_2016,
	title = {Evaluating replicability of laboratory experiments in economics},
	volume = {351},
	rights = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/351/6280/1433},
	doi = {10.1126/science.aaf0918},
	abstract = {Another social science looks at itself
Experimental economists have joined the reproducibility discussion by replicating selected published experiments from two top-tier journals in economics. Camerer et al. found that two-thirds of the 18 studies examined yielded replicable estimates of effect size and direction. This proportion is somewhat lower than unaffiliated experts were willing to bet in an associated prediction market, but roughly in line with expectations from sample sizes and P values.
Science, this issue p. 1433
The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs.
By several metrics, economics experiments do replicate, although not as often as predicted.
By several metrics, economics experiments do replicate, although not as often as predicted.},
	pages = {1433--1436},
	number = {6280},
	journaltitle = {Science},
	author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	urldate = {2021-04-08},
	date = {2016-03-25},
	langid = {english},
	pmid = {26940865},
	note = {Publisher: American Association for the Advancement of Science
Section: Report},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HKLWDV85/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JL6W7KV5/1433.html:text/html},
}

@article{mcintosh_exploratory_2017,
	title = {Exploratory reports: A new article type for Cortex},
	volume = {96},
	issn = {0010-9452},
	url = {https://www.sciencedirect.com/science/article/pii/S0010945217302393},
	doi = {10.1016/j.cortex.2017.07.014},
	shorttitle = {Exploratory reports},
	pages = {A1--A4},
	journaltitle = {Cortex},
	shortjournal = {Cortex},
	author = {{McIntosh}, Robert D.},
	urldate = {2021-04-09},
	date = {2017-11-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/S5NDFCKP/McIntosh - 2017 - Exploratory reports A new article type for Cortex.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/Z2ASUDRU/S0010945217302393.html:text/html},
}

@article{etz_statistical_nodate,
	title = {Statistical inference in behavioral research: traditional and Bayesian approaches},
	pages = {12},
	author = {Etz, Alexander and Goodman, Steven N and Vandekerckhove, Joachim},
	langid = {english},
	keywords = {toread},
	file = {Etz et al. - Statistical inference in behavioral research trad.pdf:/Users/tom/Zotero/storage/XY4KJXUX/Etz et al. - Statistical inference in behavioral research trad.pdf:application/pdf},
}

@article{forscher_chaos_1963,
	title = {Chaos in the Brickyard},
	volume = {142},
	rights = {© 1963},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/142/3590/339.1},
	doi = {10.1126/science.142.3590.339},
	pages = {339--339},
	number = {3590},
	journaltitle = {Science},
	author = {Forscher, Bernard K.},
	urldate = {2021-04-10},
	date = {1963-10-18},
	langid = {english},
	pmid = {17799464},
	note = {Publisher: American Association for the Advancement of Science
Section: Letters},
	file = {Snapshot:/Users/tom/Zotero/storage/7PRNNH3E/339.html:text/html},
}

@online{noauthor_paradoxical_nodate,
	title = {Paradoxical Effects of Thought Suppression {\textbar} Ovid},
	url = {https://oce.ovid.com/article/00005205-198707000-00001/HTML},
	urldate = {2021-04-10},
	file = {Paradoxical Effects of Thought Suppression | Ovid:/Users/tom/Zotero/storage/3KS4ZULH/HTML.html:text/html},
}

@incollection{elman_pre-registration_2020,
	edition = {1},
	title = {Pre-registration and Results-Free Review in Observational and Qualitative Research},
	isbn = {978-1-108-76251-9 978-1-108-48677-4 978-1-108-70828-9},
	url = {https://www.cambridge.org/core/product/identifier/9781108762519%23CN-bp-9/type/book_part},
	pages = {221--264},
	booktitle = {The Production of Knowledge},
	publisher = {Cambridge University Press},
	author = {Jacobs, Alan M.},
	editor = {Elman, Colin and Gerring, John and Mahoney, James},
	urldate = {2021-04-10},
	date = {2020-03-31},
	langid = {english},
	doi = {10.1017/9781108762519.009},
	file = {Jacobs - 2020 - Pre-registration and Results-Free Review in Observ.pdf:/Users/tom/Zotero/storage/Q6C3VINX/Jacobs - 2020 - Pre-registration and Results-Free Review in Observ.pdf:application/pdf},
}

@article{button_preventing_2016,
	title = {Preventing the ends from justifying the means: withholding results to address publication bias in peer-review},
	volume = {4},
	issn = {2050-7283},
	url = {https://doi.org/10.1186/s40359-016-0167-7},
	doi = {10.1186/s40359-016-0167-7},
	shorttitle = {Preventing the ends from justifying the means},
	abstract = {The evidence that many of the findings in the published literature may be unreliable is compelling. There is an excess of positive results, often from studies with small sample sizes, or other methodological limitations, and the conspicuous absence of null findings from studies of a similar quality. This distorts the evidence base, leading to false conclusions and undermining scientific progress. Central to this problem is a peer-review system where the decisions of authors, reviewers, and editors are more influenced by impressive results than they are by the validity of the study design. To address this, {BMC} Psychology is launching a pilot to trial a new ‘results-free’ peer-review process, whereby editors and reviewers are blinded to the study’s results, initially assessing manuscripts on the scientific merits of the rationale and methods alone. The aim is to improve the reliability and quality of published research, by focusing editorial decisions on the rigour of the methods, and preventing impressive ends justifying poor means.},
	pages = {1--7},
	number = {59},
	journaltitle = {{BMC} Psychology},
	shortjournal = {{BMC} Psychology},
	author = {Button, Katherine S. and Bal, Liz and Clark, Anna and Shipley, Tim},
	urldate = {2021-04-10},
	date = {2016-12-01},
	keywords = {Transparency, Publication bias, Peer review, Results-free review},
	file = {Full Text PDF:/Users/tom/Zotero/storage/8HARBZMF/Button et al. - 2016 - Preventing the ends from justifying the means wit.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6UNIIK6T/s40359-016-0167-7.html:text/html},
}

@article{nosek_registered_2014,
	title = {Registered reports: A method to increase the credibility of published results.},
	volume = {45},
	issn = {2151-2590},
	url = {https://psycnet.apa.org/fulltext/2014-20922-001.pdf},
	doi = {10.1027/1864-9335/a000192},
	shorttitle = {Registered reports},
	pages = {137--141},
	number = {3},
	journaltitle = {Social Psychology},
	author = {Nosek, Brian A. and Lakens, Daniël},
	urldate = {2021-04-10},
	date = {2014},
	note = {Publisher: Germany: Hogrefe Publishing},
	file = {Snapshot:/Users/tom/Zotero/storage/ITQ3QUU5/2014-20922-001.html:text/html;Submitted Version:/Users/tom/Zotero/storage/EP6VV4NJ/Nosek and Lakens - Registered reports A method to increase the credi.pdf:application/pdf},
}

@article{locascio_impact_2019,
	title = {The impact of results blind science publishing on statistical consultation and collaboration},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1505658},
	doi = {10.1080/00031305.2018.1505658},
	abstract = {The author has previously proposed results blind manuscript evaluation ({RBME}) as a method of ameliorating often cited problems of statistical inference and scientific publication, notably publication bias, overuse/misuse of null hypothesis significance testing ({NHST}), and irreproducibility of reported scientific results. In {RBME}, manuscripts submitted to scientific journals are assessed for suitability for publication without regard to their reported results. Criteria for publication are based exclusively on the substantive importance of the research question addressed in the study, conveyed in the Introduction section of the manuscript, and the quality of the methodology, as reported in the Methods section. Practically, this policy is implemented by a two stage process whereby the editor initially distributes only the Introduction and Methods sections of a submitted manuscript to reviewers and a provisional decision regarding acceptance is made, followed by a second stage in which the complete manuscript is distributed for review but only if the decision of the first stage is for acceptance. The present paper expands upon this recommendation by addressing implications of this proposed policy with respect to statistical consultation and collaboration in research. It is suggested that under {RBME}, statisticians will become more integrated into research endeavors and called upon sooner for their input.},
	pages = {346--351},
	issue = {sup1},
	journaltitle = {The American Statistician},
	author = {Locascio, Joseph J.},
	urldate = {2021-04-11},
	date = {2019-03-29},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1505658},
	keywords = {Significance testing, Publication bias, Publishing, Results-Blind},
	file = {Full Text PDF:/Users/tom/Zotero/storage/GNWC69E6/Locascio - 2019 - The Impact of Results Blind Science Publishing on .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/M2HBBMX5/00031305.2018.html:text/html},
}

@article{dal-re_compliance_2016,
	title = {Compliance with prospective trial registration guidance remained low in high-impact journals and has implications for primary end point reporting},
	volume = {75},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435616000482},
	doi = {10.1016/j.jclinepi.2016.01.017},
	abstract = {Objectives
To examine compliance with International Committee of Medical Journal Editors' ({ICMJE}) policy on prospective trial registration along with predictors of compliance.
Study Design and Setting
Cross-sectional analysis of all articles reporting trial results published in the six highest-impact general medicine journals in January–June 2014 that were registered in a public trial registry. The main outcome measure was compliance with {ICMJE} policy. The time frame for trial primary end point ascertainment was used to assess whether retrospective registration could have allowed changing of primary end points following an interim analysis.
Results
Forty of 144 (28\%) articles did not comply with the {ICMJE} policy. Trials of non–{FDA}-regulated interventions were less compliant than trials of {FDA}-regulated interventions (i.e., medicines, medical devices) (42\% vs. 21\%; P = 0.016). Twenty-nine of these 40 (72\%; 20\% overall) were registered before any interim analysis of primary end points could have been conducted; 11 (28\%; 8\% overall) were registered after primary end point ascertainment, such that investigators could have had the opportunity to conduct an interim analysis before trial registration.
Conclusion
Twenty-eight percent of trials published in high-impact journals were retrospectively registered including nearly 10\% that were registered after primary end point ascertainment could have had taken place. Prospective registration should be prompted and enforced to ensure transparency and accountability in clinical research.},
	pages = {100--107},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Dal-Ré, Rafael and Ross, Joseph S. and Marušić, Ana},
	urldate = {2021-04-11},
	date = {2016-07-01},
	langid = {english},
	keywords = {Clinical trials, {ICMJE}, Compliance, Journal editors, Nonregulated intervention trials, Policy, Prospective registration},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/5DIKBSQH/Dal-Ré et al. - 2016 - Compliance with prospective trial registration gui.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/4WACCXUA/S0895435616000482.html:text/html},
}

@incollection{bem_writing_2004,
	location = {Washington, {DC}, {US}},
	edition = {2},
	title = {Writing the empirical journal article},
	isbn = {978-1-59147-035-9},
	abstract = {The purpose of this chapter is to enhance the chances that academic psychologists get their empirical articles published. Because I write, review, and edit primarily for journals in personality and social psychology, I have drawn most of my examples from those areas. Colleagues assure me, however, that the guidelines set forth are also pertinent for articles in experimental psychology and biopsychology. Similarly, this chapter focuses on an empirical study, but the general writing suggestions apply as well to the theoretical articles, literature reviews, and methodological contributions that also appear in psychology journals. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {185--219},
	booktitle = {The Compleat Academic: A Career Guide},
	publisher = {American Psychological Association},
	author = {Bem, Daryl J.},
	date = {2004},
	keywords = {Scientific Communication, Experimental Psychology, Written Communication},
	file = {Snapshot:/Users/tom/Zotero/storage/Z38GMX7Q/2003-06256-010.html:text/html},
}

@report{forestier_ego_2021,
	title = {Ego depletion: A review of criticisms along with new perspectives for the replicable investigation of self-control fatigue as a multicomponent phenomenon},
	url = {https://psyarxiv.com/spm9a/},
	shorttitle = {Ego depletion},
	abstract = {The replication crisis in psychology has led to question popular psychological phenomena such as ego depletion, which has been criticized after studies failed to replicate. Here, we describe limitations in the literature that contributed to these failures and suggest how they may be addressed. At the theoretical level, the literature focuses on two out of at least eight identified auxiliary hypotheses. Thus, the majority of the hypotheses related to the three core assumptions of the ego-depletion theory have been overlooked, thereby preventing the rejection of the theory as a whole. At the experimental level, we argue that the low replicability of ego-depletion studies could be explained by the absence of a comprehensive, integrative, and falsifiable definition of self-control, which is central to the concept of ego depletion; by an unclear or absent distinction between ego depletion and mental fatigue, two phenomena that rely on different processes; and by the low validity of the tasks used to induce ego depletion. Finally, we make conceptual and practical suggestions for a more rigorous investigation of ego depletion, discuss the necessity to take into account its dynamic and multicomponent nature, and suggest using the term self-control fatigue instead.},
	institution = {{PsyArXiv}},
	author = {Forestier, Cyril and Chanaleilles, Margaux de and Boisgontier, Matthieu P. and Chalabaev, Aïna},
	urldate = {2021-04-12},
	date = {2021-04-12},
	doi = {10.31234/osf.io/spm9a},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, replicability, Cognitive Psychology, Social and Personality Psychology, Self-regulation, ego depletion, self-control fatigue},
	file = {Full Text PDF:/Users/tom/Zotero/storage/WZ2BJRVP/Forestier et al. - 2021 - Ego depletion A review of criticisms along with n.pdf:application/pdf},
}

@article{reis_frequency_2021,
	title = {Frequency of receiving requested data for a systematic review and associated factors: a cross-sectional study},
	volume = {0},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2021.1910029},
	doi = {10.1080/08989621.2021.1910029},
	shorttitle = {Frequency of receiving requested data for a systematic review and associated factors},
	abstract = {This study aimed to assess the frequency of receiving requested data for a systematic review and associated factors. We contacted the authors of studies in need of additional data via email. The primary outcome was the success in receiving the requested data according to the time until receipt. We estimated the hazard ratio ({HR}) and 95\% confidence interval ({CI}) for success in each variable compared to the reference category, with weighted Cox proportional hazards models using Stata (version 14.2). Out of 164 studies contacted, 110 replied (67.1\%), and 51 sent requested data (31.1\%). Median time to receive a response or withdraw contact was 36.0 days (interquartile range: 17.5, 142.5). Higher success ratio was observed in studies published as scientific papers ({HR} = 3.01, 95\% {CI} = [1.18, 7.70]), in more than one publication ({HR} = 2.00, 95\% {CI} = [1.14, 3.51]), and contacted in the personal email ({HR} = 2.85, 95\% {CI} = [1.34, 6.07]). Three or more contact attempts led to lower success ratio ({HR} = 0.19, 95\% {CI} = [0.11, 0.35]) than one or two. Requesting data for a systematic review was time-consuming and effective in three out of ten studies. Fewer contacts were more successful than insisting.},
	pages = {null},
	issue = {ja},
	journaltitle = {Accountability in Research},
	author = {Reis, Natália Dutra dos and Ferreira, Carolina Müller and Silva, Marcus Tolentino and Galvão, Taís Freire},
	urldate = {2021-04-12},
	date = {2021-03-29},
	pmid = {33779432},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2021.1910029},
	keywords = {Systematic review, Survey, Research integrity, Data sharing, Electronic mail},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PBHE945F/Reis et al. - 2021 - Frequency of receiving requested data for a system.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KZ6LB3JT/08989621.2021.html:text/html},
}

@article{haig_what_2020,
	title = {What can psychology's statistics reformers learn from the error-statistical perspective?},
	volume = {2},
	issn = {2590-2601},
	url = {https://www.sciencedirect.com/science/article/pii/S2590260120300072},
	doi = {10.1016/j.metip.2020.100020},
	abstract = {In this article, I critically evaluate two major contemporary proposals for reforming statistical thinking in psychology: The recommendation that psychology should employ the “new statistics” in its research practice, and the alternative proposal that it should embrace Bayesian statistics. I do this from the vantage point of the modern error-statistical perspective, which emphasizes the importance of the severe testing of knowledge claims. I also show how this error-statistical perspective improves our understanding of the nature of science by adopting a workable process of falsification and by structuring inquiry in terms of a hierarchy of models. Before concluding, I briefly discuss the importance of the philosophy of statistics for improving our understanding of statistical thinking.},
	pages = {100020},
	journaltitle = {Methods in Psychology},
	shortjournal = {Methods in Psychology},
	author = {Haig, Brian D.},
	urldate = {2021-04-13},
	date = {2020-11-01},
	langid = {english},
	keywords = {Bayesian statistics, toread, Falsificationism, Hierarchy of models, Philosophy of statistics, The error-statistical perspective, The new statistics},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/UMXAUIRT/Haig - 2020 - What can psychology's statistics reformers learn f.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/7U53C3EW/S2590260120300072.html:text/html},
}

@article{streiner_best_2015,
	title = {Best (but oft-forgotten) practices: the multiple problems of multiplicity—whether and how to correct for many statistical tests},
	volume = {102},
	issn = {0002-9165},
	url = {https://doi.org/10.3945/ajcn.115.113548},
	doi = {10.3945/ajcn.115.113548},
	shorttitle = {Best (but oft-forgotten) practices},
	abstract = {Testing many null hypotheses in a single study results in an increased probability of detecting a significant finding just by chance (the problem of multiplicity). Debates have raged over many years with regard to whether to correct for multiplicity and, if so, how it should be done. This article first discusses how multiple tests lead to an inflation of the α level, then explores the following different contexts in which multiplicity arises: testing for baseline differences in various types of studies, having \&gt;1 outcome variable, conducting statistical tests that produce \&gt;1 P value, taking multiple “peeks” at the data, and unplanned, post hoc analyses (i.e., “data dredging,” “fishing expeditions,” or “P-hacking”). It then discusses some of the methods that have been proposed for correcting for multiplicity, including single-step procedures (e.g., Bonferroni); multistep procedures, such as those of Holm, Hochberg, and Šidák; false discovery rate control; and resampling approaches. Note that these various approaches describe different aspects and are not necessarily mutually exclusive. For example, resampling methods could be used to control the false discovery rate or the family-wise error rate (as defined later in this article). However, the use of one of these approaches presupposes that we should correct for multiplicity, which is not universally accepted, and the article presents the arguments for and against such “correction.” The final section brings together these threads and presents suggestions with regard to when it makes sense to apply the corrections and how to do so.},
	pages = {721--728},
	number = {4},
	journaltitle = {The American Journal of Clinical Nutrition},
	shortjournal = {The American Journal of Clinical Nutrition},
	author = {Streiner, David L},
	urldate = {2021-04-13},
	date = {2015-10-01},
	keywords = {toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/Y5ENIBZZ/Streiner - 2015 - Best (but oft-forgotten) practices the multiple p.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RSRSQ4NQ/4564678.html:text/html},
}

@article{schorfheide_use_2012,
	title = {On the use of holdout samples for model selection},
	volume = {102},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.102.3.477},
	doi = {10.1257/aer.102.3.477},
	abstract = {Researchers often hold out data from the estimation of econometric models to use for external validation. However, the use of holdout samples is suboptimal from a Bayesian perspective, which prescribes using the entire sample to form posterior model weights. This paper examines a possible rationale for the use of holdout samples: data-inspired modifications of structural models are likely to lead to an exaggeration of model fit. The use of holdout samples can, in principle, set an incentive for the modeler not to exaggerate model fit.},
	pages = {477--481},
	number = {3},
	journaltitle = {American Economic Review},
	author = {Schorfheide, Frank and Wolpin, Kenneth I.},
	urldate = {2021-04-14},
	date = {2012-05},
	langid = {english},
	keywords = {and Selection, Model Evaluation, Validation},
	file = {Snapshot:/Users/tom/Zotero/storage/AWMURNFK/articles.html:text/html},
}

@article{gilovich_hot_1985,
	title = {The hot hand in basketball: On the misperception of random sequences},
	volume = {17},
	issn = {0010-0285},
	url = {https://www.sciencedirect.com/science/article/pii/0010028585900106},
	doi = {10.1016/0010-0285(85)90010-6},
	shorttitle = {The hot hand in basketball},
	abstract = {We investigate the origin and the validity of common beliefs regarding “the hot hand” and “streak shooting” in the game of basketball. Basketball players and fans alike tend to believe that a player's chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses of the shooting records of the Philadelphia 76ers provided no evidence for a positive correlation between the outcomes of successive shots. The same conclusions emerged from free-throw records of the Boston Celtics, and from a controlled shooting experiment with the men and women of Cornell's varsity teams. The outcomes of previous shots influenced Cornell players' predictions but not their performance. The belief in the hot hand and the “detection” of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly representative of their generating process.},
	pages = {295--314},
	number = {3},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Gilovich, Thomas and Vallone, Robert and Tversky, Amos},
	urldate = {2021-04-14},
	date = {1985-07-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/T587YNLI/Gilovich et al. - 1985 - The hot hand in basketball On the misperception o.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/B6BVREVL/0010028585900106.html:text/html},
}

@article{white_imputation_2018,
	title = {Imputation in U.S. manufacturing data and its implications for productivity dispersion},
	volume = {100},
	issn = {00346535},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=130616467&site=ehost-live&scope=site},
	doi = {10.1162/rest_a_00678},
	abstract = {In the U.S. Census Bureau's 2002 and 2007 Censuses of Manufactures, 79\% and 73\% of observations, respectively, have imputed data for at least one variable used to compute total factor productivity ({TFP}). The bureau primarily imputes for missing values using mean-imputation methods, which can reduce the underlying variance of the imputed variables. For five variables entering {TFP}, we show that dispersion is significantly smaller in the Census mean-imputed versus the nonimputed data. We use classification and regression trees ({CART}) to produce multiple imputations with observed data for similar plants. For 90\% of the 473 industries in 2002 and 84\% of the 471 industries in 2007, we find that {TFP} dispersion increases as we move from Census mean-imputed data to nonimputed data to the {CART}-imputed data.},
	pages = {502--509},
	number = {3},
	journaltitle = {Review of Economics \& Statistics},
	shortjournal = {Review of Economics \& Statistics},
	author = {White, T. Kirk and Reiter, Jerome P. and Petrin, Amil},
	urldate = {2021-04-14},
	date = {2018-07},
	note = {Publisher: {MIT} Press},
	keywords = {{INDUSTRIAL} efficiency, {INDUSTRIAL} productivity, {MANUFACTURING} industries, {PRODUCTION} standards, {RANDOM} forest (Algorithms), {REGRESSION} trees},
	file = {EBSCO Full Text:/Users/tom/Zotero/storage/DDSBH49Y/White et al. - 2018 - Imputation in U.s. Manufacturing Data and Its Impl.pdf:application/pdf},
}

@book{goldacre_bad_2008,
	location = {London},
	title = {Bad science},
	isbn = {978-0-00-724019-7},
	pagetotal = {338},
	publisher = {Fourth Estate},
	author = {Goldacre, Ben},
	date = {2008},
	note = {{OCLC}: ocn259713114},
	keywords = {Errors, Scientific, Popular works, Pseudoscience},
}

@article{button_power_2013,
	title = {Power failure: why small sample size undermines the reliability of neuroscience},
	volume = {14},
	rights = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn3475},
	doi = {10.1038/nrn3475},
	shorttitle = {Power failure},
	abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between ∼8\% and ∼31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
	pages = {365--376},
	number = {5},
	journaltitle = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
	urldate = {2021-04-14},
	date = {2013-05},
	langid = {english},
	note = {Number: 5
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7ZG67GVP/Button et al. - 2013 - Power failure why small sample size undermines th.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/MTZDJNMK/nrn3475.html:text/html},
}

@article{delgado-rodriguez_bias_2004,
	title = {Bias},
	volume = {58},
	issn = {0143-005X},
	url = {https://jech.bmj.com/lookup/doi/10.1136/jech.2003.008466},
	doi = {10.1136/jech.2003.008466},
	pages = {635--641},
	number = {8},
	journaltitle = {Journal of Epidemiology \& Community Health},
	shortjournal = {Journal of Epidemiology \& Community Health},
	author = {Delgado-Rodriguez, M},
	urldate = {2021-04-14},
	date = {2004-08-01},
	langid = {english},
	file = {Delgado-Rodriguez - 2004 - Bias.pdf:/Users/tom/Zotero/storage/9N8ARJYN/Delgado-Rodriguez - 2004 - Bias.pdf:application/pdf},
}

@article{ellenberg_selection_1994,
	title = {Selection bias in observational and experimental studies},
	volume = {13},
	rights = {Copyright © 1994 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780130518},
	doi = {https://doi.org/10.1002/sim.4780130518},
	abstract = {There has been a heightened awareness of the dangers of selection bias over the past two decades. Certainly coverage in statistical and ‘statistics for medicine’, and epidemiology textbooks have allocated pages to warn investigators and readers of investigations to be aware of its presence. The scientific community has not, however, yet accepted the necessity for critical assessment of the method of sample selection in the planning and execution of studies as a fundamental underpinning of observational and experimental studies. To wit, we are faced with a plethora of research studies receiving funding, being published in peer-reviewed journals and influencing future studies, that may be reporting entirely spurious associations. It is the intent of this paper to present examples of selection bias in a variety of areas which have resulted in misleading or entirely incorrect results. We hope to help make such research scientifically ‘politically incorrect’ to the degree that the scientific community ‘just says no’ to such studies, either proposed or reported.},
	pages = {557--567},
	number = {5},
	journaltitle = {Statistics in Medicine},
	author = {Ellenberg, Jonas H.},
	urldate = {2021-04-14},
	date = {1994},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780130518},
	file = {Snapshot:/Users/tom/Zotero/storage/A4MMEGTZ/sim.html:text/html},
}

@report{maier_comparing_2021,
	title = {Comparing theories with the ising model of explanatory coherence (imec)},
	url = {https://psyarxiv.com/shaef/},
	abstract = {Theories are among the most important tools of science. Lewin (1943) already noted “[t]here is nothing as practical as a good theory”. Although psychologists discussed problems of theory in their discipline for a long time, weak theories are still widespread in most subfields. One possible reason for this is that psychologists lack the tools to systematically assess the quality of their theories. Thagard (1989) developed a computational model for formal theory evaluation based on the concept of explanatory coherence. However, there are possible improvements to Thagard’s (1989) model and it is not available in software that psychologists typically use. Therefore, we developed a new implementation of explanatory coherence based on the Ising model. We demonstrate the capabilities of this new Ising Model of Explanatory Coherence ({IMEC}) on several examples from psychology and other sciences. It is also available in the R-package {IMEC} so that it can help scientists to evaluate the quality of their theories in practice.},
	institution = {{PsyArXiv}},
	author = {Maier, Maximilian and Dongen, Noah van and Borsboom, Denny},
	urldate = {2021-04-16},
	date = {2021-04-13},
	doi = {10.31234/osf.io/shaef},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, toread, Explanatory Coherence, Ising Model, Theory Appraisal, Theory Development},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MTYN4KHU/Maier et al. - 2021 - Comparing Theories with the Ising Model of Explana.pdf:application/pdf},
}

@online{noauthor_harking_nodate,
	title = {{HARKing}: Conceptualizations, harms, and two fundamental remedies. - {PsycNET}},
	url = {/doiLanding?doi=10.1037%2Fteo0000182},
	shorttitle = {{HARKing}},
	abstract = {{APA} {PsycNet} {DoiLanding} page},
	urldate = {2021-04-18},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/IZ4HDNBV/doiLanding.html:text/html},
}

@article{brandt_replication_2014,
	title = {The Replication Recipe: What makes for a convincing replication?},
	volume = {50},
	issn = {0022-1031},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103113001819},
	doi = {10.1016/j.jesp.2013.10.005},
	shorttitle = {The Replication Recipe},
	abstract = {Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.},
	pages = {217--224},
	journaltitle = {Journal of Experimental Social Psychology},
	shortjournal = {Journal of Experimental Social Psychology},
	author = {Brandt, Mark J. and {IJzerman}, Hans and Dijksterhuis, Ap and Farach, Frank J. and Geller, Jason and Giner-Sorolla, Roger and Grange, James A. and Perugini, Marco and Spies, Jeffrey R. and van 't Veer, Anna},
	urldate = {2021-04-19},
	date = {2014-01-01},
	langid = {english},
	keywords = {Replication, Pre-registration, Research method, Solid Science, Statistical power},
}

@article{dienes_bayesian_2011,
	title = {Bayesian versus orthodox statistics: which side are you on?},
	volume = {6},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691611406920},
	doi = {10.1177/1745691611406920},
	shorttitle = {Bayesian versus orthodox statistics},
	abstract = {Researchers are often confused about what can be inferred from significance tests. One problem occurs when people apply Bayesian intuitions to significance testing?two approaches that must be firmly separated. This article presents some common situations in which the approaches come to different conclusions; you can see where your intuitions initially lie. The situations include multiple testing, deciding when to stop running participants, and when a theory was thought of relative to finding out results. The interpretation of nonsignificant results has also been persistently problematic in a way that Bayesian inference can clarify. The Bayesian and orthodox approaches are placed in the context of different notions of rationality, and I accuse myself and others as having been irrational in the way we have been using statistics on a key notion of rationality. The reader is shown how to apply Bayesian inference in practice, using free online software, to allow more coherent inferences from data.},
	pages = {274--290},
	number = {3},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Dienes, Zoltan},
	urldate = {2021-04-20},
	date = {2011-05-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/HH692W94/Dienes - 2011 - Bayesian Versus Orthodox Statistics Which Side Ar.pdf:application/pdf},
}

@audio{punk_human_2005,
	title = {Human After All},
	publisher = {Virgin},
	editora = {Punk, Daft},
	editoratype = {collaborator},
	date = {2005},
}

@article{vinkers_methodological_2021,
	title = {The methodological quality of 176,620 randomized controlled trials published between 1966 and 2018 reveals a positive trend but also an urgent need for improvement},
	volume = {19},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001162},
	doi = {10.1371/journal.pbio.3001162},
	abstract = {Many randomized controlled trials ({RCTs}) are biased and difficult to reproduce due to methodological flaws and poor reporting. There is increasing attention for responsible research practices and implementation of reporting guidelines, but whether these efforts have improved the methodological quality of {RCTs} (e.g., lower risk of bias) is unknown. We, therefore, mapped risk-of-bias trends over time in {RCT} publications in relation to journal and author characteristics. Meta-information of 176,620 {RCTs} published between 1966 and 2018 was extracted. The risk-of-bias probability (random sequence generation, allocation concealment, blinding of patients/personnel, and blinding of outcome assessment) was assessed using a risk-of-bias machine learning tool. This tool was simultaneously validated using 63,327 human risk-of-bias assessments obtained from 17,394 {RCTs} evaluated in the Cochrane Database of Systematic Reviews ({CDSR}). Moreover, {RCT} registration and {CONSORT} Statement reporting were assessed using automated searches. Publication characteristics included the number of authors, journal impact factor ({JIF}), and medical discipline. The annual number of published {RCTs} substantially increased over 4 decades, accompanied by increases in authors (5.2 to 7.8) and institutions (2.9 to 4.8). The risk of bias remained present in most {RCTs} but decreased over time for allocation concealment (63\% to 51\%), random sequence generation (57\% to 36\%), and blinding of outcome assessment (58\% to 52\%). Trial registration (37\% to 47\%) and the use of the {CONSORT} Statement (1\% to 20\%) also rapidly increased. In journals with a higher impact factor ({\textgreater}10), the risk of bias was consistently lower with higher levels of {RCT} registration and the use of the {CONSORT} Statement. Automated risk-of-bias predictions had accuracies above 70\% for allocation concealment (70.7\%), random sequence generation (72.1\%), and blinding of patients/personnel (79.8\%), but not for blinding of outcome assessment (62.7\%). In conclusion, the likelihood of bias in {RCTs} has generally decreased over the last decades. This optimistic trend may be driven by increased knowledge augmented by mandatory trial registration and more stringent reporting guidelines and journal requirements. Nevertheless, relatively high probabilities of bias remain, particularly in journals with lower impact factors. This emphasizes that further improvement of {RCT} registration, conduct, and reporting is still urgently needed.},
	pages = {e3001162},
	number = {4},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Vinkers, Christiaan H. and Lamberink, Herm J. and Tijdink, Joeri K. and Heus, Pauline and Bouter, Lex and Glasziou, Paul and Moher, David and Damen, Johanna A. and Hooft, Lotty and Otte, Willem M.},
	urldate = {2021-04-20},
	date = {2021-04-19},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Medicine and health sciences, Randomized controlled trials, Medical risk factors, Scientific publishing, Systematic reviews, Bibliometrics, Research reporting guidelines, toread, Machine learning},
	file = {Full Text PDF:/Users/tom/Zotero/storage/57Z48KNT/Vinkers et al. - 2021 - The methodological quality of 176,620 randomized c.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZAF4CAKM/article.pdf:application/pdf},
}

@article{soric_statistical_1989,
	title = {Statistical “Discoveries” and Effect-Size Estimation},
	volume = {84},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.1989.10478811},
	doi = {10.1080/01621459.1989.10478811},
	abstract = {Current methods of statistical inference are correct but incomplete. Small probability (α) of wrong null-hypothesis rejections can be misunderstood. Definitive rejections of null hypotheses, as well as interval assessments of effect sizes, are impossible in single cases with significance probabilities like .05. Sufficiently large sets of independent experiments and attained significance levels (p-values) should be registered. From such data it is possible to calculate least upper bounds for proportions of fallacies in sets of null-hypothesis rejections or effect-size assessments. A provisional rejection of a null hypothesis in a one-tailed test, or a one-sided confidence interval showing a nonzero effect, is here called a discovery. Consider a large number n of independent experiments with r discoveries. For r rejections of null hypotheses the proportion of fallacies Q has least upper bound Q max = (n/r — 1)α/(1 — α) {\textless} 1. For r confidence intervals, the proportion of fallacies is E = αn/r (assuming that no alternative is in the “wrong” direction).},
	pages = {608--610},
	number = {406},
	journaltitle = {Journal of the American Statistical Association},
	author = {Sorić, Branko},
	urldate = {2021-04-20},
	date = {1989-06-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.1989.10478811},
	keywords = {Statistical inference, Confidence intervals, Experimental results, statistical verification of, Null hypothesis, rejection of, Significance tests},
	file = {Snapshot:/Users/tom/Zotero/storage/D79ARMVI/01621459.1989.html:text/html},
}

@book{noauthor_leviathan_2017,
	title = {Leviathan and the Air-Pump},
	isbn = {978-0-691-17816-5},
	url = {https://press.princeton.edu/books/paperback/9780691178165/leviathan-and-the-air-pump},
	urldate = {2021-04-20},
	date = {2017-11-14},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/H9PGJEGD/leviathan-and-the-air-pump.html:text/html},
}

@article{vanpaemel_really_2019,
	title = {The really risky registered modeling report: incentivizing strong tests and {HONEST} modeling in cognitive science},
	volume = {2},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-019-00056-9},
	doi = {10.1007/s42113-019-00056-9},
	shorttitle = {The really risky registered modeling report},
	abstract = {In the Registered Modeling Report format, authors specify the experimental design, models, and inference mechanism before they have seen the data and reviewers evaluate the level of detail and quality of the proposed plan. While useful, the Registered Modeling Report is limited in its ability to incentivize strong tests. I propose an extension to the Registered Modeling Report format, the Really Risky Registered Modeling Report, in which reviewers are required to evaluate whether a bad fit between the model predictions and the empirical data is plausible. The two crucial additions are that authors include prior predictions in the protocol and that reviewers set a data prior in Stage 1. Only protocols containing predictions that are not almost guaranteed to be confirmed when brought in contact with empirical data are eligible for in principle acceptance. Adopting the Really Risky Registered Modeling Report will lead to strong model tests and solid evidence.},
	pages = {218--222},
	number = {3},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Vanpaemel, Wolf},
	urldate = {2021-04-20},
	date = {2019-12-01},
	langid = {english},
	file = {Vanpaemel - 2019 - The Really Risky Registered Modeling Report Incen.pdf:/Users/tom/Zotero/storage/LCEFC8IE/Vanpaemel - 2019 - The Really Risky Registered Modeling Report Incen.pdf:application/pdf},
}

@article{appelbaum_journal_2018,
	title = {Journal article reporting standards for quantitative research in psychology: The {APA} Publications and Communications Board task force report.},
	volume = {73},
	issn = {1935-990X},
	url = {https://psycnet.apa.org/fulltext/2018-00750-002.pdf},
	doi = {10.1037/amp0000191},
	shorttitle = {Journal article reporting standards for quantitative research in psychology},
	pages = {3--25},
	number = {1},
	journaltitle = {American Psychologist},
	author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and Mayo-Wilson, Evan and Nezu, Arthur M. and Rao, Stephen M.},
	urldate = {2021-04-21},
	date = {2018},
	note = {Publisher: {US}: American Psychological Association},
	file = {Appelbaum et al. - 2018 - Journal article reporting standards for quantitati.pdf:/Users/tom/Zotero/storage/D8XFBUJJ/Appelbaum et al. - 2018 - Journal article reporting standards for quantitati.pdf:application/pdf},
}

@article{couture_funder-imposed_2018,
	title = {A funder-imposed data publication requirement seldom inspired data sharing},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0199789},
	doi = {10.1371/journal.pone.0199789},
	abstract = {Growth of the open science movement has drawn significant attention to data sharing and availability across the scientific community. In this study, we tested the ability to recover data collected under a particular funder-imposed requirement of public availability. We assessed overall data recovery success, tested whether characteristics of the data or data creator were indicators of recovery success, and identified hurdles to data recovery. Overall the majority of data were not recovered (26\% recovery of 315 data projects), a similar result to journal-driven efforts to recover data. Field of research was the most important indicator of recovery success, but neither home agency sector nor age of data were determinants of recovery. While we did not find a relationship between recovery of data and age of data, age did predict whether we could find contact information for the grantee. The main hurdles to data recovery included those associated with communication with the researcher; loss of contact with the data creator accounted for half (50\%) of unrecoverable datasets, and unavailability of contact information accounted for 35\% of unrecoverable datasets. Overall, our results suggest that funding agencies and journals face similar challenges to enforcement of data requirements. We advocate that funding agencies could improve the availability of the data they fund by dedicating more resources to enforcing compliance with data requirements, providing data-sharing tools and technical support to awardees, and administering stricter consequences for those who ignore data sharing preconditions.},
	pages = {e0199789},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Couture, Jessica L. and Blake, Rachael E. and {McDonald}, Gavin and Ward, Colette L.},
	urldate = {2021-04-22},
	date = {2018-07-06},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open data, Open science, Science policy, Data management, Language, Communications, Public policy, Research funding},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XGQH26BA/Couture et al. - 2018 - A funder-imposed data publication requirement seld.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DKQYU67L/article.html:text/html},
}

@article{clemens_meaning_2017-1,
	title = {The meaning of failed replications: a review and proposal},
	volume = {31},
	rights = {© 2015 John Wiley \& Sons Ltd},
	issn = {1467-6419},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12139},
	doi = {https://doi.org/10.1111/joes.12139},
	shorttitle = {The meaning of failed replications},
	abstract = {The welcome rise of replication tests in economics has not been accompanied by a consensus standard for determining what constitutes a replication. A discrepant replication, in current usage of the term, can signal anything from an unremarkable disagreement over methods to scientific incompetence or misconduct. This paper proposes a standard for classifying one study as a replication of some other study. It is a standard that places the burden of proof on a study to demonstrate that it should have obtained identical results to the original, a conservative standard that is already used implicitly by many researchers. It contrasts this standard with decades of unsuccessful attempts to harmonize terminology, and argues that many prominent results described as replication tests should not be described as such. Adopting a conservative standard like this one can improve incentives for researchers, encouraging more and better replication tests.},
	pages = {326--342},
	number = {1},
	journaltitle = {Journal of Economic Surveys},
	author = {Clemens, Michael A.},
	urldate = {2021-04-24},
	date = {2017},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/joes.12139},
	keywords = {Ethics, Open data, Replication, Robustness, Transparency},
	file = {Snapshot:/Users/tom/Zotero/storage/SSEISGQS/joes.html:text/html;Submitted Version:/Users/tom/Zotero/storage/TNDCPL24/Clemens - 2017 - The Meaning of Failed Replications A Review and P.pdf:application/pdf},
}

@article{macleod_mdar_2021,
	title = {The {MDAR} (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences},
	volume = {118},
	rights = {Copyright © 2021 the Author(s). Published by {PNAS}.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 ({CC} {BY}).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/118/17/e2103238118},
	doi = {10.1073/pnas.2103238118},
	abstract = {Transparency in reporting benefits scientific communication on many levels. While specific needs and expectations vary across fields, the effective interpretation and use of research findings relies on the availability of core information about research materials, study design, data, and experimental and analytical methods. For preclinical research, transparency in reporting is a key focus in response to concerns of replication failure. The inconsistent reporting of key elements of experimental and analytical design, alongside ambiguous description of reagents and lack of access to underlying data and code, has been shown to impair replication (1) and raise doubt about the robustness of results (2, 3). In response to early concerns about replication of published results, funders, publishers, and other stakeholders have called for improvements in reporting transparency (4⇓⇓–7). Several initiatives ensued, including journal policies and joint efforts by journals, funders, and other stakeholders (8⇓–10). One of these initiatives, the Transparency and Openness Promotion ({TOP}) guidelines (11), outlines a policy framework at the journal level that over 1,000 journals and publishers have adopted.

The National Academies have focused on reproducibility and replicability* challenges through several recent initiatives leading to consensus reports, including Reproducibility and Replicability in Science (12), Open Science by Design: Realizing a Vision for 21st Century Research (13), and Fostering Integrity in Research (14). Each of these reports concludes that lack of reporting transparency is one factor which contributes to these systemic problems. Building on these findings, the National Academies convened a public workshop in September 2019 titled “Enhancing Scientific Reproducibility in Biomedical Research Through Transparent Reporting.” The workshop was designed to discuss the current state of transparency in reporting biomedical research and to explore the possibility of improving the harmonization of guidelines across journals and funding agencies.

During this workshop, we provided … 

[↵][1]1To whom correspondence may be addressed: Email: Malcolm.Macleod\{at\}ed.ac.uk (correspondence regarding the development and evaluation of the guideline) or mellor.david\{at\}gmail.com (for further information about implementation and stewardship).

 [1]: \#xref-corresp-1-1},
	number = {17},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Macleod, Malcolm and Collings, Andrew M. and Graf, Chris and Kiermer, Veronique and Mellor, David and Swaminathan, Sowmya and Sweet, Deborah and Vinson, Valda},
	urldate = {2021-04-27},
	date = {2021-04-27},
	langid = {english},
	note = {Publisher: National Academy of Sciences
Section: At the National Academies},
	keywords = {toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HYE72ZXV/Macleod et al. - 2021 - The MDAR (Materials Design Analysis Reporting) Fra.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TS3VZREA/e2103238118.html:text/html},
}

@article{vable_code_2021,
	title = {Code review as a simple trick to enhance reproducibility, accelerate learning, and improve the quality of your team’s research},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwab092},
	doi = {10.1093/aje/kwab092},
	abstract = {Programming for data wrangling and statistical analysis is an essential technical tool of modern Epidemiology, yet many Epidemiologists receive limited formal training in strategies to optimize the quality of our code. In complex projects, coding mistakes are easy to make, even for skilled practitioners. Such mistakes can lead to invalid research claims that reduce the credibility of the field. Code review is a straightforward technique used by the software industry to reduce the likelihood of coding bugs. The systematic implementation of code review in epidemiologic research projects could not only improve science, but also decrease stress, accelerate learning, contribute to team building, and codify best practices. In this paper, we argue for the importance of code review and provide some recommendations for successful implementation: [1] for the research lab, [2] for the code author (the initial programmer), and [3] for the code reviewer. We outline a feasible implementation of code review, though other successful implementations are possible to accommodate the resources and workflow of different research groups, including other practices to improve code quality. Code review isn’t always glamorous, but it is critically important for science and reproducibility. Humans are fallible; that’s why we need code review.},
	issue = {kwab092},
	journaltitle = {American Journal of Epidemiology},
	shortjournal = {American Journal of Epidemiology},
	author = {Vable, Anusha M and Diehl, Scott F and Glymour, M Maria},
	urldate = {2021-04-27},
	date = {2021-04-08},
	keywords = {toread},
	file = {Snapshot:/Users/tom/Zotero/storage/XGDYLWNJ/6218064.html:text/html},
}

@online{mayo-wilson_collectivist_2020,
	title = {Collectivist Foundations for Bayesian Statistics},
	url = {http://philsci-archive.pitt.edu/18592/},
	abstract = {What (if anything) justifies the use of Bayesian statistics in science?  The traditional answer is that Bayesian statistics is simply an instance of orthodox expected utility theory.  Thus, Bayesian statistical methods, like principles of utility theory, are justified by norms of individual rationality.  In particular, most Bayesians argue that a scientist's credences must satisfy the probability axioms if she adheres to norms of practical and epistemic (individual) rationality.  We argue that, to justify Bayesian statistics as a tool for science, it is necessary that a scientist's public credences (i.e., her degrees of belief qua scientist) obey the probability axioms. We claim that norms of collective science help justify this restricted view, termed public probabilism.},
	type = {Preprint},
	author = {Mayo-Wilson, Conor and Saraf, Aditya},
	urldate = {2021-04-27},
	date = {2020-12-06},
	langid = {english},
	keywords = {toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HNSNJYLE/Mayo-Wilson and Saraf - 2020 - Collectivist Foundations for Bayesian Statistics.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/NLIXDBDZ/18592.html:text/html},
}

@article{imker_examination_2021,
	title = {An examination of data reuse practices within highly cited articles of faculty at a research university},
	volume = {47},
	issn = {0099-1333},
	url = {https://www.sciencedirect.com/science/article/pii/S0099133321000604},
	doi = {10.1016/j.acalib.2021.102369},
	abstract = {Data sharing and reuse are regarded as important components of the research workflow and key elements in open science. While reuse is well-documented in some circumstances, the utility of data sharing for all domains is less clear, and limited evidence of wide-spread demand can make it challenging to justify effort and funds required to format, document, share, and preserve data. This paper describes a project that: (1) surveyed authors of highly cited papers published in 2015 at the University of Illinois at Urbana-Champaign in nine {STEM} disciplines to determine if data were generated for their article and their knowledge of reuse by other researchers, and (2) surveyed authors who cited these 2015 articles to ascertain whether they reused data from the original article and how that data was obtained. The project goal was to better understand data reuse in practice and to explore if research data from an initial publication was reused in subsequent publications. While the results revealed reuse in many situations (and deemed important in these cases), the survey results and researcher supplied comments also indicated that data does not play the same role in all studies or even in studies that build on previous ones.},
	pages = {102369},
	number = {4},
	journaltitle = {The Journal of Academic Librarianship},
	shortjournal = {The Journal of Academic Librarianship},
	author = {Imker, Heidi J. and Luong, Hoa and Mischo, William H. and Schlembach, Mary C. and Wiley, Chris},
	urldate = {2021-04-27},
	date = {2021-07-01},
	langid = {english},
	keywords = {Data management, Data sharing, toread, Data reuse, Data services, Scopus {API}},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/WU2B2VQE/Imker et al. - 2021 - An examination of data reuse practices within high.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/CMQSWAAU/S0099133321000604.html:text/html},
}

@report{collentine_meta-analyzing_2021,
	title = {Meta-analyzing the multiverse: a peek under the hood of selective reporting},
	url = {https://psyarxiv.com/43yae/},
	shorttitle = {Preprint - Meta-Analyzing the Multiverse},
	abstract = {There are arbitrary decisions to be made (i.e., researcher degrees of freedom) in the execution and reporting of most research. These decisions allow for many possible outcomes from a single study. Selective reporting of results from this ‘multiverse’ of outcomes, whether intentional (\_p\_-hacking) or not, can lead to inflated effect size estimates and false positive results in the literature. In this study, we examine and illustrate the consequences of researcher degrees of freedom in primary research, both for primary outcomes and for subsequent meta-analyses. We used a set of 10 preregistered multi-lab direct replication projects from psychology (Registered Replication Reports) with a total of 14 primary outcome variables, 236 labs and 37,602 participants. By exploiting researcher degrees of freedom in each project, we were able to compute between 3,840 and 2,621,440 outcomes per lab. We show that researcher degrees of freedom in primary research can cause substantial variability in effect size that we denote the Underlying Multiverse Variability ({UMV}). In our data, the median {UMV} across labs was 0.1 standard deviations (interquartile range = 0.09 – 0.15). In one extreme case, the effect size estimate could change by \_d\_ = 1.27, evidence that \_p\_-hacking in some (rare) cases can provide support for almost any conclusion. We also show that researcher degrees of freedom in primary research provide another source of uncertainty in meta-analysis beyond those usually estimated. This would not be a large concern for meta-analysis if researchers made all arbitrary decisions at random. However, emulating selective reporting of lab results led to inflation of meta-analytic average effect size estimates in our data by as much as 0.1 - 0.48 standard deviations, depending to a large degree on the number of possible outcomes at the lab level (i.e., multiverse size). Our results illustrate the importance of making research decisions transparent (e.g., through preregistration and multiverse analysis), evaluating studies for selective reporting, and whenever feasible  making raw data available.},
	institution = {{PsyArXiv}},
	author = {Collentine, Anton Olsson and Aert, Robbie C. M. van and Bakker, Marjan and Wicherts, Jelte},
	urldate = {2021-04-27},
	date = {2021-04-27},
	doi = {10.31234/osf.io/43yae},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/F6WXY8W7/Collentine et al. - 2021 - Preprint - Meta-Analyzing the Multiverse A Peek U.pdf:application/pdf},
}

@article{lakens_improving_2021,
	title = {Improving transparency, falsifiability, and rigor by making hypothesis tests machine-readable},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245920970949},
	doi = {10.1177/2515245920970949},
	abstract = {Making scientific information machine-readable greatly facilitates its reuse. Many scientific articles have the goal to test a hypothesis, so making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine-readable. We believe there are two benefits to specifying a hypothesis test in such a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis tests become more transparent, falsifiable, and rigorous. Second, scientists benefit if information related to hypothesis tests in scientific articles is easily findable and reusable, for example, to perform meta-analyses, conduct peer review, and examine metascientific research questions. We examine what a machine-readable hypothesis test should look like and demonstrate the feasibility of machine-readable hypothesis tests in a real-life example using the fully operational prototype R package scienceverse.},
	pages = {2515245920970949},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Lakens, Daniël and {DeBruine}, Lisa M.},
	urldate = {2021-04-29},
	date = {2021-04-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {hypothesis testing, metadata, scholarly communication, toread, machine readability},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/ML2ABD65/Lakens and DeBruine - 2021 - Improving Transparency, Falsifiability, and Rigor .pdf:application/pdf},
}

@article{butte_trials_2021,
	title = {Trials and tribulations—11 reasons why we need to promote clinical trials data sharing},
	volume = {4},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2020.35043},
	doi = {10.1001/jamanetworkopen.2020.35043},
	abstract = {With the release of the sharing clinical trial data consensus study report by the National Academy of Medicine in 2015, the International Committee of Medical Journal Editors ({ICMJE}) launched an effort that eventually led to requiring authors to submit data sharing statements with their manuscripts starting in mid-2018. Data sharing statements are meant to show readers how to access the deidentified individual-participant data ({IPD}) that underlie the results shown in a manuscript. Danchev et al explored how successful that effort has been.Starting with 486 clinical trial articles published in {JAMA}, New England Journal of Medicine, and The Lancet that contained a data sharing statement (or were required to have one), Danchev et al found that only 2 deidentified {IPD} data sets were publicly available on a journal website (both were associated with the same trial), and only 17 were provided in a secure repository, totaling just 4\% of the articles. Funding for articles without data sharing statements came from both the National Institutes of Health ({NIH}) and non-{NIH} sources. Although many authors stated that data would be made available in public or private repositories or conditionally by request, most of these data sets never end up being available to the public.},
	pages = {e2035043--e2035043},
	number = {1},
	journaltitle = {{JAMA} Network Open},
	shortjournal = {{JAMA} Network Open},
	author = {Butte, Atul J.},
	urldate = {2021-04-29},
	date = {2021-01-28},
	keywords = {toread},
	file = {Full Text:/Users/tom/Zotero/storage/3NWDDWJC/Butte - 2021 - Trials and Tribulations—11 Reasons Why We Need to .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VNTZNCIS/2775663.html:text/html},
}

@article{horton_offline_2015,
	title = {Offline: What is medicine's 5 sigma?},
	volume = {385},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(15)60696-1/abstract},
	doi = {10.1016/S0140-6736(15)60696-1},
	shorttitle = {Offline},
	abstract = {“A lot of what is published is incorrect.” I'm not allowed to say who made this remark
because we were asked to observe Chatham House rules. We were also asked not to take
photographs of slides. Those who worked for government agencies pleaded that their
comments especially remain unquoted, since the forthcoming {UK} election meant they
were living in “purdah”—a chilling state where severe restrictions on freedom of speech
are placed on anyone on the government's payroll. Why the paranoid concern for secrecy
and non-attribution? Because this symposium—on the reproducibility and reliability
of biomedical research, held at the Wellcome Trust in London last week—touched on
one of the most sensitive issues in science today: the idea that something has gone
fundamentally wrong with one of our greatest human creations.},
	pages = {1380},
	number = {9976},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Horton, Richard},
	urldate = {2021-05-08},
	date = {2015-04-11},
	note = {Publisher: Elsevier},
	file = {Full Text PDF:/Users/tom/Zotero/storage/4EHJ7XHM/Horton - 2015 - Offline What is medicine's 5 sigma.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N3W2U3RU/fulltext.html:text/html},
}

@article{hudson_should_2021,
	title = {Should we strive to make science bias-free? A philosophical assessment of the reproducibility crisis},
	issn = {1572-8587},
	url = {https://doi.org/10.1007/s10838-020-09548-w},
	doi = {10.1007/s10838-020-09548-w},
	shorttitle = {Should we strive to make science bias-free?},
	abstract = {Recently, many scientists have become concerned about an excessive number of failures to reproduce statistically significant effects. The situation has become dire enough that the situation has been named the ‘reproducibility crisis’. After reviewing the relevant literature to confirm the observation that scientists do indeed view replication as currently problematic, I explain in philosophical terms why the replication of empirical phenomena, such as statistically significant effects, is important for scientific progress. Following that explanation, I examine various diagnoses of the reproducibility crisis, and argue that for the majority of scientists the crisis is due, at least in part, to a form of publication bias. This conclusion sets the stage for an assessment of the view that evidential relations in science are inherently value-laden, a view championed by Heather Douglas and Kevin Elliott. I argue, in response to Douglas and Elliott, and as motivated by the meta-scientific resistance scientists harbour to a publication bias, that if we advocate the value-ladenness of science the result would be a deepening of the reproducibility crisis.},
	journaltitle = {Journal for General Philosophy of Science},
	shortjournal = {J Gen Philos Sci},
	author = {Hudson, Robert},
	urldate = {2021-05-13},
	date = {2021-04-22},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/9KZJHZCF/Hudson - 2021 - Should We Strive to Make Science Bias-Free A Phil.pdf:application/pdf},
}

@article{proulx_beyond_nodate,
	title = {Beyond statistical ritual: theory in psychological science},
	pages = {32},
	author = {Proulx, Travis and Morey, Richard},
	langid = {english},
	keywords = {toread},
	file = {Proulx - Beyond Statistical Ritual Theory in Psychological.pdf:/Users/tom/Zotero/storage/VMFKPLKZ/Proulx - Beyond Statistical Ritual Theory in Psychological.pdf:application/pdf},
}

@article{stewart_natural_2021,
	title = {The natural selection of good science},
	rights = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01111-x},
	doi = {10.1038/s41562-021-01111-x},
	abstract = {Scientists in some fields are concerned that many published results are false. Recent models predict selection for false positives as the inevitable result of pressure to publish, even when scientists are penalized for publications that fail to replicate. We model the cultural evolution of research practices when laboratories are allowed to expend effort on theory, enabling them, at a cost, to identify hypotheses that are more likely to be true, before empirical testing. Theory can restore high effort in research practice and suppress false positives to a technical minimum, even without replication. The mere ability to choose between two sets of hypotheses, one with greater prior chance of being correct, promotes better science than can be achieved with effortless access to the set of stronger hypotheses. Combining theory and replication can have synergistic effects. On the basis of our analysis, we propose four simple recommendations to promote good science.},
	pages = {1--9},
	journaltitle = {Nature Human Behaviour},
	author = {Stewart, Alexander J. and Plotkin, Joshua B.},
	urldate = {2021-05-17},
	date = {2021-05-17},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	file = {Snapshot:/Users/tom/Zotero/storage/BVWHIKPF/s41562-021-01111-x.html:text/html},
}

@article{wortzel_trends_2020,
	title = {Trends in mental health clinical research: Characterizing the {ClinicalTrials}.gov registry from 2007–2018},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0233996},
	doi = {10.1371/journal.pone.0233996},
	shorttitle = {Trends in mental health clinical research},
	abstract = {While the epidemiologic burden of mental health disorders in the United States has been well described over the past decade, we know relatively little about trends in how these disorders are being studied through clinical research. We examined all {US} interventional mental health trials submitted to {ClinicalTrials}.gov between October 1, 2007 and April 30, 2018 to identify trends in trial characteristics, comparisons with non-mental health trials, and trial attributes associated with discontinuation and results reporting. International data were excluded to minimize potential confounding. Over this period, mental health and non-mental health trials grew at similar rates, though Industry and {US} government-funded trials declined and academic medical center/hospital/other ({AMC}/Hosp/Oth) funded trials grew faster in mental health research. The proportion of trials with safeguards against bias, including blinding and oversight by data monitoring committees ({DMCs}), decreased. This occurred during growth in the proportion of trials studying behavioral and non-pharmacological interventions, which often cannot be blinded and do not require {DMC} oversight. There was concurrent decline in pharmaceutical trials. There was significant growth in trials studying Non-{DSM} (Diagnostic and Statistical Manual-5) conditions (e.g. suicidality and wellness), as well as substance use, anxiety, and neurocognitive disorders. One in 12 trials was discontinued. Trial discontinuation was associated with industry and {AMC}/Hosp/Oth funders, pharmaceutical interventions, and lack of {DMC} oversight. Only 29.9\% of completed trials reported results to the registry. Decreased results reporting was associated with behavioral interventions, phase 1 trials, and industry and {AMC}/Hosp/Oth funders. The main implications of these data are that funding is shifting away from traditional government and industry sources, there is increasing interest in non-pharmacological treatments and Non-{DSM} conditions, and there are changing norms in trial design characteristics regarding safeguards against bias. These trends can guide researchers and funding bodies when considering the trajectory of future mental health research.},
	pages = {e0233996},
	number = {6},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Wortzel, Joshua R. and Turner, Brandon E. and Weeks, Brannon T. and Fragassi, Christopher and Ramos, Virginia and Truong, Thanh and Li, Desiree and Sahak, Omar and Lee, Hochang Benjamin},
	urldate = {2021-05-18},
	date = {2020-06-05},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Mental health and psychiatry, Medicine and health sciences, Governments, Behavior, United States, Government funding of science, Diagnostic medicine, Pediatrics},
	file = {Full Text PDF:/Users/tom/Zotero/storage/DAIBINFE/Wortzel et al. - 2020 - Trends in mental health clinical research Charact.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7X9TK826/article.html:text/html},
}

@report{roche_quality_2021,
	title = {The quality of open datasets shared by researchers in ecology and evolution is moderately repeatable and slow to change},
	url = {https://ecoevorxiv.org/d63js/},
	abstract = {We assessed the quality of 362 open datasets shared by 100 principal investigators ({PIs}) in ecology and evolution to identify predictors of data quality. Datasets generally scored low on completeness and reusability, but these metrics were slightly higher for more recently archived datasets and {PIs} with less seniority. Journal data sharing policies had no effect on data quality, whereas {PI} identity explained the largest proportion of the variance in both data completeness (27.8\%) and reusability (22.0\%), suggesting that a {PI}’s training and lab culture are key determinants of data quality. Thus, greater incentives and training for individual researchers could help improve data sharing practices.},
	institution = {{EcoEvoRxiv}},
	author = {Roche, Dominique and Berberi, Ilias and Dhane, Fares and Lauzon, Félix and Soeharjono, Sandrine and Dakin, Roslyn and Binning, Sandra},
	urldate = {2021-05-20},
	date = {2021-05-18},
	doi = {10.32942/osf.io/d63js},
	note = {type: article},
	keywords = {open science, reproducibility, metascience, data sharing, Life Sciences, {FAIR} data, Biology, Ecology and Evolutionary Biology, Other Ecology and Evolutionary Biology, public data archiving},
	file = {Full Text PDF:/Users/tom/Zotero/storage/85X7ZBQ4/Roche et al. - 2021 - The quality of open datasets shared by researchers.pdf:application/pdf},
}

@article{wieringa_rethinking_2018,
	title = {Rethinking bias and truth in evidence-based health care},
	volume = {24},
	rights = {© 2018 The Authors Journal of Evaluation in Clinical Practice Published by John Wiley \& Sons, Ltd.},
	issn = {1365-2753},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jep.13010},
	doi = {https://doi.org/10.1111/jep.13010},
	abstract = {In modern philosophy, the concept of truth has been problematized from different angles, yet in evidence-based health care ({EBHC}), it continues to operate hidden and almost undisputed through the linked concept of “bias.” To prevent unwarranted relativism and make better inferences in clinical practice, clinicians may benefit from a closer analysis of existing assumptions about truth, validity, and reality. In this paper, we give a brief overview of several important theories of truth, notably the ideal limit theorem (which assumes an ultimate and absolute truth towards which scientific inquiry progresses), the dominant way truth is conceptualized in the discourse and practice of {EBHC}. We draw on Belgian philosopher Isabelle Stengers' work to demonstrate that bias means one thing if one assumes a world of hard facts “out there,” waiting to be collected. It means something different if one takes a critical view of the knowledge-power complex in research trials. Bias appears to have both an unproductive aspect and a productive aspect as argued by Stengers and others: Facts are not absolute but result from an interest, or interesse: a bias towards a certain line of questioning that cannot be eliminated. The duality that Stengers' view invokes draws attention to and challenges the assumptions underlying the ideal limit theory of truth in several ways. Most importantly, it casts doubt on the ideal limit theory as it applies to the single case scenario of the clinical encounter, the cornerstone of {EBHC}. To the extent that the goal of {EBHC} is to support inferencing in the clinical encounter, then the ideal limit as the sole concept of truth appears to be conceptually insufficient. We contend that {EBHC} could usefully incorporate a more pluralist understanding of truth and bias and provide an example how this would work out in a clinical scenario.},
	pages = {930--938},
	number = {5},
	journaltitle = {Journal of Evaluation in Clinical Practice},
	author = {Wieringa, Sietse and Engebretsen, Eivind and Heggen, Kristin and Greenhalgh, Trish},
	urldate = {2021-05-21},
	date = {2018},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jep.13010},
	keywords = {science, epistemology, evidence-based medicine, practical reasoning},
	file = {Full Text PDF:/Users/tom/Zotero/storage/4JZ9EYQB/Wieringa et al. - 2018 - Rethinking bias and truth in evidence-based health.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SY8IBQD9/jep.html:text/html},
}

@article{sterne_statistical_2002,
	title = {Statistical methods for assessing the influence of study characteristics on treatment effects in ‘meta-epidemiological’ research},
	volume = {21},
	rights = {Copyright © 2002 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1184},
	doi = {https://doi.org/10.1002/sim.1184},
	abstract = {Biases in systematic reviews and meta-analyses may be examined in ‘meta-epidemiological’ studies, in which the influence of trial characteristics such as measures of study quality on treatment effect estimates is explored. Published studies to date have analysed data from collections of meta-analyses with binary outcomes, using logistic regression models that assume that there is no between- or within-meta-analysis heterogeneity. Using data from a study of publication bias (39 meta-analyses, 394 published and 88 unpublished trials) and language bias (29 meta-analyses, 297 English language trials and 52 non-English language trials), we compare results from logistic regression models, with and without robust standard errors to allow for clustering on meta-analysis, with results using a ‘meta-meta-analytic’ approach that can allow for between- and within-meta-analysis heterogeneity. We also consider how to allow for the confounding effects of different trial characteristics. We show that both within- and between meta-analysis heterogeneity may be of importance in the analysis of meta-epidemiological studies, and that confounding exists between the effects of publication status and trial quality. Copyright © 2002 John Wiley \& Sons, Ltd.},
	pages = {1513--1524},
	number = {11},
	journaltitle = {Statistics in Medicine},
	author = {Sterne, Jonathan A. C. and Jüni, Peter and Schulz, Kenneth F. and Altman, Douglas G. and Bartlett, Christopher and Egger, Matthias},
	urldate = {2021-05-26},
	date = {2002},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1184},
	keywords = {publication bias, meta-analysis, clinical trials, language bias, trial quality},
	file = {Full Text PDF:/Users/tom/Zotero/storage/G3R9SV2C/Sterne et al. - 2002 - Statistical methods for assessing the influence of.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8CJK3Z86/sim.html:text/html},
}

@article{chalmers_avoidable_2009,
	title = {Avoidable waste in the production and reporting of research evidence},
	volume = {374},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(09)60329-9/abstract},
	doi = {10.1016/S0140-6736(09)60329-9},
	abstract = {Without accessible and usable reports, research cannot help patients and their clinicians.
In a published Personal View,1 a medical researcher with myeloma reflected on the
way that the results of four randomised trials relevant to his condition had still
not been published, years after preliminary findings had been presented in meeting
abstracts:},
	pages = {86--89},
	number = {9683},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Chalmers, Iain and Glasziou, Paul},
	urldate = {2021-05-31},
	date = {2009-07-04},
	pmid = {19525005},
	note = {Publisher: Elsevier},
	file = {Chalmers and Glasziou - 2009 - Avoidable waste in the production and reporting of.pdf:/Users/tom/Zotero/storage/GVIQEGA5/Chalmers and Glasziou - 2009 - Avoidable waste in the production and reporting of.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PPQMFEW9/fulltext.html:text/html},
}

@report{fife_understanding_2019,
	title = {Understanding the exploratory/confirmatory data analysis continuum: moving beyond the "replication crisis"},
	url = {https://psyarxiv.com/5vfq6/},
	shorttitle = {Understanding the exploratory/confirmatory data analysis continuum},
	abstract = {In light of the “replication crisis,” some advocate for stricter standards and greater transparency in research methods. These efforts push toward a data analysis approach called “confirmatory data analysis” ({CDA}). However, some (e.g., Baumeister, 2016; Finkel, Eastwick, \& Reis, 2017; Goldin-Meadow, 2016) argued emphasizing {CDA} may restrict creativity and discovery. These scholars argued (sometimes inadvertently) for greater freedom to pursue an approach called “exploratory data analysis” ({EDA}). Unfortunately, many of those who push against stricter {CDA} standards don’t realize {EDA} even exists, or fail to understand the philosophy and proper tools for exploration. In this paper, the meaning, tools, and ethics associated with {EDA}, {CDA}, and a relatively unknown but important approach called “rough {CDA}” are clarified. Important distinctions are developed between {EDA}/rough {CDA}/{CDA} and problematic analysis activities including p-hacking, {HARKing}, and data mining.  {CDA}, {EDA}, p-hacking, {HARKing}, etc., are situated in a unified (graphical) framework that clarifies ethical boundaries associated with each. In short, the proper data analytic approach depends on (1) intentions, and (2) transparency. Most psychological research isn’t at a maturity level to justify {CDA}, and researchers have historically utilized tools that are mismatched to their research agenda. In the conclusion, guidelines are presented about how these typologies can be integrated into a cumulative research program that can help psychology move beyond the replication crisis.},
	institution = {{PsyArXiv}},
	author = {Fife, Dustin and Rodgers, Joseph Lee},
	urldate = {2021-06-02},
	date = {2019-12-18},
	doi = {10.31234/osf.io/5vfq6},
	note = {type: article},
	keywords = {Meta-science, open science, preregistration, Social and Behavioral Sciences, Quantitative Methods, replication crisis, Statistical Methods, Theory and Philosophy of Science, {CDA}, confirmatory data analysis, {EDA}, exploratory data analysis, Quantitative Psychology, rough {CDA}, Science and Technology Policy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/8PW9VUC7/Fife and Rodgers - 2019 - Understanding the ExploratoryConfirmatory Data An.pdf:application/pdf},
}

@article{collins_joy_2021,
	title = {Joy and rigor in behavioral science},
	volume = {164},
	issn = {0749-5978},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597821000327},
	doi = {10.1016/j.obhdp.2021.03.002},
	abstract = {In the past decade, behavioral science has seen the introduction of beneficial reforms to reduce false positive results. Serving as the motivational backdrop for the present research, we wondered whether these reforms might have unintended negative consequences for researchers’ behavior and emotional experiences. In an experiment simulating the research process, Study 1 (N = 449 researchers) suggested that engaging in a pre-registration task impeded the discovery of an interesting but non-hypothesized result. Study 2 (N = 400 researchers) indicated that relative to confirmatory research, researchers found exploratory research more enjoyable, motivating, and interesting; and less anxiety-inducing, frustrating, boring, and scientific. These studies raise the possibility that emphasizing confirmation can shift researchers away from exploration, and that such a shift could degrade the subjective experience of conducting research. Study 3 (N = 314 researchers) introduced a scale to measure “prediction preoccupation”—the feeling of heightened concern over, and fixation with, confirming predictions.},
	pages = {179--191},
	journaltitle = {Organizational Behavior and Human Decision Processes},
	shortjournal = {Organizational Behavior and Human Decision Processes},
	author = {Collins, Hanne K. and Whillans, Ashley V. and John, Leslie K.},
	urldate = {2021-06-08},
	date = {2021-05-01},
	langid = {english},
	keywords = {Open science, Pre-registration, Confirmation, Exploration, Career satisfaction, Diversity, False positives},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/I93A83ZF/Collins et al. - 2021 - Joy and rigor in behavioral science.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/N42C64BI/S0749597821000327.html:text/html},
}

@article{montoya_opening_2021,
	title = {Opening the door to registered reports: census of journals publishing registered reports (2013–2020)},
	volume = {7},
	issn = {2474-7394},
	url = {https://doi.org/10.1525/collabra.24404},
	doi = {10.1525/collabra.24404},
	shorttitle = {Opening the door to registered reports},
	abstract = {Registered reports are a new publication workflow where the decision to publish is made prior to data collection and analysis and thus cannot be dependent on the outcome of the study. An increasing number of journals have adopted this new mechanism, but previous research suggests that submission rates are still relatively low. We conducted a census of journals publishing registered reports (N = 278) using independent coders to collect information from submission guidelines, with the goal of documenting journals’ early adoption of registered reports. Our results show that the majority of journals adopting registered reports are in psychology, and it typically takes about a year to publish the first registered report after adopting. Still, many journals have not published their first registered report. There is high variability in impact of journals adopting registered reports. Many journals do not include concrete information about policies that address concerns about registered reports (e.g., exploratory analysis); however, those that do typically allow these practices with some restrictions. Additionally, other open science practices are commonly encouraged or required as part of the registered report process, especially open data and materials. Overall, many journals did not include many of the fields coded by the research team, which could be a barrier to submission for some authors. Though the majority of journals allow authors to be anonymous during the review process, a sizable portion do not, which could also be a barrier to submission. We conclude with future directions and implications for authors of registered reports, journals that have already adopted registered reports, and journals that may consider adopting registered reports in the future.},
	pages = {24404},
	number = {1},
	journaltitle = {Collabra: Psychology},
	shortjournal = {Collabra: Psychology},
	author = {Montoya, Amanda Kay and Krenzer, William Leo Donald and Fossum, Jessica Louise},
	urldate = {2021-06-09},
	date = {2021-06-08},
	file = {Full Text PDF:/Users/tom/Zotero/storage/6X7BBCRC/Montoya et al. - 2021 - Opening the Door to Registered Reports Census of .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/P7NH78ZB/Opening-the-Door-to-Registered-Reports-Census-of.html:text/html},
}

@article{glass_primary_1976,
	title = {Primary, secondary, and meta-analysis of research},
	volume = {5},
	issn = {0013-189X},
	url = {https://www.jstor.org/stable/1174772},
	doi = {10.2307/1174772},
	pages = {3--8},
	number = {10},
	journaltitle = {Educational Researcher},
	author = {Glass, Gene V.},
	urldate = {2021-06-11},
	date = {1976},
	note = {Publisher: [American Educational Research Association, Sage Publications, Inc.]},
	file = {Glass - 1976 - Primary, Secondary, and Meta-Analysis of Research.pdf:/Users/tom/Zotero/storage/766Q24B4/Glass - 1976 - Primary, Secondary, and Meta-Analysis of Research.pdf:application/pdf},
}

@article{sohn_statistical_1998,
	title = {Statistical Significance and Replicability: Why the Former Does not Presage the Latter},
	volume = {8},
	issn = {0959-3543},
	url = {https://doi.org/10.1177/0959354398083001},
	doi = {10.1177/0959354398083001},
	shorttitle = {Statistical Significance and Replicability},
	abstract = {In spite of arguments to the contrary, psychologists, it is shown here, believe statistical significance ({SS}) signifies that a finding will replicate. The most visible argument that {SS} is not an index of replicability, one that is based in notions of Bayesian statistical inference, is considered and shown to be flawed. Two different arguments are presented that demonstrate the irrelevance of {SS} to replicability: (a) {SS} may not be taken as a sign of the truth of the research hypothesis; and (b) statistical significance tests do not generate verifiable predictions of replication attempts. Direct tests of replicability and effect-size measures of replicability are shown to have comparable problems. A solution to the replicability problem is proposed for atheoretical research that replaces `once-and-for-all' tests of replicability with the requirement that the treatment effect be demonstrable (a) in the individual, (b) on a continuing basis, and (c) in a way that is clearly discernible. The question of the role of significance testing in research is answered by denying a role to such testing.},
	pages = {291--311},
	number = {3},
	journaltitle = {Theory \& Psychology},
	shortjournal = {Theory \& Psychology},
	author = {Sohn, David},
	urldate = {2021-06-20},
	date = {1998-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd},
	keywords = {statistical significance, null hypothesis testing, replicability and replication, significance testing},
}

@book{galef_scout_2021,
	location = {New York},
	title = {The scout mindset: why some people see things clearly and others don't},
	isbn = {978-0-7352-1755-3 978-0-593-18926-9},
	shorttitle = {The scout mindset},
	abstract = {"When it comes to what we believe, humans see what they want to see. In other words, we have what Julia Galef calls a "soldier" mindset. From tribalism and wishful thinking, to rationalizing in our personal lives and everything in between, we are driven to defend the ideas we most want to believe--and shoot down those we don't. But if we want to get things right more often, argues Galef, we should train ourselves to have a "scout" mindset. Unlike the soldier, a scout's goal isn't to defend one side over the other. It's to go out, survey the territory, and come back with as accurate a map as possible. Regardless of what they hope to be the case, above all, the scout wants to know what's actually true. In The Scout Mindset, Galef shows that what makes scouts better at getting things right isn't that they're smarter or more knowledgeable than everyone else. It's a handful of emotional skills, habits, and ways of looking at the world--which anyone can learn. With fascinating examples ranging from how to survive being stranded in the middle of the ocean, to how Jeff Bezos avoids overconfidence, to how superforecasters outperform {CIA} operatives, to Reddit threads and modern partisan politics, Galef explores why our brains deceive us and what we can do to change the way we think"--},
	publisher = {Portfolio},
	author = {Galef, Julia},
	date = {2021},
	langid = {english},
	keywords = {Cognition, Critical thinking, Skepticism},
	file = {Galef - 2021 - The scout mindset why some people see things clea.pdf:/Users/tom/Zotero/storage/8JVW36G9/Galef - 2021 - The scout mindset why some people see things clea.pdf:application/pdf},
}

@article{lewandowsky_dont_2016,
	title = {Don’t let transparency damage science},
	volume = {529},
	doi = {10.1038/529459a},
	pages = {459--461},
	number = {7587},
	journaltitle = {Nature},
	author = {Lewandowsky, Stephan and Bishop, Dorothy},
	date = {2016},
	langid = {english},
	file = {Lewandowsky and Bishop - Don’t let transparency damage science.pdf:/Users/tom/Zotero/storage/ZTLELFQN/Lewandowsky and Bishop - Don’t let transparency damage science.pdf:application/pdf},
}

@article{meyer_practical_2018,
	title = {Practical tips for ethical data sharing},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245917747656},
	doi = {10.1177/2515245917747656},
	abstract = {This Tutorial provides practical dos and don’ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say—and what not to say—in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing “public” data.},
	pages = {131--144},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Meyer, Michelle N.},
	urldate = {2021-06-21},
	date = {2018-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {data sharing, morality, research ethics, {IRB}, responsible conduct of research},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/78DEU2AM/Meyer - 2018 - Practical Tips for Ethical Data Sharing.pdf:application/pdf},
}

@article{magee_dawn_2014,
	title = {The dawn of open access to phylogenetic data},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0110268},
	doi = {10.1371/journal.pone.0110268},
	abstract = {The scientific enterprise depends critically on the preservation of and open access to published data. This basic tenet applies acutely to phylogenies (estimates of evolutionary relationships among species). Increasingly, phylogenies are estimated from increasingly large, genome-scale datasets using increasingly complex statistical methods that require increasing levels of expertise and computational investment. Moreover, the resulting phylogenetic data provide an explicit historical perspective that critically informs research in a vast and growing number of scientific disciplines. One such use is the study of changes in rates of lineage diversification (speciation – extinction) through time. As part of a meta-analysis in this area, we sought to collect phylogenetic data (comprising nucleotide sequence alignment and tree files) from 217 studies published in 46 journals over a 13-year period. We document our attempts to procure those data (from online archives and by direct request to corresponding authors), and report results of analyses (using Bayesian logistic regression) to assess the impact of various factors on the success of our efforts. Overall, complete phylogenetic data for of these studies are effectively lost to science. Our study indicates that phylogenetic data are more likely to be deposited in online archives and/or shared upon request when: (1) the publishing journal has a strong data-sharing policy; (2) the publishing journal has a higher impact factor, and; (3) the data are requested from faculty rather than students. Importantly, our survey spans recent policy initiatives and infrastructural changes; our analyses indicate that the positive impact of these community initiatives has been both dramatic and immediate. Although the results of our study indicate that the situation is dire, our findings also reveal tremendous recent progress in the sharing and preservation of phylogenetic data.},
	pages = {e110268},
	number = {10},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Magee, Andrew F. and May, Michael R. and Moore, Brian R.},
	urldate = {2021-06-22},
	date = {2014-10-24},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Scientific publishing, Bibliometrics, Archives, Multiple alignment calculation, Phylogenetic analysis, Phylogenetics, Sequence alignment, Undergraduates},
	file = {Full Text PDF:/Users/tom/Zotero/storage/JF9HQYTA/Magee et al. - 2014 - The Dawn of Open Access to Phylogenetic Data.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GC6BC5ZR/article.pdf:application/pdf},
}

@article{martone_data_2018,
	title = {Data sharing in psychology.},
	volume = {73},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000242},
	doi = {10.1037/amp0000242},
	abstract = {Routine data sharing, defined here as the publication of the primary data and any supporting materials required to interpret the data acquired as part of a research study, is still in its infancy in psychology, as in many domains. Nevertheless, with increased scrutiny on reproducibility and more funder mandates requiring sharing of data, the issues surrounding data sharing are moving beyond whether data sharing is a benefit or a bane to science, to what data should be shared and how. Here, we present an overview of these issues, specifically focusing on the sharing of so-called “long tail” data, that is, data generated by individual laboratories as part of largely hypothesis-driven research. We draw on experiences in other domains to discuss attitudes toward data sharing, cost-benefits, best practices and infrastructure. We argue that the publishing of data sets is an integral component of 21st-century scholarship. Moreover, although not all issues around how and what to share have been resolved, a consensus on principles and best practices for effective data sharing and the infrastructure for sharing many types of data are largely in place.},
	pages = {111--125},
	number = {2},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Martone, Maryann E. and Garcia-Castro, Alexander and {VandenBos}, Gary R.},
	urldate = {2021-06-22},
	date = {2018-02},
	langid = {english},
	file = {Martone et al. - 2018 - Data sharing in psychology..pdf:/Users/tom/Zotero/storage/T3CBU2V9/Martone et al. - 2018 - Data sharing in psychology..pdf:application/pdf},
}

@article{cummings_but_2019,
	title = {But what do participants want? Comment on the “Data Sharing in Psychology” special section (2018).},
	volume = {74},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000408},
	doi = {10.1037/amp0000408},
	shorttitle = {But what do participants want?},
	abstract = {This commentary addresses a recent special section on data sharing (i.e., open data) in the February–March 2018 American Psychologist. In 4 articles, the authors outline how open data can positively impact psychology and provide guidelines for adopting open data practices, which we believe is to be commended. However, this special issue has not acknowledged a crucial concern in the open data debate: the views and desires of participants. Participants are the backbone of psychological research and an important stakeholder in open data issues. We review research that has studied participants’ opinions of open data and outline concerns regarding open data raised by some groups of participants. We conclude with recommendations, including a call to psychological researchers to move beyond opinion and instead to empirically examine the impact of open data. We believe psychology is a discipline uniquely poised to execute these recommendations and guide researchers’ understandings of how to appropriately and ethically implement open data practices across multiple disciplines.},
	pages = {245--247},
	number = {2},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Cummings, Jorden A. and Day, T. Eugene},
	urldate = {2021-06-22},
	date = {2019-02},
	langid = {english},
	file = {Cummings and Day - 2019 - But what do participants want Comment on the “Dat.pdf:/Users/tom/Zotero/storage/ZYKQA3KW/Cummings and Day - 2019 - But what do participants want Comment on the “Dat.pdf:application/pdf},
}

@article{ross_ethical_2018,
	title = {Ethical aspects of data sharing and research participant protections.},
	volume = {73},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000240},
	doi = {10.1037/amp0000240},
	pages = {138--145},
	number = {2},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Ross, Michael W. and Iguchi, Martin Y. and Panicker, Sangeeta},
	urldate = {2021-06-22},
	date = {2018-02},
	langid = {english},
	file = {Ross et al. - 2018 - Ethical aspects of data sharing and research parti.pdf:/Users/tom/Zotero/storage/B778I5SU/Ross et al. - 2018 - Ethical aspects of data sharing and research parti.pdf:application/pdf},
}

@article{houtkoop_data_2018,
	title = {Data sharing in psychology: a survey on barriers and preconditions},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245917751886},
	doi = {10.1177/2515245917751886},
	shorttitle = {Data sharing in psychology},
	abstract = {Despite its potential to accelerate academic progress in psychological science, public data sharing remains relatively uncommon. In order to discover the perceived barriers to public data sharing and possible means for lowering them, we conducted a survey, which elicited responses from 600 authors of articles in psychology. The results confirmed that data are shared only infrequently. Perceived barriers included respondents’ belief that sharing is not a common practice in their fields, their preference to share data only upon request, their perception that sharing requires extra work, and their lack of training in sharing data. Our survey suggests that strong encouragement from institutions, journals, and funders will be particularly effective in overcoming these barriers, in combination with educational materials that demonstrate where and how data can be shared effectively.},
	pages = {70--85},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Houtkoop, Bobby Lee and Chambers, Chris and Macleod, Malcolm and Bishop, Dorothy V. M. and Nichols, Thomas E. and Wagenmakers, Eric-Jan},
	urldate = {2021-06-22},
	date = {2018-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {open science, open data, open materials, preregistered, data availability, open practices, public data sharing},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/YN267QP6/Houtkoop et al. - 2018 - Data Sharing in Psychology A Survey on Barriers a.pdf:application/pdf},
}

@article{greenland_quality_1994,
	title = {Quality scores are useless and potentially misleading},
	volume = {140},
	issn = {1476-6256, 0002-9262},
	url = {https://academic.oup.com/aje/article/99755/Quality},
	doi = {10.1093/oxfordjournals.aje.a117250},
	shorttitle = {Quality scores are useless and potentially misleading},
	pages = {300--301},
	number = {3},
	journaltitle = {American Journal of Epidemiology},
	author = {Greenland, Sander},
	urldate = {2021-06-22},
	date = {1994-08-01},
	langid = {english},
	file = {Greenland - 1994 - Quality Scores Are Useless and Potentially Mislead.pdf:/Users/tom/Zotero/storage/BB85TS7U/Greenland - 1994 - Quality Scores Are Useless and Potentially Mislead.pdf:application/pdf},
}

@article{sterne_robins-i_2016,
	title = {{ROBINS}-I: a tool for assessing risk of bias in non-randomised studies of interventions},
	volume = {355},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/355/bmj.i4919},
	doi = {10.1136/bmj.i4919},
	shorttitle = {{ROBINS}-I},
	abstract = {{\textless}p{\textgreater}Non-randomised studies of the effects of interventions are critical to many areas of healthcare evaluation, but their results may be biased. It is therefore important to understand and appraise their strengths and weaknesses. We developed {ROBINS}-I (“Risk Of Bias In Non-randomised Studies - of Interventions”), a new tool for evaluating risk of bias in estimates of the comparative effectiveness (harm or benefit) of interventions from studies that did not use randomisation to allocate units (individuals or clusters of individuals) to comparison groups. The tool will be particularly useful to those undertaking systematic reviews that include non-randomised studies.{\textless}/p{\textgreater}},
	pages = {i4919},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Sterne, Jonathan {AC} and Hernán, Miguel A. and Reeves, Barnaby C. and Savović, Jelena and Berkman, Nancy D. and Viswanathan, Meera and Henry, David and Altman, Douglas G. and Ansari, Mohammed T. and Boutron, Isabelle and Carpenter, James R. and Chan, An-Wen and Churchill, Rachel and Deeks, Jonathan J. and Hróbjartsson, Asbjørn and Kirkham, Jamie and Jüni, Peter and Loke, Yoon K. and Pigott, Theresa D. and Ramsay, Craig R. and Regidor, Deborah and Rothstein, Hannah R. and Sandhu, Lakhbir and Santaguida, Pasqualina L. and Schünemann, Holger J. and Shea, Beverly and Shrier, Ian and Tugwell, Peter and Turner, Lucy and Valentine, Jeffrey C. and Waddington, Hugh and Waters, Elizabeth and Wells, George A. and Whiting, Penny F. and Higgins, Julian {PT}},
	urldate = {2021-06-22},
	date = {2016-10-12},
	langid = {english},
	pmid = {27733354},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/TNA99992/Sterne et al. - 2016 - ROBINS-I a tool for assessing risk of bias in non.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/43U55CVH/bmj.html:text/html},
}

@article{noauthor_challenge_2018,
	title = {The challenge of reforming nutritional epidemiologic research},
	pages = {2},
	date = {2018},
	langid = {english},
	file = {2018 - The Challenge of Reforming Nutritional Epidemiolog.pdf:/Users/tom/Zotero/storage/VD5KBDGR/2018 - The Challenge of Reforming Nutritional Epidemiolog.pdf:application/pdf},
}

@article{soderberg_initial_2021,
	title = {Initial evidence of research quality of registered reports compared with the standard publishing model},
	rights = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01142-4},
	doi = {10.1038/s41562-021-01142-4},
	abstract = {In registered reports ({RRs}), initial peer review and in-principle acceptance occur before knowing the research outcomes. This combats publication bias and distinguishes planned from unplanned research. How {RRs} could improve the credibility of research findings is straightforward, but there is little empirical evidence. Also, there could be unintended costs such as reducing novelty. Here, 353 researchers peer reviewed a pair of papers from 29 published {RRs} from psychology and neuroscience and 57 non-{RR} comparison papers. {RRs} numerically outperformed comparison papers on all 19 criteria (mean difference 0.46, scale range −4 to +4) with effects ranging from {RRs} being statistically indistinguishable from comparison papers in novelty (0.13, 95\% credible interval [−0.24, 0.49]) and creativity (0.22, [−0.14, 0.58]) to sizeable improvements in rigour of methodology (0.99, [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper quality (0.66, [0.30, 1.02]). {RRs} could improve research quality while reducing publication bias and ultimately improve the credibility of the published literature. Soderberg et al. asked scientists to peer review registered reports and standard articles post-publication, after information explicitly identifying the article type had been removed. Registered reports scored higher on some dimensions, including quality and rigour.},
	pages = {1--8},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia and Thorn, Felix Singleton and Vazire, Simine and Esterling, Kevin M. and Nosek, Brian A.},
	urldate = {2021-06-29},
	date = {2021-06-24},
	langid = {english},
	note = {Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Psychology;Publishing
Subject\_term\_id: psychology;publishing},
	file = {Snapshot:/Users/tom/Zotero/storage/Q93WMNR7/s41562-021-01142-4.html:text/html},
}

@report{neve_evolution_2021,
	title = {The Evolution of Data Sharing Practices in the Psychological Literature},
	url = {https://psyarxiv.com/3xdja/},
	abstract = {Sharing data has many benefits. However, data sharing rates remain low, for the most part well below 50\%. A variety of interventions encouraging data sharing have been proposed. We focus here on editorial policies. Kidwell et al. (2016) assessed the impact of the introduction of badges in Psychological Science; Hardwicke et al. (2018) assessed the impact of Cognition’s mandatory data sharing policy. Both studies found policies to improve data sharing practices, but only assessed the impact of the policy for up to 25 months after its implementation. We examined the effect of these policies over a longer term by reusing their data and collecting a follow-up sample including articles published up until December 31st, 2019. We fit generalized additive models as these allow for a flexible assessment of the effect of time, in particular to identify non-linear changes in the trend. These models were compared to generalized linear models to examine whether the non-linearity is needed. Descriptive results and the outputs from generalized additive and linear models were coherent with previous findings: following the policies in Cognition and Psychological Science, data sharing statement rates increased immediately and continued to increase beyond the timeframes examined previously, until reaching close to 100\%. In Clinical Psychological Science, data sharing statement rates started to increase only two years following the implementation of badges. Reusability rates jumped from close to 0\% to around 50\% but did not show changes within the pre-policy nor the post-policy timeframes. Journals that did not implement a policy showed no change in data sharing rates or reusability over time. There was variability across journals in the levels of increase, so we suggest future research should examine a larger number of policies to draw conclusions about their efficacy. We also encourage future research to investigate the barriers to data sharing specific to psychology subfields to identify the best interventions to tackle them.},
	institution = {{PsyArXiv}},
	author = {Neve, Judith and Rousselet, Guillaume},
	urldate = {2021-07-03},
	date = {2021-06-29},
	doi = {10.31234/osf.io/3xdja},
	note = {type: article},
	keywords = {Meta-science, open science, Social and Behavioral Sciences, other, Psychology, reproducibility, metascience, journal policy, metapsychology, scientific publishing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/KMPWM9M9/Neve and Rousselet - 2021 - The Evolution of Data Sharing Practices in the Psy.pdf:application/pdf},
}

@article{moher_all_2015,
	title = {All in the Family: systematic reviews, rapid reviews, scoping reviews, realist reviews, and more},
	volume = {4},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-015-0163-7},
	doi = {10.1186/s13643-015-0163-7},
	shorttitle = {All in the Family},
	pages = {183},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Systematic Reviews},
	author = {Moher, David and Stewart, Lesley and Shekelle, Paul},
	urldate = {2021-07-05},
	date = {2015-12-22},
	file = {Full Text PDF:/Users/tom/Zotero/storage/79ILQF55/Moher et al. - 2015 - All in the Family systematic reviews, rapid revie.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SFHU9EDE/s13643-015-0163-7.html:text/html},
}

@article{bar-ilan_post_2017-1,
	title = {Post retraction citations in context: a case study},
	volume = {113},
	issn = {0138-9130, 1588-2861},
	url = {http://link.springer.com/10.1007/s11192-017-2242-0},
	doi = {10.1007/s11192-017-2242-0},
	shorttitle = {Post retraction citations in context},
	pages = {547--565},
	number = {1},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Bar-Ilan, Judit and Halevi, Gali},
	urldate = {2021-07-06},
	date = {2017-10},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/6TNLJPV9/Bar-Ilan and Halevi - 2017 - Post retraction citations in context a case study.pdf:application/pdf},
}

@article{elsey_human_2018,
	title = {Human memory reconsolidation: A guiding framework and critical review of the evidence.},
	volume = {144},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000152},
	doi = {10.1037/bul0000152},
	shorttitle = {Human memory reconsolidation},
	pages = {797--848},
	number = {8},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Elsey, James W. B. and Van Ast, Vanessa A. and Kindt, Merel},
	urldate = {2021-07-06},
	date = {2018-08},
	langid = {english},
	file = {Elsey et al. - 2018 - Human memory reconsolidation A guiding framework .pdf:/Users/tom/Zotero/storage/ZAUZEUMN/Elsey et al. - 2018 - Human memory reconsolidation A guiding framework .pdf:application/pdf},
}

@report{gopalakrishna_prevalence_2021,
	title = {Prevalence of questionable research practices, research misconduct and their potential explanatory factors: a survey among academic researchers in The Netherlands},
	url = {https://osf.io/preprints/metaarxiv/vk9yt/},
	shorttitle = {Prevalence of questionable research practices, research misconduct and their potential explanatory factors},
	abstract = {Background
Prevalence of research misconduct, questionable research practices ({QRPs}) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers.
Methods 
The National Survey on Research Integrity was aimed at all disciplinary fields and academic ranks in the Netherlands. The survey enquired about engagement in fabrication, falsification and 11 {QRPs} over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used a randomized response method for questions on research misconduct. 
Results
6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% {CI}: 2.9, 5.7) and falsification 4.2\% (95\% {CI}: 2.8, 5.6). Prevalence of {QRPs} ranged from 0.6\% (95\% {CI}: 0.5, 0.9) to 17.5\% (95 \% {CI}: 16.4, 18.7) with 51.3\% (95\% {CI}: 50.1, 52.5) of respondents engaging frequently in ≥ 1 {QRP}. Being a {PhD} candidate or junior researcher increased the odds of frequently engaging in ≥ 1 {QRP}, as did being male. Scientific norm subscription (odds ratio ({OR}) 0.79; 95\% {CI}: 0.63, 1.00) and perceived likelihood of detection by reviewers ({OR} 0.62, 95\% {CI}: 0.44, 0.88) were associated with lower odds of research misconduct. Publication pressure was associated with higher odds of engaging frequently in  ≥ 1 {QRP} ({OR} 1.22, 95\% {CI}: 1.14, 1.30).
Conclusions
We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the “publish or perish” incentive system can promote research integrity.},
	institution = {{MetaArXiv}},
	author = {Gopalakrishna, Gowri and Riet, Gerben ter and Cruyff, Maarten J. L. F. and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte and Bouter, Lex},
	urldate = {2021-07-06},
	date = {2021-07-06},
	doi = {10.31222/osf.io/vk9yt},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Medicine and Health Sciences, Physical Sciences and Mathematics, Open Science, Research Integrity, Survey, Questionable Research Practices, Research Misconduct, Responsible Research Practices},
	file = {Full Text PDF:/Users/tom/Zotero/storage/9P28S2M4/Gopalakrishna et al. - 2021 - Prevalence of questionable research practices, res.pdf:application/pdf},
}

@article{otani_handbook_nodate,
	title = {Handbook of Research Methods in Human Memory},
	pages = {496},
	author = {Otani, Hajime and Schwartz, Bennett L},
	langid = {english},
	file = {Otani and Schwartz - Handbook of Research Methods in Human Memory.pdf:/Users/tom/Zotero/storage/2JPAA4GP/Otani and Schwartz - Handbook of Research Methods in Human Memory.pdf:application/pdf},
}

@book{rosenthal_essentials_2008,
	location = {Boston},
	edition = {3rd ed},
	title = {Essentials of behavioral research: methods and data analysis},
	isbn = {978-0-07-353196-0},
	shorttitle = {Essentials of behavioral research},
	pagetotal = {842},
	publisher = {{McGraw}-Hill},
	author = {Rosenthal, Robert and Rosnow, Ralph L.},
	date = {2008},
	langid = {english},
	note = {{OCLC}: ocm69645797},
	keywords = {Psychology, Research Methodology},
	file = {Rosenthal and Rosnow - 2008 - Essentials of behavioral research methods and dat.pdf:/Users/tom/Zotero/storage/MWGXAP6H/Rosenthal and Rosnow - 2008 - Essentials of behavioral research methods and dat.pdf:application/pdf},
}

@article{ioannidis_massive_2018,
	title = {Massive citations to misleading methods and research tools: Matthew effect, quotation error and citation copying},
	volume = {33},
	issn = {1573-7284},
	url = {https://doi.org/10.1007/s10654-018-0449-x},
	doi = {10.1007/s10654-018-0449-x},
	shorttitle = {Massive citations to misleading methods and research tools},
	pages = {1021--1023},
	number = {11},
	journaltitle = {European Journal of Epidemiology},
	shortjournal = {Eur J Epidemiol},
	author = {Ioannidis, John P. A.},
	urldate = {2021-07-13},
	date = {2018-11-01},
	langid = {english},
}

@article{nelson_mapping_2021,
	title = {Mapping the discursive dimensions of the reproducibility crisis: A mixed methods analysis},
	volume = {16},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0254090},
	doi = {10.1371/journal.pone.0254090},
	shorttitle = {Mapping the discursive dimensions of the reproducibility crisis},
	abstract = {To those involved in discussions about rigor, reproducibility, and replication in science, conversation about the “reproducibility crisis” appear ill-structured. Seemingly very different issues concerning the purity of reagents, accessibility of computational code, or misaligned incentives in academic research writ large are all collected up under this label. Prior work has attempted to address this problem by creating analytical definitions of reproducibility. We take a novel empirical, mixed methods approach to understanding variation in reproducibility discussions, using a combination of grounded theory and correspondence analysis to examine how a variety of authors narrate the story of the reproducibility crisis. Contrary to expectations, this analysis demonstrates that there is a clear thematic core to reproducibility discussions, centered on the incentive structure of science, the transparency of methods and data, and the need to reform academic publishing. However, we also identify three clusters of discussion that are distinct from the main body of articles: one focused on reagents, another on statistical methods, and a final cluster focused on the heterogeneity of the natural world. Although there are discursive differences between scientific and popular articles, we find no strong differences in how scientists and journalists write about the reproducibility crisis. Our findings demonstrate the value of using qualitative methods to identify the bounds and features of reproducibility discourse, and identify distinct vocabularies and constituencies that reformers should engage with to promote change.},
	pages = {e0254090},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Nelson, Nicole C. and Ichikawa, Kelsey and Chung, Julie and Malik, Momin M.},
	urldate = {2021-07-13},
	date = {2021-07-09},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Reproducibility, Scientists, Coding mechanisms, Factor analysis, Inertia, Qualitative studies, Semantics},
	file = {Full Text PDF:/Users/tom/Zotero/storage/WUDX96E8/Nelson et al. - 2021 - Mapping the discursive dimensions of the reproduci.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/U7HA4IA6/article.pdf:application/pdf},
}

@article{robinson_evidence-based_2021,
	title = {Evidence-Based Research Series-Paper 1: What Evidence-Based Research is and why is it important?},
	volume = {129},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(20)31095-7/abstract},
	doi = {10.1016/j.jclinepi.2020.07.020},
	shorttitle = {Evidence-Based Research Series-Paper 1},
	abstract = {{\textless}h2{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Objectives{\textless}/h3{\textgreater}{\textless}p{\textgreater}There is considerable actual and potential waste in research. Evidence-based research ensures worthwhile and valuable research. The aim of this series, which this article introduces, is to describe the evidence-based research approach.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Study Design and Setting{\textless}/h3{\textgreater}{\textless}p{\textgreater}In this first article of a three-article series, we introduce the evidence-based research approach. Evidence-based research is the use of prior research in a systematic and transparent way to inform a new study so that it is answering questions that matter in a valid, efficient, and accessible manner.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater}{\textless}p{\textgreater}We describe evidence-based research and provide an overview of the approach of systematically and transparently using previous research before starting a new study to justify and design the new study (article \#2 in series) and—on study completion—place its results in the context with what is already known (article \#3 in series).{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusion{\textless}/h3{\textgreater}{\textless}p{\textgreater}This series introduces evidence-based research as an approach to minimize unnecessary and irrelevant clinical health research that is unscientific, wasteful, and unethical.{\textless}/p{\textgreater}},
	pages = {151--157},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Robinson, Karen A. and Brunnhuber, Klara and Ciliska, Donna and Juhl, Carsten Bogh and Christensen, Robin and Lund, Hans},
	urldate = {2021-07-15},
	date = {2021-01-01},
	pmid = {32979491},
	note = {Publisher: Elsevier},
	file = {Snapshot:/Users/tom/Zotero/storage/IS4WCJMH/fulltext.html:text/html},
}

@article{elliott_living_2014,
	title = {Living systematic reviews: an emerging opportunity to narrow the evidence-practice gap},
	volume = {11},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001603},
	doi = {10.1371/journal.pmed.1001603},
	shorttitle = {Living systematic reviews},
	abstract = {Julian Elliott and colleagues discuss how the current inability to keep systematic reviews up-to-date hampers the translation of knowledge into action. They propose living systematic reviews as a contribution to evidence synthesis to enhance the accuracy and utility of health evidence.},
	pages = {e1001603},
	number = {2},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Elliott, Julian H. and Turner, Tari and Clavisi, Ornella and Thomas, James and Higgins, Julian P. T. and Mavergames, Chris and Gruen, Russell L.},
	urldate = {2021-07-15},
	date = {2014-02-18},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Statistical data, Data management, Metaanalysis, Systematic reviews, Decision making, Ecosystems, Health systems strengthening, Screening guidelines},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XMXBAZTF/Elliott et al. - 2014 - Living Systematic Reviews An Emerging Opportunity.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KADMXEH6/article.html:text/html},
}

@article{stanley_detecting_nodate,
	title = {Detecting publication selection bias through excess statistical significance},
	volume = {n/a},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1512},
	doi = {10.1002/jrsm.1512},
	abstract = {We introduce and evaluate three tests for publication selection bias based on excess statistical significance. The proposed tests incorporate heterogeneity explicitly in the formulas for expected and excess statistical significance. We calculate the expected proportion of statistically significant findings in the absence of selective reporting or publication bias based on each study’s standard error and meta-analysis estimates of the mean and variance of the true-effect distribution. Comparing the expected to the observed proportion of statistically significant results leads to a simple proportion of statistical significance test ({PSST}). Alternatively, we propose a direct test of excess statistical significance ({TESS}). We also combine these two tests of excess statistical significance ({TESSPSST}). Simulations show that these excess statistical significance tests often outperform the conventional Egger test for publication selection bias and the three-parameter selection model. This article is protected by copyright. All rights reserved.},
	issue = {n/a},
	journaltitle = {Research Synthesis Methods},
	author = {Stanley, T. D. and Doucouliagos, Hristos and Ioannidis, John P. A. and Carter, Evan C.},
	urldate = {2021-07-19},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1512},
	keywords = {meta-analysis, excess statistical significance, publication selection bias, statistical power},
	file = {Full Text PDF:/Users/tom/Zotero/storage/FKNTCFRZ/Stanley et al. - Detecting Publication Selection Bias Through Exces.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VSQDZ299/jrsm.html:text/html},
}

@article{miller_failures_2021-1,
	title = {Failures of memory and the fate of forgotten memories},
	volume = {181},
	issn = {1074-7427},
	url = {https://www.sciencedirect.com/science/article/pii/S1074742721000484},
	doi = {10.1016/j.nlm.2021.107426},
	abstract = {This review is intended primarily to provide cognitive benchmarks and perhaps a new mindset for behavioral neuroscientists who study memory. Forgetting, defined here broadly as all types of decreases in acquired responding to stimulus-specific eliciting cues, is commonly attributed to one or more of the following families of mechanisms: (1) associative interference by information similar to, but different from the target information, (2) spontaneous decay of memory with increasing retention intervals, (3) displacement from short-term memory by irrelevant information, and (4) inadequate retrieval cues at test. I briefly review each of these families and discuss data suggesting that many apparent instances of spontaneous forgetting and displacement from short-term memory can be viewed as variants of inadequate retrieval cues and associative interference. The potential for recovery of target information from each of these families of forgetting without further relevant training is then reviewed, with a conclusion that most forgetting is due to retrieval failure as opposed to irreversible erasure of memory. The more general point is made that there are logical problems with ever talking about attenuating or erasing a memory as a consequence of conventional forgetting or disrupted consolidation/reconsolidation. Consideration is then given to the frequently overlooked but highly beneficial consequences of most forgetting. Lastly, the major variables that moderate forgetting are summarized, including (a) the similarities of the target information including training context to the explicit retrieval cues and context present at test, (b) the similarities of potentially interfering acquired information to the retrieval cues and context present at test, and (c) the retention interval for the target information relative to that for the potentially interfering information. Appropriate manipulation of these variables can reduce forgetting, and increase forgetting when desired.},
	pages = {107426},
	journaltitle = {Neurobiology of Learning and Memory},
	shortjournal = {Neurobiology of Learning and Memory},
	author = {Miller, Ralph R.},
	urldate = {2021-07-20},
	date = {2021-05-01},
	langid = {english},
	keywords = {Memory, Associative interference, Consolidation, Forgetting, Retrieval failure, Spontaneous forgetting},
	file = {Miller - 2021 - Failures of memory and the fate of forgotten memor.pdf:/Users/tom/Zotero/storage/47AL7H8P/Miller - 2021 - Failures of memory and the fate of forgotten memor.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/EZAV5EMH/S1074742721000484.html:text/html},
}

@article{borsboom_concept_2004,
	title = {The Concept of Validity.},
	volume = {111},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.111.4.1061},
	doi = {10.1037/0033-295X.111.4.1061},
	pages = {1061--1071},
	number = {4},
	journaltitle = {Psychological Review},
	shortjournal = {Psychological Review},
	author = {Borsboom, Denny and Mellenbergh, Gideon J. and van Heerden, Jaap},
	urldate = {2021-07-20},
	date = {2004},
	langid = {english},
	file = {Borsboom et al. - 2004 - The Concept of Validity..pdf:/Users/tom/Zotero/storage/HMRSZVLH/Borsboom et al. - 2004 - The Concept of Validity..pdf:application/pdf},
}

@article{davis_peer-review_2018,
	title = {Peer-review guidelines promoting replicability and transparency in psychological science},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918806489},
	doi = {10.1177/2515245918806489},
	abstract = {More and more psychological researchers have come to appreciate the perils of common but poorly justified research practices and are rethinking commonly held standards for evaluating research. As this methodological reform expresses itself in psychological research, peer reviewers of such work must also adapt their practices to remain relevant. Reviewers of journal submissions wield considerable power to promote methodological reform, and thereby contribute to the advancement of a more robust psychological literature. We describe concrete practices that reviewers can use to encourage transparency, intellectual humility, and more valid assessments of the methods and statistics reported in articles.},
	pages = {556--573},
	number = {4},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Davis, William E. and Giner-Sorolla, Roger and Lindsay, D. Stephen and Lougheed, Jessica P. and Makel, Matthew C. and Meier, Matt E. and Sun, Jessie and Vaughn, Leigh Ann and Zelenski, John M.},
	urldate = {2021-07-22},
	date = {2018-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {p-hacking, reproducibility, publication bias, replication, peer review, validity},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/ZP5LAI2E/Davis et al. - 2018 - Peer-Review Guidelines Promoting Replicability and.pdf:application/pdf},
}

@article{mcalister_medical_nodate,
	title = {The medical review article revisited: has the science improved?},
	abstract = {Background: The validity of a review depends on its methodologic quality.
Objective: To determine the methodologic quality of recently published review articles. Design: Critical appraisal. Setting: All reviews of clinical topics published in six general medical journals in 1996.
Measurements: Explicit criteria that have been published and validated were used.
Results: Of 158 review articles, only 2 satisﬁed all 10 methodologic criteria (median number of criteria satisﬁed, 1). Less than a quarter of the articles described how evidence was identiﬁed, evaluated, or integrated; 34\% addressed a focused clinical question; and 39\% identiﬁed gaps in existing knowledge. Of the 111 reviews that made treatment recommendations, 48\% provided an estimate of the magnitude of potential beneﬁts (and 34\%, the potential adverse effects) of the treatment options, 45\% cited randomized clinical trials to support their recommendations, and only 6\% made any reference to costs. Review articles are an important element of most medical journals and a popular source of information for clinicians (1). Given the increasing volume of medical literature and the limited time for reading that busy clinicians have, reliance on review articles is likely to increase. However, concerns have been raised that narrative, nonsystematic review articles may produce biased conclusions (2, 3). Mulrow (4) examined 50 review articles published in four major medical journals (New England Journal of Medicine, Annals of Internal Medicine, {JAMA}, and Archives of Internal Medicine) in 1985–1986 and found that none fulﬁlled 8 explicit criteria for scientiﬁcally sound summaries of the evidence. As a result, she and others (5, 6) proposed criteria for conducting and evaluating review articles that would improve their quality; these criteria are the ﬁrst 10 listed in the Appendix Table. In a study of 36 review articles done by nine content experts and methodologists, these criteria were shown to yield reliable and valid estimates of the scientiﬁc quality of reviews (7, 8). We sought to describe the methods used in recently published review articles and determine whether the attention paid to the methodologic shortcomings of review articles has led to improvements in their scientiﬁc quality.
Conclusions: The methodologic quality of clinical review articles is highly variable, and many of these articles do not specify systematic methods.},
	pages = {5},
	author = {{McAlister}, Finlay A and Clark, Heather D and van Walraven, Carl and Straus, Sharon E and Lawson, Fiona M E and Moher, David and Mulrow, Cynthia D},
	langid = {english},
	file = {McAlister et al. - The Medical Review Article Revisited Has the Scie.pdf:/Users/tom/Zotero/storage/4TVWXBRY/McAlister et al. - The Medical Review Article Revisited Has the Scie.pdf:application/pdf},
}

@article{donnelly_four_2018,
	title = {Four principles to make evidence synthesis more useful for policy},
	volume = {558},
	rights = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-018-05414-4},
	doi = {10.1038/d41586-018-05414-4},
	abstract = {Reward the creation of analyses for policymakers that are inclusive, rigorous, transparent and accessible.},
	pages = {361--364},
	number = {7710},
	journaltitle = {Nature},
	author = {Donnelly, Christl A. and Boyd, Ian and Campbell, Philip and Craig, Claire and Vallance, Patrick and Walport, Mark and Whitty, Christopher J. M. and Woods, Emma and Wormald, Chris},
	urldate = {2021-07-25},
	date = {2018-06},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Comment
Number: 7710
Publisher: Nature Publishing Group
Subject\_term: Policy, Research management, Funding},
	file = {Full Text PDF:/Users/tom/Zotero/storage/LJ3RSVUS/Donnelly et al. - 2018 - Four principles to make evidence synthesis more us.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RGS8QYJL/d41586-018-05414-4.html:text/html},
}

@article{gaudino_effects_2020,
	title = {Effects of experimental interventions to improve the biomedical peer‐review process: a systematic review and meta‐analysis},
	rights = {Copyright © 2021 The Authors. Published on behalf of the American Heart Association, Inc., by Wiley Blackwell© 2021 The Authors. Published on behalf of the American Heart Association, Inc., by Wiley.},
	url = {https://www.ahajournals.org/doi/abs/10.1161/JAHA.120.019903},
	doi = {10.1161/JAHA.120.019903},
	shorttitle = {Effects of experimental interventions to improve the biomedical peer‐review process},
	abstract = {Background Quality of the peer‐review process has been tested only in small studies. We describe and summarize the randomized trials that investigated interventions aimed at improving peer‐review p...},
	journaltitle = {Journal of the American Heart Association},
	author = {Gaudino, Mario and Robinson, N. Bryce and Franco, Antonino Di and Hameed, Irbaz and Naik, Ajita and Demetres, Michelle and Girardi, Leonard N. and Frati, Giacomo and Fremes, Stephen E. and Biondi‐Zoccai, Giuseppe},
	urldate = {2021-07-26},
	date = {2020-10-22},
	file = {Full Text:/Users/tom/Zotero/storage/7QZ6266A/Gaudino et al. - 2020 - Effects of Experimental Interventions to Improve t.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9GMUEQ4W/JAHA.120.html:text/html},
}

@report{ikeda_post-publication_2020,
	title = {Post-publication peer review for real},
	url = {https://psyarxiv.com/sp3j5/},
	abstract = {The inefficiency of the current peer-review system has been discussed for many years, and now there is a surge of various countermeasures aiming to solve the problems. Post-publication peer review ({PPPR}) has emerged as one of them, and some scholars expected that it would be the definite solution. Unfortunately, a decade of trial has not turned out to be as fruitful as expected. We assessed that the biggest reason for this situation was the lack of incentives among contributors, and proposed that publishing review commentaries as independent and qualified publications in a dedicated section of a journal might solve the problem. Specifically, we took the open peer commentary section of Behavioral and Brain Sciences as a model of such an incentivised structure,  and pictured a possible implementation of this idea in the current web-based environment. Potentials of this new {PPPR} format were suggested.},
	institution = {{PsyArXiv}},
	author = {Ikeda, Koki and Yamada, Yuki and Takahashi, Kohske},
	urldate = {2021-07-26},
	date = {2020-05-26},
	doi = {10.31234/osf.io/sp3j5},
	note = {type: article},
	keywords = {Meta-science, open science, transparency, peer review, scholarly communication, academic publishing, library and information science},
	file = {Full Text PDF:/Users/tom/Zotero/storage/V6LTGFNI/Ikeda et al. - 2020 - Post-Publication Peer Review for Real.pdf:application/pdf},
}

@article{munafo_dont_2021,
	title = {Don’t let the perfect be the enemy of the good},
	volume = {19},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001327},
	doi = {10.1371/journal.pbio.3001327},
	abstract = {There is a growing interest in the factors that influence research quality and into research culture more generally. Reform must be evidence based, but experimental studies in real-world settings can be challenging. Observational evidence, even if imperfect, can be a valuable and efficient starting point to help identify the most fruitful avenues for meta-research investment.},
	pages = {e3001327},
	number = {7},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Munafò, Marcus},
	urldate = {2021-07-27},
	date = {2021-07-15},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Scientists, Behavior, Careers, Tobacco, Reflection, Ecosystems, Research quality assessment, Trainees},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HEGFW9R5/Munafò - 2021 - Don’t let the perfect be the enemy of the good.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Z37GK4RM/article.html:text/html},
}

@article{towse_making_2021,
	title = {Making data meaningful: guidelines for good quality open data},
	volume = {0},
	issn = {0022-4545},
	url = {https://doi.org/10.1080/00224545.2021.1938811},
	doi = {10.1080/00224545.2021.1938811},
	shorttitle = {Making data meaningful},
	pages = {1--8},
	number = {0},
	journaltitle = {The Journal of Social Psychology},
	author = {Towse, Andrea S. and Ellis, David A. and Towse, John N.},
	urldate = {2021-07-27},
	date = {2021-07-22},
	pmid = {34292132},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00224545.2021.1938811},
	keywords = {open science, transparency, reproducibility, data sharing, public data archiving},
	file = {Full Text PDF:/Users/tom/Zotero/storage/BAEJ3CPD/Towse et al. - 2021 - Making data meaningful guidelines for good qualit.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/LPWVNGCQ/00224545.2021.html:text/html},
}

@article{weir_dealing_2018,
	title = {Dealing with missing standard deviation and mean values in meta-analysis of continuous outcomes: a systematic review},
	volume = {18},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-018-0483-0},
	doi = {10.1186/s12874-018-0483-0},
	shorttitle = {Dealing with missing standard deviation and mean values in meta-analysis of continuous outcomes},
	abstract = {Rigorous, informative meta-analyses rely on availability of appropriate summary statistics or individual participant data. For continuous outcomes, especially those with naturally skewed distributions, summary information on the mean or variability often goes unreported. While full reporting of original trial data is the ideal, we sought to identify methods for handling unreported mean or variability summary statistics in meta-analysis.},
	pages = {25},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Weir, Christopher J. and Butcher, Isabella and Assi, Valentina and Lewis, Stephanie C. and Murray, Gordon D. and Langhorne, Peter and Brady, Marian C.},
	urldate = {2021-07-27},
	date = {2018-03-07},
	keywords = {Systematic review, Meta-analysis, Continuous outcomes, Missing mean, Missing standard deviation},
	file = {Full Text PDF:/Users/tom/Zotero/storage/P9ZJL4IF/Weir et al. - 2018 - Dealing with missing standard deviation and mean v.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8332HPV9/s12874-018-0483-0.html:text/html},
}

@article{lakens_calculating_2013,
	title = {Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and {ANOVAs}},
	volume = {0},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full},
	doi = {10.3389/fpsyg.2013.00863},
	shorttitle = {Calculating and reporting effect sizes to facilitate cumulative science},
	abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and {ANOVA}’s such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Lakens, Daniel},
	urldate = {2021-07-27},
	date = {2013},
	note = {Publisher: Frontiers},
	keywords = {power analysis, Cohen's d, effect sizes, eta-squared, sample size planning},
	file = {Full Text:/Users/tom/Zotero/storage/89RKST2E/Lakens - 2013 - Calculating and reporting effect sizes to facilita.pdf:application/pdf},
}

@article{peters_comparison_2006,
	title = {Comparison of two methods to detect publication bias in meta-analysis},
	volume = {295},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.295.6.676},
	doi = {10.1001/jama.295.6.676},
	pages = {676},
	number = {6},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Peters, Jaime L.},
	urldate = {2021-07-28},
	date = {2006-02-08},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/BNWTAFRT/Peters - 2006 - Comparison of Two Methods to Detect Publication Bi.pdf:application/pdf},
}

@article{zwetsloot_standardized_2017,
	title = {Standardized mean differences cause funnel plot distortion in publication bias assessments},
	volume = {6},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.24260},
	doi = {10.7554/eLife.24260},
	abstract = {Meta-analyses are increasingly used for synthesis of evidence from biomedical research, and often include an assessment of publication bias based on visual or analytical detection of asymmetry in funnel plots. We studied the influence of different normalisation approaches, sample size and intervention effects on funnel plot asymmetry, using empirical datasets and illustrative simulations. We found that funnel plots of the Standardized Mean Difference ({SMD}) plotted against the standard error ({SE}) are susceptible to distortion, leading to overestimation of the existence and extent of publication bias. Distortion was more severe when the primary studies had a small sample size and when an intervention effect was present. We show that using the Normalised Mean Difference measure as effect size (when possible), or plotting the {SMD} against a sample size-based precision estimate, are more reliable alternatives. We conclude that funnel plots using the {SMD} in combination with the {SE} are unsuitable for publication bias assessments and can lead to false-positive results.},
	pages = {e24260},
	journaltitle = {{eLife}},
	author = {Zwetsloot, Peter-Paul and Van Der Naald, Mira and Sena, Emily S and Howells, David W and {IntHout}, Joanna and De Groot, Joris {AH} and Chamuleau, Steven {AJ} and {MacLeod}, Malcolm R and Wever, Kimberley E},
	editor = {Teare, M Dawn},
	urldate = {2021-07-28},
	date = {2017-09-08},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {publication bias, meta-analysis, funnel plot, data simulation},
	file = {Full Text PDF:/Users/tom/Zotero/storage/WD8NCLRK/Zwetsloot et al. - 2017 - Standardized mean differences cause funnel plot di.pdf:application/pdf},
}

@incollection{higgins_assessing_2019,
	edition = {1},
	title = {Assessing risk of bias due to missing results in a synthesis},
	isbn = {978-1-119-53662-8 978-1-119-53660-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781119536604.ch13},
	pages = {349--374},
	booktitle = {Cochrane Handbook for Systematic Reviews of Interventions},
	publisher = {Wiley},
	author = {Page, Matthew J and Higgins, Julian {PT} and Sterne, Jonathan {AC}},
	editor = {Higgins, Julian P.T. and Thomas, James and Chandler, Jacqueline and Cumpston, Miranda and Li, Tianjing and Page, Matthew J. and Welch, Vivian A.},
	urldate = {2021-07-28},
	date = {2019-09-23},
	langid = {english},
	doi = {10.1002/9781119536604.ch13},
	file = {Page et al. - 2019 - Assessing risk of bias due to missing results in a.pdf:/Users/tom/Zotero/storage/YB4KQPY3/Page et al. - 2019 - Assessing risk of bias due to missing results in a.pdf:application/pdf},
}

@article{belli_influences_1989,
	title = {Influences of misleading postevent information: misinformation interference and acceptance},
	volume = {118},
	issn = {0096-3445},
	shorttitle = {Influences of misleading postevent information},
	abstract = {Because of the biasing nature of retrieval tests, evidence that the introduction of misleading postevent information will impair the memory for an original event has recently been in dispute. In two experiments, a retrieval test sensitive to both biasing effects of misinformation (misinformation acceptance) and influences of the misinformation on memory (misinformation interference) was used. Both experiments demonstrated misinformation acceptance, and one of the experiments suggested that misinformation interferes with the ability to remember the original event. Two misinformation interference hypotheses are evaluated; they suggest that the misinformation may have either impaired memory or led to confusion regarding what had occurred during the event., (C) 1989 by the American Psychological Association},
	pages = {72--85},
	number = {1},
	journaltitle = {Journal of Experimental Psychology},
	author = {Belli, Robert F.},
	date = {1989-03},
	note = {Institution: (1)Vanderbilt University.},
	file = {Belli - 1989 - Influences of misleading postevent information mi.pdf:/Users/tom/Zotero/storage/CKTRIEQJ/Belli - 1989 - Influences of misleading postevent information mi.pdf:application/pdf},
}

@article{frenda_current_2011,
	title = {Current issues and advances in misinformation research},
	volume = {20},
	issn = {0963-7214},
	url = {https://doi.org/10.1177/0963721410396620},
	doi = {10.1177/0963721410396620},
	abstract = {Eyewitnesses are often called upon to report information about what they have seen. A wealth of research from the past century has demonstrated, however, that eyewitness memory is malleable and vulnerable to distorting influences, including the effects of misinformation. In this article, we review recent developments in research related to the misinformation effect, including individual differences in susceptibility, neuroimaging approaches, and protective interview procedures that may better elicit accurate event details. We conclude with a section on related false memory research.},
	pages = {20--23},
	number = {1},
	journaltitle = {Current Directions in Psychological Science},
	shortjournal = {Curr Dir Psychol Sci},
	author = {Frenda, Steven J. and Nichols, Rebecca M. and Loftus, Elizabeth F.},
	urldate = {2021-07-28},
	date = {2011-02-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {memory, misinformation, eyewitness, law},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/M5Z6GG5N/Frenda et al. - 2011 - Current Issues and Advances in Misinformation Rese.pdf:application/pdf},
}

@article{loftus_planting_2005-1,
	title = {Planting misinformation in the human mind: A 30-year investigation of the malleability of memory},
	volume = {12},
	issn = {1072-0502, 1549-5485},
	url = {http://learnmem.cshlp.org/content/12/4/361},
	doi = {10.1101/lm.94705},
	shorttitle = {Planting misinformation in the human mind},
	abstract = {The misinformation effect refers to the impairment in memory for the past that arises after exposure to misleading information. The phenomenon has been investigated for at least 30 years, as investigators have addressed a number of issues. These include the conditions under which people are especially susceptible to the negative impact of misinformation, and conversely when are they resistant. Warnings about the potential for misinformation sometimes work to inhibit its damaging effects, but only under limited circumstances. The misinformation effect has been observed in a variety of human and nonhuman species. And some groups of individuals are more susceptible than others. At a more theoretical level, investigators have explored the fate of the original memory traces after exposure to misinformation appears to have made them inaccessible. This review of the field ends with a brief discussion of the newer work involving misinformation that has explored the processes by which people come to believe falsely that they experienced rich complex events that never, in fact, occurred.},
	pages = {361--366},
	number = {4},
	journaltitle = {Learning \& Memory},
	shortjournal = {Learn. Mem.},
	author = {Loftus, Elizabeth F.},
	urldate = {2021-07-28},
	date = {2005-01-07},
	langid = {english},
	pmid = {16027179},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Institution: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Publisher: Cold Spring Harbor Lab},
	file = {Full Text PDF:/Users/tom/Zotero/storage/9VQ2TJXN/Loftus - 2005 - Planting misinformation in the human mind A 30-ye.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/U8ZS5U7J/361.html:text/html},
}

@article{tedersoo_data_2021,
	title = {Data sharing practices and data availability upon request differ across scientific disciplines},
	volume = {8},
	rights = {2021 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-021-00981-0},
	doi = {10.1038/s41597-021-00981-0},
	abstract = {Data sharing is one of the cornerstones of modern science that enables large-scale analyses and reproducibility. We evaluated data availability in research articles across nine disciplines in Nature and Science magazines and recorded corresponding authors’ concerns, requests and reasons for declining data sharing. Although data sharing has improved in the last decade and particularly in recent years, data availability and willingness to share data still differ greatly among disciplines. We observed that statements of data availability upon (reasonable) request are inefficient and should not be allowed by journals. To improve data sharing at the time of manuscript acceptance, researchers should be better motivated to release their data with real benefits such as recognition, or bonus points in grant and job applications. We recommend that data management costs should be covered by funding agencies; publicly available research data ought to be included in the evaluation of applications; and surveillance of data sharing should be enforced by both academic publishers and funders. These cross-discipline survey data are available from the {plutoF} repository.},
	pages = {192},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Tedersoo, Leho and Küngas, Rainer and Oras, Ester and Köster, Kajar and Eenmaa, Helen and Leijen, Äli and Pedaste, Margus and Raju, Marju and Astapova, Anastasiya and Lukner, Heli and Kogermann, Karin and Sepp, Tuul},
	urldate = {2021-07-28},
	date = {2021-07-27},
	langid = {english},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Genetic databases;Molecular ecology
Subject\_term\_id: genetic-databases;molecular-ecology},
	keywords = {toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/J7X7KAEM/Tedersoo et al. - 2021 - Data sharing practices and data availability upon .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ENBWKKS2/s41597-021-00981-0.html:text/html},
}

@article{bert_refining_2019,
	title = {Refining animal research: The Animal Study Registry},
	volume = {17},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000463},
	doi = {10.1371/journal.pbio.3000463},
	shorttitle = {Refining animal research},
	abstract = {The Animal Study Registry ({ASR}; www.animalstudyregistry.org) was launched in January 2019 for preregistration of animal studies in order to increase transparency and reproducibility of bioscience research and to promote animal welfare. The registry is free of charge and is designed for exploratory and confirmatory studies within applied science as well as basic and preclinical research. The registration form helps scientists plan their study thoroughly by asking detailed questions concerning study design, methods, and statistics. With registration, the study automatically receives a digital object identifier ({DOI}) that marks it as intellectual property of the researcher. To accommodate the researchers concerns about theft of ideas, users can restrict the visibility of their registered studies for up to 5 years. The full content of the study becomes publicly accessible at the end of the embargo period. Because the platform is embedded in the infrastructure of the German Federal Government, continuity and data security are provided. By registering a study in the {ASR}, researchers can show their commitment to transparency and data quality to reviewers and editors, to third-party donors, and to the general public.},
	pages = {e3000463},
	number = {10},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Bert, Bettina and Heinl, Céline and Chmielewska, Justyna and Schwarz, Franziska and Grune, Barbara and Hensel, Andreas and Greiner, Matthias and Schönfelder, Gilbert},
	urldate = {2021-07-29},
	date = {2019-10-15},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Statistical data, Reproducibility, Drug therapy, Animal studies, Scientists, Animal husbandry, Animal welfare, Intellectual property},
	file = {Full Text PDF:/Users/tom/Zotero/storage/EPYYRIHI/Bert et al. - 2019 - Refining animal research The Animal Study Registr.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RT6GLST5/article.html:text/html},
}

@article{sim_clinical_2006-1,
	title = {Clinical trial registration: transparency is the watchword},
	volume = {367},
	issn = {0140-6736},
	url = {https://www.sciencedirect.com/science/article/pii/S0140673606687084},
	doi = {10.1016/S0140-6736(06)68708-4},
	shorttitle = {Clinical trial registration},
	pages = {1631--1633},
	number = {9523},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Sim, Ida and Chan, An-Wen and Gülmezoglu, A Metin and Evans, Tim and Pang, Tikki},
	urldate = {2021-07-29},
	date = {2006-05-20},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/HTTU65T2/Sim et al. - 2006 - Clinical trial registration transparency is the w.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/W4J2BS6F/S0140673606687084.html:text/html},
}

@article{catchpole_problem_2015,
	title = {The problem with checklists},
	volume = {24},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {2044-5415, 2044-5423},
	url = {https://qualitysafety.bmj.com/content/24/9/545},
	doi = {10.1136/bmjqs-2015-004431},
	abstract = {‘The Problem with…’ series covers controversial topics related to efforts to improve healthcare quality, including widely recommended but deceptively difficult strategies for improvement and pervasive problems that seem to resist solution.

Since the seminal studies by Gawande and colleagues1 and Pronovost et al ,2 checklists have become the go-to solution for a vast range of patient safety and quality issues in healthcare. Some see them as a quick and obvious solution to a relatively straightforward problem. For others, they illustrate a failure to understand and address the complex challenges in patient safety and quality improvement. Indeed, successes3 and failures4–6 illustrate an underlying difficulty with understanding precisely why checklists work in some cases but not in others. A recent viewpoint summarises the varying applications of checklists in aviation and healthcare, reflecting upon the dangers of making assumptions about their ‘ubiquitous utility’.7 This provided a timely “The Problem with…”8 opportunity, in which we consider the narratives that often surround the challenges faced in designing and implementing a successful checklist, and the science used to explore it.

The apparent simplicity of a checklist is understandingly tempting, with some narratives suggesting that their adoption can be used to effectively address what would appear to be intractable, complex and potentially painful systems issues. However, this simple narrative does not always reflect an understanding of the problems needing to be solved, how best to solve them or indeed the intricacies surrounding the implementation, use and impact of such a simple looking tool. More likely, what we face in introducing a checklist is a rather more complex story of gains and losses, procedural interactions and sociocultural balances (see table 1). This ‘simple’ versus ‘complex’ narrative can also be seen in the frequent aviation analogies, which imply that checklists prevented accidents (‘simple’), …},
	pages = {545--549},
	number = {9},
	journaltitle = {{BMJ} Quality \& Safety},
	shortjournal = {{BMJ} Qual Saf},
	author = {Catchpole, Ken and Russ, Stephanie},
	urldate = {2021-07-29},
	date = {2015-09-01},
	langid = {english},
	pmid = {26089207},
	note = {Publisher: {BMJ} Publishing Group Ltd
Section: The problem with…},
	keywords = {Healthcare quality improvement, Quality improvement, Quality measurement},
	file = {Full Text PDF:/Users/tom/Zotero/storage/V8WQDFFL/Catchpole and Russ - 2015 - The problem with checklists.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XD3G5Z6Y/545.html:text/html},
}

@article{richter_environmental_2009,
	title = {Environmental standardization: cure or cause of poor reproducibility in animal experiments?},
	volume = {6},
	rights = {2009 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.1312},
	doi = {10.1038/nmeth.1312},
	shorttitle = {Environmental standardization},
	abstract = {It is widely believed that environmental standardization is the best way to guarantee reproducible results in animal experiments. However, mounting evidence indicates that even subtle differences in laboratory or test conditions can lead to conflicting test outcomes. Because experimental treatments may interact with environmental conditions, experiments conducted under highly standardized conditions may reveal local 'truths' with little external validity. We review this hypothesis here and present a proof of principle based on data from a multilaboratory study on behavioral differences between inbred mouse strains. Our findings suggest that environmental standardization is a cause of, rather than a cure for, poor reproducibility of experimental outcomes. Environmental standardization can contribute to spurious and conflicting findings in the literature and unnecessary animal use. This conclusion calls for research into practicable and effective ways of systematic environmental heterogenization to attenuate these scientific, economic and ethical costs.},
	pages = {257--261},
	number = {4},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Richter, S. Helene and Garner, Joseph P. and Würbel, Hanno},
	urldate = {2021-07-29},
	date = {2009-04},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 4
Primary\_atype: Reviews
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/Users/tom/Zotero/storage/9D9C37JT/Richter et al. - 2009 - Environmental standardization cure or cause of po.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BJYNMBLU/nmeth.html:text/html},
}

@article{ioannidis_scientific_2012,
	title = {Scientific inbreeding and same-team replication: Type D personality as an example},
	volume = {73},
	issn = {0022-3999},
	url = {https://www.sciencedirect.com/science/article/pii/S0022399912002632},
	doi = {10.1016/j.jpsychores.2012.09.014},
	shorttitle = {Scientific inbreeding and same-team replication},
	abstract = {Replication is essential for validating correct results, sorting out false-positive early discoveries, and improving the accuracy and precision of estimated effects. However, some types of seemingly successful replication may foster a spurious notion of increased credibility, if they are performed by the same team and propagate or extend the same errors made by the original discoveries. Besides same-team replication, replication by other teams may also succumb to inbreeding, if it cannot fiercely maintain its independence. These patterns include obedient replication and obliged replication. I discuss these replication patterns in the context of associations and effects in the psychological sciences, drawing from the criticism of Coyne and de Voogd of the proposed association between type D personality and cardiovascular mortality and other empirical examples.},
	pages = {408--410},
	number = {6},
	journaltitle = {Journal of Psychosomatic Research},
	shortjournal = {Journal of Psychosomatic Research},
	author = {Ioannidis, John P. A.},
	urldate = {2021-07-29},
	date = {2012-12-01},
	langid = {english},
	keywords = {Replication, Bias, Allegiance bias, Confirmation bias},
	file = {Ioannidis - 2012 - Scientific inbreeding and same-team replication T.pdf:/Users/tom/Zotero/storage/6BH34SLQ/Ioannidis - 2012 - Scientific inbreeding and same-team replication T.pdf:application/pdf},
}

@article{miguel_evidence_2021,
	title = {Evidence on research transparency in economics},
	volume = {35},
	issn = {0895-3309},
	url = {https://www.aeaweb.org/articles?id=10.1257/jep.35.3.193&q=Anomalies&within%5Btitle%5D=on&journal=3&from=j},
	doi = {10.1257/jep.35.3.193},
	abstract = {A decade ago, the term "research transparency" was not on economists' radar screen, but in a few short years a scholarly movement has emerged to bring new open 
science practices, tools and norms into the mainstream of our discipline. The goal of this article is to lay out the evidence on the adoption of these approaches—in three 
specific areas: open data, pre-registration and pre-analysis plans, and journal policies—and, more tentatively, begin to assess their impacts on the quality and credibility of 
economics research. The evidence to date indicates that economics (and related quantitative social science fields) are in a period of rapid transition toward new 
transparency-enhancing norms. While solid data on the benefits of these practices in economics is still limited, in part due to their relatively recent adoption, there is 
growing reason to believe that critics' worst fears regarding onerous adoption costs have not been realized. Finally, the article presents a set of frontier questions and 
potential innovations.},
	pages = {193--214},
	number = {3},
	journaltitle = {Journal of Economic Perspectives},
	author = {Miguel, Edward},
	urldate = {2021-07-29},
	date = {2021-08},
	langid = {english},
	keywords = {Computer Programs: General, Computer Programs: General, Higher Education, Market for Economists, Hypothesis Testing: General, Data Collection and Data Estimation Methodology, Research Institutions, Role of Economics, Role of Economics, Role of Economists},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7EA2YH74/Miguel - 2021 - Evidence on Research Transparency in Economics.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B4D395HU/articles.html:text/html},
}

@article{montgomery_reporting_2018,
	title = {Reporting randomised trials of social and psychological interventions: the {CONSORT}-{SPI} 2018 Extension},
	volume = {19},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/s13063-018-2733-1},
	doi = {10.1186/s13063-018-2733-1},
	shorttitle = {Reporting randomised trials of social and psychological interventions},
	abstract = {Randomised controlled trials ({RCTs}) are used to evaluate social and psychological interventions and inform policy decisions about them. Accurate, complete, and transparent reports of social and psychological intervention {RCTs} are essential for understanding their design, conduct, results, and the implications of the findings. However, the reporting of {RCTs} of social and psychological interventions remains suboptimal. The {CONSORT} Statement has improved the reporting of {RCTs} in biomedicine. A similar high-quality guideline is needed for the behavioural and social sciences. Our objective was to develop an official extension of the Consolidated Standards of Reporting Trials 2010 Statement ({CONSORT} 2010) for reporting {RCTs} of social and psychological interventions: {CONSORT}-{SPI} 2018.},
	pages = {407},
	number = {1},
	journaltitle = {Trials},
	shortjournal = {Trials},
	author = {Montgomery, Paul and Grant, Sean and Mayo-Wilson, Evan and Macdonald, Geraldine and Michie, Susan and Hopewell, Sally and Moher, David and Lawrence Aber, J. and Altman, Doug and Bhui, Kamaldeep and Booth, Andrew and Clark, David and Craig, Peter and Eisner, Manuel and Fraser, Mark W. and Gardner, Frances and Grant, Sean and Hedges, Larry and Hollon, Steve and Hopewell, Sally and Kaplan, Robert and Kaufmann, Peter and Konstantopoulos, Spyros and Macdonald, Geraldine and Mayo-Wilson, Evan and McLeroy, Kenneth and Michie, Susan and Mittman, Brian and Moher, David and Montgomery, Paul and Nezu, Arthur and Sherman, Lawrence and Sonuga-Barke, Edmund and Thomas, James and VandenBos, Gary and Waters, Elizabeth and West, Robert and Yaffe, Joanne and {on behalf of the CONSORT-SPI Group}},
	urldate = {2021-07-30},
	date = {2018-07-31},
	keywords = {Transparency, {CONSORT}, Randomised controlled trial, Reporting guideline, Reporting standards},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NW9NXN3H/Montgomery et al. - 2018 - Reporting randomised trials of social and psycholo.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SNB6QHI8/s13063-018-2733-1.html:text/html},
}

@report{gerlicher_better_2021,
	title = {Better, worse, or different than expected - On the role of value and identity prediction errors in fear memory reactivation},
	url = {https://psyarxiv.com/qxdnw/},
	abstract = {Although reconsolidation-based interventions constitute a promising new avenue to treating fear and anxieties disorders, the success of the intervention is not guaranteed. The initiation of memory reconsolidation is dependent on whether a mismatch between the experienced and predicted outcome – a prediction error ({PE}) – occurs during fear memory reactivation. It remains, however, elusive whether any type of {PE} renders fear memories susceptible to reconsolidation disruption. Here, we investigated whether a value {PE}, elicited by an outcome that is better or worse than expected, is necessary to make fear memories susceptible to reconsolidation disruption or whether a model-based identity {PE}, i.e., a {PE} elicited by an outcome equally aversive but different than expected, would be sufficient. Blocking beta-adrenergic receptors with propranolol {HCl} after reactivation did, however, not reduce the expression of fear after either type of {PE}. Instead, we observed intact fear memory expression 24h after reactivation in the value-, identity- and a no-{PE} control group. The present results do not corroborate our earlier findings of reconsolidation disruption and point towards challenges that the field is currently facing in observing evidence for memory reconsolidation at all. We provide potential explanations for the unexpected failure of replicating reconsolidation disruption and discuss future directions.},
	institution = {{PsyArXiv}},
	author = {Gerlicher, Anna and Verweij, Sjoerd A. and Kindt, Merel},
	urldate = {2021-08-01},
	date = {2021-07-22},
	doi = {10.31234/osf.io/qxdnw},
	note = {type: article},
	keywords = {Anxiety Disorders, Clinical Psychology, Physiology, Social and Behavioral Sciences, memory, Cognitive Psychology, fear conditioning, reconsolidation, learning, Memory, Learning, anxiety, Cognitive Neuroscience, Clinical Neuroscience, fear, model-based, model-free, Neuroscience, prediction error, propranolol, Psychopharmacology, threat conditioning, Trauma and Stress},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7SE6ABSY/Gerlicher et al. - 2021 - Better, worse, or different than expected - On the.pdf:application/pdf},
}

@article{maes_elusive_2016,
	title = {The elusive nature of the blocking effect: 15 failures to replicate.},
	volume = {145},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000200},
	doi = {10.1037/xge0000200},
	shorttitle = {The elusive nature of the blocking effect},
	pages = {e49--e71},
	number = {9},
	journaltitle = {Journal of Experimental Psychology: General},
	shortjournal = {Journal of Experimental Psychology: General},
	author = {Maes, Elisa and Boddez, Yannick and Alfei, Joaquín Matías and Krypotos, Angelos-Miltiadis and D'Hooge, Rudi and De Houwer, Jan and Beckers, Tom},
	urldate = {2021-08-11},
	date = {2016-09},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/8MVG346H/Maes et al. - 2016 - The elusive nature of the blocking effect 15 fail.pdf:application/pdf},
}

@article{bakker_misreporting_2011,
	title = {The (mis)reporting of statistical results in psychology journals},
	volume = {43},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-011-0089-5},
	doi = {10.3758/s13428-011-0089-5},
	abstract = {In order to study the prevalence, nature (direction), and causes of reporting errors in psychology, we checked the consistency of reported test statistics, degrees of freedom, and p values in a random sample of high- and low-impact psychology journals. In a second study, we established the generality of reporting errors in a random sample of recent psychological articles. Our results, on the basis of 281 articles, indicate that around 18\% of statistical results in the psychological literature are incorrectly reported. Inconsistencies were more common in low-impact journals than in high-impact journals. Moreover, around 15\% of the articles contained at least one statistical conclusion that proved, upon recalculation, to be incorrect; that is, recalculation rendered the previously significant result insignificant, or vice versa. These errors were often in line with researchers’ expectations. We classified the most common errors and contacted authors to shed light on the origins of the errors.},
	pages = {666--678},
	number = {3},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Bakker, Marjan and Wicherts, Jelte M.},
	urldate = {2021-08-16},
	date = {2011-09-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/SI6X7DYC/Bakker and Wicherts - 2011 - The (mis)reporting of statistical results in psych.pdf:application/pdf},
}

@article{lishner_sorting_2021,
	title = {Sorting the file drawer: a typology for describing unpublished studies},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620979831},
	doi = {10.1177/1745691620979831},
	shorttitle = {Sorting the file drawer},
	abstract = {A typology of unpublished studies is presented to describe various types of unpublished studies and the reasons for their nonpublication. Reasons for nonpublication are classified by whether they stem from an awareness of the study results (result-dependent reasons) or not (result-independent reasons) and whether the reasons affect the publication decisions of individual researchers or reviewers/editors. I argue that result-independent reasons for nonpublication are less likely to introduce motivated reasoning into the publication decision process than are result-dependent reasons. I also argue that some reasons for nonpublication would produce beneficial as opposed to problematic publication bias. The typology of unpublished studies provides a descriptive scheme that can facilitate understanding of the population of study results across the field of psychology, within subdisciplines of psychology, or within specific psychology research domains. The typology also offers insight into different publication biases and research-dissemination practices and can guide individual researchers in organizing their own file drawers of unpublished studies.},
	pages = {1745691620979831},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Lishner, David A.},
	urldate = {2021-08-25},
	date = {2021-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {publication bias, meta-analysis, methodology, toread, file drawer, unpublished studies},
	file = {Lishner - 2021 - Sorting the File Drawer A Typology for Describing.pdf:/Users/tom/Zotero/storage/RTLZ5PB7/Lishner - 2021 - Sorting the File Drawer A Typology for Describing.pdf:application/pdf},
}

@article{peels_replication_2021,
	title = {Replication and trustworthiness},
	volume = {0},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2021.1963708},
	doi = {10.1080/08989621.2021.1963708},
	abstract = {This paper explores various relations that exist between replication and trustworthiness. After defining “trust”, “trustworthiness”, “replicability”, “replication study”, and “successful replication”, we consider, respectively, how trustworthiness relates to each of the three main kinds of replication: reproductions, direct replications, and conceptual replications. Subsequently, we explore how trustworthiness relates to the intentionality of a replication. After that, we discuss whether the trustworthiness of research findings depends merely on evidential considerations or also on what is at stake. We conclude by adding replication to the other issues that should be considered in assessing the trustworthiness of research findings: (1) the likelihood of the findings before the primary study was done (that is, the prior probability of the findings), (2) the study size and the methodological quality of the primary study, (3) the number of replications that were performed and the quality and consistency of their aggregated findings, and (4) what is at stake.},
	pages = {1--11},
	number = {0},
	journaltitle = {Accountability in Research},
	author = {Peels, Rik and Bouter, Lex},
	urldate = {2021-08-25},
	date = {2021-08-04},
	pmid = {34346793},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2021.1963708},
	keywords = {replicability, reproducibility, Replication, trust, trustworthiness},
	file = {Full Text PDF:/Users/tom/Zotero/storage/P5RE9BXB/Peels and Bouter - 2021 - Replication and trustworthiness.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GTMJP93M/08989621.2021.html:text/html},
}

@article{hicks_open_2021,
	title = {Open science, the replication crisis, and environmental public health},
	volume = {0},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2021.1962713},
	doi = {10.1080/08989621.2021.1962713},
	abstract = {Concerns about a crisis of mass irreplicability across scientific fields (“the replication crisis”) have stimulated a movement for open science, encouraging or even requiring researchers to publish their raw data and analysis code. Recently, a rule at the {US} Environmental Protection Agency ({US} {EPA}) would have imposed a strong open data requirement. The rule prompted significant public discussion about whether open science practices are appropriate for fields of environmental public health. The aims of this paper are to assess (1) whether the replication crisis extends to fields of environmental public health; and (2) in general whether open science requirements can address the replication crisis. There is little empirical evidence for or against mass irreplicability in environmental public health specifically. Without such evidence, strong claims about whether the replication crisis extends to environmental public health – or not – seem premature. By distinguishing three concepts – reproducibility, replicability, and robustness – it is clear that open data initiatives can promote reproducibility and robustness but do little to promote replicability. I conclude by reviewing some of the other benefits of open science, and offer some suggestions for funding streams to mitigate the costs of adoption of open science practices in environmental public health.},
	pages = {1--29},
	number = {0},
	journaltitle = {Accountability in Research},
	author = {Hicks, Daniel J.},
	urldate = {2021-08-25},
	date = {2021-07-30},
	pmid = {34330172},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2021.1962713},
	keywords = {open science, environmental policy, environmental public health, Replication crisis},
	file = {Full Text PDF:/Users/tom/Zotero/storage/S4MPXZBP/Hicks - 2021 - Open science, the replication crisis, and environm.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/A7B5GX58/08989621.2021.html:text/html},
}

@article{bryan_behavioural_2021,
	title = {Behavioural science is unlikely to change the world without a heterogeneity revolution},
	volume = {5},
	rights = {2021 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01143-3},
	doi = {10.1038/s41562-021-01143-3},
	abstract = {In the past decade, behavioural science has gained influence in policymaking but suffered a crisis of confidence in the replicability of its findings. Here, we describe a nascent heterogeneity revolution that we believe these twin historical trends have triggered. This revolution will be defined by the recognition that most treatment effects are heterogeneous, so the variation in effect estimates across studies that defines the replication crisis is to be expected as long as heterogeneous effects are studied without a systematic approach to sampling and moderation. When studied systematically, heterogeneity can be leveraged to build more complete theories of causal mechanism that could inform nuanced and dependable guidance to policymakers. We recommend investment in shared research infrastructure to make it feasible to study behavioural interventions in heterogeneous and generalizable samples, and suggest low-cost steps researchers can take immediately to avoid being misled by heterogeneity and begin to learn from it instead.},
	pages = {980--989},
	number = {8},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Bryan, Christopher J. and Tipton, Elizabeth and Yeager, David S.},
	urldate = {2021-08-25},
	date = {2021-08},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Human behaviour;Policy;Research management;Science, technology and society
Subject\_term\_id: human-behaviour;policy;research-management;science-technology-and-society},
	keywords = {toread},
	file = {Bryan et al. - 2021 - Behavioural science is unlikely to change the worl.pdf:/Users/tom/Zotero/storage/EUNNMQQ8/Bryan et al. - 2021 - Behavioural science is unlikely to change the worl.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/R9EIK9HN/s41562-021-01143-3.html:text/html},
}

@report{heirene_preregistration_2021,
	title = {Preregistration specificity \& adherence: A review of preregistered gambling studies \& cross-disciplinary comparison},
	url = {https://psyarxiv.com/nj4es/},
	shorttitle = {Preregistration specificity \& adherence},
	abstract = {Study preregistration is one of several “open science” practices (e.g., open data, preprints) that researchers use to improve the transparency and rigour of their research. As more researchers adopt preregistration as a regular research practice, examining the nature and content of preregistrations can help identify strengths and weaknesses of current practices. The value of preregistration, in part, relates to the specificity of the study plan and the extent to which investigators adhere to this plan. We identified 53 preregistrations from the gambling studies field meeting our predefined eligibility criteria and scored their level of specificity using a 23-item protocol developed to measure the extent to which a clear and exhaustive preregistration plan restricts various researcher degrees of freedom ({RDoF}; i.e., the many methodological choices available to researchers when collecting and analysing data, and when reporting their findings). We also scored studies on a 32-item protocol that measured adherence to the preregistered plan in the study manuscript. We found that gambling preregistrations had low specificity levels on most {RDoF}. However, a comparison with a sample of cross-disciplinary preregistrations (N = 52; Bakker et al., 2020) indicated that gambling preregistrations scored higher on 12 (of 29) items. Thirteen (65\%) of the 20 associated published articles or preprints deviated from the protocol without declaring as much (the mean number of undeclared deviations per article was 2.25, {SD} = 2.34). Overall, while we found improvements in specificity and adherence over time (2017-2020), our findings suggest the purported benefits of preregistration—including increasing transparency and reducing {RDoF}—are not fully achieved by current practices. Using our findings, we provide 10 practical recommendations that can be used to support and refine preregistration practices.},
	institution = {{PsyArXiv}},
	author = {Heirene, Robert and {LaPlante}, Debi and Louderback, Eric R. and Keen, Brittany and Bakker, Marjan and Serafimovska, Anastasia and Gainsbury, Sally Melissa},
	urldate = {2021-08-25},
	date = {2021-07-16},
	doi = {10.31234/osf.io/nj4es},
	note = {type: article},
	keywords = {Meta-science, Clinical Psychology, Social and Behavioral Sciences, Open science, Preregistration, Gambling, toread, Addiction},
	file = {Full Text PDF:/Users/tom/Zotero/storage/BM4ZRIQN/Heirene et al. - 2021 - Preregistration specificity & adherence A review .pdf:application/pdf},
}

@report{dellsen_epistemic_2021,
	title = {An epistemic advantage of accommodation over prediction},
	url = {http://philsci-archive.pitt.edu/19298/},
	abstract = {Many philosophers have argued that a hypothesis is better confirmed by some data if the hypothesis was not specifically designed to fit the data. ‘Prediction’, they argue, is superior to ‘accommodation’. Others deny that there is any epistemic advantage to prediction, and conclude that prediction and accommodation are epistemically on a par. This paper argues that there is a respect in which accommodation is superior to prediction. Specifically, the information that the data was accommodated rather than predicted suggests that the data is less likely to have been manipulated or fabricated, which in turn increases the likelihood that the hypothesis is correct in light of the data. In some cases, this epistemic advantage of accommodation may even outweigh whatever epistemic advantage there might be to prediction, making accommodation epistemically superior to prediction all things considered.},
	type = {Preprint},
	author = {Dellsén, Finnur},
	urldate = {2021-08-25},
	date = {2021},
	langid = {english},
	note = {type: article},
	file = {Dellsén - 2021 - An Epistemic Advantage of Accommodation over Predi.pdf:/Users/tom/Zotero/storage/Y2TWZ55P/Dellsén - 2021 - An Epistemic Advantage of Accommodation over Predi.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/IDKFMFW4/19298.html:text/html},
}

@article{spybrook_study_2021,
	title = {Study registration for the field of prevention science: considering options and paths forward},
	issn = {1573-6695},
	url = {https://doi.org/10.1007/s11121-021-01290-z},
	doi = {10.1007/s11121-021-01290-z},
	shorttitle = {Study registration for the field of prevention science},
	abstract = {The practice of prospectively registering the details of intervention studies in a public database or registry is gaining momentum across disciplines as a strategy for increasing the transparency, credibility, and accessibility of study findings. In this article, we consider five registries that may be relevant for registration of intervention studies in the field of prevention science: {ClinicalTrials}.gov, the American Economic Association Registry of Randomized Controlled Trials ({AEA} {RCT} Registry), the Open Science Framework Preregistration ({OSF} Preregistration), the Registry for International Development Impact Evaluations ({RIDIE}), and the Registry of Efficacy and Effectiveness Studies ({REES}). We examine the five registries in terms of substantive focus, study designs, and contents of registry entries. We consider two paths forward for prospective registration of intervention studies in the field of prevention science: Path A: register all studies in {ClinicalTrials}.gov and Path B: allow individual researchers to select the registry with the “best fit.” Lastly, we consider how the field might begin to establish norms around registration.},
	journaltitle = {Prevention Science},
	shortjournal = {Prev Sci},
	author = {Spybrook, Jessaca and Maynard, Rebecca and Anderson, Dustin},
	urldate = {2021-08-26},
	date = {2021-08-12},
	langid = {english},
	file = {Spybrook et al. - 2021 - Study registration for the field of prevention sci.pdf:/Users/tom/Zotero/storage/AB5ISDTA/Spybrook et al. - 2021 - Study registration for the field of prevention sci.pdf:application/pdf},
}

@article{toczydlowski_poor_2021,
	title = {Poor data stewardship will hinder global genetic diversity surveillance},
	volume = {118},
	rights = {Copyright © 2021 the Author(s). Published by {PNAS}.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 ({CC} {BY}).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/118/34/e2107934118},
	doi = {10.1073/pnas.2107934118},
	abstract = {Genomic data are being produced and archived at a prodigious rate, and current studies could become historical baselines for future global genetic diversity analyses and monitoring programs. However, when we evaluated the potential utility of genomic data from wild and domesticated eukaryote species in the world’s largest genomic data repository, we found that most archived genomic datasets (86\%) lacked the spatiotemporal metadata necessary for genetic biodiversity surveillance. Labor-intensive scouring of a subset of published papers yielded geospatial coordinates and collection years for only 33\% (39\% if place names were considered) of these genomic datasets. Streamlined data input processes, updated metadata deposition policies, and enhanced scientific community awareness are urgently needed to preserve these irreplaceable records of today’s genetic biodiversity and to plug the growing metadata gap.},
	number = {34},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Toczydlowski, Rachel H. and Liggins, Libby and Gaither, Michelle R. and Anderson, Tanner J. and Barton, Randi L. and Berg, Justin T. and Beskid, Sofia G. and Davis, Beth and Delgado, Alonso and Farrell, Emily and Ghoojaei, Maryam and Himmelsbach, Nan and Holmes, Ann E. and Queeno, Samantha R. and Trinh, Thienthanh and Weyand, Courtney A. and Bradburd, Gideon S. and Riginos, Cynthia and Toonen, Robert J. and Crandall, Eric D.},
	urldate = {2021-08-26},
	date = {2021-08-24},
	langid = {english},
	pmid = {34404731},
	note = {{ISBN}: 9782107934112
Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {metadata, biodiversity, conservation, genomic, management},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ABZJKMVC/Toczydlowski et al. - 2021 - Poor data stewardship will hinder global genetic d.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KJ3VYC64/e2107934118.html:text/html},
}

@report{schulz_devil_2021,
	title = {The devil is in the details: Reporting and transparent research practices in sports medicine and orthopedic clinical trials},
	rights = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2021.07.20.21260565v1},
	shorttitle = {The devil is in the details},
	abstract = {Introduction While transparent reporting of clinical trials is essential to assess the risk of bias and translate research findings into clinical practice, earlier studies have shown that deficiencies are common. This study examined current clinical trial reporting and transparent research practices in sports medicine and orthopedics.
Methods The sample included clinical trials published in the top 25\% of sports medicine and orthopedics journals over eight months. Two independent reviewers assessed pre-registration, open data and criteria related to scientific rigor, the study sample, and data analysis.
Results The sample included 163 clinical trials from 27 journals. While the majority of trials mentioned rigor criteria, essential details were often missing. Sixty percent (confidence interval [{CI}] 53-68\%) of trials reported sample size calculations, but only 32\% ({CI} 25-39\%) justified the expected effect size. Few trials indicated the blinding status of all main stakeholders (4\%; {CI} 1-7\%). Only 18\% ({CI} 12-24\%) included information on randomization type, method, and concealed allocation. Most trials reported participants’ sex/gender (95\%; {CI} 92-98\%) and information on inclusion and exclusion criteria (78\%; {CI} 72-84\%). Only 20\% ({CI} 14-26\%) of trials were pre-registered. No trials deposited data in open repositories.
Conclusions These results will aid the sports medicine and orthopedics community in developing tailored interventions to improve reporting. While authors typically mention blinding, randomization and other factors, essential details are often missing. Greater acceptance of open science practices, like pre-registration and open data, is needed. These practices have been widely encouraged, we discuss systemic interventions that may improve clinical trial reporting.
Registration https://doi.org/10.17605/{OSF}.{IO}/9648H},
	pages = {2021.07.20.21260565},
	author = {Schulz, Robert and Langen, Georg and Prill, Robert and Cassel, Michael and Weissgerber, Tracey},
	urldate = {2021-08-26},
	date = {2021-07-23},
	langid = {english},
	doi = {10.1101/2021.07.20.21260565},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Type: article},
	file = {Full Text PDF:/Users/tom/Zotero/storage/H4AECLH7/Schulz et al. - 2021 - The devil is in the details Reporting and transpa.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9FTM8S3J/2021.07.20.html:text/html},
}

@report{nust_codecheck_2021,
	title = {{CODECHECK}: an Open Science initiative for the independent\&nbsp;execution of computations underlying research articles during peer review to improve reproducibility},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	url = {https://f1000research.com/articles/10-253},
	shorttitle = {{CODECHECK}},
	abstract = {The traditional scientific paper falls short of effectively communicating computational research. \&nbsp;To help improve this situation, we propose a system by which the computational workflows underlying research articles are checked. The {CODECHECK} system uses open infrastructure and tools and can be integrated into review and publication processes in multiple ways. We describe these integrations along multiple dimensions (importance, who, openness, when). In collaboration with academic publishers and conferences, we demonstrate {CODECHECK} with 25 reproductions of diverse scientific publications. These {CODECHECKs} show that asking for reproducible workflows during a collaborative review can effectively improve executability. While {CODECHECK} has clear limitations, it may represent a building block in Open Science and publishing ecosystems for improving the reproducibility, appreciation, and, potentially, the quality of non-textual research artefacts. The {CODECHECK} website can be accessed here: https://codecheck.org.uk/.},
	number = {10:253},
	institution = {F1000Research},
	author = {Nüst, Daniel and Eglen, Stephen J.},
	urldate = {2021-08-26},
	date = {2021-07-20},
	langid = {english},
	doi = {10.12688/f1000research.51738.2},
	note = {Type: article},
	keywords = {reproducibility, peer review, data sharing, Open Science, quality control, code sharing, reproducible research, scholarly publishing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NNKBJ4J7/Nüst and Eglen - 2021 - CODECHECK an Open Science initiative for the inde.pdf:application/pdf},
}

@article{buxton_avoiding_2021-1,
	title = {Avoiding wasted research resources in conservation science},
	volume = {3},
	issn = {2578-4854},
	url = {https://conbio.onlinelibrary.wiley.com/doi/abs/10.1111/csp2.329},
	doi = {10.1111/csp2.329},
	abstract = {Scientific evidence is fundamental for guiding effective conservation action to curb biodiversity loss. Yet, research resources in conservation are often wasted due to biased allocation of research effort, irrelevant or low-priority questions, flawed studies, inaccessible research outputs, and biased or poor-quality reporting. We outline a striking example of wasted research resources, highlight a powerful case of data rescue/reuse, and discuss an exemplary model of evidence-informed conservation. We suggest that funding agencies, research institutions, {NGOs}, publishers, and researchers are part of the problem and solutions, and outline recommendations to curb the waste of research resources, including knowledge co-creation and open science practices.},
	pages = {e329},
	number = {2},
	journaltitle = {Conservation Science and Practice},
	author = {Buxton, Rachel T. and Nyboer, Elizabeth A. and Pigeon, Karine E. and Raby, Graham D. and Rytwinski, Trina and Gallagher, Austin J. and Schuster, Richard and Lin, Hsien-Yung and Fahrig, Lenore and Bennett, Joseph R. and Cooke, Steven J. and Roche, Dominique G.},
	urldate = {2021-08-26},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://conbio.onlinelibrary.wiley.com/doi/pdf/10.1111/csp2.329},
	keywords = {open science, research data management, toread, co-production, data rescue and reuse, evidence-informed decision making, {FAIR} data},
	file = {Full Text PDF:/Users/tom/Zotero/storage/EJB3Q6EN/Buxton et al. - 2021 - Avoiding wasted research resources in conservation.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JNDX6CPG/csp2.html:text/html},
}

@article{howick_overly_2021,
	title = {Do overly complex reporting guidelines remove the focus from good clinical trials?},
	volume = {374},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/374/bmj.n1793},
	doi = {10.1136/bmj.n1793},
	abstract = {{\textless}p{\textgreater}\textit{The ever increasing emphasis on complex reporting guidelines is getting in the way of designing and conducting good clinical trials}, \textit{say}\textbf{\textit{Jeremy Howick}}\textit{,}\textbf{\textit{Rebecca Webster}}, \textit{and}\textbf{\textit{J André Knottnerus}}\textit{. But}\textbf{\textit{David Moher}}\textit{argues that, while following the guidelines can be frustrating, such complexity remains necessary and is improving research, not impeding it}{\textless}/p{\textgreater}},
	pages = {n1793},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Howick, Jeremy and Webster, Rebecca and Knottnerus, J. André and Moher, David},
	urldate = {2021-08-26},
	date = {2021-08-16},
	langid = {english},
	pmid = {34400403},
	note = {Publisher: British Medical Journal Publishing Group
Section: Head to Head},
	file = {Full Text PDF:/Users/tom/Zotero/storage/6VPQB5SW/Howick et al. - 2021 - Do overly complex reporting guidelines remove the .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/T8JDKL2R/bmj.html:text/html},
}

@article{greenland_invited_2017,
	title = {Invited commentary: the need for cognitive science in methodology},
	volume = {186},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwx259},
	doi = {10.1093/aje/kwx259},
	shorttitle = {Invited commentary},
	abstract = {There is no complete solution for the problem of abuse of statistics, but methodological training needs to cover cognitive biases and other psychosocial factors affecting inferences. The present paper discusses 3 common cognitive distortions: 1) dichotomania, the compulsion to perceive quantities as dichotomous even when dichotomization is unnecessary and misleading, as in inferences based on whether a P value is “statistically significant”; 2) nullism, the tendency to privilege the hypothesis of no difference or no effect when there is no scientific basis for doing so, as when testing only the null hypothesis; and 3) statistical reification, treating hypothetical data distributions and statistical models as if they reflect known physical laws rather than speculative assumptions for thought experiments. As commonly misused, null-hypothesis significance testing combines these cognitive problems to produce highly distorted interpretation and reporting of study results. Interval estimation has so far proven to be an inadequate solution because it involves dichotomization, an avenue for nullism. Sensitivity and bias analyses have been proposed to address reproducibility problems (Am J Epidemiol. 2017;186(6):646–647); these methods can indeed address reification, but they can also introduce new distortions via misleading specifications for bias parameters. P values can be reframed to lessen distortions by presenting them without reference to a cutoff, providing them for relevant alternatives to the null, and recognizing their dependence on all assumptions used in their computation; they nonetheless require rescaling for measuring evidence. I conclude that methodological development and training should go beyond coverage of mechanistic biases (e.g., confounding, selection bias, measurement error) to cover distortions of conclusions produced by statistical methods and psychosocial forces.},
	pages = {639--645},
	number = {6},
	journaltitle = {American Journal of Epidemiology},
	shortjournal = {American Journal of Epidemiology},
	author = {Greenland, Sander},
	urldate = {2021-08-26},
	date = {2017-09-15},
	file = {Full Text PDF:/Users/tom/Zotero/storage/STWF8ZCJ/Greenland - 2017 - Invited Commentary The Need for Cognitive Science.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AS9HS4D9/3886035.html:text/html},
}

@article{hrynaszkiewicz_developing_2020,
	title = {Developing a research data policy framework for all journals and publishers},
	volume = {19},
	rights = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {1683-1470},
	url = {http://datascience.codata.org/article/10.5334/dsj-2020-005/},
	doi = {10.5334/dsj-2020-005},
	abstract = {Article: Developing a Research Data Policy Framework for All Journals and Publishers},
	pages = {5},
	number = {1},
	journaltitle = {Data Science Journal},
	author = {Hrynaszkiewicz, Iain and Simons, Natasha and Hussain, Azhar and Grant, Rebecca and Goudie, Simon},
	urldate = {2021-08-26},
	date = {2020-02-21},
	langid = {english},
	note = {Number: 1
Publisher: Ubiquity Press},
	file = {Full Text PDF:/Users/tom/Zotero/storage/CWPPN95G/Hrynaszkiewicz et al. - 2020 - Developing a Research Data Policy Framework for Al.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/H33QS5V5/dsj-2020-005.html:text/html},
}

@article{mayo-wilson_evaluating_2021,
	title = {Evaluating implementation of the Transparency and Openness Promotion ({TOP}) guidelines: the {TRUST} process for rating journal policies, procedures, and practices},
	volume = {6},
	issn = {2058-8615},
	url = {https://doi.org/10.1186/s41073-021-00112-8},
	doi = {10.1186/s41073-021-00112-8},
	shorttitle = {Evaluating implementation of the Transparency and Openness Promotion ({TOP}) guidelines},
	abstract = {The Transparency and Openness Promotion ({TOP}) Guidelines describe modular standards that journals can adopt to promote open science. The {TOP} Factor is a metric to describe the extent to which journals have adopted the {TOP} Guidelines in their policies. Systematic methods and rating instruments are needed to calculate the {TOP} Factor. Moreover, implementation of these open science policies depends on journal procedures and practices, for which {TOP} provides no standards or rating instruments.},
	pages = {9},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Research Integrity and Peer Review},
	author = {Mayo-Wilson, Evan and Grant, Sean and Supplee, Lauren and Kianersi, Sina and Amin, Afsah and {DeHaven}, Alex and Mellor, David},
	urldate = {2021-08-26},
	date = {2021-06-02},
	keywords = {Open science, Reproducibility, Research transparency, toread, {TOP} factor, {TOP} guidelines},
	file = {Full Text PDF:/Users/tom/Zotero/storage/X7H53V4W/Mayo-Wilson et al. - 2021 - Evaluating implementation of the Transparency and .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/2SKNPEWU/s41073-021-00112-8.html:text/html},
}

@article{dijkers_beginners_2019,
	title = {A beginner’s guide to data stewardship and data sharing},
	volume = {57},
	rights = {2019 International Spinal Cord Society},
	issn = {1476-5624},
	url = {https://www.nature.com/articles/s41393-018-0232-6},
	doi = {10.1038/s41393-018-0232-6},
	abstract = {A narrative review of principles, benefits and disadvantages, as well as methods of research data sharing.},
	pages = {169--182},
	number = {3},
	journaltitle = {Spinal Cord},
	author = {Dijkers, Marcel P.},
	urldate = {2021-08-26},
	date = {2019-03},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 3
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Medical research;Outcomes research
Subject\_term\_id: medical-research;outcomes-research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/4EUCK5IS/Dijkers - 2019 - A beginner’s guide to data stewardship and data sh.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ARUQNUNB/s41393-018-0232-6.html:text/html},
}

@article{kennedy_evidence_2019,
	title = {The Evidence Project risk of bias tool: assessing study rigor for both randomized and non-randomized intervention studies},
	volume = {8},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-018-0925-0},
	doi = {10.1186/s13643-018-0925-0},
	shorttitle = {The Evidence Project risk of bias tool},
	abstract = {Different tools exist for assessing risk of bias of intervention studies for systematic reviews. We present a tool for assessing risk of bias across both randomized and non-randomized study designs. The tool was developed by the Evidence Project, which conducts systematic reviews and meta-analyses of behavioral interventions for {HIV} in low- and middle-income countries.},
	pages = {3},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Systematic Reviews},
	author = {Kennedy, Caitlin E. and Fonner, Virginia A. and Armstrong, Kevin A. and Denison, Julie A. and Yeh, Ping Teresa and O’Reilly, Kevin R. and Sweat, Michael D.},
	urldate = {2021-08-26},
	date = {2019-01-03},
	keywords = {Critical appraisal, Quality assessment, Rigor assessment, Rigor score, Risk of bias, Study quality, Study rigor},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PFT8BHEY/Kennedy et al. - 2019 - The Evidence Project risk of bias tool assessing .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DB397ECA/s13643-018-0925-0.html:text/html},
}

@article{james_methodology_2016,
	title = {A methodology for systematic mapping in environmental sciences},
	volume = {5},
	issn = {2047-2382},
	url = {https://doi.org/10.1186/s13750-016-0059-6},
	doi = {10.1186/s13750-016-0059-6},
	abstract = {Systematic mapping was developed in social sciences in response to a lack of empirical data when answering questions using systematic review methods, and a need for a method to describe the literature across a broad subject of interest. Systematic mapping does not attempt to answer a specific question as do systematic reviews, but instead collates, describes and catalogues available evidence (e.g. primary, secondary, theoretical, economic) relating to a topic or question of interest. The included studies can be used to identify evidence for policy-relevant questions, knowledge gaps (to help direct future primary research) and knowledge clusters (sub-sets of evidence that may be suitable for secondary research, for example systematic review). Evidence synthesis in environmental sciences faces similar challenges to those found in social sciences. Here we describe the translation of systematic mapping methodology from social sciences for use in environmental sciences. We provide the first process-based methodology for systematic maps, describing the stages involved: establishing the review team and engaging stakeholders; setting the scope and question; setting inclusion criteria for studies; scoping stage; protocol development and publication; searching for evidence; screening evidence; coding; production of a systematic map database; critical appraisal (optional); describing and visualising the findings; report production and supporting information. We discuss the similarities and differences in methodology between systematic review and systematic mapping and provide guidance for those choosing which type of synthesis is most suitable for their requirements. Furthermore, we discuss the merits and uses of systematic mapping and make recommendations for improving this evolving methodology in environmental sciences.},
	pages = {7},
	number = {1},
	journaltitle = {Environmental Evidence},
	shortjournal = {Environmental Evidence},
	author = {James, Katy L. and Randall, Nicola P. and Haddaway, Neal R.},
	urldate = {2021-08-26},
	date = {2016-04-26},
	keywords = {Evidence review, Evidence-based environmental management, Knowledge clusters, Knowledge gaps, Systematic evidence synthesis, Systematic mapping},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7HF35SRV/James et al. - 2016 - A methodology for systematic mapping in environmen.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KJAB2TVI/s13750-016-0059-6.html:text/html},
}

@article{mckinnon_sustainability_2015,
	title = {Sustainability: Map the evidence},
	volume = {528},
	rights = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/528185a},
	doi = {10.1038/528185a},
	shorttitle = {Sustainability},
	abstract = {Too many studies go unread. Collate them to enable synthesis and guide decision-making in sustainability, urge Madeleine C. {McKinnon} and colleagues.},
	pages = {185--187},
	number = {7581},
	journaltitle = {Nature},
	author = {{McKinnon}, Madeleine C. and Cheng, Samantha H. and Garside, Ruth and Masuda, Yuta J. and Miller, Daniel C.},
	urldate = {2021-08-26},
	date = {2015-12},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7581
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Conservation biology;Developing world;Policy
Subject\_term\_id: conservation;developing-world;policy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3J4942DR/McKinnon et al. - 2015 - Sustainability Map the evidence.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/EFPVYNXF/528185a.html:text/html},
}

@article{grant_typology_2009,
	title = {A typology of reviews: an analysis of 14 review types and associated methodologies},
	volume = {26},
	issn = {1471-1842},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1471-1842.2009.00848.x},
	doi = {10.1111/j.1471-1842.2009.00848.x},
	shorttitle = {A typology of reviews},
	abstract = {Background and objectives: The expansion of evidence-based practice across sectors has lead to an increasing variety of review types. However, the diversity of terminology used means that the full potential of these review types may be lost amongst a confusion of indistinct and misapplied terms. The objective of this study is to provide descriptive insight into the most common types of reviews, with illustrative examples from health and health information domains. Methods: Following scoping searches, an examination was made of the vocabulary associated with the literature of review and synthesis (literary warrant). A simple analytical framework—Search, {AppraisaL}, Synthesis and Analysis ({SALSA})—was used to examine the main review types. Results: Fourteen review types and associated methodologies were analysed against the {SALSA} framework, illustrating the inputs and processes of each review type. A description of the key characteristics is given, together with perceived strengths and weaknesses. A limited number of review types are currently utilized within the health information domain. Conclusions: Few review types possess prescribed and explicit methodologies and many fall short of being mutually exclusive. Notwithstanding such limitations, this typology provides a valuable reference point for those commissioning, conducting, supporting or interpreting reviews, both within health information and the wider health care domain.},
	pages = {91--108},
	number = {2},
	journaltitle = {Health Information \& Libraries Journal},
	author = {Grant, Maria J. and Booth, Andrew},
	urldate = {2021-08-26},
	date = {2009},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1471-1842.2009.00848.x},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XYICTYB5/Grant and Booth - 2009 - A typology of reviews an analysis of 14 review ty.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BCB3L6IT/j.1471-1842.2009.00848.html:text/html},
}

@article{campbell_synthesis_2020,
	title = {Synthesis without meta-analysis ({SWiM}) in systematic reviews: reporting guideline},
	volume = {368},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution ({CC} {BY} 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/368/bmj.l6890},
	doi = {10.1136/bmj.l6890},
	shorttitle = {Synthesis without meta-analysis ({SWiM}) in systematic reviews},
	abstract = {{\textless}p{\textgreater}In systematic reviews that lack data amenable to meta-analysis, alternative synthesis methods are commonly used, but these methods are rarely reported. This lack of transparency in the methods can cast doubt on the validity of the review findings. The Synthesis Without Meta-analysis ({SWiM}) guideline has been developed to guide clear reporting in reviews of interventions in which alternative synthesis methods to meta-analysis of effect estimates are used. This article describes the development of the {SWiM} guideline for the synthesis of quantitative data of intervention effects and presents the nine {SWiM} reporting items with accompanying explanations and examples.{\textless}/p{\textgreater}},
	pages = {l6890},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Campbell, Mhairi and {McKenzie}, Joanne E. and Sowden, Amanda and Katikireddi, Srinivasa Vittal and Brennan, Sue E. and Ellis, Simon and Hartmann-Boyce, Jamie and Ryan, Rebecca and Shepperd, Sasha and Thomas, James and Welch, Vivian and Thomson, Hilary},
	urldate = {2021-08-26},
	date = {2020-01-16},
	langid = {english},
	pmid = {31948937},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/YJ38UBMJ/Campbell et al. - 2020 - Synthesis without meta-analysis (SWiM) in systemat.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/D7SLGCJ7/bmj.html:text/html},
}

@article{murad_guidelines_2017,
	title = {Guidelines for reporting meta-epidemiological methodology research},
	volume = {22},
	rights = {© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2017. All rights reserved. No commercial use is permitted unless otherwise expressly granted.. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/},
	issn = {2515-446X, 2515-4478},
	url = {https://ebm.bmj.com/content/22/4/139},
	doi = {10.1136/ebmed-2017-110713},
	abstract = {{\textless}p{\textgreater}Published research should be reported to evidence users with clarity and transparency that facilitate optimal appraisal and use of evidence and allow replication by other researchers. Guidelines for such reporting are available for several types of studies but not for meta-epidemiological methodology studies. Meta-epidemiological studies adopt a systematic review or meta-analysis approach to examine the impact of certain characteristics of clinical studies on the observed effect and provide empirical evidence for hypothesised associations. The unit of analysis in meta-epidemiological studies is a study, not a patient. The outcomes of meta-epidemiological studies are usually not clinical outcomes. In this guideline, we adapt items from the Preferred Reporting Items for Systematic Reviews and Meta-analyses ({PRISMA}) to fit the context of meta-epidemiological studies.{\textless}/p{\textgreater}},
	pages = {139--142},
	number = {4},
	journaltitle = {{BMJ} Evidence-Based Medicine},
	author = {Murad, Mohammad Hassan and Wang, Zhen},
	urldate = {2021-08-26},
	date = {2017-08-01},
	langid = {english},
	pmid = {28701372},
	note = {Publisher: Royal Society of Medicine
Section: {EBM} Primer},
	keywords = {epidemiology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ZJIGJ6GS/Murad and Wang - 2017 - Guidelines for reporting meta-epidemiological meth.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9UNMJZJQ/139.html:text/html},
}

@article{nasserie_assessment_2021,
	title = {Assessment of the frequency and variety of persistent symptoms among patients with covid-19: a systematic review},
	volume = {4},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2021.11417},
	doi = {10.1001/jamanetworkopen.2021.11417},
	shorttitle = {Assessment of the frequency and variety of persistent symptoms among patients with covid-19},
	abstract = {Infection with {COVID}-19 has been associated with long-term symptoms, but the frequency, variety, and severity of these complications are not well understood. Many published commentaries have proposed plans for pandemic control that are primarily based on mortality rates among older individuals without considering long-term morbidity among individuals of all ages. Reliable estimates of such morbidity are important for patient care, prognosis, and development of public health policy.To conduct a systematic review of studies examining the frequency and variety of persistent symptoms after {COVID}-19 infection.A search of {PubMed} and Web of Science was conducted to identify studies published from January 1, 2020, to March 11, 2021, that examined persistent symptoms after {COVID}-19 infection. Persistent symptoms were defined as those persisting for at least 60 days after diagnosis, symptom onset, or hospitalization or at least 30 days after recovery from the acute illness or hospital discharge. Search terms included {COVID}-19, {SARS}-{CoV}-2, coronavirus, 2019-{nCoV}, long-term, after recovery, long-haul, persistent, outcome, symptom, follow-up, and longitudinal. All English-language articles that presented primary data from cohort studies that reported the prevalence of persistent symptoms among individuals with {SARS}-{CoV}-2 infection and that had clearly defined and sufficient follow-up were included. Case reports, case series, and studies that described symptoms only at the time of infection and/or hospitalization were excluded. A structured framework was applied to appraise study quality.A total of 1974 records were identified; of those, 1247 article titles and abstracts were screened. After removal of duplicates and exclusions, 92 full-text articles were assessed for eligibility; 47 studies were deemed eligible, and 45 studies reporting 84 clinical signs or symptoms were included in the systematic review. Of 9751 total participants, 5266 (54.0\%) were male; 30 of 45 studies reported mean or median ages younger than 60 years. Among 16 studies, most of which comprised participants who were previously hospitalized, the median proportion of individuals experiencing at least 1 persistent symptom was 72.5\% (interquartile range [{IQR}], 55.0\%-80.0\%). Individual symptoms occurring most frequently included shortness of breath or dyspnea (26 studies; median frequency, 36.0\%; {IQR}, 27.6\%-50.0\%), fatigue or exhaustion (25 studies; median frequency, 40.0\%; {IQR}, 31.0\%-57.0\%), and sleep disorders or insomnia (8 studies; median 29.4\%, {IQR}, 24.4\%-33.0\%). There were wide variations in the design and quality of the studies, which had implications for interpretation and often limited direct comparability and combinability. Major design differences included patient populations, definitions of time zero (ie, the beginning of the follow-up interval), follow-up lengths, and outcome definitions, including definitions of illness severity.This systematic review found that {COVID}-19 symptoms commonly persisted beyond the acute phase of infection, with implications for health-associated functioning and quality of life. Current studies of symptom persistence are highly heterogeneous, and future studies need longer follow-up, improved quality, and more standardized designs to reliably quantify risks.},
	pages = {e2111417--e2111417},
	number = {5},
	journaltitle = {{JAMA} Network Open},
	shortjournal = {{JAMA} Network Open},
	author = {Nasserie, Tahmina and Hittle, Michael and Goodman, Steven N.},
	urldate = {2021-08-26},
	date = {2021-05-26},
	keywords = {toread},
	file = {Full Text:/Users/tom/Zotero/storage/59M2UQ2B/Nasserie et al. - 2021 - Assessment of the Frequency and Variety of Persist.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/93FFY7I5/2780376.html:text/html},
}

@article{zon_uniform_2021,
	title = {A uniform format for manuscript submission},
	volume = {184},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(21)00073-8},
	doi = {10.1016/j.cell.2021.01.030},
	abstract = {{\textless}p{\textgreater}Many scientists spend unnecessary time reformatting papers to submit them to different journals. We propose a uniform submission format that we hope journals will include in their options for submission. Widespread adoption of this uniform submission format could shorten the submission and publishing process, freeing up time for research.{\textless}/p{\textgreater}},
	pages = {1654--1656},
	number = {7},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Zon, Leonard I. and Boisvert, Jason D. and Moreau, Hadley and Chan, Iris and Weiss, Jodi and Barbano, Julia and Smith, Mackenzie and Weber, Margaret and Prasad, Meera and Stanhope, Meredith and Freeman, Rebecca and Modhurima, Rodsy and Freyer, Shannon and {McConnell}, Alicia and Choudhuri, Avik and Bornhorst, Dorothee and Hagedorn, Elliott and Ablain, Julien and Rossmann, Marlies and Fazio, Maurizio and Fairchild, Michael and Sporrij, Audrey and Avagyan, Serine},
	urldate = {2021-08-26},
	date = {2021-04-01},
	pmid = {33798436},
	note = {Publisher: Elsevier},
	file = {Full Text PDF:/Users/tom/Zotero/storage/G3FS9E84/Zon et al. - 2021 - A uniform format for manuscript submission.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N4SESW87/S0092-8674(21)00073-8.html:text/html},
}

@article{rodgers_psychologys_2018,
	title = {Psychology’s replication crisis as scientific opportunity: a précis for policymakers},
	volume = {5},
	issn = {2372-7322},
	url = {https://doi.org/10.1177/2372732217749254},
	doi = {10.1177/2372732217749254},
	shorttitle = {Psychology’s replication crisis as scientific opportunity},
	abstract = {Psychological science is in the midst of what has been referred to as a “replication crisis.” The realization that many individual findings do not replicate in new studies has led to questioning the scientific method and the integrity of psychological science. We review the history of the replication crisis, and its positive and negative effects. Most of the elements of the replication crisis are re-emergent issues that methodologists have studied in the past, but to which researchers have become increasingly sensitized. Ultimately, we argue the value of the replication crisis, in that it has led to positive self-examination within our science and to the development of new and innovative methodology. The field is emerging from the replication crisis with a realization of the importance of multiple replication efforts, and an improved ethic of openness and transparency in the conduct of research.},
	pages = {134--141},
	number = {1},
	journaltitle = {Policy Insights from the Behavioral and Brain Sciences},
	shortjournal = {Policy Insights from the Behavioral and Brain Sciences},
	author = {Rodgers, Joseph Lee and Shrout, Patrick E.},
	urldate = {2021-08-26},
	date = {2018-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
	keywords = {Bayesian methods, replication crisis, meta-analysis, methodology, toread, {NHST}, policymakers},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/9D3EC6VG/Rodgers and Shrout - 2018 - Psychology’s Replication Crisis as Scientific Oppo.pdf:application/pdf},
}

@article{fife_graph_2021,
	title = {A graph for every analysis: Mapping visuals onto common analyses using flexplot},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-020-01520-2},
	doi = {10.3758/s13428-020-01520-2},
	shorttitle = {A graph for every analysis},
	abstract = {For decades, statisticians and methodologists have insisted researchers utilize graphical analysis much more heavily. Despite cogent and passionate recommendations, there has been no graphical revolution. Instead, researchers rely heavily on misleading graphics that violate visual processing heuristics. Perhaps the main reason for the persistence of deceptive graphics is software; most software familiar to psychological researchers suffer from poor defaults and limited capabilities. Also, visualization is ancillary to statistical analysis, providing an incentive to not produce graphics at all. In this paper, we argue that every statistical analysis must have an accompanying graphic, and we introduce the point-and-click software Flexplot, available both in {JASP} and Jamovi. We then present the theoretical framework that guides Flexplot, as well as show how to perform the most common statistical analyses in psychological literature.},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Fife, Dustin A. and Longo, Gabrielle and Correll, Michael and Tremoulet, Patrice D.},
	urldate = {2021-08-26},
	date = {2021-02-25},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/CV7GQJP7/Fife et al. - 2021 - A graph for every analysis Mapping visuals onto c.pdf:application/pdf},
}

@article{fife_eight_2020,
	title = {The eight steps of data analysis: a graphical framework to promote sound statistical analysis},
	volume = {15},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620917333},
	doi = {10.1177/1745691620917333},
	shorttitle = {The eight steps of data analysis},
	abstract = {Data analysis is a risky endeavor, particularly among people who are unaware of its dangers. According to some researchers, ?statistical conclusions validity? threatens all research subjected to the dark arts of statistical magic. Although traditional statistics classes may advise against certain practices (e.g., multiple comparisons, small sample sizes, violating normality), they may fail to cover others (e.g., outlier detection and violating linearity). More common, perhaps, is that researchers may fail to remember them. In this article, rather than rehashing old warnings and diatribes against this practice or that, I instead advocate a general statistical-analysis strategy. This graphic-based eight-step strategy promises to resolve the majority of statistical traps researchers may fall into?without having to remember large lists of problematic statistical practices. These steps will assist in preventing both false positives and false negatives and yield critical insights about the data that would have otherwise been missed. I conclude with an applied example that shows how the eight steps reveal interesting insights that would not be detected with standard statistical practices.},
	pages = {1054--1075},
	number = {4},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Fife, Dustin},
	urldate = {2021-08-26},
	date = {2020-07-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/6FFPQNH8/Fife - 2020 - The Eight Steps of Data Analysis A Graphical Fram.pdf:application/pdf},
}

@article{khalil_conducting_2021,
	title = {Conducting high quality scoping reviews-challenges and solutions},
	volume = {130},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435620311483},
	doi = {10.1016/j.jclinepi.2020.10.009},
	abstract = {Objectives
Scoping reviews are being increasingly used by researchers. The objective of this article was to outline some challenges and potential solutions to improve the conduct and reporting of scoping reviews.
Study Design and Setting
The {JBI} scoping review methodology group consists of 9 experts in the field of scoping reviews. This article summarizes the key issues facing reviewers who conduct scoping reviews and those who use the results from scoping reviews and may engage in consultations during their development.
Results
Several key issues have been identified for reviewers as challenges in conducting scoping reviews. Challenges may be faced throughout the conduct of the review, from developing the a priori protocol to finalizing the review report for publication and developing implications or recommendations for research, policy, and practice from the results of the review. Challenges to publishing scoping reviews may stem from a lack of understanding of scoping reviews by journal editors, authors, and peer reviewers to extending the conclusion drawn from these reviews to generate recommendations for practice and policy.
Conclusion
By identifying and overcoming challenges to the conduct and reporting of scoping reviews, reviewers may better ensure that scoping reviews are effective in meeting the objectives of scoping reviews.},
	pages = {156--160},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Khalil, Hanan and Peters, Micah {DJ}. and Tricco, Andrea C. and Pollock, Danielle and Alexander, Lyndsay and {McInerney}, Patricia and Godfrey, Christina M. and Munn, Zachary},
	urldate = {2021-08-26},
	date = {2021-02-01},
	langid = {english},
	keywords = {Reporting, Methodology, Quality, Scoping reviews},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/QY7FB8VG/Khalil et al. - 2021 - Conducting high quality scoping reviews-challenges.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/ZSAP24GA/S0895435620311483.html:text/html},
}

@article{ulrich_questionable_2020,
	title = {Questionable research practices may have little effect on replicability},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.58237},
	doi = {10.7554/eLife.58237},
	abstract = {This article examines why many studies fail to replicate statistically significant published results. We address this issue within a general statistical framework that also allows us to include various questionable research practices ({QRPs}) that are thought to reduce replicability. The analyses indicate that the base rate of true effects is the major factor that determines the replication rate of scientific results. Specifically, for purely statistical reasons, replicability is low in research domains where true effects are rare (e.g., search for effective drugs in pharmacology). This point is under-appreciated in current scientific and media discussions of replicability, which often attribute poor replicability mainly to {QRPs}.},
	pages = {e58237},
	journaltitle = {{eLife}},
	author = {Ulrich, Rolf and Miller, Jeff},
	editor = {Rodgers, Peter and Thompson, William Hedley and Francis, Gregory},
	urldate = {2021-08-26},
	date = {2020-09-15},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {p-hacking, replicability, false positives, meta-research, base rate of true effects, mathematical modelling of research process},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7HYMHM9L/Ulrich and Miller - 2020 - Questionable research practices may have little ef.pdf:application/pdf},
}

@incollection{rosnow_ethical_2011,
	location = {New York, {NY}, {US}},
	title = {Ethical principles in data analysis: An overview},
	isbn = {978-1-84872-855-4 978-1-84872-854-7},
	series = {Multivariate applications series},
	shorttitle = {Ethical principles in data analysis},
	abstract = {This chapter is intended to serve as a conceptual and historical backdrop to the discussions of particular ethical issues and quantitative methods in the chapters that follow in this Handbook. Before we focus more specifically on modern-day events that fired up concerns about ethical issues, it may be illuminating to give a sense of how the consequences of those events, as indeed even the need for this Handbook, can be understood as a piece in a larger philosophical mosaic. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {37--58},
	booktitle = {Handbook of ethics in quantitative methodology},
	publisher = {Routledge/Taylor \& Francis Group},
	author = {Rosnow, Ralph L. and Rosenthal, Robert},
	date = {2011},
	keywords = {Quantitative Methods, Statistical Analysis, History, Experimental Ethics, Philosophies, Professional Ethics},
	file = {Snapshot:/Users/tom/Zotero/storage/TGJ28SXF/2011-03489-003.html:text/html},
}

@online{noauthor_handbook_nodate,
	title = {Handbook of Ethics in Quantitative Methodology},
	url = {https://www.routledge.com/Handbook-of-Ethics-in-Quantitative-Methodology/Panter-Sterba/p/book/9781848728554},
	abstract = {This comprehensive Handbook is the first to provide a practical, interdisciplinary review of ethical issues as they relate to quantitative methodology including how to present evidence for reliability and validity, what comprises an adequate tested population, and what constitutes scientific knowledge for eliminating biases. The book uses an ethical framework that emphasizes the human cost of quantitative decision making to help researchers understand the specific implications of their choices.},
	titleaddon = {Routledge \& {CRC} Press},
	urldate = {2021-08-26},
	langid = {english},
}

@article{borsboom_theory_2021-1,
	title = {Theory construction methodology: a practical framework for building theories in psychology},
	volume = {16},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620969647},
	doi = {10.1177/1745691620969647},
	shorttitle = {Theory construction methodology},
	abstract = {This article aims to improve theory formation in psychology by developing a practical methodology for constructing explanatory theories: theory construction methodology ({TCM}). {TCM} is a sequence of five steps. First, the theorist identifies a domain of empirical phenomena that becomes the target of explanation. Second, the theorist constructs a prototheory, a set of theoretical principles that putatively explain these phenomena. Third, the prototheory is used to construct a formal model, a set of model equations that encode explanatory principles. Fourth, the theorist investigates the explanatory adequacy of the model by formalizing its empirical phenomena and assessing whether it indeed reproduces these phenomena. Fifth, the theorist studies the overall adequacy of the theory by evaluating whether the identified phenomena are indeed reproduced faithfully and whether the explanatory principles are sufficiently parsimonious and substantively plausible. We explain {TCM} with an example taken from research on intelligence (the mutualism model of intelligence), in which key elements of the method have been successfully implemented. We discuss the place of {TCM} in the larger scheme of scientific research and propose an outline for a university curriculum that can systematically educate psychologists in the process of theory formation.},
	pages = {756--766},
	number = {4},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Borsboom, Denny and van der Maas, Han L. J. and Dalege, Jonas and Kievit, Rogier A. and Haig, Brian D.},
	urldate = {2021-08-26},
	date = {2021-07-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/84FCLXEK/Borsboom et al. - 2021 - Theory Construction Methodology A Practical Frame.pdf:application/pdf},
}

@article{yarkoni_big_2009,
	title = {Big correlations in little studies: inflated fmri correlations reflect low statistical power—commentary on vul et al. (2009)},
	volume = {4},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1111/j.1745-6924.2009.01127.x},
	doi = {10.1111/j.1745-6924.2009.01127.x},
	shorttitle = {Big correlations in little studies},
	abstract = {Vul, Harris, Winkielman, and Pashler (2009, this issue) argue that correlations in many cognitive neuroscience studies are grossly inﬂated due to a widespread tendency to use nonindependent analyses. In this article, I argue that Vul et al.’s primary conclusion is correct, but for different reasons than they suggest. I demonstrate that the primary cause of grossly inﬂated correlations in wholebrain {fMRI} analyses is not nonindependence, but the pernicious combination of small sample sizes and stringent alpha-correction levels. Far from defusing Vul et al.’s conclusions, the simulations presented suggest that the level of inﬂation may be even worse than Vul et al.’s empirical analysis would suggest.},
	pages = {294--298},
	number = {3},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Yarkoni, Tal},
	urldate = {2021-08-26},
	date = {2009-05},
	langid = {english},
	file = {Yarkoni - 2009 - Big Correlations in Little Studies Inflated fMRI .pdf:/Users/tom/Zotero/storage/TMVYYJXU/Yarkoni - 2009 - Big Correlations in Little Studies Inflated fMRI .pdf:application/pdf},
}

@article{anvari_replicability_2018,
	title = {The replicability crisis and public trust in psychological science},
	volume = {3},
	issn = {2374-3603, 2374-3611},
	url = {https://www.tandfonline.com/doi/full/10.1080/23743603.2019.1684822},
	doi = {10.1080/23743603.2019.1684822},
	abstract = {Replication failures of past ﬁndings in several scientiﬁc disciplines, including psychology, medicine, and experimental economics, have created a “crisis of conﬁdence” among scientists. Psychological science has been at the forefront of tackling these issues, with discussions about replication failures and scientiﬁc self-criticisms of questionable research practices ({QRPs}) increasingly taking place in public forums. How this replicability crisis impacts the public’s trust is a question yet to be answered by research. Whereas some researchers believe that the public’s trust will be positively impacted or maintained, others believe trust will be diminished. Because it is our ﬁeld of expertise, we focus on trust in psychological science. We performed a study testing how public trust in past and future psychological research would be impacted by being informed about (i) replication failures (replications group), (ii) replication failures and criticisms of {QRPs} ({QRPs} group), and (iii) replication failures, criticisms of {QRPs}, and proposed reforms (reforms group). Results from a mostly European sample (N = 1129) showed that, compared to a control group, people in the replications, {QRPs}, and reforms groups self-reported less trust in past research. Regarding trust in future research, the replications and {QRPs} groups did not signiﬁcantly diﬀer from the control group. Surprisingly, the reforms group had less trust in future research than the control group. Nevertheless, people in the replications, {QRPs}, and reforms groups did not signiﬁcantly diﬀer from the control group in how much they believed future research in psychological science should be supported by public funding. Potential explanations are discussed.},
	pages = {266--286},
	number = {3},
	journaltitle = {Comprehensive Results in Social Psychology},
	shortjournal = {Comprehensive Results in Social Psychology},
	author = {Anvari, Farid and Lakens, Daniël},
	urldate = {2021-08-27},
	date = {2018-09-02},
	langid = {english},
	file = {Anvari and Lakens - 2018 - The replicability crisis and public trust in psych.pdf:/Users/tom/Zotero/storage/U8USEZM6/Anvari and Lakens - 2018 - The replicability crisis and public trust in psych.pdf:application/pdf},
}

@article{meade_identifying_2012,
	title = {Identifying careless responses in survey data},
	pages = {437--455},
	journaltitle = {Psychological Methods},
	author = {Meade, A. W. Citation and Craig, S. B.},
	date = {2012},
	file = {Meade and Craig - 2012 - Identifying careless responses in survey data.pdf:/Users/tom/Zotero/storage/F5A5WQP9/Meade and Craig - 2012 - Identifying careless responses in survey data.pdf:application/pdf},
}

@article{altman_association_2015,
	title = {Association, correlation and causation},
	volume = {12},
	rights = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3587},
	doi = {10.1038/nmeth.3587},
	abstract = {Correlation implies association, but not causation. Conversely, causation implies association, but not correlation.},
	pages = {899--900},
	number = {10},
	journaltitle = {Nature Methods},
	author = {Altman, Naomi and Krzywinski, Martin},
	urldate = {2021-08-30},
	date = {2015-10-01},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 10
Primary\_atype: News
Publisher: Nature Publishing Group
Subject\_term: Publishing;Research data;Statistical methods
Subject\_term\_id: publishing;research-data;statistical-methods},
	keywords = {toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/I7RCXEED/Altman and Krzywinski - 2015 - Association, correlation and causation.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JYDKDCB7/nmeth.html:text/html},
}

@article{black_noise_1986,
	title = {Noise},
	volume = {41},
	issn = {1540-6261},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1986.tb04513.x},
	doi = {10.1111/j.1540-6261.1986.tb04513.x},
	abstract = {The effects of noise on the world, and on our views of the world, are profound. Noise in the sense of a large number of small events is often a causal factor much more powerful than a small number of large events can be. Noise makes trading in financial markets possible, and thus allows us to observe prices for financial assets. Noise causes markets to be somewhat inefficient, but often prevents us from taking advantage of inefficiencies. Noise in the form of uncertainty about future tastes and technology by sector causes business cycles, and makes them highly resistant to improvement through government intervention. Noise in the form of expectations that need not follow rational rules causes inflation to be what it is, at least in the absence of a gold standard or fixed exchange rates. Noise in the form of uncertainty about what relative prices would be with other exchange rates makes us think incorrectly that changes in exchange rates or inflation rates cause changes in trade or investment flows or economic activity. Most generally, noise makes it very difficult to test either practical or academic theories about the way that financial or economic markets work. We are forced to act largely in the dark.},
	pages = {528--543},
	number = {3},
	journaltitle = {The Journal of Finance},
	author = {Black, Fischer},
	urldate = {2021-08-30},
	date = {1986},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.1986.tb04513.x},
	file = {Full Text PDF:/Users/tom/Zotero/storage/78TNXCK2/Black - 1986 - Noise.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UGUEHBVT/j.1540-6261.1986.tb04513.html:text/html},
}

@article{yanai_novel_2021,
	title = {Novel predictions arise from contradictions},
	volume = {22},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-021-02371-6},
	doi = {10.1186/s13059-021-02371-6},
	pages = {153},
	number = {1},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Yanai, Itai and Lercher, Martin},
	urldate = {2021-08-30},
	date = {2021-05-11},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MTYRPLEU/Yanai and Lercher - 2021 - Novel predictions arise from contradictions.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5GQHGGPU/s13059-021-02371-6.html:text/html},
}

@article{cornell_annals_2017,
	title = {Annals Understanding Clinical Research: Evaluating the Meaning of a Summary Estimate in a Meta-analysis},
	volume = {167},
	issn = {0003-4819},
	url = {https://www.acpjournals.org/doi/10.7326/M17-1454},
	doi = {10.7326/M17-1454},
	shorttitle = {Annals Understanding Clinical Research},
	pages = {275--277},
	number = {4},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Cornell, John E. and Liao, Joshua M. and Stack, Catharine B. and Mulrow, Cynthia D.},
	urldate = {2021-08-30},
	date = {2017-08-15},
	note = {Publisher: American College of Physicians},
	file = {Full Text PDF:/Users/tom/Zotero/storage/YURJKHBU/Cornell et al. - 2017 - Annals Understanding Clinical Research Evaluating.pdf:application/pdf},
}

@report{peterson_arguments_2021,
	title = {Arguments against efficiency in science},
	url = {https://osf.io/preprints/socarxiv/u78e5/},
	abstract = {A recent commentary critiqued the embrace of performance metrics at research universities. Drawing on our research studying the metascience movement, we suggest that the drive to maximize efficiency in science is increasingly extending beyond performance metrics, into labs themselves. Because institutional and public audiences are predisposed to viewing science in simple terms, it can be challenging for scientists to articulate counterarguments to policies that increase transparency and accountability in the name of efficiency. This short piece offers a sketch of an argument against treating efficiency as the lodestar for science.},
	institution = {{SocArXiv}},
	author = {Peterson, David and Panofsky, Aaron},
	urldate = {2021-08-30},
	date = {2021-05-12},
	doi = {10.31235/osf.io/u78e5},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, metascience, replication crisis, Science, and Technology, Knowledge, Sociology, efficiency, performance metrics, science policy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3RPT8BDG/Peterson and Panofsky - 2021 - Arguments against efficiency in science.pdf:application/pdf},
}

@article{chauvin_accuracy_2019,
	title = {Accuracy in detecting inadequate research reporting by early career peer reviewers using an online {CONSORT}-based peer-review tool ({COBPeer}) versus the usual peer-review process: a cross-sectional diagnostic study},
	volume = {17},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-019-1436-0},
	doi = {10.1186/s12916-019-1436-0},
	shorttitle = {Accuracy in detecting inadequate research reporting by early career peer reviewers using an online {CONSORT}-based peer-review tool ({COBPeer}) versus the usual peer-review process},
	abstract = {The peer review process has been questioned as it may fail to allow the publication of high-quality articles. This study aimed to evaluate the accuracy in identifying inadequate reporting in {RCT} reports by early career researchers ({ECRs}) using an online {CONSORT}-based peer-review tool ({COBPeer}) versus the usual peer-review process.},
	pages = {205},
	number = {1},
	journaltitle = {{BMC} Medicine},
	shortjournal = {{BMC} Medicine},
	author = {Chauvin, Anthony and Ravaud, Philippe and Moher, David and Schriger, David and Hopewell, Sally and Shanahan, Daniel and Alam, Sabina and Baron, Gabriel and Regnaux, Jean-Philippe and Crequit, Perrine and Martinez, Valeria and Riveros, Carolina and Le Cleach, Laurence and Recchioni, Alessandro and Altman, Douglas G. and Boutron, Isabelle},
	urldate = {2021-08-30},
	date = {2019-11-19},
	keywords = {Randomized controlled trials, Reporting, toread, {CONSORT} statement, Peer reviewers},
	file = {Full Text PDF:/Users/tom/Zotero/storage/A4ALZT4U/Chauvin et al. - 2019 - Accuracy in detecting inadequate research reportin.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/A8B6Z53V/s12916-019-1436-0.html:text/html},
}

@article{boutron_future_2020,
	title = {Future of evidence ecosystem series: 1. Introduction Evidence synthesis ecosystem needs dramatic change},
	volume = {123},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S089543561930592X},
	doi = {10.1016/j.jclinepi.2020.01.024},
	shorttitle = {Future of evidence ecosystem series},
	abstract = {Objectives
This article presents why the planning, conduct, and reporting of systematic reviews and meta-analyses of therapeutic interventions are suboptimal.
Study Design and Setting
We present an overview of the limitations of the current system of evidence synthesis for therapeutic interventions.
Results
Systematic reviews and meta-analyses are a cornerstone of health care decisions. However, despite the increasing a number of published systematic reviews of therapeutic interventions, the current evidence synthesis ecosystem is not properly addressing stakeholders’ needs. The current production process leads to a series of disparate systematic reviews because of erratic and inefficient planning with a process that is not always comprehensive and is prone to bias. Evidence synthesis depends on the quality of primary research, so primary research that is not available is biased or selectively reported raises important concerns. Moreover, the lack of interactions between the community of primary research producers and systematic reviewers impedes the optimal use of data. The context has considerably evolved, with ongoing research innovations, a new medical approach with the end of the one-size-fits-all approach, more available data, and new patient expectations. All these changes must be introduced into the future evidence ecosystem.
Conclusion
Dramatic changes are needed to enable this future ecosystem to become user driven and user oriented and more useful for decision-making.},
	pages = {135--142},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Boutron, Isabelle and Créquit, Perrine and Williams, Hywel and Meerpohl, Joerg and Craig, Jonathan C. and Ravaud, Philippe},
	urldate = {2021-08-30},
	date = {2020-07-01},
	langid = {english},
	keywords = {Systematic review, Meta-analysis, Methods, Decision-making, Evidence synthesis ecosystem, Waste in research},
}

@article{tierney_framework_2021,
	title = {A framework for prospective, adaptive meta-analysis ({FAME}) of aggregate data from randomised trials},
	volume = {18},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003629},
	doi = {10.1371/journal.pmed.1003629},
	abstract = {Background The vast majority of systematic reviews are planned retrospectively, once most eligible trials have completed and reported, and are based on aggregate data that can be extracted from publications. Prior knowledge of trial results can introduce bias into both review and meta-analysis methods, and the omission of unpublished data can lead to reporting biases. We present a collaborative framework for prospective, adaptive meta-analysis ({FAME}) of aggregate data to provide results that are less prone to bias. Also, with {FAME}, we monitor how evidence from trials is accumulating, to anticipate the earliest opportunity for a potentially definitive meta-analysis. Methodology We developed and piloted {FAME} alongside 4 systematic reviews in prostate cancer, which allowed us to refine the key principles. These are to: (1) start the systematic review process early, while trials are ongoing or yet to report; (2) liaise with trial investigators to develop a detailed picture of all eligible trials; (3) prospectively assess the earliest possible timing for reliable meta-analysis based on the accumulating aggregate data; (4) develop and register (or publish) the systematic review protocol before trials produce results and seek appropriate aggregate data; (5) interpret meta-analysis results taking account of both available and unavailable data; and (6) assess the value of updating the systematic review and meta-analysis. These principles are illustrated via a hypothetical review and their application to 3 published systematic reviews. Conclusions {FAME} can reduce the potential for bias, and produce more timely, thorough and reliable systematic reviews of aggregate data.},
	pages = {e1003629},
	number = {5},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Tierney, Jayne F. and Fisher, David J. and Vale, Claire L. and Burdett, Sarah and Rydzewska, Larysa H. and Rogozińska, Ewelina and Godolphin, Peter J. and White, Ian R. and Parmar, Mahesh K. B.},
	urldate = {2021-08-30},
	date = {2021-05-06},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Medical risk factors, Metaanalysis, Systematic reviews, Cancer treatment, Metastasis, Prostate cancer, Prostate gland, Radiation therapy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/WRFM85QI/Tierney et al. - 2021 - A framework for prospective, adaptive meta-analysi.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XEL2G5AT/article.html:text/html},
}

@article{noauthor_value_2021,
	title = {The value of evidence synthesis},
	volume = {5},
	rights = {2021 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01131-7},
	doi = {10.1038/s41562-021-01131-7},
	abstract = {Science is a cumulative enterprise, and systematic evidence synthesis is invaluable for appraising what is known and what is not known on a specific research question. We strongly encourage the submission of systematic reviews and meta-analyses to Nature Human Behaviour.},
	pages = {539--539},
	number = {5},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	urldate = {2021-08-30},
	date = {2021-05},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 5
Primary\_atype: Editorial
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NLIMZ3MQ/2021 - The value of evidence synthesis.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8DFRPEKA/s41562-021-01131-7.html:text/html},
}

@article{dohoo_biasis_2014,
	title = {Bias—Is it a problem, and what should we do?},
	volume = {113},
	issn = {0167-5877},
	url = {https://www.sciencedirect.com/science/article/pii/S0167587713003024},
	doi = {10.1016/j.prevetmed.2013.10.008},
	series = {Special Issue: Schwabe Symposium 2012},
	abstract = {Observational studies are prone to two types of errors: random and systematic. Random error arises as a result of variation between samples that might be drawn in a study and can be reduced by increasing the sample size. Systematic error arises from problems with the study design or the methods used to obtain the study data and is not influenced by sample size. Over the last 20 years, veterinary epidemiologists have made great progress in dealing more effectively with random error (particularly through the use of multilevel models) but paid relatively little attention to systematic error. Systematic errors can arise from unmeasured confounders, selection bias and information bias. Unmeasured confounders include both factors which are known to be confounders but which were not measured in a study and factors which are not known to be confounders. Confounders can bias results toward or away from the null. The impact of selection bias can also be difficult to predict and can be negligible or large. Although the direction of information bias is generally toward the null, this cannot be guaranteed and its impact might be very large. Methods of dealing with systematic errors include: qualitative assessment, quantitative bias analysis and incorporation of bias parameters into the statistical analyses.},
	pages = {331--337},
	number = {3},
	journaltitle = {Preventive Veterinary Medicine},
	shortjournal = {Preventive Veterinary Medicine},
	author = {Dohoo, Ian R.},
	urldate = {2021-08-30},
	date = {2014-02-15},
	langid = {english},
	keywords = {Bias, Confounding, Misclassification bias, Quantitative bias adjustment, Selection bias, Systematic error},
}

@online{noauthor_bias_nodate,
	title = {Bias {\textbar} Journal of Epidemiology \& Community Health},
	url = {https://jech.bmj.com/content/58/8/635},
	urldate = {2021-08-30},
}

@article{grimes_bias_2002,
	title = {Bias and causal associations in observational research},
	volume = {359},
	issn = {0140-6736},
	url = {https://www.sciencedirect.com/science/article/pii/S0140673602074512},
	doi = {10.1016/S0140-6736(02)07451-2},
	abstract = {Readers of medical literature need to consider two types of validity, internal and external. Internal validity means that the study measured what it set out to; external validity is the ability to generalise from the study to the reader's patients. With respect to internal validity, selection bias, information bias, and confounding are present to some degree in all observational research. Selection bias stems from an absence of comparability between groups being studied. Information bias results from incorrect determination of exposure, outcome, or both. The effect of information bias depends on its type. If information is gathered differently for one group than for another, bias results. By contrast, non-differential misclassification tends to obscure real differences. Confounding is a mixing or blurring of effects: a researcher attempts to relate an exposure to an outcome but actually measures the effect of a third factor (the confounding variable). Confounding can be controlled in several ways: restriction, matching, stratification, and more sophisticated multivariate techniques. If a reader cannot explain away study results on the basis of selection, information, or confounding bias, then chance might be another explanation. Chance should be examined last, however, since these biases can account for highly significant, though bogus results. Differentiation between spurious, indirect, and causal associations can be difficult. Criteria such as temporal sequence, strength and consistency of an association, and evidence of a dose-response effect lend support to a causal link.},
	pages = {248--252},
	number = {9302},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Grimes, David A and Schulz, Kenneth F},
	urldate = {2021-08-30},
	date = {2002-01-19},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/BGQWIAWU/Grimes and Schulz - 2002 - Bias and causal associations in observational rese.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/4JXEVINI/S0140673602074512.html:text/html},
}

@article{dohoo_biasis_2014-1,
	title = {Bias—Is it a problem, and what should we do?},
	volume = {113},
	issn = {0167-5877},
	url = {https://www.sciencedirect.com/science/article/pii/S0167587713003024},
	doi = {10.1016/j.prevetmed.2013.10.008},
	series = {Special Issue: Schwabe Symposium 2012},
	abstract = {Observational studies are prone to two types of errors: random and systematic. Random error arises as a result of variation between samples that might be drawn in a study and can be reduced by increasing the sample size. Systematic error arises from problems with the study design or the methods used to obtain the study data and is not influenced by sample size. Over the last 20 years, veterinary epidemiologists have made great progress in dealing more effectively with random error (particularly through the use of multilevel models) but paid relatively little attention to systematic error. Systematic errors can arise from unmeasured confounders, selection bias and information bias. Unmeasured confounders include both factors which are known to be confounders but which were not measured in a study and factors which are not known to be confounders. Confounders can bias results toward or away from the null. The impact of selection bias can also be difficult to predict and can be negligible or large. Although the direction of information bias is generally toward the null, this cannot be guaranteed and its impact might be very large. Methods of dealing with systematic errors include: qualitative assessment, quantitative bias analysis and incorporation of bias parameters into the statistical analyses.},
	pages = {331--337},
	number = {3},
	journaltitle = {Preventive Veterinary Medicine},
	shortjournal = {Preventive Veterinary Medicine},
	author = {Dohoo, Ian R.},
	urldate = {2021-08-30},
	date = {2014-02-15},
	langid = {english},
	keywords = {Bias, Confounding, Misclassification bias, Quantitative bias adjustment, Selection bias, Systematic error},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/IYXWKPUP/Dohoo - 2014 - Bias—Is it a problem, and what should we do.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/B9624LJV/S0167587713003024.html:text/html},
}

@article{heyes_what_2021,
	title = {What Happened to Mirror Neurons?},
	issn = {1745-6924},
	doi = {10.1177/1745691621990638},
	abstract = {Ten years ago, Perspectives in Psychological Science published the Mirror Neuron Forum, in which authors debated the role of mirror neurons in action understanding, speech, imitation, and autism and asked whether mirror neurons are acquired through visual-motor learning. Subsequent research on these themes has made significant advances, which should encourage further, more systematic research. For action understanding, multivoxel pattern analysis, patient studies, and brain stimulation suggest that mirror-neuron brain areas contribute to low-level processing of observed actions (e.g., distinguishing types of grip) but not to high-level action interpretation (e.g., inferring actors' intentions). In the area of speech perception, although it remains unclear whether mirror neurons play a specific, causal role in speech perception, there is compelling evidence for the involvement of the motor system in the discrimination of speech in perceptually noisy conditions. For imitation, there is strong evidence from patient, brain-stimulation, and brain-imaging studies that mirror-neuron brain areas play a causal role in copying of body movement topography. In the area of autism, studies using behavioral and neurological measures have tried and failed to find evidence supporting the "broken-mirror theory" of autism. Furthermore, research on the origin of mirror neurons has confirmed the importance of domain-general visual-motor associative learning rather than canalized visual-motor learning, or motor learning alone.},
	pages = {1745691621990638},
	journaltitle = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Heyes, Cecilia and Catmur, Caroline},
	date = {2021-07-09},
	pmid = {34241539},
	keywords = {neuroscience, toread, action understanding, associative learning, autism, mirror neurons, social cognition},
	file = {Submitted Version:/Users/tom/Zotero/storage/CTHN9LUQ/Heyes and Catmur - 2021 - What Happened to Mirror Neurons.pdf:application/pdf},
}

@article{greenhalgh_will_2020,
	title = {Will {COVID}-19 be evidence-based medicine’s nemesis?},
	volume = {17},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003266},
	doi = {10.1371/journal.pmed.1003266},
	pages = {e1003266},
	number = {6},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Greenhalgh, Trisha},
	urldate = {2021-08-31},
	date = {2020-06-30},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Research design, Drug research and development, toread, Complex systems, {COVID} 19, Evidence based medicine, Ontologies, Pandemics, Public and occupational health},
	file = {Full Text PDF:/Users/tom/Zotero/storage/2I7T6IS3/Greenhalgh - 2020 - Will COVID-19 be evidence-based medicine’s nemesis.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6GXIWA83/article.html:text/html},
}

@article{hagger_nomological_2017,
	title = {On Nomological Validity and Auxiliary Assumptions: The Importance of Simultaneously Testing Effects in Social Cognitive Theories Applied to Health Behavior and Some Guidelines},
	volume = {8},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2017.01933},
	doi = {10.3389/fpsyg.2017.01933},
	shorttitle = {On Nomological Validity and Auxiliary Assumptions},
	abstract = {Tests of social cognitive theories provide informative data on the factors that relate to health behavior, and the processes and mechanisms involved. In the present article, we contend that tests of social cognitive theories should adhere to the principles of nomological validity, defined as the degree to which predictions in a formal theoretical network are confirmed. We highlight the importance of nomological validity tests to ensure theory predictions can be disconfirmed through observation. We argue that researchers should be explicit on the conditions that lead to theory disconfirmation, and identify any auxiliary assumptions on which theory effects may be conditional. We contend that few researchers formally test the nomological validity of theories, or outline conditions that lead to model rejection and the auxiliary assumptions that may explain findings that run counter to hypotheses, raising potential for ‘falsification evasion.’ We present a brief analysis of studies (k = 122) testing four key social cognitive theories in health behavior to illustrate deficiencies in reporting theory tests and evaluations of nomological validity. Our analysis revealed that few articles report explicit statements suggesting that their findings support or reject the hypotheses of the theories tested, even when findings point to rejection. We illustrate the importance of explicit a priori specification of fundamental theory hypotheses and associated auxiliary assumptions, and identification of the conditions which would lead to rejection of theory predictions. We also demonstrate the value of confirmatory analytic techniques, meta-analytic structural equation modeling, and Bayesian analyses in providing robust converging evidence for nomological validity. We provide a set of guidelines for researchers on how to adopt and apply the nomological validity approach to testing health behavior models.},
	pages = {1933},
	journaltitle = {Frontiers in Psychology},
	author = {Hagger, Martin S. and Gucciardi, Daniel F. and Chatzisarantis, Nikos L. D.},
	urldate = {2021-08-31},
	date = {2017},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PYY8ANAA/Hagger et al. - 2017 - On Nomological Validity and Auxiliary Assumptions.pdf:application/pdf},
}

@report{tunc_falsificationist_2020-1,
	title = {A falsificationist treatment of auxiliary hypotheses in social and behavioral sciences: systematic replications framework},
	url = {https://psyarxiv.com/pdm7y/},
	shorttitle = {A falsificationist treatment of auxiliary hypotheses in social and behavioral sciences},
	abstract = {Auxiliary hypotheses ({AHs}) are indispensable in hypothesis-testing, because without them specification of testable predictions and consequently falsification is impossible. However, as {AHs} enter the test along with the main hypothesis, non-corroborative findings are ambiguous. Due to this ambiguity, {AHs} may also be employed to deflect falsification by providing “alternative explanations” of findings. This is not fatal to the extent that {AHs} are independently validated and safely relegated to background knowledge. But this is not always possible, especially in the so-called “softer” sciences where often theories are loosely organized, measurements are noisy, and constructs are vague. The Systematic Replications Framework ({SRF}) provides a methodological solution by disentangling the implications of the findings for the main hypothesis and the {AHs} through pre-planned series of systematically interlinked close and conceptual replications. {SRF} facilitates testing alternative explanations associated with different {AHs} and thereby increases test severity across a battery of tests. In this way, {SRF} assesses whether the corroboration of a hypothesis is conditional on particular {AHs}, and thus allows for a more objective evaluation of its empirical support and whether post hoc modifications to the theory are progressive or degenerative in the Lakatosian sense. Finally, {SRF} has several advantages over randomization-based systematic replication proposals, which generally assume a problematic neo-operationalist approach that prescribes exploration-oriented strategies in confirmatory contexts.},
	institution = {{PsyArXiv}},
	author = {Tunç, Duygu Uygun and Tunç, Mehmet Necip},
	urldate = {2021-08-31},
	date = {2020-05-13},
	doi = {10.31234/osf.io/pdm7y},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Quantitative Methods, replication, Theory and Philosophy of Science, adversarial collaboration, auxiliary hypotheses, Duhem-Quine Thesis, empirical underdetermination, falsificationism, Meta-Psychology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/G75VCVEF/Tunç and Tunç - 2020 - A Falsificationist Treatment of Auxiliary Hypothes.pdf:application/pdf},
}

@article{maner_into_2016,
	title = {Into the wild: Field research can increase both replicability and real-world impact},
	volume = {66},
	issn = {0022-1031},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103115001286},
	doi = {10.1016/j.jesp.2015.09.018},
	series = {Rigorous and Replicable Methods in Social Psychology},
	shorttitle = {Into the wild},
	abstract = {Field research has the potential to substantially increase both the replicability and the impact of psychological science. Field methods sometimes are characterized by features – relatively high levels of participant diversity, relative lack of control over extraneous variables, greater focus on behavioral dependent variables, less room for researcher degrees of freedom, and lower likelihood of publication bias – that can increase the veracity and robustness of published research. Moreover, field studies can help extend psychological research in valuable ways to applied domains such as health, law, education, and business. Consequently, field studies, especially those that integrate an applied perspective, can provide information directly relevant for tackling important social problems. Incorporating field data into lines of basic research can increase not just the replicability, but also the relevance and impact of one's science.},
	pages = {100--106},
	journaltitle = {Journal of Experimental Social Psychology},
	shortjournal = {Journal of Experimental Social Psychology},
	author = {Maner, Jon K.},
	urldate = {2021-08-31},
	date = {2016-09-01},
	langid = {english},
	keywords = {Replication, Applied psychology, Research methods, toread, Field research},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/IM4ZHBPS/Maner - 2016 - Into the wild Field research can increase both re.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/2LPUYSNN/S0022103115001286.html:text/html},
}

@article{van_den_bergh_cautionary_2021,
	title = {A cautionary note on estimating effect size},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245921992035},
	doi = {10.1177/2515245921992035},
	abstract = {An increasingly popular approach to statistical inference is to focus on the estimation of effect size. Yet this approach is implicitly based on the assumption that there is an effect while ignoring the null hypothesis that the effect is absent. We demonstrate how this common null-hypothesis neglect may result in effect size estimates that are overly optimistic. As an alternative to the current approach, a spike-and-slab model explicitly incorporates the plausibility of the null hypothesis into the estimation process. We illustrate the implications of this approach and provide an empirical example.},
	pages = {2515245921992035},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {van den Bergh, Don and Haaf, Julia M. and Ly, Alexander and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
	urldate = {2021-08-31},
	date = {2021-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {modeling, open materials, effect size, Bayesian estimation, shrinkage},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/UJ9ZYB44/van den Bergh et al. - 2021 - A Cautionary Note on Estimating Effect Size.pdf:application/pdf},
}

@article{davis-stober_estimation_2018,
	title = {Estimation accuracy in the psychological sciences},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0207239},
	doi = {10.1371/journal.pone.0207239},
	abstract = {Sample means comparisons are a fundamental and ubiquitous approach to interpreting experimental psychological data. Yet, we argue that the sample and effect sizes in published psychological research are frequently so small that sample means are insufficiently accurate to determine whether treatment effects have occurred. Generally, an estimator should be more accurate than any benchmark that systematically ignores information about the relations among experimental conditions. We consider two such benchmark estimators: one that randomizes the relations among conditions and another that always assumes no treatment effects. We show conditions under which these benchmark estimators estimate the true parameters more accurately than sample means. This perverse situation can occur even when effects are statistically significant at traditional levels. Our argument motivates the need for regularized estimates, such as those used in lasso, ridge, and hierarchical Bayes techniques.},
	pages = {e0207239},
	number = {11},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Davis-Stober, Clintin P. and Dana, Jason and Rouder, Jeffrey N.},
	urldate = {2021-08-31},
	date = {2018-11-26},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Clinical psychology, Measurement, Behavior, Experimental psychology, Psychologists, Cognitive psychology, Normal distribution, African American people},
	file = {Full Text PDF:/Users/tom/Zotero/storage/2BUZAJNQ/Davis-Stober et al. - 2018 - Estimation accuracy in the psychological sciences.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9EVCTRLH/article.html:text/html},
}

@article{van_agteren_systematic_2021,
	title = {A systematic review and meta-analysis of psychological interventions to improve mental wellbeing},
	volume = {5},
	rights = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01093-w},
	doi = {10.1038/s41562-021-01093-w},
	abstract = {Our current understanding of the efficacy of psychological interventions in improving mental states of wellbeing is incomplete. This study aimed to overcome limitations of previous reviews by examining the efficacy of distinct types of psychological interventions, irrespective of their theoretical underpinning, and the impact of various moderators, in a unified systematic review and meta-analysis. Four-hundred-and-nineteen randomized controlled trials from clinical and non-clinical populations (n = 53,288) were identified for inclusion. Mindfulness-based and multi-component positive psychological interventions demonstrated the greatest efficacy in both clinical and non-clinical populations. Meta-analyses also found that singular positive psychological interventions, cognitive and behavioural therapy-based, acceptance and commitment therapy-based, and reminiscence interventions were impactful. Effect sizes were moderate at best, but differed according to target population and moderator, most notably intervention intensity. The evidence quality was generally low to moderate. While the evidence requires further advancement, the review provides insight into how psychological interventions can be designed to improve mental wellbeing. This meta-analysis of 419 randomized controlled trials found that various types of psychological interventions could improve mental wellbeing in clinical and non-clinical populations. Effect sizes tended to be small to moderate and were influenced by various moderators.},
	pages = {631--652},
	number = {5},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {van Agteren, Joep and Iasiello, Matthew and Lo, Laura and Bartholomaeus, Jonathan and Kopsaftis, Zoe and Carey, Marissa and Kyrios, Michael},
	urldate = {2021-08-31},
	date = {2021-05},
	langid = {english},
	note = {Number: 5
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Disease prevention;Medical humanities;Outcomes research;Quality of life
Subject\_term\_id: disease-prevention;medical-humanities;outcomes-research;quality-of-life},
	file = {Snapshot:/Users/tom/Zotero/storage/G2WYFZB5/s41562-021-01093-w.html:text/html},
}

@article{meehl_cliometric_2002,
	title = {Cliometric metatheory: ii. criteria scientists use in theory appraisal and why it is rational to do so},
	volume = {91},
	issn = {0033-2941},
	url = {https://doi.org/10.2466/pr0.2002.91.2.339},
	doi = {10.2466/pr0.2002.91.2.339},
	shorttitle = {Cliometric metatheory},
	abstract = {Definitive tests of theories are often impossible in the life sciences because auxiliary assumptions are problematic. In the appraisal of competing theories, history of science shows that scientists use various theory characteristics such as aspects of parsimony, the number, qualitative diversity, novelty, and numerical precision of facts derived, number of misderived facts, and reducibility relations to other accepted theories. Statistical arguments are offered to show why, given minimal assumptions about the world and the mind, many of these attributes are expectable correlates of verisimilitude. A statistical composite of these attributes could provide an actuarial basis for theory appraisal (cliometric metatheory).},
	pages = {339--404},
	number = {2},
	journaltitle = {Psychological Reports},
	shortjournal = {Psychol Rep},
	author = {Meehl, Paul E.},
	urldate = {2021-08-31},
	date = {2002-10-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/PPMNGU89/Meehl - 2002 - Cliometric Metatheory II. Criteria Scientists Use.pdf:application/pdf},
}

@article{faust_why_2005,
	title = {Why Paul Meehl will revolutionize the philosophy of science and why it should matter to psychologists},
	volume = {61},
	issn = {1097-4679},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jclp.20185},
	doi = {10.1002/jclp.20185},
	abstract = {Paul Meehl was a person of remarkable genius who made many seminal contributions to psychology and other fields. One of his most important, but less widely known potential contributions is the codevelopment, and the extension and elaboration of meta-science, or the science of science. Meta-science involves applying more rigorous methods (than are usually used) to the study of episodes in the history of science, or the historical track record, in order to address long-standing questions in the philosophy and history of science, aid in the selection of optimally effective methodology, and assist the scientist in higher level and complex integrative judgments (e.g., theory evaluation). Psychologists, given their methodological sophistication, can be major contributors to meta-scientific efforts and to the development of this field. © 2005 Wiley Periodicals, Inc. J Clin Psychol 61: 1355–1366, 2005.},
	pages = {1355--1366},
	number = {10},
	journaltitle = {Journal of Clinical Psychology},
	author = {Faust, David},
	urldate = {2021-08-31},
	date = {2005},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jclp.20185},
	keywords = {toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/5TYWNTKI/Faust - 2005 - Why Paul Meehl will revolutionize the philosophy o.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/EX2C98NR/jclp.html:text/html},
}

@article{rohrer_thinking_2018,
	title = {Thinking clearly about correlations and causation: graphical causal models for observational data},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245917745629},
	doi = {10.1177/2515245917745629},
	shorttitle = {Thinking clearly about correlations and causation},
	abstract = {Correlation does not imply causation; but often, observational data are the only option, even though the research question at hand involves causality. This article discusses causal inference based on observational data, introducing readers to graphical causal models that can provide a powerful tool for thinking more clearly about the interrelations between variables. Topics covered include the rationale behind the statistical control of third variables, common procedures for statistical control, and what can go wrong during their implementation. Certain types of third variables—colliders and mediators—should not be controlled for because that can actually move the estimate of an association away from the value of the causal effect of interest. More subtle variations of such harmful control include using unrepresentative samples, which can undermine the validity of causal conclusions, and statistically controlling for mediators. Drawing valid causal inferences on the basis of observational data is not a mechanistic procedure but rather always depends on assumptions that require domain knowledge and that can be more or less plausible. However, this caveat holds not only for research based on observational data, but for all empirical research endeavors.},
	pages = {27--42},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Rohrer, Julia M.},
	urldate = {2021-08-31},
	date = {2018-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {toread, directed acyclic graphs},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/STBY7GW2/Rohrer - 2018 - Thinking Clearly About Correlations and Causation.pdf:application/pdf},
}

@report{maatman_psychologys_2021,
	title = {Psychology's theory crisis, and why formal modelling cannot solve it},
	url = {https://psyarxiv.com/puqvs/},
	abstract = {In light of psychology’s ‘theory crisis’, multiple authors have recently argued that adopting the formalization of theories and/or formal modelling is a necessary or useful step towards stronger psychological theory. In this article, I instead argue that formal modelling cannot solve the core problem the psychological ‘theory crisis’ refers to, which are the currently high degrees of contrastive and holistic underdetermination of our theories by our data. I do so by first introducing underdetermination as an explanatory framework for determining the evidential import of research findings for theories, and showing how both broader theoretical considerations and informal assumptions are key to this process. Then, I derive the aforementioned core problem from current theory crisis literature and tentatively explore its possible solutions. Lastly, I show that formal modelling is neither a necessary nor sufficient solution for either contrastive or holistic underdetermination, and that its uncritical adoption might instead worsen the crisis.},
	institution = {{PsyArXiv}},
	author = {Maatman, Freek Oude},
	urldate = {2021-08-31},
	date = {2021-07-12},
	doi = {10.31234/osf.io/puqvs},
	note = {type: article},
	keywords = {Meta-science, theory, evidence, toread, formal modelling, formalization, theory crisis, underdetermination},
	file = {Full Text PDF:/Users/tom/Zotero/storage/SY5BGRCI/Maatman - 2021 - Psychology's Theory Crisis, and Why Formal Modelli.pdf:application/pdf},
}

@article{ioannidis_meta-research_2010,
	title = {Meta-research: The art of getting it wrong},
	volume = {1},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.19},
	doi = {10.1002/jrsm.19},
	shorttitle = {Meta-research},
	abstract = {Meta-analysis has major strengths, but sometimes it can often lead to wrong and misleading answers. In this {SRSM} presidential address, I discuss some case studies that exemplify these problems, including examples from meta-analyses of both clinical trials and observational associations. I also discuss issues of effect size estimation, bias (in particular significance-chasing biases), and credibility in meta-research. I examine the factors that affect the credibility of meta-analyses, including magnitude of effects, multiplicity of analyses, scale of data, flexibility of analyses, reporting, and conflicts of interest. Under the current circumstances, a survey of expert meta-analysts attending the {SRSM} meeting showed that most of them believe that the true effect is practically equally likely to lie within the 95\% confidence interval of a meta-analysis or outside of it. Finally, I address the placement of meta-analysis in the wider current research agenda and make a plea for adoption of more prospective meta-designs. In many/most/all fields, all primary original research may be designed, executed, and interpreted as a prospective meta-analysis. Copyright © 2011 John Wiley \& Sons, Ltd.},
	pages = {169--184},
	number = {3},
	journaltitle = {Research Synthesis Methods},
	author = {Ioannidis, John P. A.},
	urldate = {2021-08-31},
	date = {2010},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.19},
	keywords = {meta-analysis, bias, reporting bias, effect size},
	file = {Full Text PDF:/Users/tom/Zotero/storage/CM3REBMT/Ioannidis - 2010 - Meta-research The art of getting it wrong.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/YZSMCERQ/jrsm.html:text/html},
}

@report{david_historical_2013,
	location = {Rochester, {NY}},
	title = {The Historical Origins of 'Open Science': An Essay on Patronage, Reputation and Common Agency Contracting in the Scientific Revolution},
	url = {https://papers.ssrn.com/abstract=2209188},
	shorttitle = {The Historical Origins of 'Open Science'},
	abstract = {This essay examines the economics of patronage in the production of knowledge and its influence upon the historical formation of key elements in the ethos and organizational structure of publicly funded 'open science.' The emergence during the late sixteenth and early seventeenth centuries of the idea and practice of 'open science' was a distinctive and vital organizational aspect of the Scientific Revolution. It represented a break from the previously dominant ethos of secrecy in the pursuit of Nature's Secrets, to a new set of norms, incentives, and organizational structures that reinforced scientific researchers' commitments to rapid disclosure of new knowledge. The rise of 'cooperative rivalries' in the revelation of new knowledge, is seen as a functional response to heightened asymmetric information problems posed for the Renaissance system of court-patronage of the arts and sciences; pre-existing informational asymmetries had been exacerbated by the claims of mathematicians and the increasing practical reliance upon new mathematical techniques in a variety of 'contexts of application.' Reputational competition among Europe's noble patrons motivated much of their efforts to attract to their courts the most prestigious natural philosophers, was no less crucial in the workings of that system than was the concern among their would-be clients to raise their peer-based reputational status. In late Renaissance Europe, the feudal legacy of fragmented political authority had resulted in relations between noble patrons and their savantclients that resembled the situation modern economists describe as `common agency contracting in substitutes' - competition among incompletely informed principals for the dedicated services of multiple agents. These conditions tended to result in contract terms (especially with regard to autonomy and financial support) that left agent client members of the nascent scientific communities better positioned to retain larger information rents on their specialized knowledge. This encouraged entry into their emerging disciplines, and enabled them collectively to develop a stronger degree of professional autonomy for their programs of inquiry within the increasingly specialized and formal scientific academies (such the Académie royale des Sciences and the Royal Society) that had attracted the patronage of rival absolutist States of Western Europe during the latter part of the seventeenth century. The institutionalization of 'open science' that took place within those settings is shown to have continuities with the use by scientists of the earlier humanist academies, and with the logic of regal patronage, rather than being driven by the material requirements of new observational and experimental techniques.},
	number = {{ID} 2209188},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {David, Paul A.},
	urldate = {2021-08-31},
	date = {2013-01-30},
	langid = {english},
	keywords = {open science, asymmetric information, common agency contracting, evolution of institutions, invisible colleges, new economics of science, patronage, principal-agent problems, scientific academies, social networks},
	file = {Snapshot:/Users/tom/Zotero/storage/HTP7MJRY/papers.html:text/html},
}

@article{ioannidis_reasons_2008,
	title = {Reasons or excuses for avoiding meta-analysis in forest plots},
	volume = {336},
	rights = {© {BMJ} Publishing Group Ltd 2008},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/content/336/7658/1413},
	doi = {10.1136/bmj.a117},
	abstract = {{\textless}p{\textgreater}Heterogeneous data are a common problem in meta-analysis. \textbf{John Ioannidis}, \textbf{Nikolaos Patsopoulos}, and \textbf{Hannah Rothstein} show that final synthesis is possible and desirable in most cases {\textless}/p{\textgreater}},
	pages = {1413--1415},
	number = {7658},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Ioannidis, John P. A. and Patsopoulos, Nikolaos A. and Rothstein, Hannah R.},
	urldate = {2021-08-31},
	date = {2008-06-19},
	langid = {english},
	pmid = {18566080},
	note = {Publisher: British Medical Journal Publishing Group
Section: Analysis},
	file = {Full Text PDF:/Users/tom/Zotero/storage/BHISBTUX/Ioannidis et al. - 2008 - Reasons or excuses for avoiding meta-analysis in f.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DBFVXHVA/1413.html:text/html},
}

@article{sayre_reproducibility_2018,
	title = {The reproducibility crisis and academic libraries},
	url = {https://crl.acrl.org/index.php/crl/article/view/16846},
	doi = {https://doi.org/10.5860/crl.79.1.2},
	abstract = {In recent years, evidence has emerged from disciplines ranging from biology to economics that many scientific studies are not reproducible. This evidence has led to declarations in both the scientific and lay press that science is experiencing a “reproducibility crisis” and that this crisis has significant impacts on both science and society, including misdirected effort, funding, and policy implemented on the basis of irreproducible research. In many cases, academic libraries are the natural organizations to lead efforts to implement recommendations from journals, funders, and societies to improve research reproducibility. In this editorial, we introduce the reproducibility crisis, define reproducibility and replicability, and then discusses how academic libraries can lead institutional support for reproducible research.},
	author = {Sayre, Franklin and Riegelman, Amy},
	urldate = {2021-08-31},
	date = {2018-01-03},
	langid = {american},
	file = {Full Text:/Users/tom/Zotero/storage/74GBDN24/Sayre and Riegelman - 2018 - The Reproducibility Crisis and Academic Libraries .pdf:application/pdf},
}

@article{logg_pre-registration_2021,
	title = {Pre-registration: Weighing costs and benefits for researchers},
	volume = {167},
	issn = {0749-5978},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597821000649},
	doi = {10.1016/j.obhdp.2021.05.006},
	shorttitle = {Pre-registration},
	abstract = {In the past decade, the social and behavioral sciences underwent a methodological revolution, offering practical prescriptions for improving the replicability and reproducibility of research results. One key to reforming science is a simple and scalable practice: pre-registration. Pre-registration constitutes pre-specifying an analysis plan prior to data collection. A growing chorus of articles discusses the prescriptive, field-wide benefits of pre-registration. To increase adoption, however, scientists need to know who currently pre-registers and understand perceived barriers to doing so. Thus, we weigh costs and benefits of pre-registration. Our survey of researchers reveals generational differences in who pre-registers and uncertainty regarding how pre-registration benefits individual researchers. We leverage these data to directly address researchers’ uncertainty by clarifying why pre-registration improves the research process itself. Finally, we discuss how to pre-register and compare available resources. The present work examines the who, why, and how of pre-registration in order to weigh the costs and benefits of pre-registration to researchers and motivate continued adoption.},
	pages = {18--27},
	journaltitle = {Organizational Behavior and Human Decision Processes},
	shortjournal = {Organizational Behavior and Human Decision Processes},
	author = {Logg, Jennifer M. and Dorison, Charles A.},
	urldate = {2021-08-31},
	date = {2021-11-01},
	langid = {english},
	keywords = {Replication, Open science, Pre-registration, Methodology},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/634BSU2M/Logg and Dorison - 2021 - Pre-registration Weighing costs and benefits for .pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/NTWNHV3L/S0749597821000649.html:text/html},
}

@online{noauthor_full_nodate,
	title = {Full article: What difference might retractions make? An estimate of the potential epistemic cost of retractions on meta-analyses},
	url = {https://www.tandfonline.com/doi/full/10.1080/08989621.2021.1947810},
	urldate = {2021-08-31},
	file = {Full article\: What difference might retractions make? An estimate of the potential epistemic cost of retractions on meta-analyses:/Users/tom/Zotero/storage/F4FRDXM3/08989621.2021.html:text/html},
}

@article{rosenblatt_sharing_2015,
	title = {Sharing of clinical trial data: benefits, risks, and uniform principles},
	volume = {162},
	issn = {0003-4819},
	url = {http://annals.org/article.aspx?doi=10.7326/M14-1299},
	doi = {10.7326/M14-1299},
	shorttitle = {Sharing of clinical trial data},
	pages = {306},
	number = {4},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Rosenblatt, Michael and Jain, Sachin H. and Cahill, Matthew},
	urldate = {2021-08-31},
	date = {2015-02-17},
	langid = {english},
	file = {Rosenblatt et al. - 2015 - Sharing of Clinical Trial Data Benefits, Risks, a.pdf:/Users/tom/Zotero/storage/8KTUHT33/Rosenblatt et al. - 2015 - Sharing of Clinical Trial Data Benefits, Risks, a.pdf:application/pdf},
}

@article{goodman_clinical_2015,
	title = {Clinical trial data sharing: what do we do now?},
	volume = {162},
	issn = {0003-4819},
	url = {https://www.acpjournals.org/doi/full/10.7326/M15-0021},
	doi = {10.7326/M15-0021},
	shorttitle = {Clinical trial data sharing},
	pages = {308--309},
	number = {4},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Goodman, Steven N.},
	urldate = {2021-08-31},
	date = {2015-02-17},
	note = {Publisher: American College of Physicians},
	keywords = {toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/VMUS3N9A/Goodman - 2015 - Clinical Trial Data Sharing What Do We Do Now.pdf:application/pdf},
}

@article{auspurg_has_2021,
	title = {Has the credibility of the social sciences been credibly destroyed? Reanalyzing the “many analysts, one data set” project},
	volume = {7},
	issn = {2378-0231},
	url = {https://doi.org/10.1177/23780231211024421},
	doi = {10.1177/23780231211024421},
	shorttitle = {Has the credibility of the social sciences been credibly destroyed?},
	abstract = {In 2018, Silberzahn, Uhlmann, Nosek, and colleagues published an article in which 29 teams analyzed the same research question with the same data: Are soccer referees more likely to give red cards to players with dark skin tone than light skin tone? The results obtained by the teams differed extensively. Many concluded from this widely noted exercise that the social sciences are not rigorous enough to provide definitive answers. In this article, we investigate why results diverged so much. We argue that the main reason was an unclear research question: Teams differed in their interpretation of the research question and therefore used diverse research designs and model specifications. We show by reanalyzing the data that with a clear research question, a precise definition of the parameter of interest, and theory-guided causal reasoning, results vary only within a narrow range. The broad conclusion of our reanalysis is that social science research needs to be more precise in its ?estimands? to become credible.},
	pages = {23780231211024421},
	journaltitle = {Socius},
	shortjournal = {Socius},
	author = {Auspurg, Katrin and Brüderl, Josef},
	urldate = {2021-08-31},
	date = {2021-01-01},
	note = {Publisher: {SAGE} Publications},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/8SG5DVUK/Auspurg and Brüderl - 2021 - Has the Credibility of the Social Sciences Been Cr.pdf:application/pdf},
}

@article{pham_preregistration_2021,
	title = {Preregistration is neither sufficient nor necessary for good science},
	volume = {31},
	issn = {1532-7663},
	url = {https://myscp.onlinelibrary.wiley.com/doi/abs/10.1002/jcpy.1209},
	doi = {10.1002/jcpy.1209},
	abstract = {To address widespread perceptions of a reproducibility crisis in the social sciences, a growing number of scholars recommend the systematic preregistration of empirical studies. The purpose of this article is to contribute to an epistemological dialogue on the value of preregistration in consumer research by identifying the limitations, drawbacks, and potential adverse effects of a preregistration system. After a brief review of some of the implementation challenges that commonly arise with preregistration, we raise three levels of issues with a system of preregistration. First, we identify its limitations as a means of advancing consumer knowledge, thus questioning the sufficiency of preregistration in promoting good consumer science. Second, we elaborate on why consumer science can progress even in the absence of preregistration, thereby also questioning the necessity of preregistration in promoting good consumer science. Third, we discuss serious potential adverse effects of preregistration, both at the individual researcher level and at the level of the field as a whole. We conclude by offering a broader perspective on the narrower role that preregistration can play within the general pursuit of building robust and useful knowledge about consumers.},
	pages = {163--176},
	number = {1},
	journaltitle = {Journal of Consumer Psychology},
	author = {Pham, Michel Tuan and Oh, Travis Tae},
	urldate = {2021-08-31},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://myscp.onlinelibrary.wiley.com/doi/pdf/10.1002/jcpy.1209},
	keywords = {Open science, Preregistration, Consumer research, Reproducibility crisis},
	file = {Snapshot:/Users/tom/Zotero/storage/T4UQQFMS/jcpy.html:text/html},
}

@article{guyatt_grade_2008,
	title = {{GRADE}: an emerging consensus on rating quality of evidence and strength of recommendations},
	volume = {336},
	rights = {© {BMJ} Publishing Group Ltd 2008},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/content/336/7650/924},
	doi = {10.1136/bmj.39489.470347.AD},
	shorttitle = {{GRADE}},
	abstract = {{\textless}p{\textgreater}Guidelines are inconsistent in how they rate the quality of evidence and the strength of recommendations. This article explores the advantages of the {GRADE} system, which is increasingly being adopted by organisations worldwide {\textless}/p{\textgreater}},
	pages = {924--926},
	number = {7650},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Guyatt, Gordon H. and Oxman, Andrew D. and Vist, Gunn E. and Kunz, Regina and Falck-Ytter, Yngve and Alonso-Coello, Pablo and Schünemann, Holger J.},
	urldate = {2021-08-31},
	date = {2008-04-24},
	langid = {english},
	pmid = {18436948},
	note = {Publisher: British Medical Journal Publishing Group
Section: Analysis},
	file = {Full Text PDF:/Users/tom/Zotero/storage/B5U9LRN2/Guyatt et al. - 2008 - GRADE an emerging consensus on rating quality of .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N7YL2N8T/924.html:text/html},
}

@article{bravo-biosca_experimental_2020,
	title = {Experimental innovation policy},
	volume = {20},
	issn = {1531-3468},
	url = {https://www.journals.uchicago.edu/doi/full/10.1086/705644},
	doi = {10.1086/705644},
	abstract = {Experimental approaches are increasingly being adopted across many policy fields, but innovation policy has been lagging. This paper reviews the case for policy experimentation in this field, describes the different types of experiments that can be undertaken, discusses some of the unique challenges to the use of experimental approaches in innovation policy, and summarizes some of the emerging lessons, with a focus on randomized trials. The paper concludes describing how at the Innovation Growth Lab we have been working with governments across the {OECD} to help them overcome the barriers to policy experimentation to make their policies more impactful.},
	pages = {191--232},
	journaltitle = {Innovation Policy and the Economy},
	author = {Bravo-Biosca, Albert},
	urldate = {2021-08-31},
	date = {2020-12-01},
	note = {Publisher: The University of Chicago Press},
	file = {Bravo-Biosca - 2020 - Experimental innovation policy.pdf:/Users/tom/Zotero/storage/XQZ74YFR/Bravo-Biosca - 2020 - Experimental innovation policy.pdf:application/pdf},
}

@article{saltelli_physics_2021,
	title = {Physics to the rescue?},
	url = {http://arxiv.org/abs/2107.07239},
	abstract = {A vast body of literature addresses the complex nature of science's reproducibility crisis. In contrast with this perceived complexity, some recent papers from the discipline of physics suggests that irreproducibility does not point to a systemic crisis, but is, on the contrary, a sign that the science system works properly. These works, while acknowledging the difference between physics and other disciplines mired in the reproducibility crisis, hint that all disciplines could learn from Physics. The present work suggests that this optimistic message, when addressed to struggling disciplines, may invite complacency over other relevant dimensions of crisis, delay its solution, and get into the way of a truly joint effort from all disciplines to tackle the important social and environmental predicaments of the present age.},
	journaltitle = {{arXiv}:2107.07239 [physics]},
	author = {Saltelli, Andrea and Di Fiore, Monica and Spanò, Francesco},
	urldate = {2021-08-31},
	date = {2021-07-15},
	eprinttype = {arxiv},
	eprint = {2107.07239},
	keywords = {toread, 01A99, A.1, Physics - Physics and Society},
	file = {arXiv Fulltext PDF:/Users/tom/Zotero/storage/N33BIQ3T/Saltelli et al. - 2021 - Physics to the rescue.pdf:application/pdf;arXiv.org Snapshot:/Users/tom/Zotero/storage/FKW93UZL/2107.html:text/html},
}

@article{hilgard_maximal_2021,
	title = {Maximal positive controls: A method for estimating the largest plausible effect size},
	volume = {93},
	issn = {0022-1031},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103120304224},
	doi = {10.1016/j.jesp.2020.104082},
	shorttitle = {Maximal positive controls},
	abstract = {Effect sizes in social psychology are generally not large and are limited by error variance in manipulation and measurement. Effect sizes exceeding these limits are implausible and should be viewed with skepticism. Maximal positive controls, experimental conditions that should show an obvious and predictable effect, can provide estimates of the upper limits of plausible effect sizes on a measure. In this work, maximal positive controls are conducted for three measures of aggressive cognition, and the effect sizes obtained are compared to studies found through systematic review. Questions are raised regarding the plausibility of certain reports with effect sizes comparable to, or in excess of, the effect sizes found in maximal positive controls. Maximal positive controls may provide a means to identify implausible study results at lower cost than direct replication.},
	pages = {104082},
	journaltitle = {Journal of Experimental Social Psychology},
	shortjournal = {Journal of Experimental Social Psychology},
	author = {Hilgard, Joseph},
	urldate = {2021-08-31},
	date = {2021-03-01},
	langid = {english},
	keywords = {Aggression, Violent video games, Aggressive thought, Positive controls, Scientific self-correction},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/Q39KJCTS/S0022103120304224.html:text/html;Submitted Version:/Users/tom/Zotero/storage/XLZAUUC5/Hilgard - 2021 - Maximal positive controls A method for estimating.pdf:application/pdf},
}

@article{mellor_progression_2021,
	title = {Progression from external pilot to definitive randomised controlled trial: a methodological review of progression criteria reporting},
	volume = {11},
	rights = {© Author(s) (or their employer(s)) 2021. Re-use permitted under {CC} {BY}. Published by {BMJ}.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported ({CC} {BY} 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
	issn = {2044-6055, 2044-6055},
	url = {https://bmjopen.bmj.com/content/11/6/e048178},
	doi = {10.1136/bmjopen-2020-048178},
	shorttitle = {Progression from external pilot to definitive randomised controlled trial},
	abstract = {Objectives Prespecified progression criteria can inform the decision to progress from an external randomised pilot trial to a definitive randomised controlled trial. We assessed the characteristics of progression criteria reported in external randomised pilot trial protocols and results publications, including whether progression criteria were specified a priori and mentioned in prepublication peer reviewer reports.
Study design Methodological review.
Methods We searched four journals through {PubMed}: British Medical Journal Open, Pilot and Feasibility Studies, Trials and Public Library of Science One. Eligible publications reported external randomised pilot trial protocols or results, were published between January 2018 and December 2019 and reported progression criteria. We double data extracted 25\% of the included publications. Here we report the progression criteria characteristics.
Results We included 160 publications (123 protocols and 37 completed trials). Recruitment and retention were the most frequent indicators contributing to progression criteria. Progression criteria were mostly reported as distinct thresholds (eg, achieving a specific target; 133/160, 83\%). Less than a third of the planned and completed pilot trials that included qualitative research reported how these findings would contribute towards progression criteria (34/108, 31\%). The publications seldom stated who established the progression criteria (12/160, 7.5\%) or provided rationale or justification for progression criteria (44/160, 28\%). Most completed pilot trials reported the intention to proceed to a definitive trial (30/37, 81\%), but less than half strictly met all of their progression criteria (17/37, 46\%). Prepublication peer reviewer reports were available for 153/160 publications (96\%). Peer reviewer reports for 86/153 (56\%) publications mentioned progression criteria, with peer reviewers of 35 publications commenting that progression criteria appeared not to be specified.
Conclusions Many external randomised pilot trial publications did not adequately report or propose prespecified progression criteria to inform whether to proceed to a future definitive randomised controlled trial.},
	pages = {e048178},
	number = {6},
	journaltitle = {{BMJ} Open},
	author = {Mellor, Katie and Eddy, Saskia and Peckham, Nicholas and Bond, Christine M. and Campbell, Michael J. and Lancaster, Gillian A. and Thabane, Lehana and Eldridge, Sandra M. and Dutton, Susan J. and Hopewell, Sally},
	urldate = {2021-08-31},
	date = {2021-06-01},
	langid = {english},
	pmid = {34183348},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research methods},
	keywords = {clinical trials, protocols \& guidelines, statistics \& research methods},
	file = {Full Text PDF:/Users/tom/Zotero/storage/Q2M4J2GX/Mellor et al. - 2021 - Progression from external pilot to definitive rand.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/A6DT5I4M/e048178.html:text/html},
}

@article{funder_evaluating_2019,
	title = {Evaluating Effect Size in Psychological Research: Sense and Nonsense},
	volume = {2},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245919847202},
	doi = {10.1177/2515245919847202},
	shorttitle = {Evaluating Effect Size in Psychological Research},
	abstract = {Effect sizes are underappreciated and often misinterpreted—the most common mistakes being to describe them in ways that are uninformative (e.g., using arbitrary standards) or misleading (e.g., squaring effect-size rs). We propose that effect sizes can be usefully evaluated by comparing them with well-understood benchmarks or by considering them in terms of concrete consequences. In that light, we conclude that when reliably estimated (a critical consideration), an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size r of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size r of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size (r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication. Our goal is to help advance the treatment of effect sizes so that rather than being numbers that are ignored, reported without interpretation, or interpreted superficially or incorrectly, they become aspects of research reports that can better inform the application and theoretical development of psychological research.},
	pages = {156--168},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Funder, David C. and Ozer, Daniel J.},
	urldate = {2021-08-31},
	date = {2019-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {evaluation, benchmarks, correlation, effect size},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/D8Y7KNQN/Funder and Ozer - 2019 - Evaluating Effect Size in Psychological Research .pdf:application/pdf},
}

@article{fergusson_post-randomisation_2002,
	title = {Post-randomisation exclusions: the intention to treat principle and excluding patients from analysis},
	volume = {325},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1124168/},
	shorttitle = {Post-randomisation exclusions},
	pages = {652--654},
	number = {7365},
	journaltitle = {{BMJ} : British Medical Journal},
	shortjournal = {{BMJ}},
	author = {Fergusson, Dean and Aaron, Shawn D and Guyatt, Gordon and Hébert, Paul},
	urldate = {2021-08-31},
	date = {2002-09-21},
	pmid = {12242181},
	pmcid = {PMC1124168},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/JWZVFJCQ/Fergusson et al. - 2002 - Post-randomisation exclusions the intention to tr.pdf:application/pdf},
}

@article{lonsdorf_dont_2017,
	title = {Don’t fear ‘fear conditioning’: Methodological considerations for the design and analysis of studies on human fear acquisition, extinction, and return of fear},
	volume = {77},
	issn = {0149-7634},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763416308466},
	doi = {10.1016/j.neubiorev.2017.02.026},
	shorttitle = {Don’t fear ‘fear conditioning’},
	abstract = {The so-called ‘replicability crisis’ has sparked methodological discussions in many areas of science in general, and in psychology in particular. This has led to recent endeavours to promote the transparency, rigour, and ultimately, replicability of research. Originating from this zeitgeist, the challenge to discuss critical issues on terminology, design, methods, and analysis considerations in fear conditioning research is taken up by this work, which involved representatives from fourteen of the major human fear conditioning laboratories in Europe. This compendium is intended to provide a basis for the development of a common procedural and terminology framework for the field of human fear conditioning. Whenever possible, we give general recommendations. When this is not feasible, we provide evidence-based guidance for methodological decisions on study design, outcome measures, and analyses. Importantly, this work is also intended to raise awareness and initiate discussions on crucial questions with respect to data collection, processing, statistical analyses, the impact of subtle procedural changes, and data reporting specifically tailored to the research on fear conditioning.},
	pages = {247--285},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	shortjournal = {Neuroscience \& Biobehavioral Reviews},
	author = {Lonsdorf, Tina B. and Menz, Mareike M. and Andreatta, Marta and Fullana, Miguel A. and Golkar, Armita and Haaker, Jan and Heitland, Ivo and Hermann, Andrea and Kuhn, Manuel and Kruse, Onno and Meir Drexler, Shira and Meulders, Ann and Nees, Frauke and Pittig, Andre and Richter, Jan and Römer, Sonja and Shiban, Youssef and Schmitz, Anja and Straube, Benjamin and Vervliet, Bram and Wendt, Julia and Baas, Johanna M. P. and Merz, Christian J.},
	urldate = {2021-08-31},
	date = {2017-06-01},
	langid = {english},
	keywords = {Statistics, Methods, Terminology, Individual differences, Conditioning, Extinction, Return of fear, Design, Replicability crisis},
	file = {Full Text:/Users/tom/Zotero/storage/L5NLXSSA/Lonsdorf et al. - 2017 - Don’t fear ‘fear conditioning’ Methodological con.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/SAMTV7K8/S0149763416308466.html:text/html},
}

@article{norris_science_2019,
	title = {Science as behaviour: Using a behaviour change approach to increase uptake of open science},
	volume = {34},
	issn = {0887-0446},
	url = {https://doi.org/10.1080/08870446.2019.1679373},
	doi = {10.1080/08870446.2019.1679373},
	shorttitle = {Science as behaviour},
	pages = {1397--1406},
	number = {12},
	journaltitle = {Psychology \& Health},
	author = {Norris, Emma and O’Connor, Daryl B.},
	urldate = {2021-08-31},
	date = {2019-12-02},
	pmid = {31661325},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/08870446.2019.1679373},
	file = {Full Text PDF:/Users/tom/Zotero/storage/S4JZ8IH8/Norris and O’Connor - 2019 - Science as behaviour Using a behaviour change app.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B9NKBWRK/08870446.2019.html:text/html},
}

@article{milat_intervention_2020,
	title = {Intervention Scalability Assessment Tool: A decision support tool for health policy makers and implementers},
	volume = {18},
	issn = {1478-4505},
	url = {https://doi.org/10.1186/s12961-019-0494-2},
	doi = {10.1186/s12961-019-0494-2},
	shorttitle = {Intervention Scalability Assessment Tool},
	abstract = {Promising health interventions tested in pilot studies will only achieve population-wide impact if they are implemented at scale across communities and health systems. Scaling up effective health interventions is vital as not doing so denies the community the most effective services and programmes. However, there remains a paucity of practical tools to assess the suitability of health interventions for scale-up. The Intervention Scalability Assessment Tool ({ISAT}) was developed to support policy-makers and practitioners to make systematic assessments of the suitability of health interventions for scale-up.},
	pages = {1},
	number = {1},
	journaltitle = {Health Research Policy and Systems},
	shortjournal = {Health Research Policy and Systems},
	author = {Milat, Andrew and Lee, Karen and Conte, Kathleen and Grunseit, Anne and Wolfenden, Luke and van Nassau, Femke and Orr, Neil and Sreeram, Padmaja and Bauman, Adrian},
	urldate = {2021-08-31},
	date = {2020-01-03},
	keywords = {Implementation, assessment support tool, scalability, scale-up},
	file = {Full Text PDF:/Users/tom/Zotero/storage/WZWF6Y3N/Milat et al. - 2020 - Intervention Scalability Assessment Tool A decisi.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/CT4GXXAZ/s12961-019-0494-2.html:text/html},
}

@article{craig_developing_2008,
	title = {Developing and evaluating complex interventions: the new Medical Research Council guidance},
	volume = {337},
	rights = {© {BMJ} Publishing Group Ltd 2008},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/337/bmj.a1655},
	doi = {10.1136/bmj.a1655},
	shorttitle = {Developing and evaluating complex interventions},
	abstract = {{\textless}p{\textgreater}Evaluating complex interventions is complicated. The Medical Research Council9s evaluation framework (2000) brought welcome clarity to the task. Now the council has updated its guidance{\textless}/p{\textgreater}},
	pages = {a1655},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Craig, Peter and Dieppe, Paul and Macintyre, Sally and Michie, Susan and Nazareth, Irwin and Petticrew, Mark},
	urldate = {2021-08-31},
	date = {2008-09-29},
	langid = {english},
	pmid = {18824488},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3V6UMBNR/Craig et al. - 2008 - Developing and evaluating complex interventions t.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9DR7ZTJG/bmj.html:text/html},
}

@article{minary_which_2019,
	title = {Which design to evaluate complex interventions? Toward a methodological framework through a systematic review},
	volume = {19},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-019-0736-6},
	doi = {10.1186/s12874-019-0736-6},
	shorttitle = {Which design to evaluate complex interventions?},
	abstract = {Evaluation of complex interventions ({CI}) is challenging for health researchers and requires innovative approaches. The objective of this work is to present the main methods used to evaluate {CI}.},
	pages = {92},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Minary, Laetitia and Trompette, Justine and Kivits, Joëlle and Cambon, Linda and Tarquinio, Cyril and Alla, François},
	urldate = {2021-08-31},
	date = {2019-05-07},
	keywords = {Research methods, Study design, Health behaviour, Public health},
	file = {Full Text PDF:/Users/tom/Zotero/storage/DEI45GGC/Minary et al. - 2019 - Which design to evaluate complex interventions To.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/NLBC4UVS/s12874-019-0736-6.html:text/html},
}

@article{rubenson_tie_nodate,
	title = {Tie my hands loosely: pre-analysis plans in political science},
	issn = {0730-9384, 1471-5457},
	url = {https://www.cambridge.org/core/journals/politics-and-the-life-sciences/article/abs/tie-my-hands-loosely-preanalysis-plans-in-political-science/F64D9D9957D225C8ECFB5655F833FCD9?utm_source=Twitter&utm_medium=&utm_campaign=PLS_Aug21},
	doi = {10.1017/pls.2021.23},
	shorttitle = {Tie my hands loosely},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS073093842100023X/resource/name/{firstPage}-S073093842100023Xa.jpg},
	pages = {1--24},
	journaltitle = {Politics and the Life Sciences},
	author = {Rubenson, Daniel},
	urldate = {2021-08-31},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Full Text PDF:/Users/tom/Zotero/storage/INR83PQJ/Rubenson - Tie My Hands Loosely Pre-analysis Plans in Politi.pdf:application/pdf},
}

@article{littell_campbell_2018,
	title = {The Campbell Collaboration: providing better evidence for a better world},
	volume = {28},
	issn = {1049-7315},
	url = {https://doi.org/10.1177/1049731517703748},
	doi = {10.1177/1049731517703748},
	shorttitle = {The campbell collaboration},
	abstract = {In this article, we trace the development of the Campbell Collaboration and its renewed efforts to build a world library of accurate, synthesized evidence to inform policy and practice and improve human well-being worldwide. Campbell systematic reviews and related evidence synthesis products provide unbiased summaries of entire bodies of empirical evidence, making them uniquely useful sources of information for policy and practice. With recent changes in organizational structure and new leadership, the Campbell Collaboration is poised to dramatically increase the production, dissemination, and use of rigorous syntheses of research on social, economic, and behavioral interventions. Campbell provides opportunities for social work scholars, practitioners, and consumers to contribute to knowledge about the processes and outcomes of social, behavioral, and economic interventions.},
	pages = {6--12},
	number = {1},
	journaltitle = {Research on Social Work Practice},
	shortjournal = {Research on Social Work Practice},
	author = {Littell, Julia H. and White, Howard},
	urldate = {2021-08-31},
	date = {2018-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {meta-analysis, systematic review, evidence-based practice},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/E8UDFDA9/Littell and White - 2018 - The Campbell Collaboration Providing Better Evide.pdf:application/pdf},
}

@article{scammacca_meta-analysis_2014,
	title = {Meta-analysis with complex research designs: dealing with dependence from multiple measures and multiple group comparisons},
	volume = {84},
	issn = {0034-6543},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4191743/},
	doi = {10.3102/0034654313500826},
	shorttitle = {Meta-analysis with complex research designs},
	abstract = {Previous research has shown that treating dependent effect sizes as independent inflates the variance of the mean effect size and introduces bias by giving studies with more effect sizes more weight in the meta-analysis. This article summarizes the different approaches to handling dependence that have been advocated by methodologists, some of which are more feasible to implement with education research studies than others. A case study using effect sizes from a recent meta-analysis of reading interventions is presented to compare the results obtained from different approaches to dealing with dependence. Overall, mean effect sizes and variance estimates were found to be similar, but estimates of indexes of heterogeneity varied. Meta-analysts are advised to explore the effect of the method of handling dependence on the heterogeneity estimates before conducting moderator analyses and to choose the approach to dependence that is best suited to their research question and their data set.},
	pages = {328--364},
	number = {3},
	journaltitle = {Review of educational research},
	shortjournal = {Rev Educ Res},
	author = {Scammacca, Nancy and Roberts, Greg and Stuebing, Karla K.},
	urldate = {2021-08-31},
	date = {2014-09-01},
	pmid = {25309002},
	pmcid = {PMC4191743},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/VC2W4Q2X/Scammacca et al. - 2014 - Meta-Analysis With Complex Research Designs Deali.pdf:application/pdf},
}

@article{foo_practical_nodate,
	title = {A practical guide to question formation, systematic searching and study screening for literature reviews in ecology and evolution},
	volume = {n/a},
	issn = {2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13654},
	doi = {10.1111/2041-210X.13654},
	abstract = {Well-conducted systematic reviews are invaluable for synthesising research findings. The conclusions of a review depend on how the research question was formulated, how relevant studies were found and how studies were selected for synthesis. Here, we present a practical guide for ecologists and evolutionary biologists on formulating a question for a systematic review, and finding a representative sample of research findings. We explain the steps involved using a worked example and practical training exercises. Throughout this guide we share tricks of the trade, included rules of thumb and software that we have found useful. We hope our paper helps demystify the systematic search process and encourages more researchers to adopt a systematic and reproducible approach when searching the literature.},
	issue = {n/a},
	journaltitle = {Methods in Ecology and Evolution},
	author = {Foo, Yong Zhi and O'Dea, Rose E. and Koricheva, Julia and Nakagawa, Shinichi and Lagisz, Malgorzata},
	urldate = {2021-08-31},
	langid = {english},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13654},
	keywords = {meta-analysis, systematic review, Boolean, narrative review, screening, systematic search},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NGURDD7A/Foo et al. - A practical guide to question formation, systemati.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/72HWAKW9/2041-210X.html:text/html},
}

@article{gough_clarifying_2019,
	title = {Clarifying differences between reviews within evidence ecosystems},
	volume = {8},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-019-1089-2},
	doi = {10.1186/s13643-019-1089-2},
	abstract = {This paper builds on a 2012 paper by the same authors which argued that the types and brands of systematic review do not sufficiently differentiate between the many dimensions of different review questions and review methods (Gough et al., Syst Rev 1:28, 2012). The current paper extends this argument by considering the dynamic contexts, or ‘evidence ecosystems’, within which reviews are undertaken; the fact that these ecosystems are constantly changing; and the relevance of this broader context for understanding ‘dimensions of difference’ in the unfolding development and refinement of review methods.},
	pages = {170},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Systematic Reviews},
	author = {Gough, David and Thomas, James and Oliver, Sandy},
	urldate = {2021-08-31},
	date = {2019-07-15},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7R4ZWJFF/Gough et al. - 2019 - Clarifying differences between reviews within evid.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/4CYKBYEN/s13643-019-1089-2.html:text/html},
}

@article{oxman_science_1993,
	title = {The science of reviewing research},
	volume = {703},
	issn = {1749-6632},
	url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1993.tb26342.x},
	doi = {10.1111/j.1749-6632.1993.tb26342.x},
	pages = {125--134},
	number = {1},
	journaltitle = {Annals of the New York Academy of Sciences},
	author = {Oxman, Andrew D. and Guyatt, Gordon H.},
	urldate = {2021-08-31},
	date = {1993},
	langid = {english},
	note = {\_eprint: https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.1993.tb26342.x},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7IE2CEF7/Oxman and Guyatt - 1993 - The Science of Reviewing Researcha.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/U5TPGWUB/j.1749-6632.1993.tb26342.html:text/html},
}

@article{page_letter_2019,
	title = {Letter re: stratification of meta-analyses based on risk of bias is appropriate and does not induce selection bias},
	volume = {115},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(19)30382-8/abstract},
	doi = {10.1016/j.jclinepi.2019.06.003},
	shorttitle = {Letter re},
	abstract = {We read, “Stratification by quality induces selection bias in a meta-analysis of clinical
trials” by Stone et al. [1] with interest. We think that the authors have misunderstood
the implications of their findings and that their conclusions could mislead authors
of systematic reviews.},
	pages = {175--176},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Page, Matthew J. and Hróbjartsson, Asbjørn and Hansen, Camilla and Sterne, Jonathan A. C.},
	urldate = {2021-08-31},
	date = {2019-11-01},
	pmid = {31278020},
	note = {Publisher: Elsevier},
	file = {Full Text PDF:/Users/tom/Zotero/storage/Z8YA2EIG/Page et al. - 2019 - Letter re stratification of meta-analyses based o.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ANFS7RTZ/fulltext.html:text/html},
}

@article{dai_methodological_2019,
	title = {Methodological quality of public health guideline recommendations on vitamin D and calcium : a systematic review protocol},
	volume = {9},
	rights = {© Author(s) (or their employer(s)) 2019. Re-use permitted under {CC} {BY}-{NC}. No commercial re-use. See rights and permissions. Published by {BMJ}.. This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {2044-6055, 2044-6055},
	url = {https://bmjopen.bmj.com/content/9/11/e031840},
	doi = {10.1136/bmjopen-2019-031840},
	shorttitle = {Methodological quality of public health guideline recommendations on vitamin D and calcium},
	abstract = {Introduction Current recommendations for vitamin D and calcium in dietary guidelines and bone health guidelines vary significantly among countries and professional organisations. It is unknown whether the methods used to develop these recommendations followed a rigourous process and how the differences in methods used may affect the recommended intakes of vitamin D and calcium. The objectives of this study are (1) collate and compare recommendations for vitamin D and calcium across guidelines, (2) appraise methodological quality of the guideline recommendations and (3) identify methodological factors that may affect the recommended intakes for vitamin D and calcium. This study will make a significant contribution to enhancing the methodological rigour in public health guidelines for vitamin D and calcium recommendations.
Methods and analyses We will conduct a systematic review to evaluate vitamin D and calcium recommendations for osteoporosis prevention in generally healthy middle-aged and older adults. Methodological assessment will be performed for each guideline against those outlined in the 2014 {WHO} handbook for guideline development. A systematic search strategy will be applied to locate food-based dietary guidelines and bone health guidelines indexed in various electronic databases, guideline repositories and grey literature from 1 January 2009 to 28 February 2019. Descriptive statistics will be used to summarise the data on intake recommendation and on proportion of guidelines consistent with the {WHO} criteria. Logistic regression, if feasible, will be used to assess the relationships between the methodological factors and the recommendation intakes.
Ethics and dissemination Ethics approval is not required as we will only extract published data or information from the published guidelines. Results of this review will be disseminated through conference presentations and peer-reviewed publications.
{PROSPERO} registration number {CRD}42019126452},
	pages = {e031840},
	number = {11},
	journaltitle = {{BMJ} Open},
	author = {Dai, Zhaoli and Kroeger, Cynthia M. and {McDonald}, Sally and Page, Matthew J. and {McKenzie}, Joanne E. and Allman-Farinelli, Margaret and Raubenheimer, David and Bero, Lisa},
	urldate = {2021-08-31},
	date = {2019-11-01},
	langid = {english},
	pmid = {31699738},
	note = {Publisher: British Medical Journal Publishing Group
Section: Health policy},
	keywords = {calcium, guideline development methods, public health guidelines, vitamin D},
	file = {Full Text PDF:/Users/tom/Zotero/storage/YNT4Y6M8/Dai et al. - 2019 - Methodological quality of public health guideline .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UQJGFIT5/e031840.html:text/html},
}

@article{page_methods_2015,
	title = {Methods to select results to include in meta-analyses deserve more consideration in systematic reviews},
	volume = {68},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435615001055},
	doi = {10.1016/j.jclinepi.2015.02.009},
	abstract = {Objectives
To investigate how often systematic reviewers encounter multiple trial effect estimates that are available for inclusion in a particular meta-analysis (multiplicity of results) and the methods they use to select effect estimates.
Study Design and Setting
We randomly sampled Cochrane and {MEDLINE}-indexed non-Cochrane reviews published between January 2010 and January 2012. The first presented meta-analysis of an effect measure for a continuous outcome in each review was identified, and methods to select results to include in this meta-analysis were extracted from review protocols and reviews. All effect estimates that were available for inclusion in the meta-analyses were extracted from trial reports.
Results
We examined 44 reviews. Multiplicity of results was common, occurring in 49\% of trial reports (n = 210). Prespecification of decision rules to select results from multiple measurement scales and intervention/control groups (in multi-arm trials) was uncommon (19\% and 14\% of 21 review protocols, respectively). Overall, 70\% of reviews included at least one randomized controlled trial with multiplicity of results, but this occurred less frequently in reviews with a protocol (risk difference, −25\%; 95\% confidence interval: −52\%, 1\%).
Conclusion
Systematic reviewers are likely to encounter multiplicity of results in the included trials. We recommend that systematic reviewers always consider predefining methods to select results to include in meta-analyses. Methods focusing on selection of measurement scales and how to deal with multi-arm trials would be most valuable.},
	pages = {1282--1291},
	number = {11},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Page, Matthew J. and {McKenzie}, Joanne E. and Chau, Marisa and Green, Sally E. and Forbes, Andrew},
	urldate = {2021-08-31},
	date = {2015-11-01},
	langid = {english},
	keywords = {Bias, Randomized controlled trials, Reporting, Research methodology, Systematic review, Meta-analysis},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/W58528DP/S0895435615001055.html:text/html},
}

@article{lopez-lopez_dealing_2018,
	title = {Dealing with effect size multiplicity in systematic reviews and meta-analyses},
	volume = {9},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1310},
	doi = {10.1002/jrsm.1310},
	abstract = {Systematic reviews often encounter primary studies that report multiple effect sizes based on data from the same participants. These have the potential to introduce statistical dependency into the meta-analytic data set. In this paper, we provide a tutorial on dealing with effect size multiplicity within studies in the context of meta-analyses of intervention and association studies, recommending a three-step approach. The first step is to define the research question and consider the extent to which it mainly reflects interest in mean effect sizes (which we term a convergent approach) or an interest in exploring heterogeneity (which we term a divergent approach). A second step is to identify the types of multiplicities that appear in the initial database of effect sizes relevant to the research question, and we propose a categorization scheme to differentiate them. The third step is to select a strategy for dealing with each type of multiplicity. The researcher can choose between a reductionist meta-analytic approach, which is characterized by inclusion of a single effect size per study, and an integrative approach, characterized by inclusion of multiple effect sizes per study. We present an overview of available analysis strategies for dealing with effect size multiplicity within studies and provide recommendations intended to help researchers decide which strategy might be preferable in particular situations. Last, we offer caveats and cautions about addressing the challenges multiplicity poses for systematic reviews and meta-analyses.},
	pages = {336--351},
	number = {3},
	journaltitle = {Research Synthesis Methods},
	author = {López-López, José A. and Page, Matthew J. and Lipsey, Mark W. and Higgins, Julian P. T.},
	urldate = {2021-08-31},
	date = {2018},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1310},
	keywords = {meta-analysis, systematic review, effect size, multiplicity, dependency},
	file = {Full Text PDF:/Users/tom/Zotero/storage/GUTYSA7U/López-López et al. - 2018 - Dealing with effect size multiplicity in systemati.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ELCZQ5IJ/jrsm.html:text/html},
}

@article{may_bias_2020,
	title = {Bias in Science: Natural and Social},
	doi = {10.1007/s11229-020-02937-0},
	shorttitle = {Bias in Science},
	pages = {1--22},
	journaltitle = {Synthese},
	author = {May, Joshua},
	date = {2020},
	note = {Publisher: Springer (Springer Science+Business Media B.V.)},
	file = {Snapshot:/Users/tom/Zotero/storage/3AMN6KWW/MAYBIS.html:text/html;Submitted Version:/Users/tom/Zotero/storage/2YAQAAVV/May - 2020 - Bias in Science Natural and Social.pdf:application/pdf},
}

@article{trafimow_badly_2016,
	title = {Badly specified theories are not responsible for the replication crisis in social psychology: Comment on Klein},
	volume = {26},
	issn = {0959-3543},
	url = {https://doi.org/10.1177/0959354316637136},
	doi = {10.1177/0959354316637136},
	shorttitle = {Badly specified theories are not responsible for the replication crisis in social psychology},
	abstract = {Klein (2014) argues that the replication crisis in social psychology is due—at least in large part—to the tendency of psychological theories to be ill-specified. We disagree. First, we use both historical and contemporary examples to show that high-quality replication is possible even in the absence of a well-specified theory; and, second, we argue that it is typically auxiliary assumptions, rather than theories themselves, that need to be more clearly specified in order to understand the implications of a given replication effort.},
	pages = {540--548},
	number = {4},
	journaltitle = {Theory \& Psychology},
	shortjournal = {Theory \& Psychology},
	author = {Trafimow, David and Earp, Brian D.},
	urldate = {2021-08-31},
	date = {2016-08-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd},
	keywords = {replication crisis, theory, toread, auxiliary assumptions, historical examples},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/5JV4AV4A/Trafimow and Earp - 2016 - Badly specified theories are not responsible for t.pdf:application/pdf},
}

@article{rohrer_precise_2021,
	title = {Precise answers to vague questions: issues with interactions},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/25152459211007368},
	doi = {10.1177/25152459211007368},
	shorttitle = {Precise answers to vague questions},
	abstract = {Psychological theories often invoke interactions but remain vague regarding the details. As a consequence, researchers may not know how to properly test them and may potentially run analyses that reliably return the wrong answer to their research question. We discuss three major issues regarding the prediction and interpretation of interactions. First, interactions can be removable in the sense that they appear or disappear depending on scaling decisions, with consequences for a variety of situations (e.g., binary or categorical outcomes, bounded scales with floor and ceiling effects). Second, interactions may be conceptualized as changes in slope or changes in correlations, and because these two phenomena do not necessarily coincide, researchers might draw wrong conclusions. Third, interactions may or may not be causally identified, and this determines which interpretations are valid. Researchers who remain unaware of these distinctions might accidentally analyze their data in a manner that returns the technically correct answer to the wrong question. We illustrate all issues with examples from psychology and issue recommendations for how to best address them in a productive manner.},
	pages = {25152459211007368},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Rohrer, Julia M. and Arslan, Ruben C.},
	urldate = {2021-08-31},
	date = {2021-04-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {open materials, causality, assumptions, interaction, measurement scale},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/PW4536R6/Rohrer and Arslan - 2021 - Precise Answers to Vague Questions Issues With In.pdf:application/pdf},
}

@article{fiedler_what_2017,
	title = {What constitutes strong psychological science? The (neglected) role of diagnosticity and a priori theorizing},
	volume = {12},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691616654458},
	doi = {10.1177/1745691616654458},
	shorttitle = {What constitutes strong psychological science?},
	abstract = {A Bayesian perspective on Ioannidis’s (2005) memorable statement that “Most Published Research Findings Are False” suggests a seemingly inescapable trade-off: It appears as if research hypotheses are based either on safe ground (high prior odds), yielding valid but unsurprising results, or on unexpected and novel ideas (low prior odds), inspiring risky and surprising findings that are inevitably often wrong. Indeed, research of two prominent types, sexy hypothesis testing and model testing, is often characterized by low priors (due to astounding hypotheses and conjunctive models) as well as low-likelihood ratios (due to nondiagnostic predictions of the yin-or-yang type). However, the trade-off is not inescapable: An alternative research approach, theory-driven cumulative science, aims at maximizing both prior odds and diagnostic hypothesis testing. The final discussion emphasizes the value of pluralistic science, within which exploratory phenomenon-driven research can play a similarly strong part as strict theory-testing science.},
	pages = {46--61},
	number = {1},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Fiedler, Klaus},
	urldate = {2021-08-31},
	date = {2017-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {toread, model testing, phenomenon-driven research, sexy-hypothesis testing, theory-driven cumulative science},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/87N878ED/Fiedler - 2017 - What Constitutes Strong Psychological Science The.pdf:application/pdf},
}

@article{gampa_ideological_2019,
	title = {(Ideo)Logical Reasoning: Ideology Impairs Sound Reasoning},
	volume = {10},
	issn = {1948-5506},
	url = {https://doi.org/10.1177/1948550619829059},
	doi = {10.1177/1948550619829059},
	shorttitle = {(Ideo)Logical Reasoning},
	abstract = {Beliefs shape how people interpret information and may impair how people engage in logical reasoning. In three studies, we show how ideological beliefs impair people’s ability to (1) recognize logical validity in arguments that oppose their political beliefs and (2) recognize the lack of logical validity in arguments that support their political beliefs. We observed belief bias effects among liberals and conservatives who evaluated the logical soundness of classically structured logical syllogisms supporting liberal or conservative beliefs. Both liberals and conservatives frequently evaluated the logical structure of entire arguments based on the believability of arguments’ conclusions, leading to predictable patterns of logical errors. As a result, liberals were better at identifying flawed arguments supporting conservative beliefs and conservatives were better at identifying flawed arguments supporting liberal beliefs. These findings illuminate one key mechanism for how political beliefs distort people’s abilities to reason about political topics soundly.},
	pages = {1075--1083},
	number = {8},
	journaltitle = {Social Psychological and Personality Science},
	shortjournal = {Social Psychological and Personality Science},
	author = {Gampa, Anup and Wojcik, Sean P. and Motyl, Matt and Nosek, Brian A. and Ditto, Peter H.},
	urldate = {2021-09-03},
	date = {2019-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {belief bias, ideology, logical reasoning, political psychology, syllogistic reasoning},
	file = {Submitted Version:/Users/tom/Zotero/storage/XJ6VZZ6D/Gampa et al. - 2019 - (Ideo)Logical Reasoning Ideology Impairs Sound Re.pdf:application/pdf},
}

@article{juni_assessing_2001-1,
	title = {Assessing the quality of controlled clinical trials},
	volume = {323},
	rights = {© 2001 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/323/7303/42},
	doi = {10.1136/bmj.323.7303.42},
	abstract = {This is the first in a series of four articles 

The quality of controlled trials is of obvious relevance to systematic reviews. If the “raw material” is flawed then the conclusions of systematic reviews cannot be trusted. Many reviewers formally assess the quality of primary trials by following the recommendations of the Cochrane Collaboration and other experts. 1 2 However, the methodology for both the assessment of quality and its incorporation into systematic reviews and meta-analysis are a matter of ongoing debate.3-5 In this article we discuss the concept of study quality and the methods used to assess quality.

\#\#\#\# Components of internal and external validity of controlled clinical trials

Internal validity —extent to which systematic error (bias) is minimised in clinical trials

Quality is a multidimensional concept, which could relate to the design, conduct, and analysis of a trial, its clinical relevance, or quality of reporting.6 The validity of the findings generated by a study clearly is an important dimension of quality. In the 1950s the social scientist Campbell proposed a useful distinction between internal and external validity (see box below). 7 8 Internal validity implies that the differences observed between groups of patients allocated to different …},
	pages = {42--46},
	number = {7303},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Jüni, Peter and Altman, Douglas G. and Egger, Matthias},
	urldate = {2021-09-09},
	date = {2001-07-07},
	langid = {english},
	pmid = {11440947},
	note = {Publisher: British Medical Journal Publishing Group
Section: Education and debate},
	file = {Full Text PDF:/Users/tom/Zotero/storage/JI6CE4M6/Jüni et al. - 2001 - Assessing the quality of controlled clinical trial.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/2GT9DQJV/42.html:text/html},
}

@article{kennedy-shaffer_before_2019,
	title = {Before p {\textless} 0.05 to Beyond p {\textless} 0.05: Using History to Contextualize p-Values and Significance Testing},
	volume = {73},
	issn = {0003-1305},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6693672/},
	doi = {10.1080/00031305.2018.1537891},
	shorttitle = {Before p {\textless} 0.05 to Beyond p {\textless} 0.05},
	abstract = {As statisticians and scientists consider a world beyond p {\textless} 0.05, it is important to not lose sight of how we got to this point. Although significance testing and p-values are often presented as prescriptive procedures, they came about through a process of refinement and extension to other disciplines. Ronald A. Fisher and his contemporaries formalized these methods in the early twentieth century and Fisher’s 1925 Statistical Methods for Research Workers brought the techniques to experimentalists in a variety of disciplines. Understanding how these methods arose, spread, and were argued over since then illuminates how p {\textless} 0.05 came to be a standard for scientific inference, the advantage it offered at the time, and how it was interpreted. This historical perspective can inform the work of statisticians today by encouraging thoughtful consideration of how their work, including proposed alternatives to the p-value, will be perceived and used by scientists. And it can engage students more fully and encourage critical thinking rather than rote applications of formulae. Incorporating history enables students, practitioners, and statisticians to treat the discipline as an ongoing endeavor, crafted by fallible humans, and provides a deeper understanding of the subject and its consequences for science and society.},
	pages = {82--90},
	issue = {Suppl 1},
	journaltitle = {The American statistician},
	shortjournal = {Am Stat},
	author = {Kennedy-Shaffer, Lee},
	urldate = {2021-09-09},
	date = {2019},
	pmid = {31413381},
	pmcid = {PMC6693672},
	keywords = {toread},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/5M7ZKZUQ/Kennedy-Shaffer - 2019 - Before p  0.05 to Beyond p  0.05 Using History .pdf:application/pdf},
}

@article{moher_ensuring_2021,
	title = {Ensuring the success of data sharing in Canada},
	volume = {6},
	url = {https://www.facetsjournal.com/doi/full/10.1139/facets-2021-0031},
	doi = {10.1139/facets-2021-0031},
	pages = {1534--1538},
	journaltitle = {{FACETS}},
	author = {Moher, David and Cobey, Kelly D.},
	urldate = {2021-09-09},
	date = {2021-01-01},
	note = {Publisher: Canadian Science Publishing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/SXAQISYU/Moher and Cobey - 2021 - Ensuring the success of data sharing in Canada.pdf:application/pdf},
}

@article{simonsohn_just_2013,
	title = {Just post it: the lesson from two cases of fabricated data detected by statistics alone},
	volume = {24},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797613480366},
	doi = {10.1177/0956797613480366},
	shorttitle = {Just post it},
	abstract = {I argue that requiring authors to post the raw data supporting their published results has the benefit, among many others, of making fraud much less likely to go undetected. I illustrate this point by describing two cases of suspected fraud I identified exclusively through statistical analysis of reported means and standard deviations. Analyses of the raw data behind these published results provided invaluable confirmation of the initial suspicions, ruling out benign explanations (e.g., reporting errors, unusual distributions), identifying additional signs of fabrication, and also ruling out one of the suspected fraud’s explanations for his anomalous results. If journals, granting agencies, universities, or other entities overseeing research promoted or required data posting, it seems inevitable that fraud would be reduced.},
	pages = {1875--1888},
	number = {10},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Simonsohn, Uri},
	urldate = {2021-09-10},
	date = {2013-10-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {data sharing, decision making, judgment, data posting, fake data, scientific communication},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/REP5KEEA/Simonsohn - 2013 - Just Post It The Lesson From Two Cases of Fabrica.pdf:application/pdf},
}

@article{voytek_virtuous_2016,
	title = {The virtuous cycle of a data ecosystem},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005037},
	doi = {10.1371/journal.pcbi.1005037},
	pages = {e1005037},
	number = {8},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Voytek, Bradley},
	urldate = {2021-09-10},
	date = {2016-08-04},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Social psychology, Peer review, Metaanalysis, Scientists, Culture, Ecosystems, Brain electrophysiology, Data mining},
	file = {Full Text PDF:/Users/tom/Zotero/storage/BTKXGR4E/Voytek - 2016 - The Virtuous Cycle of a Data Ecosystem.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/2QBE3S8R/article.html:text/html},
}

@article{anderson_biasing_2021,
	title = {Biasing the input: A yoked-scientist demonstration of the distorting effects of optional stopping on Bayesian inference},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-021-01618-1},
	doi = {10.3758/s13428-021-01618-1},
	shorttitle = {Biasing the input},
	abstract = {Prior work by Michael R. Dougherty and colleagues (Yu et al., 2014) shows that when a scientist monitors the p value during data collection and uses a critical p as the signal to stop collecting data, the resulting p is distorted due to Type I error-rate inflation. They argued similarly that the use of a critical Bayes factor ({BF}(crit)) for stopping distorts the obtained Bayes factor ({BF}), a position that has met with controversy. The present paper clarified that when {BF}(crit) is used as a stopping criterion, the sample becomes biased in that data consistent with large effects have a greater chance to be included than do other data, thus biasing the input to Bayesian inference. We report simulations of yoked pairs of scientists in which Scientist A uses {BF}(crit) to optionally stop, while Scientist B, sampling from the same population, stops when A stops. Thus, optional stopping is compared not to a hypothetical in which no stopping occurs, but to a situation in which B stops for reasons unrelated to the characteristics of B's sample. The results indicated that optional stopping biased the input for Bayesian inference. We also simulated the use of effect-size stabilization as a stopping criterion and found no bias in that case.},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Anderson, Richard B. and Crawford, Jennifer C. and Bailey, Michael H.},
	urldate = {2021-09-11},
	date = {2021-09-07},
	langid = {english},
	file = {Anderson et al. - 2021 - Biasing the input A yoked-scientist demonstration.pdf:/Users/tom/Zotero/storage/ZP24T9LC/Anderson et al. - 2021 - Biasing the input A yoked-scientist demonstration.pdf:application/pdf},
}

@article{cairo_gray_2020,
	title = {Gray (literature) matters: evidence of selective hypothesis reporting in social psychological research},
	volume = {46},
	issn = {0146-1672},
	url = {https://doi.org/10.1177/0146167220903896},
	doi = {10.1177/0146167220903896},
	shorttitle = {Gray (literature) matters},
	abstract = {Selective reporting practices ({SRPs})?adding, dropping, or altering study elements when preparing reports for publication?are thought to increase false positives in scientific research. Yet analyses of {SRPs} have been limited to self-reports or analyses of pre-registered and published studies. To assess {SRPs} in social psychological research more broadly, we compared doctoral dissertations defended between 1999 and 2017 with the publications based on those dissertations. Selective reporting occurred in nearly 50\% of studies. Fully supported dissertation hypotheses were 3 times more likely to be published than unsupported hypotheses, while unsupported hypotheses were nearly 4 times more likely to be dropped from publications. Few hypotheses were found to be altered or added post hoc. Dissertation studies with fewer supported hypotheses were more likely to remove participants or measures from publications. Selective hypothesis reporting and dropped measures significantly predicted greater hypothesis support in published studies, supporting concerns that {SRPs} may increase Type 1 error risk.},
	pages = {1344--1362},
	number = {9},
	journaltitle = {Personality and Social Psychology Bulletin},
	shortjournal = {Pers Soc Psychol Bull},
	author = {Cairo, Athena H. and Green, Jeffrey D. and Forsyth, Donelson R. and Behler, Anna Maria C. and Raldiris, Tarah L.},
	urldate = {2021-09-13},
	date = {2020-09-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/A3SFUCFF/Cairo et al. - 2020 - Gray (Literature) Matters Evidence of Selective H.pdf:application/pdf},
}

@article{bruce_impact_2016,
	title = {Impact of interventions to improve the quality of peer review of biomedical journals: a systematic review and meta-analysis},
	volume = {14},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-016-0631-5},
	doi = {10.1186/s12916-016-0631-5},
	shorttitle = {Impact of interventions to improve the quality of peer review of biomedical journals},
	abstract = {The peer review process is a cornerstone of biomedical research. We aimed to evaluate the impact of interventions to improve the quality of peer review for biomedical publications.},
	pages = {85},
	number = {1},
	journaltitle = {{BMC} Medicine},
	shortjournal = {{BMC} Medicine},
	author = {Bruce, Rachel and Chauvin, Anthony and Trinquart, Ludovic and Ravaud, Philippe and Boutron, Isabelle},
	urldate = {2021-09-14},
	date = {2016-06-10},
	keywords = {Systematic review, Meta-analysis, Peer reviewers, Peer review process},
	file = {Full Text PDF:/Users/tom/Zotero/storage/6RF95I6S/Bruce et al. - 2016 - Impact of interventions to improve the quality of .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VFCCWF23/s12916-016-0631-5.html:text/html},
}

@article{van_calster_methodology_2021,
	title = {Methodology over metrics: current scientific standards are a disservice to patients and society},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435621001700},
	doi = {10.1016/j.jclinepi.2021.05.018},
	shorttitle = {Methodology over metrics},
	abstract = {Covid-19 research made it painfully clear that the scandal of poor medical research, as denounced by Altman in 1994, persists today. The overall quality of medical research remains poor, despite longstanding criticisms. The problems are well known, but the research community fails to properly address them. We suggest that most problems stem from an underlying paradox: although methodology is undeniably the backbone of high-quality and responsible research, science consistently undervalues methodology. The focus remains more on the destination (research claims and metrics) than on the journey. Notwithstanding, research should serve society more than the reputation of those involved. While we notice that many initiatives are being established to improve components of the research cycle, these initiatives are too disjointed. The overall system is monolithic and slow to adapt. We assert that top-down action is needed from journals, universities, funders and governments to break the cycle and put methodology first. These actions should involve the widespread adoption of registered reports, balanced research funding between innovative, incremental and methodological research projects, full recognition and demystification of peer review, improved methodological review of reports, adherence to reporting guidelines, and investment in methodological education and research. Currently, the scientific enterprise is doing a major disservice to patients and society.},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Van Calster, Ben and Wynants, Laure and Riley, Richard D and van Smeden, Maarten and Collins, Gary S},
	urldate = {2021-09-14},
	date = {2021-05-30},
	langid = {english},
	keywords = {Reporting, Methodology, Research quality},
	file = {Full Text:/Users/tom/Zotero/storage/KRX9V3UH/Van Calster et al. - 2021 - Methodology over metrics current scientific stand.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/TXXAY6HB/S0895435621001700.html:text/html},
}

@article{bastian_stronger_2014,
	title = {A stronger post-publication culture is needed for better science},
	volume = {11},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001772},
	doi = {10.1371/journal.pmed.1001772},
	abstract = {Hilda Bastian considers post-publication commenting and the cultural changes that are needed to better capture this intellectual effort. Please see later in the article for the Editors' Summary},
	pages = {e1001772},
	number = {12},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Bastian, Hilda},
	urldate = {2021-09-15},
	date = {2014-12-30},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Citation analysis, Science policy, Internet, Peer review, Scientists, Culture, Research quality assessment, Sexual and gender issues},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ST69PGDN/Bastian - 2014 - A Stronger Post-Publication Culture Is Needed for .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/NMFJYXG6/article.html:text/html},
}

@report{allard_science_2021,
	title = {Science needs systematic replicability audits},
	url = {https://psyarxiv.com/wusdr/},
	abstract = {The credibility revolution in social science has highlighted the importance of conducting replication studies. Despite this growing awareness, the value of direct replications is still hotly debated. In this article, we identify three main functions served by replication. We argue that replications are valuable when they target important or influential studies, when they provide a general estimate of the replicability rate of a population of published articles, and when they create incentives favoring replicable research. We therefore argue that the scientific community should organize systematic large-scale replication audits of two subsets of journals’ published articles: a subset of the most-cited articles, and a subset of randomly selected articles that would provide an estimate of the replicability of the journals' articles. These replicability audits should pave the way for more general quality audits of scientific journals.},
	institution = {{PsyArXiv}},
	author = {Allard, Aurélien and Vazire, Simine},
	urldate = {2021-09-15},
	date = {2021-09-15},
	doi = {10.31234/osf.io/wusdr},
	note = {type: article},
	keywords = {Meta-science, Open Science, Philosophy of Science, Replicability Crisis, Replications, Science Audits},
	file = {Full Text PDF:/Users/tom/Zotero/storage/54KLAGRH/Allard and Vazire - 2021 - Science Needs Systematic Replicability Audits.pdf:application/pdf},
}

@article{wakeling_no_2020,
	title = {‘No comment’? A study of commenting on {PLOS} articles},
	volume = {46},
	issn = {0165-5515},
	url = {https://doi.org/10.1177/0165551518819965},
	doi = {10.1177/0165551518819965},
	shorttitle = {‘No comment’?},
	abstract = {Article–commenting functionality allows users to add publicly visible comments to an article on a publisher’s website. As well as facilitating forms of post-publication peer review, for publishers of open-access mega-journals (large, broad scope, open-access journals that seek to publish all technically or scientifically sound research) comments are also thought to serve as a means for the community to discuss and communicate the significance and novelty of the research, factors which are not assessed during peer review. In this article we present the results of an analysis of commenting on articles published by the Public Library of Science ({PLOS}), publisher of the first and best-known mega-journal {PLOS} {ONE}, between 2003 and 2016. We find that while overall commenting rates are low, and have declined since 2010, there is substantial variation across different {PLOS} titles. Using a typology of comments developed for this research, we also find that only around half of comments engage in an academic discussion of the article and that these discussions are most likely to focus on the paper’s technical soundness. Our results suggest that publishers are yet to encourage significant numbers of readers to leave comments, with implications for the effectiveness of commenting as a means of collecting and communicating community perceptions of an article’s importance.},
	pages = {82--100},
	number = {1},
	journaltitle = {Journal of Information Science},
	shortjournal = {Journal of Information Science},
	author = {Wakeling, Simon and Willett, Peter and Creaser, Claire and Fry, Jenny and Pinfield, Stephen and Spezi, Valerie and Bonne, Marc and Founti, Christina and Medina Perea, Itzelle},
	urldate = {2021-09-15},
	date = {2020-02-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd},
	keywords = {Commenting, mega-journal, {PLOS}},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/4CMW5YE6/Wakeling et al. - 2020 - ‘No comment’ A study of commenting on PLOS articl.pdf:application/pdf},
}

@article{hardwicke_postretrieval_2016,
	title = {Postretrieval new learning does not reliably induce human memory updating via reconsolidation},
	volume = {113},
	rights = {©  . http://www.pnas.org/preview\_site/misc/userlicense.xhtml},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/113/19/5206},
	doi = {10.1073/pnas.1601440113},
	abstract = {Reconsolidation theory proposes that retrieval can destabilize an existing memory trace, opening a time-dependent window during which that trace is amenable to modification. Support for the theory is largely drawn from nonhuman animal studies that use invasive pharmacological or electroconvulsive interventions to disrupt a putative postretrieval restabilization (“reconsolidation”) process. In human reconsolidation studies, however, it is often claimed that postretrieval new learning can be used as a means of “updating” or “rewriting” existing memory traces. This proposal warrants close scrutiny because the ability to modify information stored in the memory system has profound theoretical, clinical, and ethical implications. The present study aimed to replicate and extend a prominent 3-day motor-sequence learning study [Walker {MP}, Brakefield T, Hobson {JA}, Stickgold R (2003) Nature 425(6958):616–620] that is widely cited as a convincing demonstration of human reconsolidation. However, in four direct replication attempts (n = 64), we did not observe the critical impairment effect that has previously been taken to indicate disruption of an existing motor memory trace. In three additional conceptual replications (n = 48), we explored the broader validity of reconsolidation-updating theory by using a declarative recall task and sequences similar to phone numbers or computer passwords. Rather than inducing vulnerability to interference, memory retrieval appeared to aid the preservation of existing sequence knowledge relative to a no-retrieval control group. These findings suggest that memory retrieval followed by new learning does not reliably induce human memory updating via reconsolidation.},
	pages = {5206--5211},
	number = {19},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Hardwicke, Tom E. and Taqi, Mahdi and Shanks, David R.},
	urldate = {2021-09-15},
	date = {2016-05-10},
	langid = {english},
	pmid = {27114514},
	note = {Publisher: National Academy of Sciences
Section: Social Sciences},
	keywords = {replication, memory updating, reconsolidation, forgetting, sequence learning},
	file = {Full Text PDF:/Users/tom/Zotero/storage/TJ7RPHUI/Hardwicke et al. - 2016 - Postretrieval new learning does not reliably induc.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/E99FX7FE/5206.html:text/html},
}

@article{hardwicke_reply_2016,
	title = {Reply to Walker and Stickgold: Proposed boundary conditions on memory reconsolidation will require empirical verification},
	volume = {113},
	rights = {©  . http://www.pnas.org/preview\_site/misc/userlicense.xhtml},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/113/28/E3993},
	doi = {10.1073/pnas.1608235113},
	shorttitle = {Reply to Walker and Stickgold},
	abstract = {Broadly speaking, nonreplications of the kind we reported (1) can occur for three reasons: ( i ) the original finding was a false-positive, ( ii ) the replication was a false-negative, or ( iii ) some unanticipated variable moderated the effect. The authors of the original study, Walker and Stickgold (2), now propose several potential moderators (3).

We welcome such discussion; however, we should note that both parties are now “hypothesizing after the results are known” (4), and are therefore in an exploratory (“hypothesis-generating”), rather than confirmatory (“hypothesis-testing”) phase of scientific inquiry (5). Any post hoc conjectures will require empirical verification.

Walker and Stickgold (3) note that participant age and session time had a … 

[↵][1]1To whom correspondence should be addressed. Email: t.hardwicke.12\{at\}ucl.ac.uk.

 [1]: \#xref-corresp-1-1},
	pages = {E3993--E3994},
	number = {28},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Hardwicke, Tom E. and Shanks, David R.},
	urldate = {2021-09-15},
	date = {2016-07-12},
	langid = {english},
	pmid = {27364010},
	note = {Publisher: National Academy of Sciences
Section: Letter},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3KQMWDFV/Hardwicke and Shanks - 2016 - Reply to Walker and Stickgold Proposed boundary c.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B5Z3CUR6/E3993.html:text/html},
}

@article{leys_how_2019,
	title = {How to Classify, Detect, and Manage Univariate and Multivariate Outliers, With Emphasis on Pre-Registration},
	volume = {32},
	rights = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2397-8570},
	url = {http://www.rips-irsp.com/articles/10.5334/irsp.289/},
	doi = {10.5334/irsp.289},
	abstract = {Researchers often lack knowledge about how to deal with outliers when analyzing their data. Even more frequently, researchers do not pre-specify how they plan to manage outliers. In this paper we aim to improve research practices by outlining what you need to know about outliers. We start by providing a functional definition of outliers. We then lay down an appropriate nomenclature/classification of outliers. This nomenclature is used to understand what kinds of outliers can be encountered and serves as a guideline to make appropriate decisions regarding the conservation, deletion, or recoding of outliers. These decisions might impact the validity of statistical inferences as well as the reproducibility of our experiments. To be able to make informed decisions about outliers you first need proper detection tools. We remind readers why the most common outlier detection methods are problematic and recommend the use of the median absolute deviation to detect univariate outliers, and of the Mahalanobis-{MCD} distance to detect multivariate outliers. An R package was created that can be used to easily perform these detection tests. Finally, we promote the use of pre-registration to avoid flexibility in data analysis when handling outliers.

 

Publishers note: due to a typesetting error, this paper was originally published with incorrect table numbering, where tables 2, 3, and 4 were incorrectly labelled. This was corrected soon after publication.},
	pages = {5},
	number = {1},
	journaltitle = {International Review of Social Psychology},
	author = {Leys, Christophe and Delacre, Marie and Mora, Youri L. and Lakens, Daniël and Ley, Christophe},
	urldate = {2021-09-16},
	date = {2019-04-30},
	langid = {english},
	note = {Number: 1
Publisher: Ubiquity Press},
	keywords = {preregistration, outliers, Malahanobis distance, median absolute deviation, minimum covariance determinant, robust detection},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PII2SUD6/Leys et al. - 2019 - How to Classify, Detect, and Manage Univariate and.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AGBBI8DH/irsp.289.html:text/html},
}

@article{usui_meta-analysis_2021,
	title = {Meta-analysis of variation suggests that embracing variability improves both replicability and generalizability in preclinical research},
	volume = {19},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001009},
	doi = {10.1371/journal.pbio.3001009},
	abstract = {The replicability of research results has been a cause of increasing concern to the scientific community. The long-held belief that experimental standardization begets replicability has also been recently challenged, with the observation that the reduction of variability within studies can lead to idiosyncratic, lab-specific results that cannot be replicated. An alternative approach is to, instead, deliberately introduce heterogeneity, known as “heterogenization” of experimental design. Here, we explore a novel perspective in the heterogenization program in a meta-analysis of variability in observed phenotypic outcomes in both control and experimental animal models of ischemic stroke. First, by quantifying interindividual variability across control groups, we illustrate that the amount of heterogeneity in disease state (infarct volume) differs according to methodological approach, for example, in disease induction methods and disease models. We argue that such methods may improve replicability by creating diverse and representative distribution of baseline disease state in the reference group, against which treatment efficacy is assessed. Second, we illustrate how meta-analysis can be used to simultaneously assess efficacy and stability (i.e., mean effect and among-individual variability). We identify treatments that have efficacy and are generalizable to the population level (i.e., low interindividual variability), as well as those where there is high interindividual variability in response; for these, latter treatments translation to a clinical setting may require nuance. We argue that by embracing rather than seeking to minimize variability in phenotypic outcomes, we can motivate the shift toward heterogenization and improve both the replicability and generalizability of preclinical research.},
	pages = {e3001009},
	number = {5},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Usui, Takuji and Macleod, Malcolm R. and {McCann}, Sarah K. and Senior, Alistair M. and Nakagawa, Shinichi},
	urldate = {2021-09-17},
	date = {2021-05-19},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Drug therapy, Animal models, Experimental design, Metaanalysis, Animal studies, Publication ethics, Hypothermia, Ischemic stroke},
	file = {Full Text PDF:/Users/tom/Zotero/storage/U8WHRMXB/Usui et al. - 2021 - Meta-analysis of variation suggests that embracing.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9RZ8LP5H/article.html:text/html},
}

@article{superchi_tools_2019,
	title = {Tools used to assess the quality of peer review reports: a methodological systematic review},
	volume = {19},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-019-0688-x},
	doi = {10.1186/s12874-019-0688-x},
	shorttitle = {Tools used to assess the quality of peer review reports},
	abstract = {A strong need exists for a validated tool that clearly defines peer review report quality in biomedical research, as it will allow evaluating interventions aimed at improving the peer review process in well-performed trials. We aim to identify and describe existing tools for assessing the quality of peer review reports in biomedical research.},
	pages = {48},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Superchi, Cecilia and González, José Antonio and Solà, Ivan and Cobo, Erik and Hren, Darko and Boutron, Isabelle},
	urldate = {2021-09-18},
	date = {2019-03-06},
	keywords = {Systematic review, Methods, Peer review, Quality control, Report},
	file = {Snapshot:/Users/tom/Zotero/storage/53TFV9FK/s12874-019-0688-x.html:text/html;Superchi et al. - 2019 - Tools used to assess the quality of peer review re.pdf:/Users/tom/Zotero/storage/SPKM7TAJ/Superchi et al. - 2019 - Tools used to assess the quality of peer review re.pdf:application/pdf},
}

@article{day_use_2002,
	title = {The use of dedicated methodology and statistical reviewers for peer review: A content analysis of comments to authors made by methodology and regular reviewers},
	volume = {40},
	issn = {0196-0644},
	url = {https://www.sciencedirect.com/science/article/pii/S0196064402000483},
	doi = {10.1067/mem.2002.127326},
	shorttitle = {The use of dedicated methodology and statistical reviewers for peer review},
	abstract = {Study objective: In 1997, Annals of Emergency Medicine initiated a protocol by which every original research article, in addition to each regular review, was concurrently evaluated by 1 of 2 methodology and statistical reviewers. We characterized and contrasted comments made by the methodology and regular peer reviewers. Methods: After pilot testing, interrater reliability assessment, and revision, we finalized a 99-item taxonomy of reviewer comments organized in 8 categories. Two authors, uninvolved in the writing of reviews, classified each comment from a random sample of methodology reviews from 1999. For 30 of these reviews (15 for each methodology reviewer), the 2 authors also scored all (range 2 to 5) regular reviews. Results: Sixty-five reviews by methodologist A, 60 by methodologist B, and 68 by regular reviewers were analyzed. Comments by methodologist A most frequently concerned the presentation of results (33\% of all comments) and methods (17\%). Methodologist B commented most frequently on presentation of results (28\%) and statistical methods (16\%). Regular reviewers most frequently made non-methodology/statistical comments (45\%) and comments on presentation of results (18\%). Of note, comments made by methodology and regular reviewers about methods issues were often contradictory. Conclusion: The distributions of comments made by the 2 methodology and statistical reviewers were similar, although reviewer A emphasized presentation and reviewer B stressed statistical issues. The regular reviewers (most of whom were unaware that a dedicated methodology and statistical reviewer would be reviewing the article) paid much less attention to methodology issues. The 2 dedicated methodology and statistical reviewers created reviews that were similarly focused and emphasized methodology issues that were distinct from the issues raised by regular reviewers. [Ann Emerg Med. 2002;40:329-333.]},
	pages = {329--333},
	number = {3},
	journaltitle = {Annals of Emergency Medicine},
	shortjournal = {Annals of Emergency Medicine},
	author = {Day, Frank C. and Schriger, David L. and Todd, Christopher and Wears, Robert L.},
	urldate = {2021-09-19},
	date = {2002-09-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/SAQ9F5HB/S0196064402000483.html:text/html;The use of dedicated methodology and statistical reviewers for peer review\: A content analysis of comments to authors made by methodology and regular reviewers:/Users/tom/Zotero/storage/94E7RJNB/day2002.pdf.pdf:application/pdf},
}

@article{godlee_making_2002,
	title = {Making reviewers visible: openness, accountability, and credit},
	volume = {287},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.287.21.2762},
	doi = {10.1001/jama.287.21.2762},
	abstract = {Anonymity for peer reviewers remains the overwhelming norm within biomedical journals. While acknowledging that open review is not without challenges, this article presents 4 key arguments in its favor: (1) ethical superiority, (2) lack of important adverse effects, (3) feasibility in practice, and (4) potential to balance greater accountability for reviewers with credit for the work they do. Barriers to more widespread use of open review include conservatism within the research community and the fact that openness makes editors publicly responsible for their choice of reviewers and their interpretation of reviewers' comments. Forces for change include the growing use of preprint servers combined with open commentary. I look forward to a time when open commentary and review replace the current, flawed system of closed prepublication peer review and its false reassurances about the reliability of what is published.},
	pages = {2762--2765},
	number = {21},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Godlee, Fiona},
	urldate = {2021-09-19},
	date = {2002-06-05},
	file = {Making Reviewers VisibleOpenness, Accountability, and Credit:/Users/tom/Zotero/storage/Z43D9EHL/godlee2002.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9ELUDPR3/194992.html:text/html},
}

@article{campbell_reforms_1969,
	title = {Reforms as experiments},
	volume = {24},
	issn = {1935-990X},
	doi = {10.1037/h0027982},
	abstract = {Contends that programs of social reform are not effectively assessed. This article is a preliminary effort in examining the sources of this condition and designing ways of overcoming the difficulties. The political setting of program evaluation is also considered. It is concluded that trapped and experimental administrators are not threatened by a hard-headed analysis of the reform. For such, proper administrative decisions can lay the base for useful experimental or quasi-experimental analyses. Through the ideology of allocating scarce resources by lottery, through the use of staged innovation, and through the pilot project, true experiments with randomly assigned control groups can be achieved. If the reform must be introduced across the board, the interrupted time-series design is available. If there are similar units under independent administration, a control series design adds strength. If a scarce boon must be given to the most needy or to the most deserving, quantifying this need or merit makes possible the regression discontinuity analysis." (48 ref.) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {409--429},
	number = {4},
	journaltitle = {American Psychologist},
	author = {Campbell, Donald T.},
	date = {1969},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Experimentation, Evaluation, Politics, Social Processes},
	file = {Campbell - 1969 - Reforms as experiments.pdf:/Users/tom/Zotero/storage/4YLLXBLG/Campbell - 1969 - Reforms as experiments.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/EQYNQT6G/1969-17253-001.html:text/html},
}

@article{stromland_making_2021,
	title = {Making our “meta-hypotheses” clear: heterogeneity and the role of direct replications in science},
	volume = {11},
	issn = {1879-4920},
	url = {https://doi.org/10.1007/s13194-021-00348-7},
	doi = {10.1007/s13194-021-00348-7},
	shorttitle = {Making our “meta-hypotheses” clear},
	abstract = {This paper argues that some of the discussion around meta-scientific issues can be viewed as an argument over different “meta-hypotheses” – assumptions made about how different hypotheses in a scientific literature relate to each other. I argue that, currently, such meta-hypotheses are typically left unstated except in methodological papers and that the consequence of this practice is that it is hard to determine what can be learned from a direct replication study. I argue in favor of a procedure dubbed the “limited homogeneity assumption” – assuming very little heterogeneity of effect sizes when a literature is initiated but switching to an assumption of heterogeneity once an initial finding has been successfully replicated in a direct replication study. Until that has happened, we do not allow the literature to proceed to a mature stage. This procedure will elevate the scientific status of direct replication studies in science. Following this procedure, a well-designed direct replication study is a means of falsifying an overall claim in an early phase of a literature and thus sets up a hurdle against the canonization of false facts in the behavioral sciences.},
	pages = {35},
	number = {2},
	journaltitle = {European Journal for Philosophy of Science},
	shortjournal = {Euro Jnl Phil Sci},
	author = {Strømland, Eirik},
	urldate = {2021-09-19},
	date = {2021-03-24},
	langid = {english},
	file = {Making our “meta-hypotheses” clear\: heterogeneity and the role of direct replications in science:/Users/tom/Zotero/storage/UY9L4YLW/10.1007@s13194-021-00348-7.pdf.pdf:application/pdf;Springer Full Text PDF:/Users/tom/Zotero/storage/G7I8GJIF/Strømland - 2021 - Making our “meta-hypotheses” clear heterogeneity .pdf:application/pdf},
}

@article{polanin_data-sharing_2019,
	title = {A data-sharing agreement helps to increase researchers’ willingness to share primary data: results from a randomized controlled trial},
	volume = {106},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435617313306},
	doi = {10.1016/j.jclinepi.2018.10.006},
	shorttitle = {A data-sharing agreement helps to increase researchers’ willingness to share primary data},
	abstract = {Background and Objectives
Sharing individual participant data ({IPD}) among researchers, on request, is an ethical and responsible practice. Despite numerous calls for this practice to be standard, however, research indicates that primary study authors are often unwilling to share {IPD}, even for use in a meta-analysis. This study sought to examine researchers' reservations about data sharing and to evaluate the impact of sending a data-sharing agreement on researchers’ attitudes toward sharing {IPD}.
Methods
To investigate these questions, we conducted a randomized controlled trial in conjunction with a Web-based survey. We searched for and invited primary study authors of studies included in recent meta-analyses. We emailed more than 1,200 individuals, and 247 participated. The survey asked individuals about their transparent research practices, general concerns about sharing data, attitudes toward sharing data for inclusion in a meta-analysis, and concerns about sharing data in the context of a meta-analysis. We hypothesized that participants who were randomly assigned to receive a data-sharing agreement would be more willing to share their primary study's {IPD}.
Results
Results indicated that participants who received a data-sharing agreement were more willing to share their data set, compared with control participants, even after controlling for demographics and pretest values (d = 0.65, 95\% {CI} [0.39, 0.90]). A member of the control group is 24 percent more likely to share her data set should she receive the data-sharing agreement.
Conclusions
These findings shed light on data-sharing practices, attitudes, and concerns and can be used to inform future meta-analysis projects seeking to collect {IPD}, as well as the field at large.},
	pages = {60--69},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Polanin, Joshua R. and Terzian, Mary},
	urldate = {2021-09-19},
	date = {2019-02-01},
	langid = {english},
	keywords = {Meta-analysis, Randomized controlled trial, Research transparency, Data sharing, Individual participant data},
	file = {A data-sharing agreement helps to increase researchers’ willingness to share primary data\: results from a randomized controlled trial:/Users/tom/Zotero/storage/C8VG7QNT/757d2fe0df1c93b7bc1e1773f9e1da8b.pdf.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/ZC38BBW2/S0895435617313306.html:text/html},
}

@article{guyatt_going_2008,
	title = {Going from evidence to recommendations},
	volume = {336},
	rights = {© {BMJ} Publishing Group Ltd 2008},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/content/336/7652/1049},
	doi = {10.1136/bmj.39493.646875.AE},
	abstract = {{\textless}p{\textgreater}The {GRADE} system classifies recommendations made in guidelines as either strong or weak. This article explores the meaning of these descriptions and their implications for patients, clinicians, and policy makers{\textless}/p{\textgreater}},
	pages = {1049--1051},
	number = {7652},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Guyatt, Gordon H. and Oxman, Andrew D. and Kunz, Regina and Falck-Ytter, Yngve and Vist, Gunn E. and Liberati, Alessandro and Schünemann, Holger J.},
	urldate = {2021-09-20},
	date = {2008-05-08},
	langid = {english},
	pmid = {18467413},
	note = {Publisher: British Medical Journal Publishing Group
Section: Analysis},
	file = {Full Text PDF:/Users/tom/Zotero/storage/78Y858GY/Guyatt et al. - 2008 - Going from evidence to recommendations.pdf:application/pdf;Going from evidence to recommendations:/Users/tom/Zotero/storage/P9XTGPI2/guyatt2008.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RJXXWFT2/1049.html:text/html},
}

@article{minocher_estimating_2021,
	title = {Estimating the reproducibility of social learning research published between 1955 and 2018},
	volume = {8},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.210450},
	doi = {10.1098/rsos.210450},
	abstract = {Reproducibility is integral to science, but difficult to achieve. Previous research has quantified low rates of data availability and results reproducibility across the biological and behavioural sciences. Here, we surveyed 560 empirical publications, published between 1955 and 2018 in the social learning literature, a research topic that spans animal behaviour, behavioural ecology, cultural evolution and evolutionary psychology. Data were recoverable online or through direct data requests for 30\% of this sample. Data recovery declines exponentially with time since publication, halving every 6 years, and up to every 9 years for human experimental data. When data for a publication can be recovered, we estimate a high probability of subsequent data usability (87\%), analytical clarity (97\%) and agreement of published results with reproduced findings (96\%). This corresponds to an overall rate of recovering data and reproducing results of 23\%, largely driven by the unavailability or incompleteness of data. We thus outline clear measures to improve the reproducibility of research on the ecology and evolution of social behaviour.},
	pages = {210450},
	number = {9},
	journaltitle = {Royal Society Open Science},
	author = {Minocher, Riana and Atmaca, Silke and Bavero, Claudia and {McElreath}, Richard and Beheim, Bret},
	urldate = {2021-09-22},
	date = {2021},
	note = {Publisher: Royal Society},
	keywords = {reproducibility, meta-science, data decay, social learning},
	file = {Minocher et al. - Estimating the reproducibility of social learning .pdf:/Users/tom/Zotero/storage/JXAHUNCA/Minocher et al. - Estimating the reproducibility of social learning .pdf:application/pdf},
}

@article{felgenhauer_experimentation_2021,
	title = {Experimentation and Manipulation with Preregistration},
	issn = {0899-8256},
	url = {https://www.sciencedirect.com/science/article/pii/S0899825621001226},
	doi = {10.1016/j.geb.2021.09.002},
	abstract = {Preregistration requires scientists to describe the planned research activities before their project begins. Preregistration improves transparency in empirical research and is an institutional response to scientific misconduct. This paper studies the impact of a preregistration requirement in a model in which a sender can generate information for a receiver by running private experiments. The sender can also engage in uninformative manipulation. This paper argues that a preregistration requirement can discourage p-hacking, but also result in even more detrimental faked studies.},
	journaltitle = {Games and Economic Behavior},
	shortjournal = {Games and Economic Behavior},
	author = {Felgenhauer, Mike},
	urldate = {2021-09-24},
	date = {2021-09-15},
	langid = {english},
	keywords = {Experimentation, Information acquisition, Manipulation, Persuasion},
	file = {Felgenhauer - 2021 - Experimentation and Manipulation with Preregistrat.pdf:/Users/tom/Zotero/storage/VWHQBTII/Felgenhauer - 2021 - Experimentation and Manipulation with Preregistrat.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/R9E76CRX/S0899825621001226.html:text/html},
}

@article{heesen_is_2021,
	title = {Is peer review a good idea?},
	volume = {72},
	issn = {0007-0882},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/axz029},
	doi = {10.1093/bjps/axz029},
	abstract = {Prepublication peer review should be abolished. We consider the effects that such a change will have on the social structure of science, paying particular attention to the changed incentive structure and the likely effects on the behaviour of individual scientists. We evaluate these changes from the perspective of epistemic consequentialism. We find that where the effects of abolishing prepublication peer review can be evaluated with a reasonable level of confidence based on presently available evidence, they are either positive or neutral. We conclude that on present evidence abolishing peer review weakly dominates the status quo.},
	pages = {635--663},
	number = {3},
	journaltitle = {The British Journal for the Philosophy of Science},
	author = {Heesen, Remco and Bright, Liam Kofi},
	urldate = {2021-10-04},
	date = {2021-09-01},
	note = {Publisher: The University of Chicago Press},
	file = {Full Text PDF:/Users/tom/Zotero/storage/8YSKPHNK/Heesen and Bright - 2021 - Is Peer Review a Good Idea.pdf:application/pdf;Is Peer Review a Good Idea?:/Users/tom/Zotero/storage/SLY6GHF3/heesen2019.pdf.pdf:application/pdf},
}

@article{lee_limited_2013,
	title = {The limited effectiveness of prestige as an intervention on the health of medical journal publications},
	volume = {10},
	issn = {1742-3600, 1750-0117},
	url = {https://www.cambridge.org/core/journals/episteme/article/limited-effectiveness-of-prestige-as-an-intervention-on-the-health-of-medical-journal-publications/AF4B3B7B77A8D8C00C769BC981914612},
	doi = {10.1017/epi.2013.35},
	abstract = {Under the traditional system of peer-reviewed publication, the degree of prestige conferred to authors by successful publication is tied to the degree of the intellectual rigor of its peer review process: ambitious scientists do well professionally by doing well epistemically. As a result, we should expect journal editors, in their dual role as epistemic evaluators and prestige-allocators, to have the power to motivate improved author behavior through the tightening of publication requirements. Contrary to this expectation, I will argue that the publication bias literature in academic medicine demonstrates that editor interventions have had limited effectiveness in improving the health of the publication and trial registration record, suggesting that much stronger interventions are needed.},
	pages = {387--402},
	number = {4},
	journaltitle = {Episteme},
	author = {Lee, Carole J.},
	urldate = {2021-10-04},
	date = {2013-12},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Full Text PDF:/Users/tom/Zotero/storage/TH6K2LP7/Lee - 2013 - THE LIMITED EFFECTIVENESS OF PRESTIGE AS AN INTERV.pdf:application/pdf;THE LIMITED EFFECTIVENESS OF PRESTIGE AS AN INTERVENTION ON THE HEALTH OF MEDICAL JOURNAL PUBLICATIONS:/Users/tom/Zotero/storage/N725BQWJ/lee2013.pdf.pdf:application/pdf},
}

@article{kane_reporting_2007,
	title = {Reporting in randomized clinical trials improved after adoption of the {CONSORT} statement},
	volume = {60},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435606002575},
	doi = {10.1016/j.jclinepi.2006.06.016},
	abstract = {Objective
To examine the extent to which the Consolidated Standards of Reporting Trials ({CONSORT}) reporting guidelines improved clinical trials reporting and subject attrition, which may undermine the credibility of published randomized clinical trials ({RCTs}).
Study Design and Setting
Published {RCTs} reported in two major medical journals before and after the {CONSORT} guidelines were systematically reviewed; one used the {CONSORT} statement ({JAMA}) and one did not ({NEJM}).
Results
The quality of {RCT} reporting improved for both journals, but {JAMA} showed more significant and consistent improvements in all aspects of {RCT} reporting. Subject attrition was better accounted for after the publication of {CONSORT}, although the attrition rates for various reasons actually increased. Attrition due to unknown reasons, as a percentage of total attrition, declined dramatically, from 68.7\% pre-{CONSORT} to 13.0\% post-{CONSORT}.
Conclusions
Attrition of study subjects remains a serious problem in {RCTs}. Bias from selective attrition can undermine the presumptive scientific advantage of {RCTs}. The {CONSORT} guidelines improved {RCT} reporting when they were implemented but did not substantially improve reported attrition rates.},
	pages = {241--249},
	number = {3},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Kane, Robert L. and Wang, Jye and Garrard, Judith},
	urldate = {2021-10-04},
	date = {2007-03-01},
	langid = {english},
	keywords = {Bias, Research design, Validity, {CONSORT} statement, Attrition, Randomized trials},
	file = {Kane et al. - 2007 - Reporting in randomized clinical trials improved a.pdf:/Users/tom/Zotero/storage/F6KXYY5G/Kane et al. - 2007 - Reporting in randomized clinical trials improved a.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/CUIFCA5T/S0895435606002575.html:text/html},
}

@article{moher_use_2001,
	title = {Use of the {CONSORT} Statement and quality of reports of randomized trials: A comparative before-and-after evaluation},
	volume = {285},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.285.15.1992},
	doi = {10.1001/jama.285.15.1992},
	abstract = {{ContextThe} Consolidated Standards for Reporting of Trials ({CONSORT}) statement
was developed to help improve the quality of reports of randomized controlled
trials ({RCTs}). To date, a paucity of data exists regarding whether it has
achieved this goal.{ObjectiveTo} determine whether use of the {CONSORT} statement is associated with
improvement in the quality of reports of {RCTs}.Design and {SettingComparative} before-and-after evaluation in which reports of {RCTs} published
in 1994 (pre-{CONSORT}) were compared with {RCT} reports from the same journals
published in 1998 (post-{CONSORT}). We included 211 reports from {BMJ}, {JAMA}, and The Lancet (journals that
adopted {CONSORT}) as well as The New England Journal of Medicine (a journal that did not adopt {CONSORT} and was used as a comparator).Main Outcome {MeasuresNumber} of {CONSORT} items included in a report, frequency of unclear reporting
of allocation concealment, and overall trial quality score based on the Jadad
scale, a 5-point quality assessment instrument.{ResultsCompared} with 1994, the number of {CONSORT} checklist items in reports
of {RCTs} increased in all 4 journals in 1998, and this increase was statistically
significant for the 3 adopter journals (pre-{CONSORT}, 23.4; mean change, 3.7;
95\% confidence interval [{CI}], 2.1-5.3). The frequency of unclear reporting
of allocation concealment decreased for each of the 4 journals, and this change
was statistically significant for adopters (pre-{CONSORT}, 61\%; mean change, −22\%;
95\% {CI}, −38\% to −6\%). Similarly, 3 of the 4 journals showed an
improvement in the quality score for reports of {RCTs}, and this increase was
statistically significant for adopter journals overall (pre-{CONSORT}, 2.7;
mean change, 0.4; 95\% {CI}, 0.1-0.8).{ConclusionUse} of the {CONSORT} statement is associated with improvements in the
quality of reports of {RCTs}.},
	pages = {1992--1995},
	number = {15},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Moher, David and Jones, Alison and Lepage, Leah and {for the CONSORT Group}},
	urldate = {2021-10-04},
	date = {2001-04-18},
	file = {Full Text:/Users/tom/Zotero/storage/TU77S8AY/Moher et al. - 2001 - Use of the CONSORT Statement and Quality of Report.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/T3G47T9C/193739.html:text/html;Use of the CONSORT Statement and Quality of Reports of Randomized TrialsA Comparative Before-and-After Evaluation:/Users/tom/Zotero/storage/466FDSTD/moher2001.pdf.pdf:application/pdf},
}

@article{plint_does_2006,
	title = {Does the {CONSORT} checklist improve the quality of reports of randomised controlled trials? A systematic review},
	volume = {185},
	issn = {1326-5377},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.5694/j.1326-5377.2006.tb00557.x},
	doi = {10.5694/j.1326-5377.2006.tb00557.x},
	shorttitle = {Does the {CONSORT} checklist improve the quality of reports of randomised controlled trials?},
	abstract = {Objective: To determine whether the adoption of the {CONSORT} checklist is associated with improvement in the quality of reporting of randomised controlled trials ({RCTs}). Data sources: {MEDLINE}, {EMBASE}, Cochrane {CENTRAL}, and reference lists of included studies and of experts were searched to identify eligible studies published between 1996 and 2005. Study selection: Studies were eligible if they (a) compared {CONSORT}-adopting and non-adopting journals after the publication of {CONSORT}, (b) compared {CONSORT} adopters before and after publication of {CONSORT}, or (c) a combination of (a) and (b). Outcomes examined included reports for any of the 22 items on the {CONSORT} checklist or overall trial quality. Data synthesis: 1128 studies were retrieved, of which 248 were considered possibly relevant. Eight studies were included in the review. {CONSORT} adopters had significantly better reporting of the method of sequence generation (risk ratio [{RR}], 1.67; 95\% {CI}, 1.19–2.33), allocation concealment ({RR}, 1.66; 95\% {CI}, 1.37–2.00) and overall number of {CONSORT} items than non-adopters (standardised mean difference, 0.83; 95\% {CI}, 0.46–1.19). {CONSORT} adoption had less effect on reporting of participant flow ({RR}, 1.14; 95\% {CI}, 0.89–1.46) and blinding of participants ({RR}, 1.09; 95\% {CI}, 0.84–1.43) or data analysts ({RR}, 5.44; 95\% {CI}, 0.73–36.87). In studies examining {CONSORT}-adopting journals before and after the publication of {CONSORT}, description of the method of sequence generation ({RR}, 2.78; 95\% {CI}, 1.78–4.33), participant flow ({RR}, 8.06; 95\% {CI}, 4.10–15.83), and total {CONSORT} items (standardised mean difference, 3.67 items; 95\% {CI}, 2.09–5.25) were improved after adoption of {CONSORT} by the journal. Conclusions: Journal adoption of {CONSORT} is associated with improved reporting of {RCTs}.},
	pages = {263--267},
	number = {5},
	journaltitle = {Medical Journal of Australia},
	author = {Plint, Amy C and Moher, David and Morrison, Andra and Schulz, Kenneth and Altman, Douglas G and Hill, Catherine and Gaboury, Isabelle},
	urldate = {2021-10-04},
	date = {2006},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.5694/j.1326-5377.2006.tb00557.x},
	keywords = {Statistics, epidemiology and research design},
	file = {Does the CONSORT checklist improve the quality of reports of randomised controlled trials? A systematic review:/Users/tom/Zotero/storage/JP4A9823/10.5694@j.1326-5377.2006.tb00557.x.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/YSFYM5VH/Plint et al. - 2006 - Does the CONSORT checklist improve the quality of .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PX9ZYLHZ/j.1326-5377.2006.tb00557.html:text/html},
}

@article{pearl_external_2014,
	title = {External Validity: From Do-Calculus to Transportability Across Populations},
	volume = {29},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-29/issue-4/External-Validity-From-Do-Calculus-to-Transportability-Across-Populations/10.1214/14-STS486.full},
	doi = {10.1214/14-STS486},
	shorttitle = {External Validity},
	abstract = {The generalizability of empirical findings to new environments, settings or populations, often called “external validity,” is essential in most scientific explorations. This paper treats a particular problem of generalizability, called “transportability,” defined as a license to transfer causal effects learned in experimental studies to a new population, in which only observational studies can be conducted. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between populations of interest and, using this representation, we reduce questions of transportability to symbolic derivations in the do-calculus. This reduction yields graph-based procedures for deciding, prior to observing any data, whether causal effects in the target population can be inferred from experimental findings in the study population. When the answer is affirmative, the procedures identify what experimental and observational findings need be obtained from the two populations, and how they can be combined to ensure bias-free transport.},
	pages = {579--595},
	number = {4},
	journaltitle = {Statistical Science},
	author = {Pearl, Judea and Bareinboim, Elias},
	urldate = {2021-10-05},
	date = {2014-11},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Experimental design, causal effects, External validity, generalizability},
	file = {External Validity\: From Do-Calculus to Transportability Across Populations:/Users/tom/Zotero/storage/HBJ6E6B8/pearl2014.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/JQ34D9KM/Pearl and Bareinboim - 2014 - External Validity From Do-Calculus to Transportab.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KBXYMM9R/14-STS486.html:text/html},
}

@online{noauthor_full_nodate-1,
	title = {Full article: Investigating the frequency of intrusive memories after 24 hours using a visuospatial interference intervention: a follow-up and extension},
	url = {https://www.tandfonline.com/doi/full/10.1080/20008198.2021.1953788},
	urldate = {2021-10-05},
	file = {Full article\: Investigating the frequency of intrusive memories after 24 hours using a visuospatial interference intervention\: a follow-up and extension:/Users/tom/Zotero/storage/ZP6SR5J3/20008198.2021.html:text/html},
}

@article{davies_revitalising_2005,
	title = {Revitalising rapid responses},
	volume = {330},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC558191/},
	pages = {1284},
	number = {7503},
	journaltitle = {{BMJ} : British Medical Journal},
	shortjournal = {{BMJ}},
	author = {Davies, Sharon and Delamothe, Tony},
	urldate = {2021-10-07},
	date = {2005-06-04},
	pmid = {15933340},
	pmcid = {PMC558191},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/2Y4JCNG7/Davies and Delamothe - 2005 - Revitalising rapid responses.pdf:application/pdf},
}

@article{schroter_effects_2004,
	title = {Effects of training on quality of peer review: randomised controlled trial},
	volume = {328},
	rights = {© 2004 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/328/7441/673},
	doi = {10.1136/bmj.38023.700775.AE},
	shorttitle = {Effects of training on quality of peer review},
	abstract = {Objective To determine the effects of training on the quality of peer review.
Design Single blind randomised controlled trial with two intervention groups receiving different types of training plus a control group.
Setting and participants Reviewers at a general medical journal.
Interventions Attendance at a training workshop or reception of a self taught training package focusing on what editors want from reviewers and how to critically appraise randomised controlled trials.
Main outcome measures Quality of reviews of three manuscripts sent to reviewers at four to six monthly intervals, evaluated using the validated review quality instrument; number of deliberate major errors identified; time taken to review the manuscripts; proportion recommending rejection of the manuscripts.
Results Reviewers in the self taught group scored higher in review quality after training than did the control group (score 2.85 v 2.56; difference 0.29, 95\% confidence interval 0.14 to 0.44; P = 0.001), but the difference was not of editorial significance and was not maintained in the long term. Both intervention groups identified significantly more major errors after training than did the control group (3.14 and 2.96 v 2.13; P {\textless} 0.001), and this remained significant after the reviewers' performance at baseline assessment was taken into account. The evidence for benefit of training was no longer apparent on further testing six months after the interventions. Training had no impact on the time taken to review the papers but was associated with an increased likelihood of recommending rejection (92\% and 84\% v 76\%; P = 0.002).
Conclusions Short training packages have only a slight impact on the quality of peer review. The value of longer interventions needs to be assessed.},
	pages = {673},
	number = {7441},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Schroter, Sara and Black, Nick and Evans, Stephen and Carpenter, James and Godlee, Fiona and Smith, Richard},
	urldate = {2021-10-07},
	date = {2004-03-18},
	langid = {english},
	pmid = {14996698},
	note = {Publisher: British Medical Journal Publishing Group
Section: Paper},
	file = {Effects of training on quality of peer review\: randomised controlled trial:/Users/tom/Zotero/storage/C8WAY4SF/schroter2004.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/BN6Y8Y3F/Schroter et al. - 2004 - Effects of training on quality of peer review ran.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/BK73462M/673.html:text/html},
}

@article{bergh_is_2017,
	title = {Is there a credibility crisis in strategic management research? Evidence on the reproducibility of study findings},
	volume = {15},
	issn = {1476-1270},
	url = {https://doi.org/10.1177/1476127017701076},
	doi = {10.1177/1476127017701076},
	shorttitle = {Is there a credibility crisis in strategic management research?},
	abstract = {Recent studies report an inability to replicate previously published research, leading some to suggest that scientific knowledge is facing a credibility crisis. In this essay, we provide evidence on whether strategic management research may itself be vulnerable to these concerns. We conducted a study whereby we attempted to reproduce the empirical findings of 88 articles appearing in the Strategic Management Journal using data reported in the articles themselves. About 70\% of the studies did not disclose enough data to permit independent tests of reproducibility of their findings. Of those that could be retested, almost one-third reported hypotheses as statistically significant which were no longer so and far more significant results were found to be non-significant in the reproductions than in the opposite direction. Collectively, incomplete reporting practices, disclosure errors, and possible opportunism limit the reproducibility of most studies. Until disclosure standards and requirements change to include more complete reporting and facilitate tests of reproducibility, the strategic management field appears vulnerable to a credibility crisis.},
	pages = {423--436},
	number = {3},
	journaltitle = {Strategic Organization},
	shortjournal = {Strategic Organization},
	author = {Bergh, Donald D and Sharp, Barton M and Aguinis, Herman and Li, Ming},
	urldate = {2021-10-07},
	date = {2017-08-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
	keywords = {reproducibility, replication, knowledge credibility},
	file = {Is there a credibility crisis in strategic management research? Evidence on the reproducibility of study findings:/Users/tom/Zotero/storage/EHEJJUG9/bergh2017.pdf.pdf:application/pdf;SAGE PDF Full Text:/Users/tom/Zotero/storage/4BLAXZDB/Bergh et al. - 2017 - Is there a credibility crisis in strategic managem.pdf:application/pdf},
}

@incollection{moher_using_2014,
	location = {Oxford, {UK}},
	title = {Using reporting guidelines effectively to ensure good reporting of health research},
	isbn = {978-1-118-71559-8 978-0-470-67044-6},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118715598.ch4},
	pages = {32--40},
	booktitle = {Guidelines for Reporting Health Research: A User's Manual},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Altman, Douglas G. and Simera, Iveta},
	editor = {Moher, David and Altman, Douglas G. and Schulz, Kenneth F. and Simera, Iveta and Wager, Elizabeth},
	urldate = {2021-10-08},
	date = {2014-08-22},
	langid = {english},
	doi = {10.1002/9781118715598.ch4},
	file = {Altman and Simera - 2014 - Using Reporting Guidelines Effectively to Ensure G.pdf:/Users/tom/Zotero/storage/S9MINUVI/Altman and Simera - 2014 - Using Reporting Guidelines Effectively to Ensure G.pdf:application/pdf},
}

@collection{moher_guidelines_2014,
	title = {Guidelines for Reporting Health Research: A User’s Manual},
	publisher = {John Wiley \& Sons},
	editor = {Moher, D. and Altman, D. G. and Schulz, K. F. and Simera, I. and Wager, E.},
	date = {2014},
	langid = {english},
	file = {Moher - Guidelines for Reporting Health Research.pdf:/Users/tom/Zotero/storage/S4TJRDLG/Moher - Guidelines for Reporting Health Research.pdf:application/pdf},
}

@article{gotzsche_readers_2009,
	title = {Readers as research detectives},
	volume = {10},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/1745-6215-10-2},
	doi = {10.1186/1745-6215-10-2},
	abstract = {Flaws in research papers are common but it may require arduous detective work to unravel them. Checklists are helpful, but many inconsistencies will only be revealed through repeated cross-checks of every little detail, just like in a crime case. As a major deterrent for dishonesty, raw data from all trials should be posted on a public website. This would also make it much easier to detect errors and flaws in publications, and it would allow many research projects to be performed without collecting new data. The prevailing culture of secrecy and ownership to data is not in the best interests of patients.},
	pages = {2},
	number = {1},
	journaltitle = {Trials},
	shortjournal = {Trials},
	author = {Gøtzsche, Peter C.},
	urldate = {2021-10-08},
	date = {2009-01-07},
	keywords = {Academic Ambition, Police Detective, Trial Protocol, Trial Report, Unreported Outcome},
	file = {Full Text PDF:/Users/tom/Zotero/storage/J44YW3DQ/Gøtzsche - 2009 - Readers as research detectives.pdf:application/pdf;Readers as research detectives:/Users/tom/Zotero/storage/LUGA9F7W/10.1186@1745-6215-10-2.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SPH9U2HW/1745-6215-10-2.html:text/html},
}

@article{turner_consolidated_2012,
	title = {Consolidated standards of reporting trials ({CONSORT}) and the completeness of reporting of randomised controlled trials ({RCTs}) published in medical journals},
	volume = {11},
	issn = {1469-493X},
	doi = {10.1002/14651858.MR000030.pub2},
	abstract = {{BACKGROUND}: An overwhelming body of evidence stating that the completeness of reporting of randomised controlled trials ({RCTs}) is not optimal has accrued over time. In the mid-1990s, in response to these concerns, an international group of clinical trialists, statisticians, epidemiologists, and biomedical journal editors developed the {CONsolidated} Standards Of Reporting Trials ({CONSORT}) Statement. The {CONSORT} Statement, most recently updated in March 2010, is an evidence-based minimum set of recommendations including a checklist and flow diagram for reporting {RCTs} and is intended to facilitate the complete and transparent reporting of trials and aid their critical appraisal and interpretation. In 2006, a systematic review of eight studies evaluating the "effectiveness of {CONSORT} in improving reporting quality in journals" was published.
{OBJECTIVES}: To update the earlier systematic review assessing whether journal endorsement of the 1996 and 2001 {CONSORT} checklists influences the completeness of reporting of {RCTs} published in medical journals.
{SEARCH} {METHODS}: We conducted electronic searches, known item searching, and reference list scans to identify reports of evaluations assessing the completeness of reporting of {RCTs}. The electronic search strategy was developed in {MEDLINE} and tailored to {EMBASE}. We searched the Cochrane Methodology Register and the Cochrane Database of Systematic Reviews using the Wiley interface. We searched the Science Citation Index, Social Science Citation Index, and Arts and Humanities Citation Index through the {ISI} Web of Knowledge interface. We conducted all searches to identify reports published between January 2005 and March 2010, inclusive.
{SELECTION} {CRITERIA}: In addition to studies identified in the original systematic review on this topic, comparative studies evaluating the completeness of reporting of {RCTs} in any of the following comparison groups were eligible for inclusion in this review: 1) Completeness of reporting of {RCTs} published in journals that have and have not endorsed the {CONSORT} Statement; 2) Completeness of reporting of {RCTs} published in {CONSORT}-endorsing journals before and after endorsement; or 3) Completeness of reporting of {RCTs} before and after the publication of the {CONSORT} Statement (1996 or 2001). We used a broad definition of {CONSORT} endorsement that includes any of the following: (a) requirement or recommendation in journal's 'Instructions to Authors' to follow {CONSORT} guidelines; (b) journal editorial statement endorsing the {CONSORT} Statement; or (c) editorial requirement for authors to submit a {CONSORT} checklist and/or flow diagram with their manuscript. We contacted authors of evaluations reporting data that could be included in any comparison group(s), but not presented as such in the published report and asked them to provide additional data in order to determine eligibility of their evaluation. Evaluations were not excluded due to language of publication or validity assessment.
{DATA} {COLLECTION} {AND} {ANALYSIS}: We completed screening and data extraction using standardised electronic forms, where conflicts, reasons for exclusion, and level of agreement were all automatically and centrally managed in web-based management software, {DistillerSR}(®). One of two authors extracted general characteristics of included evaluations and all data were verified by a second author. Data describing completeness of reporting were extracted by one author using a pre-specified form; a 10\% random sample of evaluations was verified by a second author. Any discrepancies were discussed by both authors; we made no modifications to the extracted data. Validity assessments of included evaluations were conducted by one author and independently verified by one of three authors. We resolved all conflicts by consensus.For each comparison we collected data on 27 outcomes: 22 items of the {CONSORT} 2001 checklist, plus four items relating to the reporting of blinding, and one item of aggregate {CONSORT} scores. Where reported, we extracted and qualitatively synthesised data on the methodological quality of {RCTs}, by scale or score.
{MAIN} {RESULTS}: Fifty-three publications reporting 50 evaluations were included. The total number of {RCTs} assessed within evaluations was 16,604 (median per evaluation 123 (interquartile range ({IQR}) 77 to 226) published in a median of six ({IQR} 3 to 26) journals. Characteristics of the included {RCT} populations were variable, resulting in heterogeneity between included evaluations. Validity assessments of included studies resulted in largely unclear judgements. The included evaluations are not {RCTs} and less than 8\% (4/53) of the evaluations reported adjusting for potential confounding factors.   Twenty-five of 27 outcomes assessing completeness of reporting in {RCTs} appeared to favour {CONSORT}-endorsing journals over non-endorsers, of which five were statistically significant. 'Allocation concealment' resulted in the largest effect, with risk ratio ({RR}) 1.81 (99\% confidence interval ({CI}) 1.25 to 2.61), suggesting that 81\% more {RCTs} published in {CONSORT}-endorsing journals adequately describe allocation concealment compared to those published in non-endorsing journals. Allocation concealment was reported adequately in 45\% (393/876) of {RCTs} in {CONSORT}-endorsing journals and in 22\% (329/1520) of {RCTs} in non-endorsing journals. Other outcomes with results that were significant include: scientific rationale and background in the 'Introduction' ({RR} 1.07, 99\% {CI} 1.01 to 1.14); 'sample size' ({RR} 1.61, 99\% {CI} 1.13 to 2.29); method used for 'sequence generation' ({RR} 1.59, 99\% {CI} 1.38 to 1.84); and an aggregate score over reported {CONSORT} items, 'total sum score' (standardised mean difference ({SMD}) 0.68 (99\% {CI} 0.38 to 0.98)).
{AUTHORS}' {CONCLUSIONS}: Evidence has accumulated to suggest that the reporting of {RCTs} remains sub-optimal. This review updates a previous systematic review of eight evaluations. The findings of this review are similar to those from the original review and demonstrate that, despite the general inadequacies of reporting of {RCTs}, journal endorsement of the {CONSORT} Statement may beneficially influence the completeness of reporting of trials published in medical journals. Future prospective studies are needed to explore the influence of the {CONSORT} Statement dependent on the extent of editorial policies to ensure adherence to {CONSORT} guidance.},
	pages = {MR000030},
	journaltitle = {The Cochrane Database of Systematic Reviews},
	shortjournal = {Cochrane Database Syst Rev},
	author = {Turner, Lucy and Shamseer, Larissa and Altman, Douglas G. and Weeks, Laura and Peters, Jodi and Kober, Thilo and Dias, Sofia and Schulz, Kenneth F. and Plint, Amy C. and Moher, David},
	date = {2012-11-14},
	pmid = {23152285},
	pmcid = {PMC7386818},
	keywords = {Randomized Controlled Trials as Topic, Publishing, Checklist, Periodicals as Topic, Reference Standards},
	file = {Consolidated standards of reporting trials (CONSORT) and the completeness of reporting of randomised controlled trials (RCTs) published in medical journals:/Users/tom/Zotero/storage/QGCV57R4/turner2012.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/5WS7W7BB/Turner et al. - 2012 - Consolidated standards of reporting trials (CONSOR.pdf:application/pdf},
}

@article{moher_reporting_2007,
	title = {Reporting research results: A moral obligation for all researchers},
	volume = {54},
	issn = {0832-610X, 1496-8975},
	url = {http://link.springer.com/10.1007/BF03022653},
	doi = {10.1007/BF03022653},
	shorttitle = {Reporting research results},
	pages = {331--335},
	number = {5},
	journaltitle = {Canadian Journal of Anesthesia/Journal canadien d'anesthésie},
	shortjournal = {Can J Anesth/J Can Anesth},
	author = {Moher, David},
	urldate = {2021-10-08},
	date = {2007-05},
	langid = {english},
	file = {Moher - 2007 - Reporting research results A moral obligation for.pdf:/Users/tom/Zotero/storage/ACFWMDKP/Moher - 2007 - Reporting research results A moral obligation for.pdf:application/pdf;Reporting research results\: A moral obligation for all researchers:/Users/tom/Zotero/storage/GDGZRHQ7/moher2007.pdf.pdf:application/pdf},
}

@article{brennen_investigating_nodate,
	title = {Investigating the frequency of intrusive memories after 24 hours using a visuospatial interference intervention: a follow-up and extension},
	volume = {12},
	issn = {2000-8198},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8366629/},
	doi = {10.1080/20008198.2021.1953788},
	shorttitle = {Investigating the frequency of intrusive memories after 24 hours using a visuospatial interference intervention},
	abstract = {This study followed up James et al.’s (2015) finding that playing a visuospatially demanding game reduced negative intrusive memories.As we did not find a similar pattern of findings the intervention may be less promising than assumed and sensitive to changes in set-up.},
	pages = {1953788},
	number = {1},
	journaltitle = {European Journal of Psychotraumatology},
	shortjournal = {Eur J Psychotraumatol},
	author = {Brennen, Tim and Blix, Ines and Nissen, Alexander and Holmes, Emily A. and Skumlien, Martine and Solberg, Øivind},
	urldate = {2021-10-13},
	pmid = {34408817},
	pmcid = {PMC8366629},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/UBQLZ6X8/Brennen et al. - Investigating the frequency of intrusive memories .pdf:application/pdf},
}

@online{committee_on_publication_ethics_core_2019,
	title = {Core Practices},
	url = {https://web.archive.org/web/20191122170830/https://publicationethics.org/core-practices},
	titleaddon = {{COPE}: Committee on Publication Ethics},
	author = {Committee on Publication Ethics},
	urldate = {2019-11-22},
	date = {2019},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/NQYMCU4W/core-practices.html:text/html},
}

@article{morawski_replication_2019,
	title = {The replication crisis: How might philosophy and theory of psychology be of use?},
	volume = {39},
	issn = {2151-3341, 1068-8471},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/teo0000129},
	doi = {10.1037/teo0000129},
	shorttitle = {The replication crisis},
	pages = {218--238},
	number = {4},
	journaltitle = {Journal of Theoretical and Philosophical Psychology},
	shortjournal = {Journal of Theoretical and Philosophical Psychology},
	author = {Morawski, Jill},
	urldate = {2021-10-18},
	date = {2019-11},
	langid = {english},
	keywords = {toread},
	file = {Morawski - 2019 - The replication crisis How might philosophy and t.pdf:/Users/tom/Zotero/storage/9MY58AED/Morawski - 2019 - The replication crisis How might philosophy and t.pdf:application/pdf},
}

@article{good_statistics_1972,
	title = {Statistics and Today's Problems},
	volume = {26},
	issn = {0003-1305},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1972.10478922},
	doi = {10.1080/00031305.1972.10478922},
	pages = {11--19},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Good, I. J.},
	urldate = {2021-10-20},
	date = {1972-06-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1972.10478922},
	file = {Snapshot:/Users/tom/Zotero/storage/C9CL44BW/00031305.1972.html:text/html;Statistics and Today's Problems:/Users/tom/Zotero/storage/WZLKCINY/good1972.pdf.pdf:application/pdf},
}

@article{liu_data_2019,
	title = {The data source of this study is Web of Science Core Collection? Not enough},
	volume = {121},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-019-03238-1},
	doi = {10.1007/s11192-019-03238-1},
	shorttitle = {The data source of this study is Web of Science Core Collection?},
	abstract = {Clarivate Analytics’ Web of Science Core Collection, a comprehensive database consisting of ten sub-datasets, is increasingly applied in academic research across over two hundred Web of Science categories. 271 English language {SCIE} and {SSCI} papers published in 2017–2018 from the category of Information Science and Library Science have mentioned “Web of Science” in the topic field. A manual check of the full texts of these papers reveals that 243 of them have used “Web of Science Core Collection” as the data source but over half of them haven’t specified the sub-datasets of Web of Science Core Collection used in the study. Since many institutions may only subscribe to a customized subset of the whole core collection, the non-transparency of the data source will hinder the reproducibility of some corresponding studies. This study suggests that researchers should specify the sub-datasets and corresponding coverage timespans when using Web of Science Core Collection as the data source.},
	pages = {1815--1824},
	number = {3},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Liu, Weishu},
	urldate = {2021-10-20},
	date = {2019-12-01},
	langid = {english},
	file = {The data source of this study is Web of Science Core Collection? Not enough:/Users/tom/Zotero/storage/XU3HZ37B/10.1007@s11192-019-03238-1.pdf.pdf:application/pdf},
}

@article{cruwell_preregistration_2021,
	title = {Preregistration in diverse contexts: a preregistration template for the application of cognitive models},
	volume = {8},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.210155},
	doi = {10.1098/rsos.210155},
	shorttitle = {Preregistration in diverse contexts},
	abstract = {In recent years, open science practices have become increasingly popular in psychology and related sciences. These practices aim to increase rigour and transparency in science as a potential response to the challenges posed by the replication crisis. Many of these reforms—including the increasingly used preregistration—have been designed for purely experimental work that tests straightforward hypotheses with standard inferential statistical analyses, such as assessing whether an experimental manipulation has an effect on a variable of interest. But psychology is a diverse field of research. The somewhat narrow focus of the prevalent discussions surrounding and templates for preregistration has led to debates on how appropriate these reforms are for areas of research with more diverse hypotheses and more intricate methods of analysis, such as cognitive modelling research within mathematical psychology. Our article attempts to bridge the gap between open science and mathematical psychology, focusing on the type of cognitive modelling that Crüwell et al. (Crüwell S, Stefan {AM}, Evans {NJ}. 2019 Robust standards in cognitive science. Comput. Brain Behav.2, 255–265) labelled model application, where researchers apply a cognitive model as a measurement tool to test hypotheses about parameters of the cognitive model. Specifically, we (i) discuss several potential researcher degrees of freedom within model application, (ii) provide the first preregistration template for model application and (iii) provide an example of a preregistered model application using our preregistration template. More broadly, we hope that our discussions and concrete proposals constructively advance the mostly abstract current debate surrounding preregistration in cognitive modelling, and provide a guide for how preregistration templates may be developed in other diverse or intricate research contexts.},
	pages = {210155},
	number = {10},
	journaltitle = {Royal Society Open Science},
	author = {Crüwell, Sophia and Evans, Nathan J.},
	urldate = {2021-10-21},
	date = {2021},
	note = {Publisher: Royal Society},
	keywords = {open science, preregistration, transparency, reproducibility, cognitive modelling},
	file = {Crüwell and Evans - Preregistration in diverse contexts a preregistra.pdf:/Users/tom/Zotero/storage/SCBVICP5/Crüwell and Evans - Preregistration in diverse contexts a preregistra.pdf:application/pdf},
}

@article{berkowitz_math_2015,
	title = {Math at home adds up to achievement in school},
	volume = {350},
	url = {https://www.science.org/doi/10.1126/science.aac7427},
	doi = {10.1126/science.aac7427},
	pages = {196--198},
	number = {6257},
	journaltitle = {Science},
	author = {Berkowitz, Talia and Schaeffer, Marjorie W. and Maloney, Erin A. and Peterson, Lori and Gregor, Courtney and Levine, Susan C. and Beilock, Sian L.},
	urldate = {2021-10-21},
	date = {2015-10-09},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ABXM6LCX/Berkowitz et al. - 2015 - Math at home adds up to achievement in school.pdf:application/pdf;Math at home adds up to achievement in school:/Users/tom/Zotero/storage/WQ7B8XL9/berkowitz2015.pdf.pdf:application/pdf},
}

@article{berkowitz_response_2016,
	title = {Response to Comment on “Math at home adds up to achievement in school”},
	volume = {351},
	url = {https://www.science.org/doi/full/10.1126/science.aad8555},
	doi = {10.1126/science.aad8555},
	pages = {1161--1161},
	number = {6278},
	journaltitle = {Science},
	author = {Berkowitz, Talia and Schaeffer, Marjorie W. and Rozek, Christopher S. and Maloney, Erin A. and Levine, Susan C. and Beilock, Sian L.},
	urldate = {2021-10-21},
	date = {2016-03-11},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Full Text PDF:/Users/tom/Zotero/storage/B4EYAL83/Berkowitz et al. - 2016 - Response to Comment on “Math at home adds up to ac.pdf:application/pdf;Response to Comment on “Math at home adds up to achievement in school”:/Users/tom/Zotero/storage/AFPRUUSS/berkowitz2016.pdf.pdf:application/pdf},
}

@article{gelman_difference_2006,
	title = {The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant},
	volume = {60},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313006X152649},
	doi = {10.1198/000313006X152649},
	pages = {328--331},
	number = {4},
	journaltitle = {The American Statistician},
	shortjournal = {The American Statistician},
	author = {Gelman, Andrew and Stern, Hal},
	urldate = {2021-10-21},
	date = {2006-11},
	langid = {english},
	file = {Gelman and Stern - 2006 - The Difference Between “Significant” and “Not Sign.pdf:/Users/tom/Zotero/storage/E2ZDSWGP/Gelman and Stern - 2006 - The Difference Between “Significant” and “Not Sign.pdf:application/pdf;The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant:/Users/tom/Zotero/storage/BXADLLFC/gelman2006.pdf.pdf:application/pdf},
}

@article{browne_cross-validation_2000,
	title = {Cross-Validation Methods},
	volume = {44},
	issn = {0022-2496},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249699912798},
	doi = {10.1006/jmps.1999.1279},
	abstract = {This paper gives a review of cross-validation methods. The original applications in multiple linear regression are considered first. It is shown how predictive accuracy depends on sample size and the number of predictor variables. Both two-sample and single-sample cross-validation indices are investigated. The application of cross-validation methods to the analysis of moment structures is then justified. An equivalence of a single-sample cross-validation index and the Akaike information criterion is pointed out. It is seen that the optimal number of parameters suggested by both single-sample and two-sample cross-validation indices will depend on sample size.},
	pages = {108--132},
	number = {1},
	journaltitle = {Journal of Mathematical Psychology},
	shortjournal = {Journal of Mathematical Psychology},
	author = {Browne, Michael W},
	urldate = {2021-10-22},
	date = {2000-03-01},
	langid = {english},
	file = {Cross-Validation Methods:/Users/tom/Zotero/storage/Z4BHJTND/browne2000.pdf.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/V4PPH8SW/S0022249699912798.html:text/html},
}

@article{agnoli_australian_2021,
	title = {Australian and Italian Psychologists’ View of Replication},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/25152459211039218},
	doi = {10.1177/25152459211039218},
	abstract = {Solutions to the crisis in confidence in the psychological literature have been proposed in many recent articles, including increased publication of replication studies, a solution that requires engagement by the psychology research community. We surveyed Australian and Italian academic research psychologists about the meaning and role of replication in psychology. When asked what they consider to be a replication study, nearly all participants (98\% of Australians and 96\% of Italians) selected options that correspond to a direct replication. Only 14\% of Australians and 8\% of Italians selected any options that included changing the experimental method. Majorities of psychologists from both countries agreed that replications are very important, that more replications should be done, that more resources should be allocated to them, and that they should be published more often. Majorities of psychologists from both countries reported that they or their students sometimes or often replicate studies, yet they also reported having no replication studies published in the prior 5 years. When asked to estimate the percentage of published studies in psychology that are replications, both Australians (with a median estimate of 13\%) and Italians (with a median estimate of 20\%) substantially overestimated the actual rate. When asked what constitute the main obstacles to replications, difficulty publishing replications was the most frequently cited obstacle, coupled with the high value given to innovative or novel research and the low value given to replication studies.},
	pages = {25152459211039218},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Agnoli, Franca and Fraser, Hannah and Singleton Thorn, Felix and Fidler, Fiona},
	urldate = {2021-10-27},
	date = {2021-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {reproducibility, replication, open data, open materials, conceptual, direct},
	file = {Agnoli et al. - 2021 - Australian and Italian Psychologists’ View of Repl.pdf:/Users/tom/Zotero/storage/TH6NG2Y2/Agnoli et al. - 2021 - Australian and Italian Psychologists’ View of Repl.pdf:application/pdf},
}

@article{goodman_have_nodate,
	title = {Have you ever meta-analysis you didn't like?},
	pages = {3},
	author = {Goodman, Steven N},
	langid = {english},
	file = {Have You Ever Meta-Analysis You Didn't Like.pdf:/Users/tom/Zotero/storage/APTN3PMR/Have You Ever Meta-Analysis You Didn't Like.pdf:application/pdf},
}

@report{bledsoe_data_2021,
	title = {Data rescue: saving environmental data from extinction},
	url = {https://ecoevorxiv.org/ra6ze/},
	shorttitle = {Data rescue},
	abstract = {1.	Historical and long-term environmental datasets are imperative to understanding how natural systems respond to our changing world, setting baselines and establishing trajectories of change. Although immensely valuable, these data are ultimately at risk of being lost unless they are actively managed, curated, and eventually archived on data repositories. 
2.	The practice of data rescue, which we define as identifying, preserving, and sharing valuable data and associated metadata at risk of loss, is an important means of ensuring the long-term viability and accessibility of such datasets. Improvements in policies and best practices around data management will hopefully limit the future need for data rescue; these changes, however, do not apply retroactively. While the concept of rescuing data is not new, the term lacks a formal definition, is often conflated with other terms (i.e., data reuse), and lacks general recommendations. 
3.	Here, we outline seven key guidelines for effective rescue of historically-collected and unmanaged datasets. We discuss how to prioritize which datasets to rescue, form effective data rescue teams, prepare the data and related metadata, and ultimately archive and share the rescued data. 
4.	In an era of rapid environmental change, the best policy solutions will require evidence from both contemporary and historical sources. It is, therefore, imperative that we identify and preserve valuable, at-risk environmental data before they are lost to science.},
	institution = {{EcoEvoRxiv}},
	author = {Bledsoe, Ellen and Burant, Joey and Higino, Gracielle and Roche, Dominique and Binning, Sandra and Finlay, Kerri and Pither, Jason and Pollock, Laura and Sunday, Jennifer and Srivastava, Diane},
	urldate = {2021-10-27},
	date = {2021-10-26},
	langid = {english},
	doi = {10.32942/osf.io/ra6ze},
	note = {type: article},
	keywords = {open science, transparency, open data, Life Sciences, Ecology and Evolutionary Biology, data archiving, historical data, long-term ecological data, long-term studies},
	file = {Full Text PDF:/Users/tom/Zotero/storage/EX2BJP58/Bledsoe et al. - 2021 - Data rescue saving environmental data from extinc.pdf:application/pdf},
}

@inproceedings{winker_letters_2013,
	location = {Chicago},
	title = {Letters and Comments Published in Response to Research: Whither Postpublication Peer Review?},
	url = {https://web.archive.org/web/20211023052251/https://peerreviewcongress.org/2013-abstracts/},
	eventtitle = {Peer Review Congress},
	booktitle = {Peer Review Congress},
	author = {Winker, Margaret A.},
	date = {2013},
	file = {Snapshot:/Users/tom/Zotero/storage/DY3RDF64/2013-abstracts.html:text/html},
}

@article{dunn_conflict_2016,
	title = {Conflict of interest disclosure in biomedical research: a review of current practices, biases, and the role of public registries in improving transparency},
	volume = {1},
	issn = {2058-8615},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4854425/},
	doi = {10.1186/s41073-016-0006-7},
	shorttitle = {Conflict of interest disclosure in biomedical research},
	abstract = {Conflicts of interest held by researchers remain a focus of attention in clinical research. Biases related to these relationships have the potential to directly impact the quality of healthcare by influencing decision-making, yet conflicts of interest remain underreported, inconsistently described, and difficult to access. Initiatives aimed at improving the disclosure of researcher conflicts of interest are still in their infancy but represent a vital reform that must be addressed before potential biases associated with conflicts of interest can be mitigated and trust in the impartiality of clinical evidence restored. In this review, we examine the prevalence of conflicts of interest, evidence of the effects that disclosed and undisclosed conflicts of interest have had on the reporting of clinical evidence, and the emerging approaches for improving the completeness and consistency of disclosures. Through this review of emerging technologies, we recognize a growing interest in publicly accessible registries for researcher conflicts of interest and propose five desiderata aimed at maximizing the value of such registries: mandates for ensuring that researchers keep their records up to date; transparent records that are made available to the public; interoperability to allow researchers, bibliographic databases, and institutions to interact with the registry; a consistent taxonomy for describing different classes of conflicts of interest; and the ability to automatically generate conflicts of interest statements for use in published articles.},
	pages = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Res Integr Peer Rev},
	author = {Dunn, Adam G. and Coiera, Enrico and Mandl, Kenneth D. and Bourgeois, Florence T.},
	urldate = {2021-10-29},
	date = {2016-05-03},
	pmid = {27158530},
	pmcid = {PMC4854425},
	file = {Conflict of interest disclosure in biomedical research\: a review of current practices, biases, and the role of public registries in improving transparency:/Users/tom/Zotero/storage/XNPQLW3D/dunn2016.pdf.pdf:application/pdf;PubMed Central Full Text PDF:/Users/tom/Zotero/storage/QBCKQU6B/Dunn et al. - 2016 - Conflict of interest disclosure in biomedical rese.pdf:application/pdf},
}

@article{beheim_treatment_2021,
	title = {Treatment of missing data determined conclusions regarding moralizing gods},
	volume = {595},
	rights = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03655-4},
	doi = {10.1038/s41586-021-03655-4},
	pages = {E29--E34},
	number = {7866},
	journaltitle = {Nature},
	author = {Beheim, Bret and Atkinson, Quentin D. and Bulbulia, Joseph and Gervais, Will and Gray, Russell D. and Henrich, Joseph and Lang, Martin and Monroe, M. Willis and Muthukrishna, Michael and Norenzayan, Ara and Purzycki, Benjamin Grant and Shariff, Azim and Slingerland, Edward and Spicer, Rachel and Willard, Aiyana K.},
	urldate = {2021-10-29},
	date = {2021-07},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7866
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: History;Religion
Subject\_term\_id: history;religion},
	keywords = {History, Religion},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3VXVLB9L/Beheim et al. - 2021 - Treatment of missing data determined conclusions r.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/WB7Z36CG/s41586-021-03655-4.html:text/html;Treatment of missing data determined conclusions regarding moralizing gods:/Users/tom/Zotero/storage/KHC8YK52/beheim2021.pdf.pdf:application/pdf},
}

@article{whitehouse_retraction_2021,
	title = {Retraction Note: Complex societies precede moralizing gods throughout world history},
	volume = {595},
	rights = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03656-3},
	doi = {10.1038/s41586-021-03656-3},
	shorttitle = {Retraction Note},
	pages = {320--320},
	number = {7866},
	journaltitle = {Nature},
	author = {Whitehouse, Harvey and François, Pieter and Savage, Patrick E. and Currie, Thomas E. and Feeney, Kevin C. and Cioni, Enrico and Purcell, Rosalind and Ross, Robert M. and Larson, Jennifer and Baines, John and ter Haar, Barend and Covey, Alan and Turchin, Peter},
	urldate = {2021-10-29},
	date = {2021-07},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7866
Primary\_atype: Amendments and Corrections
Publisher: Nature Publishing Group
Subject\_term: Anthropology;Cultural evolution;History;Religion
Subject\_term\_id: anthropology;cultural-evolution;history;religion},
	keywords = {History, Religion, Anthropology, Cultural evolution},
	file = {Full Text PDF:/Users/tom/Zotero/storage/I3MZSQQT/Whitehouse et al. - 2021 - Retraction Note Complex societies precede moraliz.pdf:application/pdf;Retraction Note\: Complex societies precede moralizing gods throughout world history:/Users/tom/Zotero/storage/DSUEH6I4/whitehouse2021.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/WRTMHZWQ/s41586-021-03656-3.html:text/html},
}

@online{noauthor_ncbi_2018,
	title = {{NCBI} Insights : {PubMed} Commons to be Discontinued},
	url = {https://ncbiinsights.ncbi.nlm.nih.gov/2018/02/01/pubmed-commons-to-be-discontinued/},
	shorttitle = {{NCBI} Insights},
	abstract = {This post was updated on February 27, 2018. Update: {NLM} appreciates all of the input we have received in response to our February 1, 2018, announcement that {PubMed} Commons is being discontinued. Th…},
	titleaddon = {{NCBI} Insights},
	urldate = {2021-10-29},
	date = {2018-02-01},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/ZC3NJUZN/pubmed-commons-to-be-discontinued.html:text/html},
}

@article{delamothe_twenty_2002,
	title = {Twenty thousand conversations: Rapid responses suggest new models of knowledge creation},
	volume = {324},
	rights = {© 2002 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/324/7347/1171},
	doi = {10.1136/bmj.324.7347.1171},
	shorttitle = {Twenty thousand conversations},
	abstract = {Conversation is a meeting of minds with different memories and habits. When minds meet, they don't just exchange facts: they transform them, reshape them, draw different implications from them, and engage in new trains of thought. Conversation doesn't just reshuffle the cards; it creates new cards.1

Four years ago we added a feature to bmj.com, which we called “rapid responses.”2 The feature allows readers to respond to articles directly via the website as they are reading them. We don't regard them as second class letters: they are just as eligible for inclusion in the paper journal as letters received in other forms. In fact, most of the letters to the editor that we now print in the paper journal started life as rapid responses.

Our original intention was to post all but the libellous, gratuitously rude, trivial, irrelevant, or incomprehensible on the website within 72 hours.We hoped that, at the very least, it would solve the problem of receiving far more letters to the editor than we could possibly print in the paper journal. We wondered whether rapid responses marked the most democratic step that the journal had ever taken.

So how has it gone? With 20 000 rapid responses now published on the website we consider the experiment a success. Particularly gratifying has been the participation of readers from outside the United Kingdom; in fact, …},
	pages = {1171--1172},
	number = {7347},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Delamothe, Tony and Smith, Richard},
	urldate = {2021-10-29},
	date = {2002-05-18},
	langid = {english},
	pmid = {12016170},
	note = {Publisher: British Medical Journal Publishing Group
Section: Editorial},
	file = {Full Text:/Users/tom/Zotero/storage/ZG8YDF7S/Delamothe and Smith - 2002 - Twenty thousand conversations Rapid responses sug.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/A44PV97R/1171.html:text/html;Twenty thousand conversations\: Rapid responses suggest new models of knowledge creation:/Users/tom/Zotero/storage/FPXQZ4IK/delamothe2002.pdf.pdf:application/pdf},
}

@article{grant_reporting_2013,
	title = {Reporting Quality of Social and Psychological Intervention Trials: A Systematic Review of Reporting Guidelines and Trial Publications},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0065442},
	doi = {10.1371/journal.pone.0065442},
	shorttitle = {Reporting Quality of Social and Psychological Intervention Trials},
	abstract = {Background Previous reviews show that reporting guidelines have improved the quality of trial reports in medicine, yet existing guidelines may not be fully suited for social and psychological intervention trials. Objective/Design We conducted a two-part study that reviewed (1) reporting guidelines for and (2) the reporting quality of social and psychological intervention trials. Data Sources (1) To identify reporting guidelines, we systematically searched multiple electronic databases and reporting guideline registries. (2) To identify trials, we hand-searched 40 journals with the 10 highest impact factors in clinical psychology, criminology, education, and social work. Eligibility (1) Reporting guidelines consisted of articles introducing a checklist of reporting standards relevant to social and psychological intervention trials. (2) Trials reported randomised experiments of complex interventions with psychological, social, or health outcomes. Results (1) We identified 19 reporting guidelines that yielded 147 reporting standards relevant to social and psychological interventions. Social and behavioural science guidelines included 89 standards not found in {CONSORT} guidelines. However, {CONSORT} guidelines used more recommended techniques for development and dissemination compared to other guidelines. (2) Our review of trials (n = 239) revealed that many standards were poorly reported, such as identification as a randomised trial in titles (20\% reported the information) and abstracts (55\%); information about blinding (15\%), sequence generation (23\%), and allocation concealment (17\%); and details about actual delivery of experimental (43\%) and control interventions (34\%), participant uptake (25\%), and service environment (28\%). Only 11 of 40 journals referenced reporting guidelines in “Instructions to Authors.” Conclusion Existing reporting guidelines have important limitations in content, development, and/or dissemination. Important details are routinely missing from trial publications; most leading journals in social and behavioural sciences do not ask authors to follow reporting standards. Findings demonstrate a need to develop a {CONSORT} extension with updated standards for social and psychological intervention trials.},
	pages = {e65442},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Grant, Sean P. and Mayo-Wilson, Evan and Melendez-Torres, G. J. and Montgomery, Paul},
	urldate = {2021-11-02},
	date = {2013-05-29},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Clinical psychology, Social psychology, Social sciences, Randomized controlled trials, Medical journals, Scientific publishing, Research reporting guidelines, Criminology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/2VLWVAET/Grant et al. - 2013 - Reporting Quality of Social and Psychological Inte.pdf:application/pdf;Reporting Quality of Social and Psychological Intervention Trials\: A Systematic Review of Reporting Guidelines and Trial Publications:/Users/tom/Zotero/storage/NZUHGBAG/grant2013.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XNYGPR9L/article.html:text/html},
}

@article{brown_issues_2018-1,
	title = {Issues with data and analyses: Errors, underlying themes, and potential solutions},
	volume = {115},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5856502/},
	doi = {10.1073/pnas.1708279115},
	shorttitle = {Issues with data and analyses},
	abstract = {Some aspects of science, taken at the broadest level, are universal in empirical research. These include collecting, analyzing, and reporting data. In each of these aspects, errors can and do occur. In this work, we first discuss the importance of focusing on statistical and data errors to continually improve the practice of science. We then describe underlying themes of the types of errors and postulate contributing factors. To do so, we describe a case series of relatively severe data and statistical errors coupled with surveys of some types of errors to better characterize the magnitude, frequency, and trends. Having examined these errors, we then discuss the consequences of specific errors or classes of errors. Finally, given the extracted themes, we discuss methodological, cultural, and system-level approaches to reducing the frequency of commonly observed errors. These approaches will plausibly contribute to the self-critical, self-correcting, ever-evolving practice of science, and ultimately to furthering knowledge.},
	pages = {2563--2570},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
	shortjournal = {Proc Natl Acad Sci U S A},
	author = {Brown, Andrew W. and Kaiser, Kathryn A. and Allison, David B.},
	urldate = {2021-11-02},
	date = {2018-03-13},
	pmid = {29531079},
	pmcid = {PMC5856502},
	file = {Issues with data and analyses\: Errors, underlying themes, and potential solutions:/Users/tom/Zotero/storage/22YTFC3D/brown2018.pdf.pdf:application/pdf;PubMed Central Full Text PDF:/Users/tom/Zotero/storage/HIYLNQSE/Brown et al. - 2018 - Issues with data and analyses Errors, underlying .pdf:application/pdf},
}

@article{eden_editors_2002,
	title = {From the Editors: Replication, Meta-Analysis, Scientific Progress, and {AMJ}'s Publication Policy},
	volume = {45},
	issn = {0001-4273},
	url = {https://www.jstor.org/stable/3069317},
	shorttitle = {From the Editors},
	pages = {841--846},
	number = {5},
	journaltitle = {The Academy of Management Journal},
	author = {Eden, Dov},
	urldate = {2021-11-12},
	date = {2002},
	note = {Publisher: Academy of Management},
}

@article{wagenmakers_seven_2021,
	title = {Seven steps toward more transparency in statistical practice},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01211-8},
	doi = {10.1038/s41562-021-01211-8},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Wagenmakers, Eric-Jan and Sarafoglou, Alexandra and Aarts, Sil and Albers, Casper and Algermissen, Johannes and Bahník, Štěpán and van Dongen, Noah and Hoekstra, Rink and Moreau, David and van Ravenzwaaij, Don and Sluga, Aljaž and Stanke, Franziska and Tendeiro, Jorge and Aczel, Balazs},
	urldate = {2021-11-18},
	date = {2021-11-11},
	langid = {english},
	keywords = {toread},
	file = {Wagenmakers et al. - 2021 - Seven steps toward more transparency in statistica.pdf:/Users/tom/Zotero/storage/5LW77FXM/Wagenmakers et al. - 2021 - Seven steps toward more transparency in statistica.pdf:application/pdf},
}

@article{hehman_doing_2021,
	title = {Doing better data visualization},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/25152459211045334},
	doi = {10.1177/25152459211045334},
	abstract = {Methods in data visualization have rapidly advanced over the past decade. Although social scientists regularly need to visualize the results of their analyses, they receive little training in how to best design their visualizations. This tutorial is for individuals whose goal is to communicate patterns in data as clearly as possible to other consumers of science and is designed to be accessible to both experienced and relatively new users of R and ggplot2. In this article, we assume some basic statistical and visualization knowledge and focus on how to visualize rather than what to visualize. We distill the science and wisdom of data-visualization expertise from books, blogs, and online forum discussion threads into recommendations for social scientists looking to convey their results to other scientists. Overarching design philosophies and color decisions are discussed before giving specific examples of code in R for visualizing central tendencies, proportions, and relationships between variables.},
	pages = {25152459211045334},
	number = {4},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Hehman, Eric and Xie, Sally Y.},
	urldate = {2021-11-18},
	date = {2021-10-01},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {toread},
	file = {Hehman and Xie - 2021 - Doing better data visualization.pdf:/Users/tom/Zotero/storage/7AN2DBVF/Hehman and Xie - 2021 - Doing better data visualization.pdf:application/pdf},
}

@article{schulz_empirical_1995,
	title = {Empirical evidence of bias. Dimensions of methodological quality associated with estimates of treatment effects in controlled trials},
	volume = {273},
	issn = {0098-7484},
	doi = {10.1001/jama.273.5.408},
	abstract = {{OBJECTIVE}: To determine if inadequate approaches to randomized controlled trial design and execution are associated with evidence of bias in estimating treatment effects.
{DESIGN}: An observational study in which we assessed the methodological quality of 250 controlled trials from 33 meta-analyses and then analyzed, using multiple logistic regression models, the associations between those assessments and estimated treatment effects.
{DATA} {SOURCES}: Meta-analyses from the Cochrane Pregnancy and Childbirth Database.
{MAIN} {OUTCOME} {MEASURES}: The associations between estimates of treatment effects and inadequate allocation concealment, exclusions after randomization, and lack of double-blinding.
{RESULTS}: Compared with trials in which authors reported adequately concealed treatment allocation, trials in which concealment was either inadequate or unclear (did not report or incompletely reported a concealment approach) yielded larger estimates of treatment effects (P {\textless} .001). Odds ratios were exaggerated by 41\% for inadequately concealed trials and by 30\% for unclearly concealed trials (adjusted for other aspects of quality). Trials in which participants had been excluded after randomization did not yield larger estimates of effects, but that lack of association may be due to incomplete reporting. Trials that were not double-blind also yielded larger estimates of effects (P = .01), with odds ratios being exaggerated by 17\%.
{CONCLUSIONS}: This study provides empirical evidence that inadequate methodological approaches in controlled trials, particularly those representing poor allocation concealment, are associated with bias. Readers of trial reports should be wary of these pitfalls, and investigators must improve their design, execution, and reporting of trials.},
	pages = {408--412},
	number = {5},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Schulz, K. F. and Chalmers, I. and Hayes, R. J. and Altman, D. G.},
	date = {1995-02-01},
	pmid = {7823387},
	keywords = {Models, Statistical, Bias, Randomized Controlled Trials as Topic, Clinical Protocols, Quality Control, Research Design},
}

@online{noauthor_introduction_nodate,
	title = {Introduction to {PRISMA} 2020 and implications for research synthesis methodologists - Page - - Research Synthesis Methods - Wiley Online Library},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1535},
	urldate = {2021-11-18},
	keywords = {toread},
	file = {Introduction to PRISMA 2020 and implications for research synthesis methodologists - Page - - Research Synthesis Methods - Wiley Online Library:/Users/tom/Zotero/storage/I6784S2T/jrsm.html:text/html},
}

@article{suls_now_2021,
	title = {Now is the time to assess the effects of open science practiceswith randomized control trials.},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000871},
	doi = {10.1037/amp0000871},
	abstract = {We call for experimental trials to test the effects of Open Science research practices, an initiative that could improve the entire scientiﬁc enterprise. Well-designed and implemented randomized controlled trials can assist in specifying the effects of different practices and inform decisions regarding their implementation.},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Suls, Jerry and Rothman, Alexander J. and Davidson, Karina W.},
	urldate = {2021-11-29},
	date = {2021-11-22},
	langid = {english},
	keywords = {toread},
	file = {Suls et al. - 2021 - Now is the time to assess the effects of open scie.pdf:/Users/tom/Zotero/storage/UCTPEY8D/Suls et al. - 2021 - Now is the time to assess the effects of open scie.pdf:application/pdf},
}

@article{holcombe_ad_2021,
	title = {Ad hominem rhetoric in scientific psychology},
	volume = {n/a},
	issn = {2044-8295},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bjop.12541},
	doi = {10.1111/bjop.12541},
	abstract = {Ad hominem discourse is largely prohibited in scientific journals. Historically, this prohibition restricted the dissemination of ad hominem discussion, but during the last decade, blogs and social media platforms became popular among researchers. With the use of social media now entrenched among researchers, there are important questions about the role of ad hominems. Ad hominems and other forms of strong criticism became particularly evident in online discussions associated with the recent replication crisis in psychology. Here, these discussions, and a few incidences of ad hominems in journal articles, are situated in the broader history of science. It is argued that explicit codes of conduct should be considered to curb certain kinds of ad hominem comments in certain fora, but that some ad hominem discussions have an important role to play in a healthier science.},
	issue = {n/a},
	journaltitle = {British Journal of Psychology},
	author = {Holcombe, Alex O.},
	urldate = {2021-12-01},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bjop.12541},
	keywords = {replication crisis, social media, meta-science},
	file = {bjop.12541.pdf:/Users/tom/Zotero/storage/84QJDHIA/bjop.12541.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/DBAZJ73H/Holcombe - Ad hominem rhetoric in scientific psychology.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/MXJX384R/bjop.html:text/html},
}

@article{hoekstra_aspiring_2021,
	title = {Aspiring to greater intellectual humility in science},
	volume = {5},
	rights = {2021 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01203-8},
	doi = {10.1038/s41562-021-01203-8},
	abstract = {The replication crisis in the social, behavioural and life sciences has spurred a reform movement aimed at increasing the credibility of scientific studies. Many of these credibility-enhancing reforms focus, appropriately, on specific research and publication practices. A less often mentioned aspect of credibility is the need for intellectual humility or being transparent about and owning the limitations of our work. Although intellectual humility is presented as a widely accepted scientific norm, we argue that current research practice does not incentivize intellectual humility. We provide a set of recommendations on how to increase intellectual humility in research articles and highlight the central role peer reviewers can play in incentivizing authors to foreground the flaws and uncertainty in their work, thus enabling full and transparent evaluation of the validity of research.},
	pages = {1602--1607},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Hoekstra, Rink and Vazire, Simine},
	urldate = {2021-12-01},
	date = {2021-10-28},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Human behaviour;Publishing;Scientific community
Subject\_term\_id: human-behaviour;publishing;scientific-community},
	keywords = {Scientific community, Publishing, Human behaviour},
	file = {Hoekstra and Vazire - 2021 - Aspiring to greater intellectual humility in scien.pdf:/Users/tom/Zotero/storage/TQ9I99PD/Hoekstra and Vazire - 2021 - Aspiring to greater intellectual humility in scien.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3JC38CGM/s41562-021-01203-8.html:text/html},
}

@article{nosek_replicability_2022,
	title = {Replicability, robustness, and reproducibility in psychological science},
	volume = {73},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157},
	doi = {10.1146/annurev-psych-020821-114157},
	abstract = {Replication—an important, uncommon, and misunderstood practice—is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.
            Expected final online publication date for the Annual Review of Psychology, Volume 73 is January 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
	pages = {719--748},
	number = {1},
	journaltitle = {Annual Review of Psychology},
	shortjournal = {Annu. Rev. Psychol.},
	author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aurélien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Struhl, Melissa Kline and Nuijten, Michèle B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Schönbrodt, Felix D. and Vazire, Simine},
	urldate = {2021-12-01},
	date = {2022-01-04},
	langid = {english},
	file = {Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf:/Users/tom/Zotero/storage/ES5EYUJX/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf:application/pdf},
}

@article{chambers_past_2021,
	title = {The past, present and future of Registered Reports},
	rights = {2021 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01193-7},
	doi = {10.1038/s41562-021-01193-7},
	abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
	pages = {1--14},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Chambers, Christopher D. and Tzavella, Loukia},
	urldate = {2021-12-01},
	date = {2021-11-15},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Culture;Publishing
Subject\_term\_id: culture;publishing},
	keywords = {Culture, Publishing},
	file = {Chambers and Tzavella - 2021 - The past, present and future of Registered Reports.pdf:/Users/tom/Zotero/storage/PQK3F6GM/Chambers and Tzavella - 2021 - The past, present and future of Registered Reports.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/G8FUQLY8/s41562-021-01193-7.html:text/html},
}

@article{andre_outlier_2021,
	title = {Outlier exclusion procedures must be blind to the researcher’s hypothesis},
	issn = {1939-2222},
	doi = {10.1037/xge0001069},
	abstract = {When researchers choose to identify and exclude outliers from their data, should they do so across all the data, or within experimental conditions? A survey of recent papers published in the Journal of Experimental Psychology: General shows that both methods are widely used, and common data visualization techniques suggest that outliers should be excluded at the condition-level. However, I highlight in the present paper that removing outliers by condition runs against the logic of hypothesis testing, and that this practice leads to unacceptable increases in false-positive rates. I demonstrate that this conclusion holds true across a variety of statistical tests, exclusion criterion and cutoffs, sample sizes, and data types, and shows in simulated experiments and in a reanalysis of existing data that by-condition exclusions can result in false-positive rates as high as 43\%. I finally demonstrate that by-condition exclusions are a specific case of a more general issue: Any outlier exclusion procedure that is not blind to the hypothesis that researchers want to test may result in inflated Type I errors. I conclude by offering best practices and recommendations for excluding outliers. ({PsycInfo} Database Record (c) 2021 {APA}, all rights reserved)},
	pages = {No Pagination Specified--No Pagination Specified},
	journaltitle = {Journal of Experimental Psychology: General},
	author = {André, Quentin},
	date = {2021},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Surveys, Statistical Analysis, Methodology, Statistical Tests, Type I Errors, Errors, Hypothesis Testing, Sample Size},
	file = {André - 2021 - Outlier exclusion procedures must be blind to the .pdf:/Users/tom/Zotero/storage/RPIR56SZ/André - 2021 - Outlier exclusion procedures must be blind to the .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Q5LKFGBV/2021-49362-001.html:text/html},
}

@article{baumeister_charting_2016,
	title = {Charting the future of social psychology on stormy seas: Winners, losers, and recommendations},
	volume = {66},
	issn = {0022-1031},
	url = {https://www.sciencedirect.com/science/article/pii/S002210311600007X},
	doi = {10.1016/j.jesp.2016.02.003},
	series = {Rigorous and Replicable Methods in Social Psychology},
	shorttitle = {Charting the future of social psychology on stormy seas},
	abstract = {Social psychology's current crisis has prompted calls for larger samples and more replications. Building on Sakaluk's (in this issue) distinction between exploration and confirmation, I argue that this shift will increase correctness of findings, but at the expense of exploration and discovery. The likely effects on the field include aversion to risk, increased difficulty in building careers and hence more capricious hiring and promotion policies, loss of interdisciplinary influence, and rising interest in small, weak findings. Winners (who stand to gain from the mooted changes) include researchers with the patience and requisite resources to assemble large samples; incompetent experimenters; destructive iconoclasts; competing subfields of psychology; and lower-ranked journals, insofar as they publish creative work with small samples. The losers are young researchers; writers of literature reviews and textbooks; flamboyant, creative researchers with lesser levels of patience; and researchers at small colleges. My position is that the field has actually done quite well in recent decades, and improvement should be undertaken as further refinement of a successful approach, in contrast to the Cassandrian view that the field's body of knowledge is hopelessly flawed and radical, revolutionary change is needed. I recommend we retain the exploratory research approach alongside the new, large-sample confirmatory work.},
	pages = {153--158},
	journaltitle = {Journal of Experimental Social Psychology},
	shortjournal = {Journal of Experimental Social Psychology},
	author = {Baumeister, Roy F.},
	urldate = {2021-12-01},
	date = {2016-09-01},
	langid = {english},
	file = {Charting the future of social psychology on stormy seas\: Winners, losers, and recommendations:/Users/tom/Zotero/storage/YX3PQ7N7/baumeister2016.pdf.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/3RU45YJP/S002210311600007X.html:text/html},
}

@article{world_medical_association_world_2013,
	title = {World Medical Association Declaration of Helsinki: ethical principles for medical research involving human subjects},
	volume = {310},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2013.281053},
	doi = {10.1001/jama.2013.281053},
	shorttitle = {World medical association declaration of helsinki},
	pages = {2191},
	number = {20},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {World Medical Association},
	urldate = {2021-12-02},
	date = {2013-11-27},
	langid = {english},
	file = {2013 - World Medical Association Declaration of Helsinki.pdf:/Users/tom/Zotero/storage/RCESB7LX/2013 - World Medical Association Declaration of Helsinki.pdf:application/pdf;World Medical Association Declaration of Helsinki\: Ethical Principles for Medical Research Involving Human Subjects:/Users/tom/Zotero/storage/3HM6FPKQ/world-medical-association-declaration-of-helsinki-2013.pdf.pdf:application/pdf},
}

@report{norris_assessing_2021,
	title = {Assessing Open Science practices in physical activity behaviour change intervention evaluations},
	rights = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2021.12.01.21267126v1},
	abstract = {Objectives: Concerns on the lack of reproducibility and transparency in science have led to a range of research practice reforms, broadly referred to as Open Science. The extent that physical activity interventions are embedding Open Science practices is currently unknown. In this study, we randomly sampled 100 reports of recent physical activity behaviour change interventions to estimate the prevalence of Open Science practices. Methods: One hundred reports of randomised controlled trial physical activity behaviour change interventions published between 2018-2021 were identified. Open Science practices were coded in identified reports, including: study pre-registration, protocol sharing, data-, materials- and analysis scripts-sharing, replication of a previous study, open access publication, funding sources and conflict of interest statements. Coding was performed by two independent researchers, with inter-rater reliability calculated using Krippendorffs alpha. Results: 78\% of the 100 reports provided details of study pre-registration and 41\% provided evidence of a published protocol. 4\% provided accessible open data, 8\% provided open materials and 1\% provided open analysis scripts. 73\% of reports were published as open access and no studies were described as replication attempts. 93\% of reports declared their sources of funding and 88\% provided conflicts of interest statements. A Krippendorffs alpha of 0.73 was obtained across all coding. Conclusion: Open data, materials, analysis and replication attempts are currently rare in physical activity behaviour change intervention reports, whereas funding source and conflict of interest declarations are common. Future physical activity research should increase the reproducibility of their methods and results by incorporating more Open Science practices.},
	pages = {2021.12.01.21267126},
	author = {Norris, Emma and Sulevani, Isra and Finnerty, Ailbhe N. and Castro, Oscar},
	urldate = {2021-12-03},
	date = {2021-12-02},
	langid = {english},
	doi = {10.1101/2021.12.01.21267126},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Type: article},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XW9UW7AS/Norris et al. - 2021 - Assessing Open Science practices in physical activ.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/NSXBVSFY/2021.12.01.html:text/html},
}

@article{hopewell_impact_2014,
	title = {Impact of peer review on reports of randomised trials published in open peer review journals: retrospective before and after study},
	volume = {349},
	rights = {© Hopewell et al 2014. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See:  http://creativecommons.org/licenses/by-nc/3.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/349/bmj.g4145},
	doi = {10.1136/bmj.g4145},
	shorttitle = {Impact of peer review on reports of randomised trials published in open peer review journals},
	abstract = {Objective To investigate the effectiveness of open peer review as a mechanism to improve the reporting of randomised trials published in biomedical journals.
Design Retrospective before and after study.
Setting {BioMed} Central series medical journals.
Sample 93 primary reports of randomised trials published in {BMC}-series medical journals in 2012.
Main outcome measures Changes to the reporting of methodological aspects of randomised trials in manuscripts after peer review, based on the {CONSORT} checklist, corresponding peer reviewer reports, the type of changes requested, and the extent to which authors adhered to these requests.
Results Of the 93 trial reports, 38\% (n=35) did not describe the method of random sequence generation, 54\% (n=50) concealment of allocation sequence, 50\% (n=46) whether the study was blinded, 34\% (n=32) the sample size calculation, 35\% (n=33) specification of primary and secondary outcomes, 55\% (n=51) results for the primary outcome, and 90\% (n=84) details of the trial protocol. The number of changes between manuscript versions was relatively small; most involved adding new information or altering existing information. Most changes requested by peer reviewers had a positive impact on the reporting of the final manuscript—for example, adding or clarifying randomisation and blinding (n=27), sample size (n=15), primary and secondary outcomes (n=16), results for primary or secondary outcomes (n=14), and toning down conclusions to reflect the results (n=27). Some changes requested by peer reviewers, however, had a negative impact, such as adding additional unplanned analyses (n=15).
Conclusion Peer reviewers fail to detect important deficiencies in reporting of the methods and results of randomised trials. The number of these changes requested by peer reviewers was relatively small. Although most had a positive impact, some were inappropriate and could have a negative impact on reporting in the final publication.},
	pages = {g4145},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Hopewell, Sally and Collins, Gary S. and Boutron, Isabelle and Yu, Ly-Mee and Cook, Jonathan and Shanyinde, Milensu and Wharton, Rose and Shamseer, Larissa and Altman, Douglas G.},
	urldate = {2021-12-06},
	date = {2014-07-01},
	langid = {english},
	pmid = {24986891},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/GZVH2YPY/Hopewell et al. - 2014 - Impact of peer review on reports of randomised tri.pdf:application/pdf;Impact of peer review on reports of randomised trials published in open peer review journals\: retrospective before and after study:/Users/tom/Zotero/storage/CG2CGJGN/hopewell2014.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/S99DWULK/bmj.html:text/html},
}

@article{rennie_lets_2016,
	title = {Let’s make peer review scientific},
	volume = {535},
	rights = {2016 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/535031a},
	doi = {10.1038/535031a},
	abstract = {Thirty years on from the first congress on peer review, Drummond Rennie reflects on the improvements brought about by research into the process — and calls for more.},
	pages = {31--33},
	number = {7610},
	journaltitle = {Nature},
	author = {Rennie, Drummond},
	urldate = {2021-12-06},
	date = {2016-07},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7610
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Communication;Medical research;Peer review
Subject\_term\_id: communication;medical-research;peer-review},
	keywords = {Peer review, Communication, Medical research},
	file = {Rennie - 2016 - Let’s make peer review scientific.pdf:/Users/tom/Zotero/storage/Y8NCDFGP/Rennie - 2016 - Let’s make peer review scientific.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SQ5XLE44/535031a.html:text/html},
}

@article{jefferson_editorial_2007,
	title = {Editorial peer review for improving the quality of reports of biomedical studies},
	issn = {1465-1858},
	url = {https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.MR000016.pub3/full},
	doi = {10.1002/14651858.MR000016.pub3},
	number = {2},
	journaltitle = {Cochrane Database of Systematic Reviews},
	author = {Jefferson, Tom and Rudin, Melanie and Folse, Suzanne Brodney and Davidoff, Frank},
	urldate = {2021-12-06},
	date = {2007},
	langid = {english},
	note = {Publisher: John Wiley \& Sons, Ltd},
	file = {Jefferson et al. - 2007 - Editorial peer review for improving the quality of.pdf:/Users/tom/Zotero/storage/XTUQ7BH4/Jefferson et al. - 2007 - Editorial peer review for improving the quality of.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PD3VKDQE/full.html:text/html},
}

@article{jefferson_effects_2002,
	title = {Effects of editorial peer review: a systematic review},
	volume = {287},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.287.21.2784},
	doi = {10.1001/jama.287.21.2784},
	abstract = {{ContextEditorial} peer review is widely used to select submissions to journals for publication and is presumed to improve their usefulness. Sufficient research
on peer review has been published to consider a synthesis of its effects.{MethodsTo} examine the evidence of the effects of editorial peer-review processes
in biomedical journals, we conducted electronic and full-text searches of
private and public databases to June 2000 and corresponded with the World
Association of Medical Editors, European Association of Science Editors, Council
of Science Editors, and researchers in the field to locate comparative studies
assessing the effects of any stage of the peer-review process that made some
attempt to control for confounding. Nineteen of 135 identified studies fulfilled
our criteria. Because of the diversity of study questions, methods, and outcomes,
we did not pool results.{ResultsNine} studies considered the effects of concealing reviewer/author identity.
Four studies suggested that concealing reviewer or author identity affected
review quality (mostly positively); however, methodological limitations make
their findings ambiguous, and other studies' results were either negative
or inconclusive. One study suggested that a statistical checklist can improve
report quality, but another failed to find an effect of publishing another
checklist. One study found no evidence that training referees improves performance
and another showed increased interrater reliability; both used open designs,
making interpretation difficult. Two studies of how journals communicate with
reviewers did not demonstrate any effect on review quality. One study failed
to show reviewer bias, but the findings may not be generalizable. One nonrandomized
study compared the quality of articles published in peer-reviewed vs other
journals. Two studies showed that editorial processes make articles more readable
and improve the quality of reporting, but the findings may have limited generalizability
to other journals.{ConclusionsEditorial} peer review, although widely used, is largely untested and
its effects are uncertain.},
	pages = {2784--2786},
	number = {21},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Jefferson, Tom and Alderson, Philip and Wager, Elizabeth and Davidoff, Frank},
	urldate = {2021-12-06},
	date = {2002-06-05},
	file = {Effects of Editorial Peer ReviewA Systematic Review:/Users/tom/Zotero/storage/YADUEDD7/jefferson2002.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/S8LM5I5E/Jefferson et al. - 2002 - Effects of Editorial Peer ReviewA Systematic Revie.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AXTVJABI/194989.html:text/html},
}

@article{taddio_quality_1994,
	title = {Quality of nonstructured and structured abstracts of original research articles in the British Medical Journal, the Canadian Medical Association Journal and the Journal of the American Medical Association},
	volume = {150},
	issn = {0820-3946},
	abstract = {{OBJECTIVE}: To assess and compare the quality of nonstructured and structured abstracts of original research articles in three medical journals.
{DESIGN}: Blind, criterion-based observational study.
{SAMPLE}: Random sample of 300 abstracts (25 abstracts per journal each year) of articles published in the British Medical Journal ({BMJ}), the Canadian Medical Association Journal and the Journal of the American Medical Association ({JAMA}) in 1988 and 1989 (nonstructured abstracts) and in 1991 and 1992 (structured abstracts).
{MAIN} {OUTCOME} {MEASURES}: The quality of abstracts was measured against 33 objective criteria, which were divided into eight categories (purpose, research design, setting, subjects, intervention, measurement of variables, results and conclusions). The quality score was determined by dividing the number of criteria present by the number applicable; the score varied from 0 to 1.
{RESULTS}: The overall mean quality scores for nonstructured and structured abstracts were 0.57 and 0.74 respectively (p {\textless} 0.001). The frequency in meeting the specific criteria was generally higher for the structured abstracts than for the nonstructured ones. The mean quality score was higher for nonstructured abstracts in {JAMA} than for those in {BMJ} (0.60 v. 0.54, p {\textless} 0.05). The scores for structured abstracts did not differ significantly between the three journals.
{CONCLUSIONS}: The findings support recommendations that promote the use of structured abstracts. Further studies should be performed to assess the effect of time on the quality of abstracts and the extent to which abstracts reflect the content of the articles.},
	pages = {1611--1615},
	number = {10},
	journaltitle = {{CMAJ}: Canadian Medical Association journal = journal de l'Association medicale canadienne},
	shortjournal = {{CMAJ}},
	author = {Taddio, A. and Pain, T. and Fassos, F. F. and Boon, H. and Ilersich, A. L. and Einarson, T. R.},
	date = {1994-05-15},
	pmid = {8174031},
	pmcid = {PMC1336964},
	keywords = {Periodicals as Topic, Abstracting and Indexing},
}

@article{mayo-wilson_peer_2021,
	title = {Peer review reduces spin in {PCORI} research reports},
	volume = {6},
	issn = {2058-8615},
	url = {https://doi.org/10.1186/s41073-021-00119-1},
	doi = {10.1186/s41073-021-00119-1},
	abstract = {The Patient-Centered Outcomes Research Institute ({PCORI}) is obligated to peer review and to post publicly “Final Research Reports” of all funded projects. {PCORI} peer review emphasizes adherence to {PCORI}’s Methodology Standards and principles of ethical scientific communication. During the peer review process, reviewers and editors seek to ensure that results are presented objectively and interpreted appropriately, e.g., free of spin.},
	pages = {16},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Research Integrity and Peer Review},
	author = {Mayo-Wilson, Evan and Phillips, Meredith L. and Connor, Avonne E. and Vander Ley, Kelly J. and Naaman, Kevin and Helfand, Mark},
	urldate = {2021-12-07},
	date = {2021-12-01},
	keywords = {Reporting bias, Peer review, Research funding, Interventions, Patient centered outcomes research institute ({PCORI}), Spin},
	file = {Mayo-Wilson et al. - 2021 - Peer review reduces spin in PCORI research reports.pdf:/Users/tom/Zotero/storage/36VRUW7I/Mayo-Wilson et al. - 2021 - Peer review reduces spin in PCORI research reports.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/EGEWY77F/s41073-021-00119-1.html:text/html},
}

@article{goodman_p-values_2001,
	title = {Of P-Values and Bayes: A Modest Proposal},
	volume = {12},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/fulltext/2001/05000/of_p_values_and_bayes__a_modest_proposal.6.aspx},
	shorttitle = {Of P-Values and Bayes},
	abstract = {An abstract is unavailable.},
	pages = {295--297},
	number = {3},
	journaltitle = {Epidemiology},
	author = {Goodman, Steven N.},
	urldate = {2021-12-08},
	date = {2001-05},
	langid = {american},
	file = {Goodman - 2001 - Of P-Values and Bayes A Modest Proposal.pdf:/Users/tom/Zotero/storage/G7KS5EME/Goodman - 2001 - Of P-Values and Bayes A Modest Proposal.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B7L34BSY/of_p_values_and_bayes__a_modest_proposal.6.html:text/html},
}

@article{bero_why_2016,
	title = {Why having a (nonfinancial) interest is not a conflict of interest},
	volume = {14},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2001221},
	doi = {10.1371/journal.pbio.2001221},
	abstract = {A current debate about conflicts of interest related to biomedical research is to question whether the focus on financial conflicts of interest overshadows “nonfinancial” interests that could put scientific judgment at equal or greater risk of bias. There is substantial evidence that financial conflicts of interest such as commercial sponsorship of research and investigators lead to systematic biases in scientific research at all stages of the research process. Conflation of “conflicts of interest” with “interests” in general serves to muddy the waters about how to manage conflicts of interest. We call for heightened disclosure of conflicts of interest and policy action beyond disclosure as the sole management strategy. We propose a different strategy to manage interests more broadly to ensure fair representation and accountability.},
	pages = {e2001221},
	number = {12},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Bero, Lisa A. and Grundy, Quinn},
	urldate = {2021-12-12},
	date = {2016-12-21},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Medical risk factors, Scientists, Conflicts of interest, Careers, Tobacco, Finance, Careers in research, Industrial research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/UWCG8E6I/Bero and Grundy - 2016 - Why Having a (Nonfinancial) Interest Is Not a Conf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZWUUCHZZ/article.html:text/html;Why Having a (Nonfinancial) Interest Is Not a Conflict of Interest:/Users/tom/Zotero/storage/6QKKLT93/bero2016.pdf.pdf:application/pdf},
}

@article{wieschowski_result_2019,
	title = {Result dissemination from clinical trials conducted at German university medical centers was delayed and incomplete},
	volume = {115},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(18)31063-1/fulltext},
	doi = {10.1016/j.jclinepi.2019.06.002},
	pages = {37--45},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Wieschowski, Susanne and Riedel, Nico and Wollmann, Katharina and Kahrass, Hannes and Müller-Ohlraun, Stephanie and Schürmann, Christopher and Kelley, Sean and Kszuk, Ute and Siegerink, Bob and Dirnagl, Ulrich and Meerpohl, Jörg and Strech, Daniel},
	urldate = {2021-12-12},
	date = {2019-11-01},
	pmid = {31195110},
	note = {Publisher: Elsevier},
	keywords = {Publication bias, Cross-sectional study, Evidence-based medicine, Good scientific practice, Result reporting, Trial registration},
	file = {Result dissemination from clinical trials conducted at German university medical centers was delayed and incomplete:/Users/tom/Zotero/storage/SIX3UNYL/10.1016@j.jclinepi.2019.06.002.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/QXITIZQD/fulltext.html:text/html;Submitted Version:/Users/tom/Zotero/storage/NHGP5Z5H/Wieschowski et al. - 2019 - Result dissemination from clinical trials conducte.pdf:application/pdf},
}

@article{khan_open_2021,
	title = {Open science failed to penetrate academic hiring practices: A cross-sectional study},
	volume = {0},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(21)00400-5/fulltext#.YbXrv03k3U8.twitter},
	doi = {10.1016/j.jclinepi.2021.12.003},
	shorttitle = {Open science failed to penetrate academic hiring practices},
	number = {0},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Khan, Hassan and Almoli, Elham and Franco, Marina Christ and Moher, David},
	urldate = {2021-12-12},
	date = {2021-12-08},
	note = {Publisher: Elsevier},
	keywords = {open science, academic institutions, job advertisements, recruitment},
	file = {Full Text PDF:/Users/tom/Zotero/storage/D6QME9TS/Khan et al. - 2021 - Open science failed to penetrate academic hiring p.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/WFTYAKMN/fulltext.html:text/html},
}

@article{roullet_traumatic_2021,
	title = {Traumatic memory reactivation with or without propranolol for {PTSD} and comorbid {MD} symptoms: a randomised clinical trial},
	volume = {46},
	issn = {1740-634X},
	doi = {10.1038/s41386-021-00984-w},
	shorttitle = {Traumatic memory reactivation with or without propranolol for {PTSD} and comorbid {MD} symptoms},
	abstract = {Post-traumatic stress disorder ({PTSD}) is difficult to treat but one promising strategy is to block memory reconsolidation of the traumatic event. This study aimed to evaluate the efficacy of traumatic memory reactivation under the influence of propranolol, a noradrenergic beta-receptor blocker, in reducing {PTSD} symptoms as well as comorbid major depression ({MD}) symptoms. We conducted a double blind, placebo-controlled, randomised clinical trial in 66 adults diagnosed with longstanding {PTSD}. Propranolol or a placebo was administered 90 min before a brief memory reactivation session, once a week for 6 consecutive weeks. Measures included the {SCID} {PTSD} module, the {PTSD} Check List ({PCL}-S) and the Beck Depression Inventory-{II} ({BDI}-{II}). {PTSD} symptoms decreased both in the pre-reactivation propranolol group (39.28\%) and the pre-reactivation placebo group (34.48 \%). During the 6 treatment sessions, {PCL}-S and {BDI}-{II} scores decreased to similar extent in both groups and there were no treatment differences. During the 3-month follow-up period, there were no treatment effects for the mean {PCL}-S and {BDI}-{II} scores. However, in patients with severe {PTSD} symptoms ({PCL}-S ≥ 65) before treatment, {PCL}-S and {BDI}-{II} scores continued to decline 3 months after the end of treatment in the propranolol group while they increased in the placebo group. Repeated traumatic memory reactivation seemed to be effective for {PTSD} and comorbid {MD} symptoms. However, the efficacy of propranolol was not greater than that of placebo 1 week post treatment. Furthermore, in this traumatic memory reactivation, {PTSD} symptom severity at baseline might have influenced the post-treatment effect of propranolol.},
	pages = {1643--1649},
	number = {9},
	journaltitle = {Neuropsychopharmacology: Official Publication of the American College of Neuropsychopharmacology},
	shortjournal = {Neuropsychopharmacology},
	author = {Roullet, Pascal and Vaiva, Guillaume and Véry, Etienne and Bourcier, Axel and Yrondi, Antoine and Dupuch, Laetitia and Lamy, Pierre and Thalamas, Claire and Jasse, Laurence and El Hage, Wissam and Birmes, Philippe},
	date = {2021-08},
	pmid = {33612830},
	pmcid = {PMC7897457},
	keywords = {Humans, Adult, Adrenergic beta-Antagonists, Depressive Disorder, Major, Propranolol, Stress Disorders, Post-Traumatic},
	file = {Full Text:/Users/tom/Zotero/storage/XA8VCJ2L/Roullet et al. - 2021 - Traumatic memory reactivation with or without prop.pdf:application/pdf;Traumatic memory reactivation with or without propranolol for PTSD and comorbid MD symptoms\: a randomised clinical trial:/Users/tom/Zotero/storage/JXDUZUWE/roullet2021.pdf.pdf:application/pdf},
}

@report{sarafoglou_is_2021,
	title = {Is preregistration worthwhile? A survey on personal experiences},
	url = {https://psyarxiv.com/6k5gr/},
	shorttitle = {Is preregistration worthwhile?},
	abstract = {The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioral sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the benefits and challenges of preregistration from the researcher's perspective. To this aim, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects, and that preregistration makes it easier to avoid questionable research practices. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. The majority of researchers with experience in preregistration reported that the benefits outweigh the challenges. However, the majority of researchers without preregistration would not consider preregistration for future projects or recommend the practice to colleagues. Our interpretation of the results is that preregistration can have positive side-effects as it adds an extra preparatory step in researchers' workflow, thus requiring researchers to think through the theoretical and practical aspects of their project.},
	institution = {{PsyArXiv}},
	author = {Sarafoglou, Alexandra and Kovacs, Marton and Bakos, Bence E. and Wagenmakers, Eric-Jan and Aczel, Balazs},
	urldate = {2021-12-22},
	date = {2021-12-20},
	langid = {english},
	doi = {10.31234/osf.io/6k5gr},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, other, Psychology, Quantitative Methods, Open Science, Meta-Science, Replication Crisis},
	file = {Sarafoglou et al. - 2021 - Is Preregistration Worthwhile A Survey on Persona.pdf:/Users/tom/Zotero/storage/IQ97EXDZ/Sarafoglou et al. - 2021 - Is Preregistration Worthwhile A Survey on Persona.pdf:application/pdf},
}

@article{erasmus_data-dredging_2021,
	title = {Data-dredging bias},
	rights = {© Author(s) (or their employer(s)) 2021. No commercial re-use. See rights and permissions. Published by {BMJ}.},
	issn = {2515-446X, 2515-4478},
	url = {https://ebm.bmj.com/content/early/2021/12/19/bmjebm-2020-111584},
	doi = {10.1136/bmjebm-2020-111584},
	abstract = {Data-dredging bias encompasses a number of more specific questionable practices (eg, fishing, p-hacking) all of which involve probing data using unplanned analyses and then reporting salient results without accurately describing the processes by which the results were generated. Almost any process of data analysis involves numerous decisions necessary to complete the analysis (eg, how to handle outliers, whether to combine groups, including/excluding covariates). Where possible, it is the best practice for these decisions to be guided by a principled approach and prespecified in a publicly available protocol. When it is not possible, authors must be transparent about the open-ended nature of their analysis.

Many different sets of choices may well be methodologically defensible and reliablewhen the specifications are made prior to the analysis. However, probing the data and selectively reporting an outcome as if it were always the intended course of analysis dramatically increases the likelihood of finding a statistically significant result when there is in fact no effect (ie, a false positive).

As an intuitive example, consider a hypothesis that a given coin is unfairly biased to heads. Suppose I flip the coin twenty times each day for a week and assume that I am allowed to (1) eliminate data from any given day; (2) consider only the first 10 flips in a day, the last 10 flips in a day, or all 20 flips; and (3) restrict my consideration to only flips that were preceded by a heads or flips that were preceded by a tails. I would be conducting a fair trial if I prespecify that I will consider only the results when the prior flip was ‘tails’ and the flip was one of the last 10 in the series for the day, but I will be excluding the results from Wednesday. This is because none of …},
	journaltitle = {{BMJ} Evidence-Based Medicine},
	author = {Erasmus, Adrian and Holman, Bennett and Ioannidis, John},
	urldate = {2021-12-22},
	date = {2021-12-20},
	langid = {english},
	pmid = {34930812},
	note = {Publisher: Royal Society of Medicine
Section: {EBM} learning},
	keywords = {methods},
	file = {Erasmus et al. - 2021 - Data-dredging bias.pdf:/Users/tom/Zotero/storage/FU52XN46/Erasmus et al. - 2021 - Data-dredging bias.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/CXFC4L54/bmjebm-2020-111584.html:text/html},
}

@article{garcia-berthou_incongruence_2004,
	title = {Incongruence between test statistics and P values in medical papers},
	volume = {4},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-4-13},
	doi = {10.1186/1471-2288-4-13},
	abstract = {Given an observed test statistic and its degrees of freedom, one may compute the observed P value with most statistical packages. It is unknown to what extent test statistics and P values are congruent in published medical papers.},
	pages = {13},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {García-Berthou, Emili and Alcaraz, Carles},
	urldate = {2022-01-08},
	date = {2004-05-28},
	keywords = {Digit Preference, Medical Paper, Probable Digit, Relative Bias, Transcription Error},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3BGGAY5T/García-Berthou and Alcaraz - 2004 - Incongruence between test statistics and P values .pdf:application/pdf;Incongruence between test statistics and P values in medical papers:/Users/tom/Zotero/storage/AYUJTUZW/10.1186@1471-2288-4-13.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RPYBXQYS/1471-2288-4-13.html:text/html},
}

@article{ioannidis_anticipating_2016,
	title = {Anticipating consequences of sharing raw data and code and of awarding badges for sharing},
	volume = {70},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(15)00326-1/fulltext},
	doi = {10.1016/j.jclinepi.2015.04.015},
	pages = {258--260},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Ioannidis, John P. A.},
	urldate = {2022-01-08},
	date = {2016-02-01},
	pmid = {26163123},
	note = {Publisher: Elsevier},
	file = {Anticipating consequences of sharing raw data and code and of awarding badges for sharing:/Users/tom/Zotero/storage/FYYMQ9P2/ioannidis2016.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/6FEX2E2V/Ioannidis - 2016 - Anticipating consequences of sharing raw data and .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/W3V2X7C6/fulltext.html:text/html},
}

@report{lin_fooled_2021,
	title = {Fooled by beautiful data: Visualization aesthetics bias trust in science, news, and social media},
	url = {https://psyarxiv.com/dnr9s/},
	shorttitle = {Fooled by beautiful data},
	abstract = {Scientists, policymakers, and the public increasingly rely on data visualizations – such as {COVID} tracking charts, weather forecast maps, and political polling graphs – to inform important decisions. The aesthetic decisions of graph-makers may produce graphs of varying visual appeal, independent of data quality. Here we tested whether the beauty of a graph influences how much people trust it. Across three studies, we sampled graphs from social media, news reports, and scientific publications, and consistently found that graph beauty predicted trust. In a fourth study, we manipulated both the graph beauty and misleadingness. We found that beauty, but not actual misleadingness, causally affected trust. These findings reveal a source of bias in the interpretation of quantitative data and indicate the importance of promoting data literacy in education.},
	institution = {{PsyArXiv}},
	author = {Lin, Chujun and Thornton, Mark Allen},
	urldate = {2022-01-08},
	date = {2021-12-17},
	langid = {english},
	doi = {10.31234/osf.io/dnr9s},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, other, Psychology, Publication Bias, Aesthetics, Beauty-is-good Stereotype, Causal Effects, Data Visualizations, Public Trust},
	file = {Full Text PDF:/Users/tom/Zotero/storage/P5J56MAC/Lin and Thornton - 2021 - Fooled by beautiful data Visualization aesthetics.pdf:application/pdf},
}

@article{gaudino_effects_2021,
	title = {Effects of Experimental Interventions to Improve the Biomedical Peer‐Review Process: A Systematic Review and Meta‐Analysis},
	volume = {10},
	url = {https://www.ahajournals.org/doi/full/10.1161/JAHA.120.019903},
	doi = {10.1161/JAHA.120.019903},
	shorttitle = {Effects of Experimental Interventions to Improve the Biomedical Peer‐Review Process},
	abstract = {Background
Quality of the peer‐review process has been tested only in small studies. We describe and summarize the randomized trials that investigated interventions aimed at improving peer‐review process of biomedical manuscripts.

Methods and Results
All randomized trials comparing different peer‐review interventions at author‐, reviewer‐, and/or editor‐level were included. Differences between traditional and intervention‐modified peer‐review processes were pooled as standardized mean difference ({SMD}) in quality based on the definitions used in the individual studies. Main outcomes assessed were quality and duration of the peer‐review process. Five‐hundred and seventy‐five studies were retrieved, eventually yielding 24 randomized trials. Eight studies evaluated the effect of interventions at author‐level, 16 at reviewer‐level, and 3 at editor‐level. Three studies investigated interventions at multiple levels. The effects of the interventions were reported as mean change in review quality, duration of the peer‐review process, acceptance/rejection rate, manuscript quality, and number of errors detected in 13, 11, 5, 4, and 3 studies, respectively. At network meta‐analysis, reviewer‐level interventions were associated with a significant improvement in review quality ({SMD}, 0.20 [0.06 to 0.33]), at the cost of increased duration of the review process ({SMD}, 0.15 [0.01 to 0.29]), except for reviewer blinding. Author‐ and editor‐level interventions did not significantly impact peer‐review quality and duration (respectively, {SMD}, 0.17 [−0.16 to 0.51] and {SMD}, 0.19 [−0.40 to 0.79] for quality, and {SMD}, 0.17 [−0.16 to 0.51] and {SMD}, 0.19 [−0.40 to 0.79] for duration).

Conclusions
Modifications of the traditional peer‐review process at reviewer‐level are associated with improved quality, at the price of longer duration. Further studies are needed.

Registration
{URL}: https://www.crd.york.ac.uk/prospero; Unique identifier: {CRD}42020187910.},
	pages = {e019903},
	number = {15},
	journaltitle = {Journal of the American Heart Association},
	author = {Gaudino, Mario and Robinson, N. Bryce and Di Franco, Antonino and Hameed, Irbaz and Naik, Ajita and Demetres, Michelle and Girardi, Leonard N. and Frati, Giacomo and Fremes, Stephen E. and Biondi‐Zoccai, Giuseppe},
	urldate = {2022-01-08},
	date = {2021-08-03},
	note = {Publisher: American Heart Association},
	keywords = {network meta‐analysis, peer‐review, review quality},
	file = {Full Text PDF:/Users/tom/Zotero/storage/2YER3SU2/Gaudino et al. - 2021 - Effects of Experimental Interventions to Improve t.pdf:application/pdf},
}

@article{mayo-wilson_peer_2021-1,
	title = {Peer review reduces spin in {PCORI} research reports},
	volume = {6},
	issn = {2058-8615},
	url = {https://doi.org/10.1186/s41073-021-00119-1},
	doi = {10.1186/s41073-021-00119-1},
	abstract = {The Patient-Centered Outcomes Research Institute ({PCORI}) is obligated to peer review and to post publicly “Final Research Reports” of all funded projects. {PCORI} peer review emphasizes adherence to {PCORI}’s Methodology Standards and principles of ethical scientific communication. During the peer review process, reviewers and editors seek to ensure that results are presented objectively and interpreted appropriately, e.g., free of spin.},
	pages = {16},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Research Integrity and Peer Review},
	author = {Mayo-Wilson, Evan and Phillips, Meredith L. and Connor, Avonne E. and Vander Ley, Kelly J. and Naaman, Kevin and Helfand, Mark},
	urldate = {2022-01-08},
	date = {2021-12-01},
	keywords = {Reporting bias, Peer review, Research funding, Interventions, Patient centered outcomes research institute ({PCORI}), Spin},
	file = {Full Text PDF:/Users/tom/Zotero/storage/N5BEFG8N/Mayo-Wilson et al. - 2021 - Peer review reduces spin in PCORI research reports.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RZ3FXJ5H/s41073-021-00119-1.html:text/html},
}

@article{ghannad_randomized_2021,
	title = {A randomized trial of an editorial intervention to reduce spin in the abstract’s conclusion of manuscripts showed no significant effect},
	volume = {130},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435620311537},
	doi = {10.1016/j.jclinepi.2020.10.014},
	abstract = {Objective
To estimate the effect of an intervention compared to the usual peer-review process on reducing spin in the abstract’s conclusion of biomedical study reports.
Study Design and Setting
We conducted a two-arm, parallel-group {RCT} in a sample of primary research manuscripts submitted to {BMJ} Open. The authors received short instructions alongside the peer reviewers’ comments in the intervention group. We assessed the presence of spin (primary outcome), types of spin, and wording change in the revised abstract’s conclusion. Outcome assessors were blinded to the intervention assignment.
Results
Of the 184 manuscripts randomized, 108 (54 intervention, 54 control) were selected for revision and could be evaluated for the presence of spin. The proportion of manuscripts with spin was 6\% lower (95\% {CI}: 24\% lower to 13\% higher) in the intervention group (57\%, 31/54) than in the control group (63\%, 34/54). The wording of the revised abstract’s conclusion was changed in 34/54 (63\%) manuscripts in the intervention group and 26/54 (48\%) in the control group. The four prespecified types of spin involved (i) selective reporting (12 in the intervention group vs. 8 in the control group), (ii) including information not supported by evidence (9 vs. 9), and (iii) interpretation not consistent with the study results (14 vs. 18), and (iv) unjustified recommendations for practice (5 vs. 11).
Conclusion
These short instructions to authors did not have a statistically significant effect on reducing spin in revised abstract conclusions, and based on the confidence interval, the existence of a large effect can be excluded. Other interventions to reduce spin in reports of original research should be evaluated.
Study registration
osf.io/xnuyt.},
	pages = {69--77},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Ghannad, Mona and Yang, Bada and Leeflang, Mariska and Aldcroft, Adrian and Bossuyt, Patrick M. and Schroter, Sara and Boutron, Isabelle},
	urldate = {2022-01-08},
	date = {2021-02-01},
	langid = {english},
	keywords = {Peer review, Editors, Meta-research, Spin, Inappropriate extrapolation of results, Misrepresentation, Overinterpretation, Publication, {RCT}},
	file = {A randomized trial of an editorial intervention to reduce spin in the abstract’s conclusion of manuscripts showed no significant effect:/Users/tom/Zotero/storage/G4PA8P2M/10.1016@j.jclinepi.2020.10.014.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/L5QL3VVC/Ghannad et al. - 2021 - A randomized trial of an editorial intervention to.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/CPMX75CN/S0895435620311537.html:text/html},
}

@article{crijns_effect_2021,
	title = {The effect of peer review on the improvement of rejected manuscripts},
	volume = {28},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2020.1869547},
	doi = {10.1080/08989621.2020.1869547},
	abstract = {Peer review is intended to improve the quality and clarity of scientific reports. Upon rejection, authors receive suggestions from knowledgeable field experts. It is unclear whether authors take full advantage of the peer review process to improve their work before publication in another journal. We identified all actionable suggestions in rejection letters of 250 randomly selected manuscripts from a prominent orthopedic journal in 2012. We searched {PubMed} and Google Scholar and compared the published text to the initial submission to determine if reviewer suggestions were addressed. Two hundred (80\%) of the 250 rejected manuscripts were published in another journal by July 2018. Among the 609 substantive actionable queries, 205 (34\%) were addressed in the published manuscripts. The suggestions most frequently addressed were in the title and abstract (48\%). Our findings suggest that authors often disregard advice from peer reviewers after rejection. Authors may regard the peer review process as particular to a journal rather than a process to optimize dissemination of useful, accurate knowledge in any media. Specialty journalsmight consider collaborating by using a single manuscript submission site that allows peer reviews to be transferred to the next journal, which helps holding authors accountable for making the suggested changes.},
	pages = {517--527},
	number = {8},
	journaltitle = {Accountability in Research},
	author = {Crijns, Tom J. and Ottenhoff, Janna S. E. and Ring, David},
	urldate = {2022-01-08},
	date = {2021-11-17},
	pmid = {33393365},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2020.1869547},
	keywords = {Peer review, actionable suggestions, publications, reviewer suggestions},
	file = {Crijns et al. - 2021 - The effect of peer review on the improvement of re.pdf:/Users/tom/Zotero/storage/CKHVSQX7/Crijns et al. - 2021 - The effect of peer review on the improvement of re.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7Y3EMSRE/08989621.2020.html:text/html},
}

@article{mayo_statistics_2021,
	title = {The statistics wars and intellectual conflicts of interest},
	issn = {1523-1739},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cobi.13861},
	doi = {10.1111/cobi.13861},
	journaltitle = {Conservation Biology},
	author = {Mayo, Deborah G.},
	urldate = {2022-01-08},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cobi.13861},
	file = {Full Text PDF:/Users/tom/Zotero/storage/R5PP62QZ/Mayo - The statistics wars and intellectual conflicts of .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8XDTXFXS/cobi.html:text/html},
}

@article{fricker_assessing_2019,
	title = {Assessing the statistical analyses used in Basic and Applied Social Psychology after their p-value ban},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1537892},
	doi = {10.1080/00031305.2018.1537892},
	abstract = {In this article, we assess the 31 articles published in Basic and Applied Social Psychology ({BASP}) in 2016, which is one full year after the {BASP} editors banned the use of inferential statistics. We discuss how the authors collected their data, how they reported and summarized their data, and how they used their data to reach conclusions. We found multiple instances of authors overstating conclusions beyond what the data would support if statistical significance had been considered. Readers would be largely unable to recognize this because the necessary information to do so was not readily available.},
	pages = {374--384},
	issue = {sup1},
	journaltitle = {The American Statistician},
	author = {Fricker, Ronald D. and Burke, Katherine and Han, Xiaoyan and Woodall, William H.},
	urldate = {2022-01-08},
	date = {2019-03-29},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1537892},
	keywords = {Psychology, Statistical significance, {NHST}, Effect size, Inference ban},
	file = {Assessing the Statistical Analyses Used in Basic and Applied Social Psychology After Their p-Value Ban:/Users/tom/Zotero/storage/GAH6RHFH/10.1080@00031305.2018.1537892.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/JNJJCMNZ/Fricker et al. - 2019 - Assessing the Statistical Analyses Used in Basic a.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UTUP49BK/00031305.2018.html:text/html},
}

@article{trafimow_editorial_2015,
	title = {Editorial},
	volume = {37},
	issn = {0197-3533},
	url = {https://doi.org/10.1080/01973533.2015.1012991},
	doi = {10.1080/01973533.2015.1012991},
	pages = {1--2},
	number = {1},
	journaltitle = {Basic and Applied Social Psychology},
	author = {Trafimow, David and Marks, Michael},
	urldate = {2022-01-08},
	date = {2015-01-02},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/01973533.2015.1012991},
	file = {Editorial:/Users/tom/Zotero/storage/YVD6M7RG/trafimow2015.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/3PHR9V7I/Trafimow and Marks - 2015 - Editorial.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SHXQKB6K/01973533.2015.html:text/html},
}

@book{cook_quasi-experimentation_1979,
	location = {Boston, {MA}},
	title = {Quasi-experimentation: Design and analysis issues for field settings.},
	publisher = {Houghton Mifflin},
	author = {Cook, T. D. and Campbell, D. T.},
	date = {1979},
}

@article{hoekstra_robust_2014,
	title = {Robust misinterpretation of confidence intervals},
	volume = {21},
	rights = {2014 Psychonomic Society, Inc.},
	issn = {1531-5320},
	url = {https://link.springer.com/article/10.3758/s13423-013-0572-3},
	doi = {10.3758/s13423-013-0572-3},
	abstract = {Null hypothesis significance testing ({NHST}) is undoubtedly the most common inferential technique used to justify claims in the social sciences. However, even staunch defenders of {NHST} agree that its outcomes are often misinterpreted. Confidence intervals ({CIs}) have frequently been proposed as a more useful alternative to {NHST}, and their use is strongly encouraged in the {APA} Manual. Nevertheless, little is known about how researchers interpret {CIs}. In this study, 120 researchers and 442 students—all in the field of psychology—were asked to assess the truth value of six particular statements involving different interpretations of a {CI}. Although all six statements were false, both researchers and students endorsed, on average, more than three statements, indicating a gross misunderstanding of {CIs}. Self-declared experience with statistics was not related to researchers’ performance, and, even more surprisingly, researchers hardly outperformed the students, even though the students had not received any education on statistical inference whatsoever. Our findings suggest that many researchers do not know the correct interpretation of a {CI}. The misunderstandings surrounding p-values and {CIs} are particularly unfortunate because they constitute the main tools by which psychologists draw conclusions from data.},
	pages = {1157--1164},
	number = {5},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Hoekstra, Rink and Morey, Richard D. and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
	urldate = {2022-01-08},
	date = {2014-10-01},
	langid = {english},
	note = {Company: Springer
Distributor: Springer
Institution: Springer
Label: Springer
Number: 5
Publisher: Springer {US}},
	file = {Robust misinterpretation of confidence intervals:/Users/tom/Zotero/storage/72NUQEPN/hoekstra2014.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SUG4HAVR/10.html:text/html},
}

@article{altman_statistics_1982,
	title = {Statistics in medical journals},
	volume = {1},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780010109},
	doi = {10.1002/sim.4780010109},
	abstract = {The general standard of statistics in medical journals is poor. This paper considers the reasons for this with illustrations of the types of error that are common. The consequences of incorrect statistics in published papers are discussed; these involve scientific and ethical issues. Suggestions are made about ways in which the standard of statistics may be improved. Particular emphasis is given to the necessity for medical journals to have proper statistical refereeing of submitted papers.},
	pages = {59--71},
	number = {1},
	journaltitle = {Statistics in Medicine},
	author = {Altman, Douglas G.},
	urldate = {2022-01-08},
	date = {1982},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780010109},
	keywords = {Medical journals, Ethics Statistical refereeing, Statistical errors, Statistical guidelines},
	file = {Snapshot:/Users/tom/Zotero/storage/UM9IZC9A/sim.html:text/html;Statistics in medical journals:/Users/tom/Zotero/storage/PETNK22A/altman1982.pdf.pdf:application/pdf},
}

@article{altman_statistical_1983,
	title = {Statistical guidelines for contributors to medical journals.},
	volume = {286},
	issn = {0267-0623},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1547706/},
	pages = {1489--1493},
	number = {6376},
	journaltitle = {British Medical Journal (Clinical research ed.)},
	shortjournal = {Br Med J (Clin Res Ed)},
	author = {Altman, D G and Gore, S M and Gardner, M J and Pocock, S J},
	urldate = {2022-01-08},
	date = {1983-05-07},
	pmid = {6405856},
	pmcid = {PMC1547706},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/8ENYB42L/Altman et al. - 1983 - Statistical guidelines for contributors to medical.pdf:application/pdf},
}

@article{bailar_guidelines_1988,
	title = {Guidelines for statistical reporting in articles for medical journals},
	volume = {108},
	issn = {0003-4819},
	url = {https://www.acpjournals.org/doi/10.7326/0003-4819-108-2-266},
	doi = {10.7326/0003-4819-108-2-266},
	pages = {266--273},
	number = {2},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Bailar, John C. and Mosteller, Frederick},
	urldate = {2022-01-08},
	date = {1988-02-01},
	note = {Publisher: American College of Physicians},
	file = {Guidelines for Statistical Reporting in Articles for Medical Journals:/Users/tom/Zotero/storage/4ZYNTXKN/bailar1988.pdf.pdf:application/pdf},
}

@incollection{smith_statistical_2005,
	title = {Statistical review for medical journals, journal's perspective},
	isbn = {978-0-470-01181-2},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0470011815.b2a17141},
	abstract = {Medical journals have moved away from case reports toward accounts of research studies, particularly randomized trials and epidemiologic studies, and statisticians have become central to medical journals and medical practice. They have played an important part in the move from opinion to evidence-based medicine. The author is Editor of the British Medical Journal and describes the contributions of statisticians to that journal's reviewing process: they are included in the team taking the final decision on whether to publish a paper. The article contains recommendations for policy on statistical review.},
	booktitle = {Encyclopedia of Biostatistics},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Smith, Richard},
	urldate = {2022-01-08},
	date = {2005},
	langid = {english},
	doi = {10.1002/0470011815.b2a17141},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0470011815.b2a17141},
	keywords = {British Medical Journal, evidence-based},
	file = {Smith - 2005 - Statistical review for medical journals, journal's.pdf:/Users/tom/Zotero/storage/IBS5V8JP/Smith - 2005 - Statistical review for medical journals, journal's.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZP3I6UWS/0470011815.html:text/html},
}

@article{finch_reform_2004,
	title = {Reform of statistical inference in psychology: The case of Memory \& Cognition},
	volume = {36},
	issn = {1532-5970},
	url = {https://doi.org/10.3758/BF03195577},
	doi = {10.3758/BF03195577},
	shorttitle = {Reform of statistical inference in psychology},
	abstract = {Geoffrey Loftus, Editor {ofMemory} \& Cognition from 1994 to 1997, strongly encouraged presentation of figures with error bars and avoidance of null hypothesis significance testing ({NHST}). The authors examined 696Memory \& Cognition articles published before, during, and after the Loftus editorship. Use of figures with bars increased to 47\% under Loftus’s editorship and then declined. Bars were rarely used for interpretation, and {NHST} remained almost universal. Analysis of 309 articles in other psychology journals confirmed that Loftus’s influence was most evident in the articles he accepted for publication, but was otherwise limited. An e-mail survey of authors of papers accepted by Loftus revealed some support for his policy, but allegiance to traditional practices as well. Reform of psychologists’ statistical practices would require more than editorial encouragement.},
	pages = {312--324},
	number = {2},
	journaltitle = {Behavior Research Methods, Instruments, \& Computers},
	shortjournal = {Behavior Research Methods, Instruments, \& Computers},
	author = {Finch, Sue and Cumming, Geoff and Williams, Jennifer and Palmer, Lee and Griffith, Elvira and Alders, Chris and Anderson, James and Goodman, Olivia},
	urldate = {2022-01-08},
	date = {2004-05-01},
	langid = {english},
	file = {Reform of statistical inference in psychology\: The case ofMemory & Cognition:/Users/tom/Zotero/storage/NQ55BQFW/finch2004.pdf.pdf:application/pdf;Springer Full Text PDF:/Users/tom/Zotero/storage/DIGW4SD8/Finch et al. - 2004 - Reform of statistical inference in psychology The.pdf:application/pdf},
}

@article{fidler_editors_2004,
	title = {Editors can lead researchers to confidence intervals, but can't make them think: statistical reform lessons from medicine},
	volume = {15},
	issn = {0956-7976},
	url = {https://doi.org/10.1111/j.0963-7214.2004.01502008.x},
	doi = {10.1111/j.0963-7214.2004.01502008.x},
	shorttitle = {Editors can lead researchers to confidence intervals, but can't make them think},
	abstract = {Since the mid-1980s, confidence intervals ({CIs}) have been standard in medical journals. We sought lessons for psychology from medicine's experience with statistical reform by investigating two attempts by Kenneth Rothman to change statistical practices. We examined 594 American Journal of Public Health ({AJPH}) articles published between 1982 and 2000 and 110 Epidemiology articles published in 1990 and 2000. Rothman's editorial instruction to report {CIs} and not p values was largely effective: In {AJPH}, sole reliance on p values dropped from 63\% to 5\%, and {CI} reporting rose from 10\% to 54\%; Epidemiology showed even stronger compliance. However, compliance was superficial: Very few authors referred to {CIs} when discussing results. The results of our survey support what other research has indicated: Editorial policy alone is not a sufficient mechanism for statistical reform. Achieving substantial, desirable change will require further guidance regarding use and interpretation of {CIs} and appropriate effect size measures. Necessary steps will include studying researchers' understanding of {CIs}, improving education, and developing empirically justified recommendations for improved statistical practice.},
	pages = {119--126},
	number = {2},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Fidler, Fiona and Thomason, Neil and Cumming, Geoff and Finch, Sue and Leeman, Joanna},
	urldate = {2022-01-08},
	date = {2004-02-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Editors Can Lead Researchers to Confidence Intervals, but Can't Make Them Think\: Statistical Reform Lessons From Medicine:/Users/tom/Zotero/storage/EEGF85EY/fidler2004.pdf.pdf:application/pdf},
}

@article{killeen_alternative_2005,
	title = {An alternative to null-hypothesis significance tests},
	volume = {16},
	issn = {0956-7976},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1473027/},
	doi = {10.1111/j.0956-7976.2005.01538.x},
	abstract = {The statistic prep estimates the probability of replicating an effect. It captures traditional publication criteria for signal-to-noise ratio, while avoiding parametric inference and the resulting Bayesian dilemma. In concert with effect size and replication intervals, prep provides all of the information now used in evaluating research, while avoiding many of the pitfalls of traditional statistical inference.},
	pages = {345--353},
	number = {5},
	journaltitle = {Psychological science},
	shortjournal = {Psychol Sci},
	author = {Killeen, Peter R.},
	urldate = {2022-01-08},
	date = {2005-05},
	pmid = {15869691},
	pmcid = {PMC1473027},
	file = {An Alternative to Null-Hypothesis Significance Tests:/Users/tom/Zotero/storage/CM5P3657/killeen2005.pdf.pdf:application/pdf;PubMed Central Full Text PDF:/Users/tom/Zotero/storage/5K4VUH6U/Killeen - 2005 - An Alternative to Null-Hypothesis Significance Tes.pdf:application/pdf},
}

@article{iverson_model-averaging_2010,
	title = {A model-averaging approach to replication: The case of prep.},
	volume = {15},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0017182},
	doi = {10.1037/a0017182},
	shorttitle = {A model-averaging approach to replication},
	pages = {172--181},
	number = {2},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Iverson, Geoffrey J. and Wagenmakers, Eric-Jan and Lee, Michael D.},
	urldate = {2022-01-08},
	date = {2010-06},
	langid = {english},
	file = {A model-averaging approach to replication\: The case of prep.:/Users/tom/Zotero/storage/IG2N9YZQ/iverson2010.pdf.pdf:application/pdf},
}

@article{maraun_killeens_2010,
	title = {Killeen's (2005) prep coefficient: Logical and mathematical problems.},
	volume = {15},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0016955},
	doi = {10.1037/a0016955},
	shorttitle = {Killeen's (2005) prep coefficient},
	pages = {182--191},
	number = {2},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Maraun, Michael and Gabriel, Stephanie},
	urldate = {2022-01-08},
	date = {2010-06},
	langid = {english},
	file = {Killeen's (2005) prep coefficient\: Logical and mathematical problems.:/Users/tom/Zotero/storage/DPHFXCKW/maraun2010.pdf.pdf:application/pdf},
}

@article{iverson_prep_2009,
	title = {prep: An agony in five Fits},
	volume = {53},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249608000801},
	doi = {10.1016/j.jmp.2008.09.004},
	pages = {195--202},
	number = {4},
	journaltitle = {Journal of Mathematical Psychology},
	shortjournal = {Journal of Mathematical Psychology},
	author = {Iverson, Geoffrey J. and Lee, Michael D. and Zhang, Shunan and Wagenmakers, Eric-Jan},
	urldate = {2022-01-08},
	date = {2009-08},
	langid = {english},
	file = {\: An agony in five Fits:/Users/tom/Zotero/storage/B73FJN6J/iverson2009.pdf.pdf:application/pdf},
}

@article{nieuwenhuis_erroneous_2011,
	title = {Erroneous analyses of interactions in neuroscience: a problem of significance},
	volume = {14},
	rights = {2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.2886},
	doi = {10.1038/nn.2886},
	shorttitle = {Erroneous analyses of interactions in neuroscience},
	abstract = {The authors analyze a large corpus of the neuroscience literature and demonstrate that nearly half of the published studies considered incorrectly compared effect sizes by comparing their significance levels.},
	pages = {1105--1107},
	number = {9},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Nieuwenhuis, Sander and Forstmann, Birte U. and Wagenmakers, Eric-Jan},
	urldate = {2022-01-08},
	date = {2011-09},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 9
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Statistics
Subject\_term\_id: statistics},
	keywords = {Statistics},
	file = {Erroneous analyses of interactions in neuroscience\: a problem of significance:/Users/tom/Zotero/storage/C3KC7YQ3/nieuwenhuis2011.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/Y83RASIB/Nieuwenhuis et al. - 2011 - Erroneous analyses of interactions in neuroscience.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/S4T6JKJL/nn.html:text/html},
}

@article{amrhein_scientists_2019,
	title = {Scientists rise up against statistical significance},
	volume = {567},
	rights = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-019-00857-9},
	doi = {10.1038/d41586-019-00857-9},
	abstract = {Valentin Amrhein, Sander Greenland, Blake {McShane} and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
	pages = {305--307},
	number = {7748},
	journaltitle = {Nature},
	author = {Amrhein, Valentin and Greenland, Sander and {McShane}, Blake},
	urldate = {2022-01-18},
	date = {2019-03},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Comment
Number: 7748
Publisher: Nature Publishing Group
Subject\_term: Research data, Research management},
	keywords = {Research data, Research management},
	file = {Full Text PDF:/Users/tom/Zotero/storage/TBNMI5AS/Amrhein et al. - 2019 - Scientists rise up against statistical significanc.pdf:application/pdf;Scientists rise up against statistical significance:/Users/tom/Zotero/storage/IDK94AKK/amrhein2019.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZUVIRAGM/d41586-019-00857-9.html:text/html},
}

@article{lakens_justify_2018,
	title = {Justify your alpha},
	volume = {2},
	rights = {2018 The Publisher},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0311-x},
	doi = {10.1038/s41562-018-0311-x},
	abstract = {In response to recommendations to redefine statistical significance to P ≤ 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
	pages = {168--171},
	number = {3},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and {DeBruine}, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and Gonzalez-Marquez, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and van Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavský, Jiří and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and {McCarthy}, Randy J. and {McConway}, Kevin and {McFarland}, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and de Oliveira, Cilene Lino and de Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and Świątkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
	urldate = {2022-01-18},
	date = {2018-03},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 3
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Human behaviour;Statistics
Subject\_term\_id: human-behaviour;statistics},
	keywords = {Statistics, Human behaviour},
	file = {Full Text:/Users/tom/Zotero/storage/F5XGTNXH/Lakens et al. - 2018 - Justify your alpha.pdf:application/pdf;Justify your alpha:/Users/tom/Zotero/storage/XCI6965H/lakens2018.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/L2QA3L5U/s41562-018-0311-x.html:text/html},
}

@article{benjamin_redefine_2018,
	title = {Redefine statistical significance},
	volume = {2},
	rights = {2017 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-017-0189-z},
	doi = {10.1038/s41562-017-0189-z},
	abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
	pages = {6--10},
	number = {1},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Björn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and {McCarthy}, Michael and Moore, Don A. and Morgan, Stephen L. and Munafó, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Schönbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
	urldate = {2022-01-18},
	date = {2018-01},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Human behaviour;Statistics
Subject\_term\_id: human-behaviour;statistics},
	keywords = {Statistics, Human behaviour},
	file = {Full Text PDF:/Users/tom/Zotero/storage/GQKEQJCS/Benjamin et al. - 2018 - Redefine statistical significance.pdf:application/pdf;Redefine statistical significance:/Users/tom/Zotero/storage/5DSCJMVX/b4ebd6c86cec6b7ef595374f8f794b3a.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3RFNTHZE/s41562-017-0189-z.html:text/html},
}

@article{mcshane_abandon_2019,
	title = {Abandon statistical significance},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1527253},
	doi = {10.1080/00031305.2018.1527253},
	abstract = {We discuss problems the null hypothesis significance testing ({NHST}) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the {NHST} paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
	pages = {235--245},
	issue = {sup1},
	journaltitle = {The American Statistician},
	author = {{McShane}, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
	urldate = {2022-01-18},
	date = {2019-03-29},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1527253},
	keywords = {Replication, Statistical significance, Null hypothesis significance testing, p-Value, Sociology of science},
	file = {Abandon Statistical Significance:/Users/tom/Zotero/storage/7ISJGAJ5/10.1080@00031305.2018.1527253.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/QJJIL6GR/McShane et al. - 2019 - Abandon Statistical Significance.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/J2GMEQV9/00031305.2018.html:text/html},
}

@article{cumming_new_2014,
	title = {The New Statistics: why and how},
	volume = {25},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797613504966},
	doi = {10.1177/0956797613504966},
	shorttitle = {The new statistics},
	abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing ({NHST}), we need to shift from reliance on {NHST} to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for {NHST}, and is aimed at building a cumulative quantitative discipline.},
	pages = {7--29},
	number = {1},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Cumming, Geoff},
	urldate = {2022-01-18},
	date = {2014-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication, research methods, meta-analysis, research integrity, estimation, statistical analysis, the new statistics},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/37R7YQ39/Cumming - 2014 - The New Statistics Why and How.pdf:application/pdf;The New Statistics\: Why and How:/Users/tom/Zotero/storage/KLB2W2PC/cumming2013.pdf.pdf:application/pdf},
}

@article{kruschke_bayesian_2018-1,
	title = {The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective},
	volume = {25},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-016-1221-4},
	doi = {10.3758/s13423-016-1221-4},
	shorttitle = {The Bayesian New Statistics},
	abstract = {In the practice of data analysis, there is a conceptual distinction between hypothesis testing, on the one hand, and estimation with quantified uncertainty on the other. Among frequentists in psychology, a shift of emphasis from hypothesis testing to estimation has been dubbed “the New Statistics” (Cumming 2014). A second conceptual distinction is between frequentist methods and Bayesian methods. Our main goal in this article is to explain how Bayesian methods achieve the goals of the New Statistics better than frequentist methods. The article reviews frequentist and Bayesian approaches to hypothesis testing and to estimation with confidence or credible intervals. The article also describes Bayesian approaches to meta-analysis, randomized controlled trials, and power analysis.},
	pages = {178--206},
	number = {1},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Kruschke, John K. and Liddell, Torrin M.},
	urldate = {2022-01-18},
	date = {2018-02-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/IHU4TVBH/Kruschke and Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, e.pdf:application/pdf;The Bayesian New Statistics\: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective:/Users/tom/Zotero/storage/ICFSIAWM/kruschke2017.pdf.pdf:application/pdf},
}

@article{cohen_statistical_1962,
	title = {The statistical power of abnormal-social psychological research: A review.},
	volume = {65},
	issn = {0096-851X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0045186},
	doi = {10.1037/h0045186},
	shorttitle = {The statistical power of abnormal-social psychological research},
	pages = {145--153},
	number = {3},
	journaltitle = {The Journal of Abnormal and Social Psychology},
	shortjournal = {The Journal of Abnormal and Social Psychology},
	author = {Cohen, Jacob},
	urldate = {2022-01-18},
	date = {1962-09},
	langid = {english},
	file = {The statistical power of abnormal-social psychological research\: A review.:/Users/tom/Zotero/storage/35M77UYB/cohen1962.pdf.pdf:application/pdf},
}

@article{wasserstein_moving_2019,
	title = {Moving to a World Beyond “p {\textless} 0.05”},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2019.1583913},
	doi = {10.1080/00031305.2019.1583913},
	pages = {1--19},
	issue = {sup1},
	journaltitle = {The American Statistician},
	author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
	urldate = {2022-01-18},
	date = {2019-03-29},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2019.1583913},
	file = {Full Text PDF:/Users/tom/Zotero/storage/F3B88KQ2/Wasserstein et al. - 2019 - Moving to a World Beyond “p  0.05”.pdf:application/pdf;Moving to a World Beyond “p < 0.05”:/Users/tom/Zotero/storage/VQW26W47/10.1080@00031305.2019.1583913.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7E3NHR3I/00031305.2019.html:text/html},
}

@article{batterham_making_2006,
	title = {Making meaningful inferences about magnitudes},
	volume = {1},
	issn = {1555-0265},
	abstract = {A study of a sample provides only an estimate of the true (population) value of an outcome statistic. A report of the study therefore usually includes an inference about the true value. Traditionally, a researcher makes an inference by declaring the value of the statistic statistically significant or nonsignificant on the basis of a P value derived from a null-hypothesis test. This approach is confusing and can be misleading, depending on the magnitude of the statistic, error of measurement, and sample size. The authors use a more intuitive and practical approach based directly on uncertainty in the true value of the statistic. First they express the uncertainty as confidence limits, which define the likely range of the true value. They then deal with the real-world relevance of this uncertainty by taking into account values of the statistic that are substantial in some positive and negative sense, such as beneficial or harmful. If the likely range overlaps substantially positive and negative values, they infer that the outcome is unclear; otherwise, they infer that the true value has the magnitude of the observed value: substantially positive, trivial, or substantially negative. They refine this crude inference by stating qualitatively the likelihood that the true value will have the observed magnitude (eg, very likely beneficial). Quantitative or qualitative probabilities that the true value has the other 2 magnitudes or more finely graded magnitudes (such as trivial, small, moderate, and large) can also be estimated to guide a decision about the utility of the outcome.},
	pages = {50--57},
	number = {1},
	journaltitle = {International Journal of Sports Physiology and Performance},
	shortjournal = {Int J Sports Physiol Perform},
	author = {Batterham, Alan M. and Hopkins, William G.},
	date = {2006-03},
	pmid = {19114737},
	keywords = {Data Interpretation, Statistical, Humans, Research Design, Confidence Intervals},
}

@article{sainani_problem_2018,
	title = {The Problem with "Magnitude-based Inference"},
	volume = {50},
	issn = {1530-0315},
	doi = {10.1249/MSS.0000000000001645},
	abstract = {{PURPOSE}: A statistical method called "magnitude-based inference" ({MBI}) has gained a following in the sports science literature, despite concerns voiced by statisticians. Its proponents have claimed that {MBI} exhibits superior type I and type {II} error rates compared with standard null hypothesis testing for most cases. I have performed a reanalysis to evaluate this claim.
{METHODS}: Using simulation code provided by {MBI}'s proponents, I estimated type I and type {II} error rates for clinical and nonclinical {MBI} for a range of effect sizes, sample sizes, and smallest important effects. I plotted these results in a way that makes transparent the empirical behavior of {MBI}. I also reran the simulations after correcting mistakes in the definitions of type I and type {II} error provided by {MBI}'s proponents. Finally, I confirmed the findings mathematically; and I provide general equations for calculating {MBI}'s error rates without the need for simulation.
{RESULTS}: Contrary to what {MBI}'s proponents have claimed, {MBI} does not exhibit "superior" type I and type {II} error rates to standard null hypothesis testing. As expected, there is a tradeoff between type I and type {II} error. At precisely the small-to-moderate sample sizes that {MBI}'s proponents deem "optimal," {MBI} reduces the type {II} error rate at the cost of greatly inflating the type I error rate-to two to six times that of standard hypothesis testing.
{CONCLUSIONS}: Magnitude-based inference exhibits worrisome empirical behavior. In contrast to standard null hypothesis testing, which has predictable type I error rates, the type I error rates for {MBI} vary widely depending on the sample size and choice of smallest important effect, and are often unacceptably high. Magnitude-based inference should not be used.},
	pages = {2166--2176},
	number = {10},
	journaltitle = {Medicine and Science in Sports and Exercise},
	shortjournal = {Med Sci Sports Exerc},
	author = {Sainani, Kristin L.},
	date = {2018-10},
	pmid = {29683920},
	keywords = {Computer Simulation, Statistics as Topic, Sports Medicine},
	file = {The Problem with "Magnitude-based Inference":/Users/tom/Zotero/storage/ZY988MVY/sainani2018.pdf.pdf:application/pdf},
}

@article{lohse_systematic_2020,
	title = {Systematic review of the use of "magnitude-based inference" in sports science and medicine},
	volume = {15},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0235318},
	abstract = {Magnitude-based inference ({MBI}) is a controversial statistical method that has been used in hundreds of papers in sports science despite criticism from statisticians. To better understand how this method has been applied in practice, we systematically reviewed 232 papers that used {MBI}. We extracted data on study design, sample size, and choice of {MBI} settings and parameters. Median sample size was 10 per group (interquartile range, {IQR}: 8-15) for multi-group studies and 14 ({IQR}: 10-24) for single-group studies; few studies reported a priori sample size calculations (15\%). Authors predominantly applied {MBI}'s default settings and chose "mechanistic/non-clinical" rather than "clinical" {MBI} even when testing clinical interventions (only 16 studies out of 232 used clinical {MBI}). Using these data, we can estimate the Type I error rates for the typical {MBI} study. Authors frequently made dichotomous claims about effects based on the {MBI} criterion of a "likely" effect and sometimes based on the {MBI} criterion of a "possible" effect. When the sample size is n = 8 to 15 per group, these inferences have Type I error rates of 12\%-22\% and 22\%-45\%, respectively. High Type I error rates were compounded by multiple testing: Authors reported results from a median of 30 tests related to outcomes; and few studies specified a primary outcome (14\%). We conclude that {MBI} has promoted small studies, promulgated a "black box" approach to statistics, and led to numerous papers where the conclusions are not supported by the data. Amidst debates over the role of p-values and significance testing in science, {MBI} also provides an important natural experiment: we find no evidence that moving researchers away from p-values or null hypothesis significance testing makes them less prone to dichotomization or over-interpretation of findings.},
	pages = {e0235318},
	number = {6},
	journaltitle = {{PloS} One},
	shortjournal = {{PLoS} One},
	author = {Lohse, Keith R. and Sainani, Kristin L. and Taylor, J. Andrew and Butson, Michael L. and Knight, Emma J. and Vickers, Andrew J.},
	date = {2020},
	pmid = {32589653},
	pmcid = {PMC7319293},
	keywords = {Science, Sports Medicine},
	file = {Full Text:/Users/tom/Zotero/storage/6TW37BGX/Lohse et al. - 2020 - Systematic review of the use of magnitude-based i.pdf:application/pdf;Systematic review of the use of "magnitude-based inference" in sports science and medicine:/Users/tom/Zotero/storage/BGPVIZDZ/10.1371@journal.pone.0235318.pdf.pdf:application/pdf},
}

@article{goodman_toward_1999,
	title = {Toward evidence-based medical statistics. 2: the Bayes Factor},
	volume = {130},
	issn = {0003-4819},
	url = {http://annals.org/article.aspx?doi=10.7326/0003-4819-130-12-199906150-00019},
	doi = {10.7326/0003-4819-130-12-199906150-00019},
	shorttitle = {Toward evidence-based medical statistics. 2},
	pages = {1005},
	number = {12},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Goodman, Steven N.},
	urldate = {2022-01-18},
	date = {1999-06-15},
	langid = {english},
	file = {Goodman - 1999 - Toward Evidence-Based Medical Statistics. 2 The B.pdf:/Users/tom/Zotero/storage/EW2M9FRB/Goodman - 1999 - Toward Evidence-Based Medical Statistics. 2 The B.pdf:application/pdf;Toward Evidence-Based Medical Statistics. 2\: The Bayes Factor:/Users/tom/Zotero/storage/X6LZ2HBW/goodman1999.pdf.pdf:application/pdf},
}

@article{giofre_influence_2017,
	title = {The influence of journal submission guidelines on authors' reporting of statistics and use of open research practices},
	volume = {12},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0175583},
	abstract = {From January 2014, Psychological Science introduced new submission guidelines that encouraged the use of effect sizes, estimation, and meta-analysis (the "new statistics"), required extra detail of methods, and offered badges for use of open science practices. We investigated the use of these practices in empirical articles published by Psychological Science and, for comparison, by the Journal of Experimental Psychology: General, during the period of January 2013 to December 2015. The use of null hypothesis significance testing ({NHST}) was extremely high at all times and in both journals. In Psychological Science, the use of confidence intervals increased markedly overall, from 28\% of articles in 2013 to 70\% in 2015, as did the availability of open data (3 to 39\%) and open materials (7 to 31\%). The other journal showed smaller or much smaller changes. Our findings suggest that journalspecific submission guidelines may encourage desirable changes in authors' practices. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	number = {4},
	journaltitle = {{PLoS} {ONE}},
	author = {Giofrè, David and Cumming, Geoff and Fresc, Luca and Boedker, Ingrid and Tressoldi, Patrizio},
	date = {2017},
	note = {Place: {US}
Publisher: Public Library of Science},
	keywords = {Statistics, Experimentation, Empirical Methods, Journal Writing},
	file = {Full Text:/Users/tom/Zotero/storage/9BY6F7ZV/Giofrè et al. - 2017 - The influence of journal submission guidelines on .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AFX7CDCB/2017-19072-001.html:text/html;The influence of journal submission guidelines on authors' reporting of statistics and use of open research practices:/Users/tom/Zotero/storage/EG93LKPQ/10.1371@journal.pone.0175583.pdf.pdf:application/pdf},
}

@article{turner_selective_2022,
	title = {Selective publication of antidepressant trials and its influence on apparent efficacy: Updated comparisons and meta-analyses of newer versus older trials},
	volume = {19},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003886},
	doi = {10.1371/journal.pmed.1003886},
	shorttitle = {Selective publication of antidepressant trials and its influence on apparent efficacy},
	abstract = {Background Valid assessment of drug efficacy and safety requires an evidence base free of reporting bias. Using trial reports in Food and Drug Administration ({FDA}) drug approval packages as a gold standard, we previously found that the published literature inflated the apparent efficacy of antidepressant drugs. The objective of the current study was to determine whether this has improved with recently approved drugs. Methods and findings Using medical and statistical reviews in {FDA} drug approval packages, we identified 30 Phase {II}/{III} double-blind placebo-controlled acute monotherapy trials, involving 13,747 patients, of desvenlafaxine, vilazodone, levomilnacipran, and vortioxetine; we then identified corresponding published reports. We compared the data from this newer cohort of antidepressants (approved February 2008 to September 2013) with the previously published dataset on 74 trials of 12 older antidepressants (approved December 1987 to August 2002). Using logistic regression, we examined the effects of trial outcome and trial cohort (newer versus older) on transparent reporting (whether published and {FDA} conclusions agreed). Among newer antidepressants, transparent publication occurred more with positive (15/15 = 100\%) than negative (7/15 = 47\%) trials ({OR} 35.1, {CI}95\% 1.8 to 693). Controlling for trial outcome, transparent publication occurred more with newer than older trials ({OR} 6.6, {CI}95\% 1.6 to 26.4). Within negative trials, transparent reporting increased from 11\% to 47\%. We also conducted and contrasted {FDA}- and journal-based meta-analyses. For newer antidepressants, {FDA}-based effect size ({ESFDA}) was 0.24 ({CI}95\% 0.18 to 0.30), while journal-based effect size ({ESJournals}) was 0.29 ({CI}95\% 0.23 to 0.36). Thus, effect size inflation, presumably due to reporting bias, was 0.05, less than for older antidepressants (0.10). Limitations of this study include a small number of trials and drugs—belonging to a single class—and a focus on efficacy (versus safety). Conclusions Reporting bias persists but appears to have diminished for newer, compared to older, antidepressants. Continued efforts are needed to further improve transparency in the scientific literature.},
	pages = {e1003886},
	number = {1},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Turner, Erick H. and Cipriani, Andrea and Furukawa, Toshi A. and Salanti, Georgia and Vries, Ymkje Anna de},
	urldate = {2022-01-19},
	date = {2022-01-19},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Randomized controlled trials, Drug therapy, Drug research and development, Metaanalysis, Publication ethics, Antidepressants, Depression, Drug interactions},
	file = {Full Text PDF:/Users/tom/Zotero/storage/S7YGJWBX/Turner et al. - 2022 - Selective publication of antidepressant trials and.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/444AQSL2/article.html:text/html},
}

@report{mcphetres_what_2020-1,
	title = {What should a preregistration contain?},
	url = {https://psyarxiv.com/cj5mh/},
	abstract = {A large amount of variation exists in beliefs about the purpose and benefits of preregistration, making it difficult to implement and evaluate, and limiting its usefulness. Additionally, no single resource exists to describe what a preregistration should contain or how it should be used. In this paper, I describe what an effective preregistration should contain and when it should be used. Specifically, preregistration should 1) restrict as many researcher degrees of freedom as possible, 2) detail all aspects of a study’s method and analysis, 3) detail information on decisions made during the planning stages, and 4) specify how the results will be used and interpreted. Further, a preregistration must be publicly verifiable and permanent. Finally, I argue that preregistration should be used in any situation where researchers intend to collect data in order to make a claim, description, decision, or inference based on that data. I also note that preregistrations which do not address each of these points do more harm than good by falsely signalling credibility and quality.},
	institution = {{PsyArXiv}},
	author = {{McPhetres}, Jonathon},
	urldate = {2022-01-19},
	date = {2020-06-01},
	langid = {english},
	doi = {10.31234/osf.io/cj5mh},
	note = {type: article},
	keywords = {Meta-science, open science, Social and Behavioral Sciences, transparency, other, Psychology, research methods, Theory and Philosophy of Science, hypothesis testing, Pre-registration, Preregistration, Social and Personality Psychology, meta-science, planning},
	file = {Full Text PDF:/Users/tom/Zotero/storage/74C2XUL6/McPhetres - 2020 - What should a preregistration contain.pdf:application/pdf},
}

@article{hurlbert_coup_2019,
	title = {Coup de grâce for a tough old bull: “statistically significant” expires},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1543616},
	doi = {10.1080/00031305.2018.1543616},
	shorttitle = {Coup de grâce for a tough old bull},
	abstract = {Many controversies in statistics are due primarily or solely to poor quality control in journals, bad statistical textbooks, bad teaching, unclear writing, and lack of knowledge of the historical literature. One way to improve the practice of statistics and resolve these issues is to do what initiators of the 2016 {ASA} statement did: take one issue at a time, have extensive discussions about the issue among statisticians of diverse backgrounds and perspectives and eventually develop and publish a broadly supported consensus on that issue. Upon completion of this task, we then move on to deal with another core issue in the same way. We propose as the next project a process that might lead quickly to a strong consensus that the term “statistically significant” and all its cognates and symbolic adjuncts be disallowed in the scientific literature except where focus is on the history of statistics and its philosophies and methodologies. Calculation and presentation of accurate p-values will often remain highly desirable though not obligatory. Supplementary materials for this article are available online in the form of an appendix listing the names and institutions of 48 other statisticians and scientists who endorse the principal propositions put forward here.},
	pages = {352--357},
	issue = {sup1},
	journaltitle = {The American Statistician},
	author = {Hurlbert, Stuart H. and Levine, Richard A. and Utts, Jessica},
	urldate = {2022-01-20},
	date = {2019-03-29},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1543616},
	keywords = {Statistical significance, Type I error, Dichotomized language, {NeoFisherian} significance assessment, p-values, Teaching of statistics},
	file = {Coup de Grâce for a Tough Old Bull\: “Statistically Significant” Expires:/Users/tom/Zotero/storage/Q33EBFPD/10.1080@00031305.2018.1543616.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/VTUPUVKB/Hurlbert et al. - 2019 - Coup de Grâce for a Tough Old Bull “Statistically.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XMYRMKH8/00031305.2018.html:text/html},
}

@article{goodman_why_2019,
	title = {Why is Getting Rid of P-Values So Hard? Musings on Science and Statistics},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1558111},
	doi = {10.1080/00031305.2018.1558111},
	shorttitle = {Why is Getting Rid of P-Values So Hard?},
	abstract = {The current concerns about reproducibility have focused attention on proper use of statistics across the sciences. This gives statisticians an extraordinary opportunity to change what are widely regarded as statistical practices detrimental to the cause of good science. However, how that should be done is enormously complex, made more difficult by the balkanization of research methods and statistical traditions across scientific subdisciplines. Working within those sciences while also allying with science reform movements—operating simultaneously on the micro and macro levels—are the key to making lasting change in applied science.},
	pages = {26--30},
	issue = {sup1},
	journaltitle = {The American Statistician},
	author = {Goodman, Steven N.},
	urldate = {2022-01-20},
	date = {2019-03-29},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1558111},
	keywords = {Statistical inference, P-values, Reproducible research, Scientific inference},
	file = {Full Text PDF:/Users/tom/Zotero/storage/FNNLTQKL/Goodman - 2019 - Why is Getting Rid of P-Values So Hard Musings on.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/83YBH3L7/00031305.2018.html:text/html;Why is Getting Rid of P-Values So Hard? Musings on Science and Statistics:/Users/tom/Zotero/storage/RSA7LVF9/10.1080@00031305.2018.1558111.pdf.pdf:application/pdf},
}

@report{sarafoglou_comparing_2022,
	title = {Comparing analysis blinding with preregistration in the many-analysts religion project},
	url = {https://psyarxiv.com/6dn8f/},
	abstract = {In psychology, preregistration is the most widely used method to ensure the confirmatory status of analyses. However, the method has disadvantages: not only is it perceived as effortful and time consuming, but reasonable deviations from the analysis plan demote the status of the study to exploratory. An alternative to preregistration is analysis blinding, where researchers develop their analysis on an altered version of the data. In this study, we compare the reported efficiency and convenience of the two methods in the context of the Many-Analysts Religion Project. In this project, 120 teams answered the same research questions on the same dataset, either preregistering their analysis (n = 61) or using analysis blinding (n = 59). Our results provide strong evidence ({BF} = 13.19) for the hypothesis that analysis blinding leads to fewer deviations from the analysis plan and if teams deviated they did so on fewer aspects. Contrary to our hypothesis, we found strong evidence ({BF} = 11.40) that both methods involved the same amount of work. Finally, we found no and moderate evidence on whether analysis blinding was perceived as less effortful and frustrating, respectively. We conclude that analysis blinding does not mean less work, but researchers can still benefit from the method since they can plan more appropriate analyses from which they deviate less frequently.},
	institution = {{PsyArXiv}},
	author = {Sarafoglou, Alexandra and Hoogeveen, Suzanne and Wagenmakers, Eric-Jan},
	urldate = {2022-01-21},
	date = {2022-01-21},
	langid = {english},
	doi = {10.31234/osf.io/6dn8f},
	note = {type: article},
	keywords = {Meta-science, Social and Behavioral Sciences, other, Psychology, Open Science, Meta-Science, Replication Crisis, Many Analysts},
	file = {Full Text PDF:/Users/tom/Zotero/storage/DXQR86LF/Sarafoglou et al. - 2022 - Comparing Analysis Blinding With Preregistration I.pdf:application/pdf},
}

@article{kiley_mechanisms_2022,
	title = {Mechanisms of memory updating: state dependency vs. reconsolidation},
	volume = {5},
	rights = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2514-4820},
	url = {http://www.journalofcognition.org/articles/10.5334/joc.198/},
	doi = {10.5334/joc.198},
	shorttitle = {Mechanisms of memory updating},
	abstract = {Reactivating a memory trace has been argued to put it in a fragile state where it must undergo a stabilization process known as reconsolidation. During this process, memories are thought to be susceptible to interference and can be updated with new information. In the spatial context paradigm, memory updating has been shown to occur when new information is presented in the same spatial context as old information, an effect attributed to a reconsolidation process. However, the integration concept holds that memory change can only occur when reactivation and test states are the same, similar to a state-dependent effect. Thus, in human episodic memory, memory updating should only be found when state is the same across the study, reactivation, and test sessions. We investigated whether memory updating can be attributed to state dependency in two experiments using mood as a state. We found evidence of memory updating only when mood was the same across all sessions of the experiments, lending support to the integration concept and posing a challenge to a reconsolidation explanation.},
	pages = {7},
	number = {1},
	journaltitle = {Journal of Cognition},
	author = {Kiley, Christopher and Parks, Colleen M.},
	urldate = {2022-01-21},
	date = {2022-01-07},
	langid = {english},
	note = {Number: 1
Publisher: Ubiquity Press},
	keywords = {memory updating, reconsolidation, episodic memory, integration concept, state dependency},
	file = {Full Text PDF:/Users/tom/Zotero/storage/2LJ4DSVJ/Kiley and Parks - 2022 - Mechanisms of Memory Updating State Dependency vs.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SUTV92AB/joc.198.html:text/html},
}

@article{bettis_creating_2016,
	title = {Creating repeatable cumulative knowledge in strategic management},
	volume = {37},
	issn = {1097-0266},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.2477},
	doi = {10.1002/smj.2477},
	pages = {257--261},
	number = {2},
	journaltitle = {Strategic Management Journal},
	author = {Bettis, Richard A. and Ethiraj, Sendil and Gambardella, Alfonso and Helfat, Constance and Mitchell, Will},
	urldate = {2022-01-21},
	date = {2016},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smj.2477},
	file = {Creating repeatable cumulative knowledge in strategic management:/Users/tom/Zotero/storage/S3NL354Y/bettis2016.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9HTWZVP6/smj.html:text/html},
}

@article{garfield_history_2006,
	title = {The history and meaning of the Journal Impact Factor},
	volume = {295},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.295.1.90},
	doi = {10.1001/jama.295.1.90},
	abstract = {I first mentioned the idea of an impact factor in Science in 1955. With support from the National Institutes of Health, the experimental Genetics Citation Index was published, and that led to the 1961 publication of the Science Citation Index. Irving H. Sher and I created the journal impact factor to help select additional source journals. To do this we simply re-sorted the author citation index into the journal citation index. From this simple exercise, we learned that initially a core group of large and highly cited journals needed to be covered in the new Science Citation Index ({SCI}). Consider that, in 2004, the Journal of Biological Chemistry published 6500 articles, whereas articles from the Proceedings of the National Academy of Sciences were cited more than 300 000 times that year. Smaller journals might not be selected if we rely solely on publication count, so we created the journal impact factor ({JIF}).},
	pages = {90--93},
	number = {1},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Garfield, Eugene},
	urldate = {2022-01-22},
	date = {2006-01-04},
	file = {Snapshot:/Users/tom/Zotero/storage/F5UZE9EE/202114.html:text/html;The History and Meaning of the Journal Impact Factor:/Users/tom/Zotero/storage/UMUMVPQM/garfield2006.pdf.pdf:application/pdf},
}

@article{malicki_systematic_2021,
	title = {Systematic review and meta-analyses of studies analysing instructions to authors from 1987 to 2017},
	volume = {12},
	rights = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26027-y},
	doi = {10.1038/s41467-021-26027-y},
	abstract = {To gain insight into changes of scholarly journals’ recommendations, we conducted a systematic review of studies that analysed journals’ Instructions to Authors ({ItAs}). We summarised results of 153 studies, and meta-analysed how often {ItAs} addressed: 1) authorship, 2) conflicts of interest, 3) data sharing, 4) ethics approval, 5) funding disclosure, and 6) International Committee of Medical Journal Editors’ Uniform Requirements for Manuscripts. For each topic we found large between-study heterogeneity. Here, we show six factors that explained most of that heterogeneity: 1) time (addressing of topics generally increased over time), 2) country (large differences found between countries), 3) database indexation (large differences found between databases), 4) impact factor (topics were more often addressed in highest than in lowest impact factor journals), 5) discipline (topics were more often addressed in Health Sciences than in other disciplines), and 6) sub-discipline (topics were more often addressed in general than in sub-disciplinary journals).},
	pages = {5840},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Malički, Mario and Jerončić, Ana and Aalbersberg, {IJsbrand} Jan and Bouter, Lex and ter Riet, Gerben},
	urldate = {2022-01-23},
	date = {2021-10-05},
	langid = {english},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Authorship;Ethics;Funding;Journalism;Publishing
Subject\_term\_id: authorship;ethics;funding;journalism;publishing},
	keywords = {Ethics, Publishing, Authorship, Funding, Journalism},
	file = {Full Text PDF:/Users/tom/Zotero/storage/AGVZCNGM/Malički et al. - 2021 - Systematic review and meta-analyses of studies ana.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RU28CW48/s41467-021-26027-y.html:text/html},
}

@article{malicki_preprint_2020,
	title = {Preprint servers’ policies, submission requirements, and transparency in reporting and research integrity recommendations},
	volume = {324},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2020.17195},
	doi = {10.1001/jama.2020.17195},
	abstract = {Preprint servers are online platforms that enable free sharing of preprints, scholarly manuscripts that have not been peer reviewed or published in a traditional publishing venue (eg, journal, conference proceeding, book). They facilitate faster dissemination of research, soliciting of feedback or collaborations, and establishing of priority of discoveries and ideas. However, they can also enable sharing of manuscripts that lack sufficient quality or methodological details necessary for research assessment, and can help spread unreliable and even fake information. Since 2010, more than 30 new preprint servers have emerged, yet research on preprint servers is still scarce. With the increase in the numbers of preprints and preprint servers, we explored servers’ policies, submission requirements, and transparency in reporting and research integrity recommendations, as the latter are often perceived as mechanisms by which academic rigor and trustworthiness are fostered and preserved.},
	pages = {1901--1903},
	number = {18},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Malički, Mario and Jerončić, Ana and ter Riet, Gerben and Bouter, Lex M. and Ioannidis, John P. A. and Goodman, Steven N. and Aalbersberg, {IJsbrand} Jan},
	urldate = {2022-01-24},
	date = {2020-11-10},
	file = {Full Text:/Users/tom/Zotero/storage/VBG4MXTJ/Malički et al. - 2020 - Preprint Servers’ Policies, Submission Requirement.pdf:application/pdf;Preprint Servers’ Policies, Submission Requirements, and Transparency in Reporting and Research Integrity Recommendations:/Users/tom/Zotero/storage/V25CIK73/10.1001@jama.2020.17195.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DP4SE2DS/2772748.html:text/html},
}

@article{windish_medicine_2007,
	title = {Medicine residents' understanding of the biostatistics and results in the medical literature},
	volume = {298},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.298.9.1010},
	doi = {10.1001/jama.298.9.1010},
	abstract = {{ContextPhysicians} depend on the medical literature to keep current with clinical information. Little is known about residents' ability to understand statistical methods or how to appropriately interpret research outcomes.{ObjectiveTo} evaluate residents' understanding of biostatistics and interpretation of research results.Design, Setting, and {ParticipantsMultiprogram} cross-sectional survey of internal medicine residents.Main Outcome {MeasurePercentage} of questions correct on a biostatistics/study design multiple-choice knowledge test.{ResultsThe} survey was completed by 277 of 367 residents (75.5\%) in 11 residency programs. The overall mean percentage correct on statistical knowledge and interpretation of results was 41.4\% (95\% confidence interval [{CI}], 39.7\%-43.3\%) vs 71.5\% (95\% {CI}, 57.5\%-85.5\%) for fellows and general medicine faculty with research training (P \&lt; .001). Higher scores in residents were associated with additional advanced degrees (50.0\% [95\% {CI}, 44.5\%-55.5\%] vs 40.1\% [95\% {CI}, 38.3\%-42.0\%]; P \&lt; .001); prior biostatistics training (45.2\% [95\% {CI}, 42.7\%-47.8\%] vs 37.9\% [95\% {CI}, 35.4\%-40.3\%]; P = .001); enrollment in a university-based training program (43.0\% [95\% {CI}, 41.0\%-45.1\%] vs 36.3\% [95\% {CI}, 32.6\%-40.0\%]; P = .002); and male sex (44.0\% [95\% {CI}, 41.4\%-46.7\%] vs 38.8\% [95\% {CI}, 36.4\%-41.1\%]; P = .004). On individual knowledge questions, 81.6\% correctly interpreted a relative risk. Residents were less likely to know how to interpret an adjusted odds ratio from a multivariate regression analysis (37.4\%) or the results of a Kaplan-Meier analysis (10.5\%). Seventy-five percent indicated they did not understand all of the statistics they encountered in journal articles, but 95\% felt it was important to understand these concepts to be an intelligent reader of the literature.{ConclusionsMost} residents in this study lacked the knowledge in biostatistics needed to interpret many of the results in published clinical research. Residency programs should include more effective biostatistics training in their curricula to successfully prepare residents for this important lifelong learning skill.},
	pages = {1010--1022},
	number = {9},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Windish, Donna M. and Huot, Stephen J. and Green, Michael L.},
	urldate = {2022-01-24},
	date = {2007-09-05},
	file = {Full Text:/Users/tom/Zotero/storage/J2FEA7EY/Windish et al. - 2007 - Medicine Residents' Understanding of the Biostatis.pdf:application/pdf;Medicine Residents' Understanding of the Biostatistics and Results in the Medical Literature:/Users/tom/Zotero/storage/3LT3XS3H/windish2007.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/48DUKA9A/208638.html:text/html},
}

@article{aiken_doctoral_2008,
	title = {Doctoral training in statistics, measurement, and methodology in psychology: replication and extension of Aiken, West, Sechrest, and Reno's (1990) survey of {PhD} programs in North America},
	volume = {63},
	issn = {0003-066X},
	doi = {10.1037/0003-066X.63.1.32},
	shorttitle = {Doctoral training in statistics, measurement, and methodology in psychology},
	abstract = {In a survey of all {PhD} programs in psychology in the United States and Canada, the authors documented the quantitative methodology curriculum (statistics, measurement, and research design) to examine the extent to which innovations in quantitative methodology have diffused into the training of {PhDs} in psychology. In all, 201 psychology {PhD} programs (86\%) participated. This survey replicated and extended a previous survey (L. S. Aiken, S. G. West, L. B. Sechrest, \& R. R. Reno, 1990), permitting examination of curriculum development. Most training supported laboratory and not field research. The median of 1.6 years of training in statistics and measurement was mainly devoted to the modally 1-year introductory statistics course, leaving little room for advanced study. Curricular enhancements were noted in statistics and to a minor degree in measurement. Additional coverage of both fundamental and innovative quantitative methodology is needed. The research design curriculum has largely stagnated, a cause for great concern. Elite programs showed no overall advantage in quantitative training. Forces that support curricular innovation are characterized. Human capital challenges to quantitative training, including recruiting and supporting young quantitative faculty, are discussed. Steps must be taken to bring innovations in quantitative methodology into the curriculum of {PhD} programs in psychology.},
	pages = {32--50},
	number = {1},
	journaltitle = {The American Psychologist},
	shortjournal = {Am Psychol},
	author = {Aiken, Leona S. and West, Stephen G. and Millsap, Roger E.},
	date = {2008-01},
	pmid = {18193979},
	keywords = {Psychology, Humans, Statistics as Topic, Research Design, Curriculum, Data Collection, Education, Medical, Graduate, North America, Surveys and Questionnaires},
	file = {Doctoral training in statistics, measurement, and methodology in psychology\: replication and extension of Aiken, West, Sechrest, and Reno's (1990) survey of PhD programs in North America:/Users/tom/Zotero/storage/JYHDDJMU/aiken2008.pdf.pdf:application/pdf},
}

@article{wixted_test_2021,
	title = {Test a witness’s memory of a suspect only once},
	volume = {22},
	issn = {1529-1006},
	url = {https://doi.org/10.1177/15291006211026259},
	doi = {10.1177/15291006211026259},
	abstract = {Eyewitness misidentifications are almost always made with high confidence in the courtroom. The courtroom is where eyewitnesses make their last identification of defendants suspected of (and charged with) committing a crime. But what did those same eyewitnesses do on the first identification test, conducted early in a police investigation? Despite testifying with high confidence in court, many eyewitnesses also testified that they had initially identified the suspect with low confidence or failed to identify the suspect at all. Presenting a lineup leaves the eyewitness with a memory trace of the faces in the lineup, including that of the suspect. As a result, the memory signal generated by the face of that suspect will be stronger on a later test involving the same witness, even if the suspect is innocent. In that sense, testing memory contaminates memory. These considerations underscore the importance of a newly proposed recommendation for conducting eyewitness identifications: Avoid repeated identification procedures with the same witness and suspect. This recommendation applies not only to additional tests conducted by police investigators but also to the final test conducted in the courtroom, in front of the judge and jury.},
	pages = {1S--18S},
	number = {1},
	journaltitle = {Psychological Science in the Public Interest},
	shortjournal = {Psychol Sci Public Interest},
	author = {Wixted, John T. and Wells, Gary L. and Loftus, Elizabeth F. and Garrett, Brandon L.},
	urldate = {2022-01-28},
	date = {2021-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {eyewitness identification, malleability of memory, wrongful convictions},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/NTQE8KCJ/Wixted et al. - 2021 - Test a Witness’s Memory of a Suspect Only Once.pdf:application/pdf},
}

@incollection{davis_eyewitness_2018,
	title = {Eyewitness Science in the 21st Century},
	isbn = {978-1-119-17017-4},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119170174.epcn116},
	abstract = {This chapter reviews the nature and findings of eyewitness science with the help of expert testimony on eyewitness performance. Eyewitness scientists have investigated a number of factors that render the task of encoding the target's features more difficult, including characteristics of the target or perpetrator, those of the circumstances of their encounter, and those of the witness. Three characteristics of the target have been most commonly investigated. These include the distinctiveness of appearance, the available view of target features, and the similarity of the target to the witness in race, age, and gender. The own-race bias ({ORB}) or cross-race effect ({CRE}) refers to impairment in identification performance regarding targets of a different race relative to those of one's own race. Important goals have been to influence public policy: to encourage the adoption of police practices that minimize the likelihood of eyewitness errors, and to educate judges and juries about the factors that influence witness accuracy.},
	pages = {1--38},
	booktitle = {Stevens' Handbook of Experimental Psychology and Cognitive Neuroscience},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Davis, Deborah and Loftus, Elizabeth F.},
	urldate = {2022-01-28},
	date = {2018},
	langid = {english},
	doi = {10.1002/9781119170174.epcn116},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119170174.epcn116},
	keywords = {expert testimony, eyewitness errors, eyewitness performance, eyewitness science, own-race bias, public policy, witness accuracy},
	file = {Davis and Loftus - 2018 - Eyewitness Science in the 21st Century.pdf:/Users/tom/Zotero/storage/JCRWEM7Y/Davis and Loftus - 2018 - Eyewitness Science in the 21st Century.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SXAVWLGT/9781119170174.html:text/html},
}

@article{ioannidis_pre-registration_2022,
	title = {Pre-registration of mathematical models},
	issn = {0025-5564},
	url = {https://www.sciencedirect.com/science/article/pii/S0025556422000037},
	doi = {10.1016/j.mbs.2022.108782},
	abstract = {Pre-registration is a research practice where a protocol is deposited in a repository before a scientific project is performed. The protocol may be publicly visible immediately upon deposition or it may remain hidden until the work is completed/published. It may include the analysis plan, outcomes, and/or information about how evaluation of performance (e.g. forecasting ability) will be made Pre-registration aims to enhance the trust one can put on scientific work. Deviations from the original plan, may still often be desirable, but pre-registration makes them transparent. While pre-registration has been advocated and used to variable extent in diverse types of research, there has been relatively little attention given to the possibility of pre-registration for mathematical modeling studies. Feasibility of pre-registration depends on the type of modeling and the ability to pre-specify processes and outcomes. In some types of modeling, in particular those that involve forecasting or other outcomes that can be appraised in the future, trust in model performance would be enhanced through pre-registration. Pre-registration can also be seen as a component of a largest suite of research practices that aim to improve documentation, transparency, and sharing – eventually allowing better reproducibility of the research work. The current commentary discusses the evolving landscape of the concept of pre-registration as it relates to different mathematical modeling activities, the potential advantages and disadvantages, feasibility issues, and realistic goals.},
	pages = {108782},
	journaltitle = {Mathematical Biosciences},
	shortjournal = {Mathematical Biosciences},
	author = {Ioannidis, John P. A.},
	urldate = {2022-01-29},
	date = {2022-01-25},
	langid = {english},
	keywords = {Bias, Reproducibility, Pre-registration, Mathematical modeling},
	file = {Full Text:/Users/tom/Zotero/storage/Z6F8PLKF/Ioannidis - 2022 - Pre-registration of mathematical models.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/M8962K53/S0025556422000037.html:text/html},
}

@article{bland_comparisons_2011,
	title = {Comparisons within randomised groups can be very misleading},
	volume = {342},
	rights = {© {BMJ} Publishing Group Ltd 2011},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/342/bmj.d561},
	doi = {10.1136/bmj.d561},
	abstract = {When we randomise trial participants into two or more intervention groups, we do this to remove bias; the groups will, on average, be comparable in every respect except the treatment which they receive. Provided the trial is well conducted, without other sources of bias, any difference in the outcome of the groups can then reasonably be attributed to the different interventions received. In a previous note we discussed the analysis of those trials in which the primary outcome measure is also measured at baseline. We discussed several valid analyses, observing that “analysis of covariance” (a regression method) is the method of choice.1

Rather than comparing the randomised groups directly, however, researchers sometimes look at the change in the measurement between baseline and the end of the trial; they test whether there was a significant change from baseline, separately in each randomised group. They may then report that this difference is significant in one group but not in the other, and conclude that this is evidence that the groups, and hence the treatments, are different. One such example was a recent trial in …},
	pages = {d561},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Bland, J. Martin and Altman, Douglas G.},
	urldate = {2022-01-31},
	date = {2011-05-06},
	langid = {english},
	pmid = {21551184},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Comparisons within randomised groups can be very misleading:/Users/tom/Zotero/storage/IF8QQWVC/bland2011.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/MTYZ69TE/Bland and Altman - 2011 - Comparisons within randomised groups can be very m.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/AHRDBLZP/bmj.html:text/html},
}

@article{ernst_regression_2017,
	title = {Regression assumptions in clinical psychology research practice—a systematic review of common misconceptions},
	volume = {5},
	issn = {2167-8359},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5436580/},
	doi = {10.7717/peerj.3323},
	abstract = {Misconceptions about the assumptions behind the standard linear regression model are widespread and dangerous. These lead to using linear regression when inappropriate, and to employing alternative procedures with less statistical power when unnecessary. Our systematic literature review investigated employment and reporting of assumption checks in twelve clinical psychology journals. Findings indicate that normality of the variables themselves, rather than of the errors, was wrongfully held for a necessary assumption in 4\% of papers that use regression. Furthermore, 92\% of all papers using linear regression were unclear about their assumption checks, violating {APA}-recommendations. This paper appeals for a heightened awareness for and increased transparency in the reporting of statistical assumption checking.},
	pages = {e3323},
	journaltitle = {{PeerJ}},
	shortjournal = {{PeerJ}},
	author = {Ernst, Anja F. and Albers, Casper J.},
	urldate = {2022-02-01},
	date = {2017-05-16},
	pmid = {28533971},
	pmcid = {PMC5436580},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/62XIDTNG/Ernst and Albers - 2017 - Regression assumptions in clinical psychology rese.pdf:application/pdf;Regression assumptions in clinical psychology research practice—a systematic review of common misconceptions:/Users/tom/Zotero/storage/QLSWKF44/18c213f8119e61143e7567a527414cb2.pdf.pdf:application/pdf},
}

@article{bland_comparisons_2011-1,
	title = {Comparisons against baseline within randomised groups are often used and can be highly misleading},
	volume = {12},
	issn = {1745-6215},
	doi = {10.1186/1745-6215-12-264},
	abstract = {{BACKGROUND}: In randomised trials, rather than comparing randomised groups directly some researchers carry out a significance test comparing a baseline with a final measurement separately in each group.
{METHODS}: We give several examples where this has been done. We use simulation to demonstrate that the procedure is invalid and also show this algebraically.
{RESULTS}: This approach is biased and invalid, producing conclusions which are, potentially, highly misleading. The actual alpha level of this procedure can be as high as 0.50 for two groups and 0.75 for three.
{CONCLUSIONS}: Randomised groups should be compared directly by two-sample methods and separate tests against baseline are highly misleading.},
	pages = {264},
	journaltitle = {Trials},
	shortjournal = {Trials},
	author = {Bland, J. Martin and Altman, Douglas G.},
	date = {2011-12-22},
	pmid = {22192231},
	pmcid = {PMC3286439},
	keywords = {Humans, Randomized Controlled Trials as Topic, Computer Simulation, Female, Probability, Aging, Back Pain, Menorrhagia},
	file = {Comparisons against baseline within randomised groups are often used and can be highly misleading:/Users/tom/Zotero/storage/WLHLIQTR/bland2011.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/LIGFTBQT/Bland and Altman - 2011 - Comparisons against baseline within randomised gro.pdf:application/pdf},
}

@article{vickers_analysing_2001,
	title = {Analysing controlled trials with baseline and follow up measurements},
	volume = {323},
	rights = {© 2001 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/323/7321/1123},
	doi = {10.1136/bmj.323.7321.1123},
	abstract = {In many randomised trials researchers measure a continuous variable at baseline and again as an outcome assessed at follow up. Baseline measurements are common in trials of chronic conditions where researchers want to see whether a treatment can reduce pre-existing levels of pain, anxiety, hypertension, and the like.

Statistical comparisons in such trials can be made in several ways. Comparison of follow up (post-treatment) scores will give a result such as “at the end of the trial, mean pain scores were 15 mm (95\% confidence interval 10 to 20 mm) lower in the treatment group.” Alternatively a change score can be calculated by subtracting the follow up score from the baseline score, leading to a statement such as “pain reductions were 20 mm (16 to 24 mm) greater on treatment than control.” If the average baseline scores are the same in each group the estimated treatment effect will be the same using these two simple approaches. If the treatment is effective the statistical significance of the treatment effect by the two methods will depend on the correlation between baseline and follow up scores. If the correlation is low using the change score will …},
	pages = {1123--1124},
	number = {7321},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Vickers, Andrew J. and Altman, Douglas G.},
	urldate = {2022-02-01},
	date = {2001-11-10},
	langid = {english},
	pmid = {11701584},
	note = {Publisher: British Medical Journal Publishing Group
Section: Education and debate},
	file = {Analysing controlled trials with baseline and follow up measurements:/Users/tom/Zotero/storage/BP4JAN4P/vickers2001.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/6C9R5QVC/Vickers and Altman - 2001 - Analysing controlled trials with baseline and foll.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/R8DMAG38/1123.html:text/html},
}

@article{altman_interaction_2003,
	title = {Interaction revisited: the difference between two estimates},
	volume = {326},
	rights = {© 2003 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/326/7382/219},
	doi = {10.1136/bmj.326.7382.219},
	shorttitle = {Interaction revisited},
	abstract = {We often want to compare two estimates of the same quantity derived from separate analyses. Thus we might want to compare the treatment effect in subgroups in a randomised trial, such as two age groups. The term for such a comparison is a test of interaction. In earlier Statistics Notes we discussed interaction in terms of heterogeneity of treatment effect.1–3 Here we revisit interaction and consider the concept more generally.

The comparison of two estimated quantities, such as means or proportions, each with its standard error, is a general method that can be applied widely. The two estimates should be independent, not obtained from the same individuals—examples are the results from subgroups in a randomised trial or from two independent studies. The samples should be large. If the estimates are E 1 and E 2 with standard errors {SE}( E 1) and {SE}( E 2), then the difference d = E 1- E 2 has standard error {SE}( d )=√[{SE}( E …},
	pages = {219},
	number = {7382},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Altman, Douglas G. and Bland, J. Martin},
	urldate = {2022-02-01},
	date = {2003-01-25},
	langid = {english},
	pmid = {12543843},
	note = {Publisher: British Medical Journal Publishing Group
Section: Education and debate},
	file = {Full Text PDF:/Users/tom/Zotero/storage/TKUK6WE6/Altman and Bland - 2003 - Interaction revisited the difference between two .pdf:application/pdf;Interaction revisited\: the difference between two estimates:/Users/tom/Zotero/storage/Q8T2DLCC/altman2003.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8C9N8XAQ/219.html:text/html},
}

@article{matthews_statistics_1996,
	title = {Statistics Notes: Interaction 2: compare effect sizes not P values},
	volume = {313},
	rights = {© 1996 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/313/7060/808},
	doi = {10.1136/bmj.313.7060.808},
	shorttitle = {Statistics Notes},
	abstract = {As we have previously described,1 the statistical term interaction relates to the non-independence of the effects of two variables on the outcome of interest. For example, in a controlled trial comparing a new treatment with a standard treatment we may want to examine whether the observed benefit was the same for different subgroups of patients. A common approach to answering this question is to analyse the data separately in each subgroup. Here we illustrate this approach and explain why it is incorrect.

One of several subgroup analyses in a trial of antenatal steroids for preventing neonatal respiratory distress syndrome2 was performed to see whether the effect of treatment was different in mothers who did or did not develop pre-eclampsia. Among mothers with preeclampsia 21.2\% (7/33) of babies whose mothers were given dexamethasone developed neonatal respiratory …},
	pages = {808},
	number = {7060},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Matthews, John N. S. and Altman, Douglas G.},
	urldate = {2022-02-01},
	date = {1996-09-28},
	langid = {english},
	pmid = {8842080},
	note = {Publisher: British Medical Journal Publishing Group
Section: Education and debate},
	file = {Full Text PDF:/Users/tom/Zotero/storage/YZ6NYCVZ/Matthews and Altman - 1996 - Statistics Notes Interaction 2 compare effect si.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/2TVZSEYV/808.html:text/html;Statistics Notes\: Interaction 2\: compare effect sizes not P values:/Users/tom/Zotero/storage/BXU3NNJU/matthews1996.pdf.pdf:application/pdf},
}

@article{altman_uncertainty_2014,
	title = {Uncertainty and sampling error},
	volume = {349},
	rights = {© {BMJ} Publishing Group Ltd 2014},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/349/bmj.g7064},
	doi = {10.1136/bmj.g7064},
	abstract = {Medical research is conducted to help to reduce uncertainty. For example, randomised controlled trials aim to answer questions relating to treatment choices for a particular group of patients. Rarely, however, does a single study remove uncertainty. There are two reasons for this: sampling error and other (non-sampling) sources of uncertainty. The word “error” comes from a Latin root meaning “to wander,” and we use it in its statistical sense of meaning variation from the average, not “mistake.” Sampling error arises because any sample may not behave quite the same as the larger population from which it was drawn. Non-sampling error arises from the many ways a research study may deviate from addressing the question that the researcher wants to answer.

Sampling error is very much the concern of the statistician, who imagines that the group of people in the study is just one of the many possible samples from the population of interest. Despite it being widely condemned,1 the dominant way of …},
	pages = {g7064},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Altman, Douglas G. and Bland, J. Martin},
	urldate = {2022-02-01},
	date = {2014-11-25},
	langid = {english},
	pmid = {25424395},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/65LIU3KC/Altman and Bland - 2014 - Uncertainty and sampling error.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9B5DERUQ/bmj.html:text/html;Uncertainty and sampling error:/Users/tom/Zotero/storage/HCANFPDS/altman2014.pdf.pdf:application/pdf},
}

@article{altman_statistics_1995,
	title = {Statistics notes: Absence of evidence is not evidence of absence},
	volume = {311},
	rights = {© 1995 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/311/7003/485},
	doi = {10.1136/bmj.311.7003.485},
	shorttitle = {Statistics notes},
	abstract = {The non-equivalence of statistical significance and clinical importance has long been recognised, but this error of interpretation remains common. Although a significant result in a large study may sometimes not be clinically important, a far greater problem arises from misinterpretation of non-significant findings. By convention a P value greater than 5\% (P{\textgreater}0.05) is called “not significant.” Randomised controlled clinical trials that do not show a significant difference between the treatments being compared are often called “negative.” This term wrongly implies that the study has shown that there is no difference, whereas usually all that has been shown is an absence of evidence of a difference. These are quite different statements.

The sample size of controlled trials is generally inadequate, with a consequent lack of power to detect real, and clinically worthwhile, differences in treatment. Freiman et al1 found that only …},
	pages = {485},
	number = {7003},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Altman, Douglas G. and Bland, J. Martin},
	urldate = {2022-02-01},
	date = {1995-08-19},
	langid = {english},
	pmid = {7647644},
	note = {Publisher: British Medical Journal Publishing Group
Section: Paper},
	file = {Full Text PDF:/Users/tom/Zotero/storage/6IHRRKSZ/Altman and Bland - 1995 - Statistics notes Absence of evidence is not evide.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Y6XUKYQR/485.html:text/html;Statistics notes\: Absence of evidence is not evidence of absence:/Users/tom/Zotero/storage/8RKAJB28/altman1995.pdf.pdf:application/pdf},
}

@article{schroter_what_2008,
	title = {What errors do peer reviewers detect, and does training improve their ability to detect them?},
	volume = {101},
	issn = {0141-0768, 1758-1095},
	url = {http://journals.sagepub.com/doi/10.1258/jrsm.2008.080062},
	doi = {10.1258/jrsm.2008.080062},
	abstract = {Objective
              To analyse data from a trial and report the frequencies with which major and minor errors are detected at a general medical journal, the types of errors missed and the impact of training on error detection.
            
            
              Design
              607 peer reviewers at the {BMJ} were randomized to two intervention groups receiving different types of training (face-to-face training or a self-taught package) and a control group. Each reviewer was sent the same three test papers over the study period, each of which had nine major and five minor methodological errors inserted.
            
            
              Setting
              {BMJ} peer reviewers.
            
            
              Main outcome measures
              The quality of review, assessed using a validated instrument, and the number and type of errors detected before and after training.
            
            
              Results
              The number of major errors detected varied over the three papers. The interventions had small effects. At baseline (Paper 1) reviewers found an average of 2.58 of the nine major errors, with no notable difference between the groups. The mean number of errors reported was similar for the second and third papers, 2.71 and 3.0, respectively. Biased randomization was the error detected most frequently in all three papers, with over 60\% of reviewers rejecting the papers identifying this error. Reviewers who did not reject the papers found fewer errors and the proportion finding biased randomization was less than 40\% for each paper.
            
            
              Conclusions
              Editors should not assume that reviewers will detect most major errors, particularly those concerned with the context of study. Short training packages have only a slight impact on improving error detection.},
	pages = {507--514},
	number = {10},
	journaltitle = {Journal of the Royal Society of Medicine},
	shortjournal = {J R Soc Med},
	author = {Schroter, Sara and Black, Nick and Evans, Stephen and Godlee, Fiona and Osorio, Lyda and Smith, Richard},
	urldate = {2022-02-01},
	date = {2008-10-01},
	langid = {english},
	file = {What errors do peer reviewers detect, and does training improve their ability to detect them?:/Users/tom/Zotero/storage/G7UXSRGH/schroter2008.pdf.pdf:application/pdf},
}

@article{pickett_stewart_2020,
	title = {The Stewart Retractions: a quantitative and qualitative analysis},
	volume = {17},
	pages = {39},
	number = {1},
	author = {Pickett, Justin T},
	date = {2020},
	langid = {english},
	file = {Pickett - 2020 - The Stewart Retractions A Quantitative and Qualit.pdf:/Users/tom/Zotero/storage/HPKKFWEW/Pickett - 2020 - The Stewart Retractions A Quantitative and Qualit.pdf:application/pdf},
}

@article{rindal_mechanisms_2017,
	title = {Mechanisms of eyewitness suggestibility: tests of the explanatory role hypothesis},
	volume = {24},
	rights = {2016 Psychonomic Society, Inc.},
	issn = {1531-5320},
	url = {https://link.springer.com/article/10.3758/s13423-016-1201-8},
	doi = {10.3758/s13423-016-1201-8},
	shorttitle = {Mechanisms of eyewitness suggestibility},
	abstract = {In a recent paper, Chrobak and Zaragoza (Journal of Experimental Psychology: General, 142(3), 827–844, 2013) proposed the explanatory role hypothesis, which posits that the likelihood of developing false memories for post-event suggestions is a function of the explanatory function the suggestion serves. In support of this hypothesis, they provided evidence that participant-witnesses were especially likely to develop false memories for their forced fabrications when their fabrications helped to explain outcomes they had witnessed. In three experiments, we test the generality of the explanatory role hypothesis as a mechanism of eyewitness suggestibility by assessing whether this hypothesis can predict suggestibility errors in (a) situations where the post-event suggestions are provided by the experimenter (as opposed to fabricated by the participant), and (b) across a variety of memory measures and measures of recollective experience. In support of the explanatory role hypothesis, participants were more likely to subsequently freely report (E1) and recollect the suggestions as part of the witnessed event (E2, source test) when the post-event suggestion helped to provide a causal explanation for a witnessed outcome than when it did not serve this explanatory role. Participants were also less likely to recollect the suggestions as part of the witnessed event (on measures of subjective experience) when their explanatory strength had been reduced by the presence of an alternative explanation that could explain the same outcome (E3, source test + warning). Collectively, the results provide strong evidence that the search for explanatory coherence influences people’s tendency to misremember witnessing events that were only suggested to them.},
	pages = {1413--1425},
	number = {5},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Rindal, Eric J. and Chrobak, Quin M. and Zaragoza, Maria S. and Weihing, Caitlin A.},
	urldate = {2022-02-06},
	date = {2017-10-01},
	langid = {english},
	note = {Company: Springer
Distributor: Springer
Institution: Springer
Label: Springer
Number: 5
Publisher: Springer {US}},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NEIG6P3P/Rindal et al. - 2017 - Mechanisms of eyewitness suggestibility tests of .pdf:application/pdf;Mechanisms of eyewitness suggestibility\: tests of the explanatory role hypothesis:/Users/tom/Zotero/storage/I8XINS3D/rindal2017.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7IJR5IWE/10.html:text/html},
}

@article{hardwicke_citation_2021,
	title = {Citation patterns following a strongly contradictory replication result: four           case studies from psychology},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/25152459211040837},
	doi = {10.1177/25152459211040837},
	shorttitle = {Citation patterns following a strongly contradictory replication result},
	abstract = {Replication studies that contradict prior findings may facilitate scientific self-correction by triggering a reappraisal of the original studies; however, the research community?s response to replication results has not been studied systematically. One approach for gauging responses to replication results is to examine how they affect citations to original studies. In this study, we explored postreplication citation patterns in the context of four prominent multilaboratory replication attempts published in the field of psychology that strongly contradicted and outweighed prior findings. Generally, we observed a small postreplication decline in the number of favorable citations and a small increase in unfavorable citations. This indicates only modest corrective effects and implies considerable perpetuation of belief in the original findings. Replication results that strongly contradict an original finding do not necessarily nullify its credibility; however, one might at least expect the replication results to be acknowledged and explicitly debated in subsequent literature. By contrast, we found substantial citation bias: The majority of articles citing the original studies neglected to cite relevant replication results. Of those articles that did cite the replication but continued to cite the original study favorably, approximately half offered an explicit defense of the original study. Our findings suggest that even replication results that strongly contradict original findings do not necessarily prompt a corrective response from the research community.},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Hardwicke, Tom E. and Szűcs, Dénes and Thibault, Robert T. and Crüwell, Sophia and van den Akker, Olmo R. and Nuijten, Michèle B. and Ioannidis, John P. A.},
	urldate = {2022-02-08},
	date = {2021-07-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/J4AGJJ3T/Hardwicke et al. - 2021 - Citation Patterns Following a Strongly Contradicto.pdf:application/pdf},
}

@article{jarvinen_blinded_2014-1,
	title = {Blinded interpretation of study results can feasibly and effectively diminish interpretation bias},
	volume = {67},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435613004861},
	doi = {10.1016/j.jclinepi.2013.11.011},
	abstract = {Objective
Controversial and misleading interpretation of data from randomized trials is common. How to avoid misleading interpretation has received little attention. Herein, we describe two applications of an approach that involves blinded interpretation of the results by study investigators.
Study Design and Settings
The approach involves developing two interpretations of the results on the basis of a blinded review of the primary outcome data (experimental treatment A compared with control treatment B). One interpretation assumes that A is the experimental intervention and another assumes that A is the control. After agreeing that there will be no further changes, the investigators record their decisions and sign the resulting document. The randomization code is then broken, the correct interpretation chosen, and the manuscript finalized. Review of the document by an external authority before finalization can provide another safeguard against interpretation bias.
Results
We found the blinded preparation of a summary of data interpretation described in this article practical, efficient, and useful.
Conclusions
Blinded data interpretation may decrease the frequency of misleading data interpretation. Widespread adoption of blinded data interpretation would be greatly facilitated were it added to the minimum set of recommendations outlining proper conduct of randomized controlled trials (eg, the Consolidated Standards of Reporting Trials statement).},
	pages = {769--772},
	number = {7},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Järvinen, Teppo L. N. and Sihvonen, Raine and Bhandari, Mohit and Sprague, Sheila and Malmivaara, Antti and Paavola, Mika and Schünemann, Holger J. and Guyatt, Gordon H.},
	urldate = {2022-02-10},
	date = {2014-07-01},
	langid = {english},
	keywords = {Bias, Data interpretations, Double-blind method, Drug evaluation/methods, Randomized controlled trials as topic/methods, Research design},
	file = {Full Text:/Users/tom/Zotero/storage/MGENR733/Järvinen et al. - 2014 - Blinded interpretation of study results can feasib.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/V86ILH4L/S0895435613004861.html:text/html},
}

@article{ortega_classification_nodate,
	title = {Classification and analysis of {PubPeer} comments: How a web journal club is used},
	volume = {n/a},
	issn = {2330-1643},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24568},
	doi = {10.1002/asi.24568},
	shorttitle = {Classification and analysis of {PubPeer} comments},
	abstract = {This study explores the use of {PubPeer} by the scholarly community, to understand the issues discussed in an online journal club, the disciplines most commented on, and the characteristics of the most prolific users. A sample of 39,985 posts about 24,779 publications were extracted from {PubPeer} in 2019 and 2020. These comments were divided into seven categories according to their degree of seriousness (Positive review, Critical review, Lack of information, Honest errors, Methodological flaws, Publishing fraud, and Manipulation). The results show that more than two-thirds of comments are posted to report some type of misconduct, mainly about image manipulation. These comments generate most discussion and take longer to be posted. By discipline, Health Sciences and Life Sciences are the most discussed research areas. The results also reveal “super commenters,” users who access the platform to systematically review publications. The study ends by discussing how various disciplines use the site for different purposes.},
	issue = {n/a},
	journaltitle = {Journal of the Association for Information Science and Technology},
	author = {Ortega, José Luis},
	urldate = {2022-02-11},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24568},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3H7TFWJ9/Ortega - Classification and analysis of PubPeer comments H.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/YM52WENH/asi.html:text/html},
}

@article{grant_typology_2009-1,
	title = {A typology of reviews: an analysis of 14 review types and associated methodologies},
	volume = {26},
	issn = {1471-1842},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1471-1842.2009.00848.x},
	doi = {10.1111/j.1471-1842.2009.00848.x},
	shorttitle = {A typology of reviews},
	abstract = {Background and objectives: The expansion of evidence-based practice across sectors has lead to an increasing variety of review types. However, the diversity of terminology used means that the full potential of these review types may be lost amongst a confusion of indistinct and misapplied terms. The objective of this study is to provide descriptive insight into the most common types of reviews, with illustrative examples from health and health information domains. Methods: Following scoping searches, an examination was made of the vocabulary associated with the literature of review and synthesis (literary warrant). A simple analytical framework—Search, {AppraisaL}, Synthesis and Analysis ({SALSA})—was used to examine the main review types. Results: Fourteen review types and associated methodologies were analysed against the {SALSA} framework, illustrating the inputs and processes of each review type. A description of the key characteristics is given, together with perceived strengths and weaknesses. A limited number of review types are currently utilized within the health information domain. Conclusions: Few review types possess prescribed and explicit methodologies and many fall short of being mutually exclusive. Notwithstanding such limitations, this typology provides a valuable reference point for those commissioning, conducting, supporting or interpreting reviews, both within health information and the wider health care domain.},
	pages = {91--108},
	number = {2},
	journaltitle = {Health Information \& Libraries Journal},
	author = {Grant, Maria J. and Booth, Andrew},
	urldate = {2022-02-15},
	date = {2009},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1471-1842.2009.00848.x},
	file = {Full Text PDF:/Users/tom/Zotero/storage/5DYUF4MA/Grant and Booth - 2009 - A typology of reviews an analysis of 14 review ty.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GR5UZYJE/j.1471-1842.2009.00848.html:text/html},
}

@article{hernan_specifying_2016,
	title = {Specifying a target trial prevents immortal time bias and other self-inflicted injuries in observational analyses},
	volume = {79},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(16)30136-6/fulltext},
	doi = {10.1016/j.jclinepi.2016.04.014},
	pages = {70--75},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Hernán, Miguel A. and Sauer, Brian C. and Hernández-Díaz, Sonia and Platt, Robert and Shrier, Ian},
	urldate = {2022-02-18},
	date = {2016-11-01},
	pmid = {27237061},
	note = {Publisher: Elsevier},
	keywords = {Selection bias, Comparative effectiveness research, Immortal time bias, Observational, studies, Target trial, Time zero},
	file = {Accepted Version:/Users/tom/Zotero/storage/LLMKA7UR/Hernán et al. - 2016 - Specifying a target trial prevents immortal time b.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PU5H7SAJ/fulltext.html:text/html},
}

@article{rittel_dilemmas_1973,
	title = {Dilemmas in a general theory of planning},
	volume = {4},
	abstract = {The search for scientific bases for confronting problems of social policy is bound to fail, because of the nature of these problems. They are "wicked" problems, whereas science has developed to deal with "tame" problems. Policy problems cannot be definitively described. Moreover, in a pluralistic society there is nothing like the undisputable public good; there is no objective definition of equity; policies that respond to social problems cannot be meaningfully correct or false; and it makes no sense to talk about "optinaal solutions" to social {probIems} unless severe qualifications are imposed first. Even worse, there are no "solutions" in the sense of definitive and objective answers.},
	pages = {155--169},
	journaltitle = {Policy Sciences},
	author = {Rittel, Horst W J and Webber, Melvin M},
	date = {1973},
	langid = {english},
	file = {Rittel - Dilemmas in a general theory of planning.pdf:/Users/tom/Zotero/storage/KTEK6R3F/Rittel - Dilemmas in a general theory of planning.pdf:application/pdf},
}

@article{bishop_can_nodate,
	title = {Can we shift belief in the ‘Law of Small Numbers’?},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.211028},
	doi = {10.1098/rsos.211028},
	abstract = {‘Sample size neglect’ is a tendency to underestimate how the variability of mean estimates changes with sample size. We studied 100 participants, from science or social science backgrounds, to test whether a training task showing different-sized samples of data points (the ‘beeswarm’ task) can help overcome this bias. Ability to judge if two samples came from the same population improved with training, and 38\% of participants reported that they had learned to wait for larger samples before making a response. Before and after training, participants completed a 12-item estimation quiz, including items testing sample size neglect (S-items). Bonus payments were given for correct responses. The quiz confirmed sample size neglect: 20\% of participants scored zero on S-items, and only two participants achieved more than 4/6 items correct. Performance on the quiz did not improve after training, regardless of how much learning had occurred on the beeswarm task. Error patterns on the quiz were generally consistent with expectation, though there were some intriguing exceptions that could not readily be explained by sample size neglect. We suggest that training with simulated data might need to be accompanied by explicit instruction to be effective in counteracting sample size neglect more generally.},
	pages = {211028},
	number = {3},
	journaltitle = {Royal Society Open Science},
	author = {Bishop, D. V. M. and Thompson, Jackie and Parker, Adam J.},
	urldate = {2022-03-03},
	note = {Publisher: Royal Society},
	keywords = {power, online training, sample size neglect, statistical reasoning},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PH88INJD/Bishop et al. - Can we shift belief in the ‘Law of Small Numbers’.pdf:application/pdf},
}

@report{wu_cultural_2022,
	title = {The cultural evolution of science},
	url = {https://osf.io/preprints/metaarxiv/2ekcr/},
	abstract = {To appear in The Oxford Handbook of Cultural Evolution, edited by Jeremy Kendal, Rachel Kendal, and Jamshid Tehrani, Oxford University Press. Expected 2023.},
	institution = {{MetaArXiv}},
	author = {Wu, Jingyi and O'Connor, Cailin and Smaldino, Paul E.},
	urldate = {2022-03-07},
	date = {2022-03-04},
	langid = {english},
	doi = {10.31222/osf.io/2ekcr},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Cultural evolution, Cultural evolution of science, models of science, natural selection, network, Other Social and Behavioral Sciences, science of science},
	file = {Full Text PDF:/Users/tom/Zotero/storage/E6QKZFLH/Wu et al. - 2022 - The Cultural Evolution of Science.pdf:application/pdf},
}

@article{jardine_evidence_2022,
	title = {The evidence for and against reactivation-induced memory updating in humans and nonhuman animals},
	volume = {136},
	issn = {0149-7634},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763422000872},
	doi = {10.1016/j.neubiorev.2022.104598},
	abstract = {Systematic investigation of reactivation-induced memory updating began in the 1960s, and a wave of research in this area followed the seminal articulation of “reconsolidation” theory in the early 2000s. Myriad studies indicate that memory reactivation can cause previously consolidated memories to become labile and sensitive to weakening, strengthening, or other forms of modification. However, from its nascent period to the present, the field has been beset by inconsistencies in researchers’ abilities to replicate seemingly established effects. Here we review these many studies, synthesizing the human and nonhuman animal literature, and suggest that these failures-to-replicate reflect a highly complex and delicately balanced memory modification system, the substrates of which must be finely tuned to enable adaptive memory updating while limiting maladaptive, inaccurate modifications. A systematic approach to the entire body of evidence, integrating positive and null findings, will yield a comprehensive understanding of the complex and dynamic nature of long-term memory storage and the potential for harnessing modification processes to treat mental disorders driven by pervasive maladaptive memories.},
	pages = {104598},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	shortjournal = {Neuroscience \& Biobehavioral Reviews},
	author = {Jardine, Kristen H. and Huff, A. Ethan and Wideman, Cassidy E. and {McGraw}, Shelby D. and Winters, Boyer D.},
	urldate = {2022-03-08},
	date = {2022-05-01},
	langid = {english},
	keywords = {Memory, Human, Retrieval, Boundary conditions, Destabilization, Labilization, Modification, Reconsolidation, Rodent, Storage},
	file = {Jardine et al. - 2022 - The evidence for and against reactivation-induced .pdf:/Users/tom/Zotero/storage/S3CNN2H5/Jardine et al. - 2022 - The evidence for and against reactivation-induced .pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/GIYKLQUW/S0149763422000872.html:text/html},
}

@article{morawski_how_2021,
	title = {How to true psychology’s objects},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211046518},
	doi = {10.1177/10892680211046518},
	abstract = {Psychology’s current crisis attends most visibly to perceived problems with statistical models, methods, publication practices, and career incentives. Rarely is close attention given to the objects of inquiry—to ontological matters—yet the crisis-related literature does features statements about the nature of psychology’s objects. Close analysis of the ontological claims reveals discrepant understandings: some researchers assume objects to be stable and singular while others posit them to be dynamic and complex. Nevertheless, both views presume the objects under scrutiny to be real. The analysis also finds each of these ontological claims to be associated not only with particular method prescriptions but also with distinct notions of the scientific self. Though both take the scientific self to be objective, one figures the scientist as not always a rational actor and, therefore, requiring some behavior regulation, while the other sees the scientist as largely capable of self-governing sustained through painstakingly acquired expertise and self-control. The fate of these prevalent assemblages of object, method, and scientific self remains to be determined, yet as conditions of possibility they portend quite different futures. Following description of the assemblages, the article ventures a futuristic portrayal of the scientific practices they each might engender.},
	pages = {10892680211046518},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Morawski, Jill},
	urldate = {2022-03-10},
	date = {2021-10-02},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication, toread, crisis, objectivity, ontology},
	file = {Morawski - 2021 - How to True Psychology’s Objects.pdf:/Users/tom/Zotero/storage/DL744WJ3/Morawski - 2021 - How to True Psychology’s Objects.pdf:application/pdf},
}

@article{mulberger_early_2022,
	title = {Early experimental psychology: how did replication work before p-hacking?},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211066468},
	doi = {10.1177/10892680211066468},
	shorttitle = {Early experimental psychology},
	pages = {10892680211066468},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Mülberger, Annette},
	urldate = {2022-03-10},
	date = {2022-02-04},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {Experimentation, Scientific Practice, toread, Controversy, Nineteenth Century, Psychophysics, Repetition},
	file = {Mülberger - 2022 - Early Experimental Psychology How did Replication.pdf:/Users/tom/Zotero/storage/9LSYIFT5/Mülberger - 2022 - Early Experimental Psychology How did Replication.pdf:application/pdf},
}

@article{callard_replication_2022,
	title = {Replication and reproduction: crises in psychology and academic labour},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211055660},
	doi = {10.1177/10892680211055660},
	shorttitle = {Replication and reproduction},
	abstract = {Discussions of the replication crisis in psychology require more substantive analysis of the crisis of academic labour and of social reproduction in the university. Both the replication crisis and the crisis of social reproduction in the university describe a failure in processes of reproducing something. The financial crisis of 2007–8 shortly preceded the emergence of the replication crisis, as well as exacerbated ongoing tendencies in the organisation and practices of university research (particularly the use of precarious contracts and the adjunctification of research). These provide two reasons to address these two named crises together. But many analyses of and responses to the replication crisis turn to research culture, often at the expense of adequate investigations of research labour. Today’s psychological sciences are made through multiple forms of labour: these include researchers, who range from senior principal investigators to sub-contracted, and exploited, research assistants; research participants/subjects, who include those providing labour for experiments via exploitative platforms including Amazon Mechanical Turk; and workers providing heterogeneous technical and administrative labour. Through understanding what is at stake for these multiple forms of labour, psychology might better analyse problems besetting psychology today, as well as develop different imaginaries and practices for how to address them.},
	pages = {10892680211055660},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Callard, Felicity},
	urldate = {2022-03-10},
	date = {2022-02-24},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication crisis, toread, academic labour, history of psychology, invisible labour, social reproduction, university, work},
	file = {Full Text:/Users/tom/Zotero/storage/4S3CIHDD/Callard - 2022 - Replication and Reproduction Crises in Psychology.pdf:application/pdf},
}

@article{flis_function_2022,
	title = {The function of literature in psychological science},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211066466},
	doi = {10.1177/10892680211066466},
	abstract = {The recent reform debates in psychological science, prompted by a widespread crisis of confidence, have exposed and destabilized the so-called myth of self-correction, that is, the problem that most scientists perceive their disciplines as self-correcting without engaging in actual practices that correct the scientific record. In this paper, building on the idea of self-correction as a myth, I propose another myth common to psychological science: the myth of self-organization. The myth of self-organization is the idea that scientific literature will organize itself into something the community adding to it would recognize as systematic knowledge; while the actual members of those communities do not engage in effective ways of organizing it. I argue for the existence of the myth self-organization by taking a historical look at how the scientific literature was construed by psychologists during the 20th century. In my view, the literature, and behaviors of scientists related to it, becomes a social institution exerting influence over the science it belongs to. I conclude with a critical discussion of self-organization through the debates about preregistration and theory formalization in psychology’s reform movement.},
	pages = {10892680211066466},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Flis, Ivan},
	urldate = {2022-03-10},
	date = {2022-01-21},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication crisis, toread, history of psychology, reform movement in psychology, scientific literature, theory formalization},
	file = {Flis - 2022 - The function of literature in psychological scienc.pdf:/Users/tom/Zotero/storage/HLFIDF79/Flis - 2022 - The function of literature in psychological scienc.pdf:application/pdf},
}

@article{vazire_where_2021,
	title = {Where are the self-correcting mechanisms in science?},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211033912},
	doi = {10.1177/10892680211033912},
	abstract = {It is often said that science is self-correcting, but the replication crisis suggests that self-correction mechanisms have fallen short. How can we know whether a particular scientific field has effective self-correction mechanisms, that is, whether its findings are credible? The usual processes that supposedly provide mechanisms for scientific self-correction, such as journal-based peer review and institutional committees, have been inadequate. We describe more verifiable indicators of a field’s commitment to self-correction. These fall under the broad headings of 1) transparency, which is already the subject of many reform efforts and 2) critical appraisal, which has received less attention and which we focus on here. Only by obtaining Observable Self-Correction Indicators ({OSCIs}) can we begin to evaluate the claim that “science is self-correcting.” We expect that the veracity of this claim varies across fields and subfields, and suggest that some fields, such as psychology and biomedicine, fall far short of an appropriate level of transparency and, especially, critical appraisal. Fields without robust, verifiable mechanisms for transparency and critical appraisal cannot reasonably be said to be self-correcting, and thus do not warrant the credibility often imputed to science as a whole.},
	pages = {10892680211033912},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Vazire, Simine and Holcombe, Alex O.},
	urldate = {2022-03-10},
	date = {2021-08-12},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication, metascience, epistemology, history of psychology},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/9I8GR6HZ/Vazire and Holcombe - 2021 - Where are the Self-Correcting Mechanisms in Scienc.pdf:application/pdf},
}

@article{gollwitzer_context_2021,
	title = {Context dependency as a predictor of replicability},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211015635},
	doi = {10.1177/10892680211015635},
	abstract = {We scrutinize the argument that unsuccessful replications—and heterogeneous effect sizes more generally—may reflect an underappreciated influence of context characteristics. Notably, while some of these context characteristics may be conceptually irrelevant (as they merely affect psychometric properties of the measured/manipulated variables), others are conceptually relevant as they qualify a theory. Here, we present a conceptual and analytical framework that allows researchers to empirically estimate the extent to which effect size heterogeneity is due to conceptually relevant versus irrelevant context characteristics. According to this framework, contextual characteristics are conceptually relevant when the observed heterogeneity of effect sizes cannot be attributed to psychometric properties. As an illustrative example, we demonstrate that the observed heterogeneity of the “moral typecasting” effect, which had been included in the {ManyLabs} 2 replication project, is more likely attributable to conceptually relevant rather than irrelevant context characteristics, which suggests that the psychological theory behind this effect may need to be specified. In general, we argue that context dependency should be taken more seriously and treated more carefully by replication research.},
	pages = {10892680211015635},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Gollwitzer, Mario and Schwabe, Johannes},
	urldate = {2022-03-10},
	date = {2021-07-20},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {replication, methodology, social psychology, toread, context dependency, hidden moderators},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/IMCFDT3G/Gollwitzer and Schwabe - 2021 - Context Dependency as a Predictor of Replicability.pdf:application/pdf},
}

@article{maiers_replication_2021,
	title = {Replication crisis – just another instance of the replication of crises in psychology? Historical retrospections and theoretical-psychological assessments},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211033915},
	doi = {10.1177/10892680211033915},
	shorttitle = {Replication crisis – just another instance of the replication of crises in psychology?},
	abstract = {The current dismay within the mainstream of nomological psychology may result from the fact that the anomaly of non-replicability has a direct bearing on its very own methodological requirements and quality criteria of empirical research. The call for more scientific rigour on the customary avenue in order to secure unambiguous empirical findings gives, however, rise to suspect that the deeper reason for this anomaly is not yet recognised: namely, the misguided regulation of a strictly objective inquiry, distorting what is present and relevant in everyday life and treating the ‘subjective’ of the subject matter as the central root of interfering factors which have to be eliminated or neutralised in the pursuit of experimental hypothesis testing. The problems of replicability would thus be a proof once again that the notorious inversion between matter and method does not really work, due to the uncircumventable characteristics of human inter-/subjectivity. In this sense, the replication crisis replicates the perennial topic of all historical discussions about a crisis in psychology – the failure of a ‘psychology without subject’.},
	pages = {10892680211033915},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Maiers, Wolfgang},
	urldate = {2022-03-10},
	date = {2021-08-28},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {toread, dualism, epistemic crisis, scientism, subject-scientific psychology, theoretical indeterminacy, unification},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/83FAQUHQ/Maiers - 2021 - Replication crisis – Just Another Instance of the .pdf:application/pdf},
}

@article{derksen_tone_2021,
	title = {The tone debate: knowledge, self, and social order},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211015636},
	doi = {10.1177/10892680211015636},
	shorttitle = {The tone debate},
	abstract = {In the replication crisis in psychology, a “tone debate” has developed. It concerns the question of how to conduct scientific debate effectively and ethically. How should scientists give critique without unnecessarily damaging relations? The increasing use of Facebook and Twitter by researchers has made this issue especially pressing, as these social technologies have greatly expanded the possibilities for conversation between academics, but there is little formal control over the debate. In this article, we show that psychologists have tried to solve this issue with various codes of conduct, with an appeal to virtues such as humility, and with practices of self-transformation. We also show that the polemical style of debate, popular in many scientific communities, is itself being questioned by psychologists. Following Shapin and Schaffer’s analysis of the ethics of Robert Boyle’s experimental philosophy in the 17th century, we trace the connections between knowledge, social order, and subjectivity as they are debated and revised by present-day psychologists.},
	pages = {10892680211015636},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Derksen, Maarten and Field, Sarahanne},
	urldate = {2022-03-10},
	date = {2021-09-22},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {open science, psychology, crisis, debate},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/MA8N2Q55/Derksen and Field - 2021 - The Tone Debate Knowledge, Self, and Social Order.pdf:application/pdf},
}

@article{nelson_psychology_2021,
	title = {Psychology exceptionalism and the multiple discovery of the replication crisis},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211046508},
	doi = {10.1177/10892680211046508},
	abstract = {This article outlines what we call the “narrative of psychology exceptionalism” in commentaries on the replication crisis: many thoughtful commentaries link the current crisis to the specificity of psychology’s history, methods, and subject matter, but explorations of the similarities between psychology and other fields are comparatively thin. Historical analyses of the replication crisis in psychology further contribute to this exceptionalism by creating a genealogy of events and personalities that shares little in common with other fields. We aim to rebalance this narrative by examining the emergence and evolution of replication discussions in psychology alongside their emergence and evolution in biomedicine. Through a mixed-methods analysis of commentaries on replication in psychology and the biomedical sciences, we find that these conversations have, from the early years of the crisis, shared a common core that centers on concerns about the effectiveness of traditional peer review, the need for greater transparency in methods and data, and the perverse incentive structure of academia. Drawing on Robert Merton’s framework for analyzing multiple discovery in science, we argue that the nearly simultaneous emergence of this narrative across fields suggests that there are shared historical, cultural, or institutional factors driving disillusionment with established scientific practices.},
	pages = {10892680211046508},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Nelson, Nicole C. and Chung, Julie and Ichikawa, Kelsey and Malik, Momin M.},
	urldate = {2022-03-10},
	date = {2021-09-23},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {reproducibility, replication, psychology, toread, biomedicine, correspondence analysis, mixed methods, multiple discovery},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/RSEKVM65/Nelson et al. - 2021 - Psychology Exceptionalism and the Multiple Discove.pdf:application/pdf},
}

@article{haig_understanding_2021,
	title = {Understanding replication in a way that is true to science},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211046514},
	doi = {10.1177/10892680211046514},
	abstract = {In this article, I critically examine a number of widely held beliefs about the nature of replication and its place in science, with particular reference to psychology. In doing so, I present a number of underappreciated understandings of the nature of science more generally. I contend that some contributors to the replication debates overstate the importance of replication in science and mischaracterize the relationship between direct and conceptual replication. I also claim that there has been a failure to appreciate sufficiently the variety of legitimate replication practices that scientists engage in. In this regard, I highlight the tendency to pay insufficient attention to methodological triangulation as an important strategy for justifying empirical claims. I argue, further, that the replication debates tend to overstate the closeness of the relationship between replication and theory construction. Some features of this relationship are spelt out with reference to the hypothetico-deductive and the abductive accounts of scientific method. Additionally, an evaluation of the status of replication in different characterizations of scientific progress is undertaken. I maintain that viewing replication as just one element of the wide array of scientific endeavors leads to the conclusion that it is not as prominent in science as is often claimed.},
	pages = {10892680211046514},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Haig, Brian D.},
	urldate = {2022-03-10},
	date = {2021-09-25},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {philosophy of science, scientific progress, toread, methodological triangulation, nature of science, phenomena detection, reliabilism, scientific method, the status of replication, theory construction, varieties of replication},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/YDWJZF2K/Haig - 2021 - Understanding Replication in a Way That Is True to.pdf:application/pdf},
}

@report{osborne_pre-registration_2022,
	title = {Pre-registration as behaviour: developing an evidence-based intervention to increase pre-registration uptake by researchers using the Behaviour Change Wheel},
	url = {https://psyarxiv.com/w3qg9/},
	shorttitle = {Pre-registration as behaviour},
	abstract = {Background: Pre-registration is an open research practice that can mitigate against questionable research practices and contribute to enhanced research outcomes.  This paper explores barriers and enablers to pre-registration, and develops an evidence-based behaviour change intervention specification to increase its uptake.  Methods: The Behaviour Change Wheel ({BCW}) framework of intervention development and {COM}-B model of behaviour were used to inform the development of a mixed-methods online questionnaire, assessing barriers and enablers to pre-registration.  Data were collected from 18/05/2020 to 12/07/2020, and explored using descriptive statistics, reflexive thematic analysis, and {COM}-B.  {BCW} was used to develop an intervention specification.  Results: Respondents were researchers (n = 105) who were mostly engaged in psychological research (71\%) and had pre-registered before (75\%).  Insufficient knowledge and skill (psychological capability), social support (social opportunity), time (physical opportunity), and incentivisation (reflective motivation) were the most substantial barriers to pre-registration, whereas belief in pre-registration contributing to desirable research outcomes (reflective motivation) was the most substantial enabler.  These findings informed the development of an intervention specification to increase pre-registration uptake by researchers.  Conclusion: This paper demonstrates the strong potential of {BCW} to facilitate open research practices. The identified barriers and enablers, intervention specification, and the behaviour change approach outlined, may be used to increase pre-registration uptake; for example, developing new or refining existing training and incentivisation interventions.  This paper may inspire others to consider the strong potential of {BCW} to facilitate open research practices and so contribute to enhanced research outcomes.},
	institution = {{PsyArXiv}},
	author = {Osborne, Christopher and Norris, Emma},
	urldate = {2022-03-10},
	date = {2022-03-08},
	langid = {english},
	doi = {10.31234/osf.io/w3qg9},
	note = {type: article},
	keywords = {Meta-science, pre-registration, open research, intervention, meta-science, toread, Behaviour Change Wheel, {COM}-B},
	file = {Full Text PDF:/Users/tom/Zotero/storage/CVE879N8/Osborne and Norris - 2022 - Pre-registration as behaviour developing an evide.pdf:application/pdf},
}

@article{chaudhary_making_2022,
	title = {Making sense of culture for the psychological sciences},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/10892680211066473},
	doi = {10.1177/10892680211066473},
	abstract = {In this article, we examine the place of culture in the human sciences with specific reference to psychology and the cultural histories of India. Despite the depth of scholarly writing on the intimate and inextricable ties between culture and psychological processes, core advancements and definitive positions in psychology have remained elusive. The privileging of a single culturally specific world-view alongside the exclusion of others has seriously hindered the authentic internationalization of psychology. We propose that linkages between culture and psychology need to be visualized as a dialogue between different cultural traditions. In the dialectics between bheda–abheda (difference and non-difference), structural-developmental, dispositional-relational, and social-collective processes will be invoked to develop our arguments for a human-science approach to the study of persons in culture. We argue that it is through the inclusion, rather than suppression, of diverse ideologies that generalizability can be best achieved. This is a call for an audit and reconstruction of psychology and its practices as an international discipline with a roadmap for theory construction and research informed by a cultural psychological approach toward human phenomena.},
	pages = {10892680211066473},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Chaudhary, Nandita and Misra, Girishwar and Bansal, Parul and Valsiner, Jaan and Singh, Tushar},
	urldate = {2022-03-10},
	date = {2022-01-20},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {toread, generalizability, cultural psychology, general psychology, Indian psychology, internationalization},
	file = {Chaudhary et al. - 2022 - Making sense of culture for the psychological scie.pdf:/Users/tom/Zotero/storage/5T3GMYZ6/Chaudhary et al. - 2022 - Making sense of culture for the psychological scie.pdf:application/pdf},
}

@article{autzen_is_2021,
	title = {Is the replication crisis a base-rate fallacy?},
	volume = {42},
	issn = {1573-1200},
	url = {https://doi.org/10.1007/s11017-022-09561-8},
	doi = {10.1007/s11017-022-09561-8},
	abstract = {Is science in the midst of a crisis of replicability and false discoveries? In a recent article, Alexander Bird offers an explanation for the apparent lack of replicability in the biomedical sciences. Bird argues that the surprise at the failure to replicate biomedical research is a result of the fallacy of neglecting the base rate. The base-rate fallacy arises in situations in which one ignores the base rate—or prior probability—of an event when assessing the probability of this event in the light of some observed evidence. By extension, the replication crisis would result from ignoring the low prior probability of biomedical hypotheses. In this paper, my response to Bird’s claim is twofold. First, I show that the argument according to which the replication crisis is due to the low prior of biomedical hypotheses is incomplete. Second, I claim that a simple base-rate fallacy model does not account for some important methodological insights that have emerged in discussions of the replication crisis.},
	pages = {233--243},
	number = {5},
	journaltitle = {Theoretical Medicine and Bioethics},
	shortjournal = {Theor Med Bioeth},
	author = {Autzen, Bengt},
	urldate = {2022-03-10},
	date = {2021-12-01},
	langid = {english},
	file = {Autzen - 2021 - Is the replication crisis a base-rate fallacy.pdf:/Users/tom/Zotero/storage/ZDNMKEBH/Autzen - 2021 - Is the replication crisis a base-rate fallacy.pdf:application/pdf},
}

@article{bird_understanding_2021,
	title = {Understanding the replication crisis as a base rate fallacy},
	volume = {72},
	issn = {0007-0882},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/axy051},
	doi = {10.1093/bjps/axy051},
	abstract = {The replication (replicability, reproducibility) crisis in social psychology and clinical medicine arises from the fact that many apparently well-confirmed experimental results are subsequently overturned by studies that aim to replicate the original study. The culprit is widely held to be poor science: questionable research practices, failure to publish negative results, bad incentives, and even fraud. In this article I argue that the high rate of failed replications is consistent with high-quality science. We would expect this outcome if the field of science in question produces a high proportion of false hypotheses prior to testing. If most of the hypotheses under test are false, then there will be many false hypotheses that are apparently supported by the outcomes of well conducted experiments and null hypothesis significance tests with a type-I error rate (α) of 5\%. Failure to recognize this is to commit the fallacy of ignoring the base rate. I argue that this is a plausible diagnosis of the replication crisis and examine what lessons we thereby learn for the future conduct of science.},
	pages = {965--993},
	number = {4},
	journaltitle = {The British Journal for the Philosophy of Science},
	author = {Bird, Alexander},
	urldate = {2022-03-10},
	date = {2021-12},
	note = {Publisher: The University of Chicago Press},
	file = {Bird - 2021 - Understanding the replication crisis as a base rat.pdf:/Users/tom/Zotero/storage/AB68VMZ6/Bird - 2021 - Understanding the replication crisis as a base rat.pdf:application/pdf},
}

@article{lewis_puzzling_nodate,
	title = {The puzzling relationship between multi-laboratory replications and meta-analyses of the published literature},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.211499},
	doi = {10.1098/rsos.211499},
	abstract = {What is the best way to estimate the size of important effects? Should we aggregate across disparate findings using statistical meta-analysis, or instead run large, multi-laboratory replications ({MLR})? A recent paper by Kvarven, Strømland and Johannesson (Kvarven et al. 2020 Nat. Hum. Behav. 4, 423–434. (doi:10.1038/s41562-019-0787-z)) compared effect size estimates derived from these two different methods for 15 different psychological phenomena. The authors reported that, for the same phenomenon, the meta-analytic estimate tended to be about three times larger than the {MLR} estimate. These results are a specific example of a broader question: What is the relationship between meta-analysis and {MLR} estimates? Kvarven et al. suggested that their results undermine the value of meta-analysis. By contrast, we argue that both meta-analysis and {MLR} are informative, and that the discrepancy between the two estimates that they observed is in fact still largely unexplained. Informed by re-analyses of Kvarven et al.’s data and by other empirical evidence, we discuss possible sources of this discrepancy and argue that understanding the relationship between estimates obtained from these two methods is an important puzzle for future meta-scientific research.},
	pages = {211499},
	number = {2},
	journaltitle = {Royal Society Open Science},
	author = {Lewis, Molly and Mathur, Maya B. and {VanderWeele}, Tyler J. and Frank, Michael C.},
	urldate = {2022-03-10},
	note = {Publisher: Royal Society},
	keywords = {meta-analysis, meta-science, toread, multi-laboratory replication},
	file = {Full Text PDF:/Users/tom/Zotero/storage/C9PD65E6/Lewis et al. - The puzzling relationship between multi-laboratory.pdf:application/pdf},
}

@article{goodman_discussion_2014,
	title = {Discussion: An estimate of the science-wise false discovery rate and application to the top medical literature},
	volume = {15},
	issn = {1465-4644},
	url = {https://doi.org/10.1093/biostatistics/kxt035},
	doi = {10.1093/biostatistics/kxt035},
	shorttitle = {Discussion},
	pages = {23--27},
	number = {1},
	journaltitle = {Biostatistics},
	shortjournal = {Biostatistics},
	author = {Goodman, Steven N.},
	urldate = {2022-03-10},
	date = {2014-01-01},
	file = {Goodman - 2014 - Discussion An estimate of the science-wise false .pdf:/Users/tom/Zotero/storage/VNIFGMW5/Goodman - 2014 - Discussion An estimate of the science-wise false .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/HAHJB622/245596.html:text/html},
}

@article{hair_randomised_2019,
	title = {A randomised controlled trial of an Intervention to Improve Compliance with the {ARRIVE} guidelines ({IICARus})},
	volume = {4},
	issn = {2058-8615},
	url = {https://doi.org/10.1186/s41073-019-0069-3},
	doi = {10.1186/s41073-019-0069-3},
	abstract = {The {ARRIVE} (Animal Research: Reporting of In Vivo Experiments) guidelines are widely endorsed but compliance is limited. We sought to determine whether journal-requested completion of an {ARRIVE} checklist improves full compliance with the guidelines.},
	pages = {12},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Research Integrity and Peer Review},
	author = {Hair, Kaitlyn and Macleod, Malcolm R. and Sena, Emily S. and Sena, Emily S. and Hair, Kaitlyn and Macleod, Malcolm R. and Howells, David and Bath, Philip and Irvine, Cadi and MacCallum, Catriona and Morrison, Gavin and Clark, Alejandra and Alvino, Gina and Dohm, Michelle and Liao, Jing and Sena, Chris and Moreland, Rosie and Cramond, Fala and Currie, Gillian L. and Bahor, Zsanett and Grill, Paula and Bannach-Brown, Alexandra and Marcu, Daniel-Cosmin and Antar, Sarah and Blazek, Katrina and Konold, Timm and Dingwall, Monica and Hohendorf, Victoria and Hosh, Mona and Gerlei, Klara Zsofia and Wever, Kimberley Elaine and Jones, Victor and Quinn, Terence J. and Karp, Natasha A. and Freymann, Jennifer and Shek, Anthony and Gregorc, Teja and Rinaldi, Arianna and Jheeta, Privjyot and Nazzal, Ahmed and Henshall, David Ewart and Storey, Joanne and Baginskaite, Julija and de Oliveira, Cilene Lino and Laban, Kamil and Charbonney, Emmanuel and Lynn, Savannah A. and Cascella, Marco and Wheater, Emily and Baker, Daniel and Cheyne, Ryan and Christopher, Edward and Roncon, Paolo and De-Souza, Evandro Araújo and Warda, Mahmoud and Corke, Sarah and Ammar, Zeinab and O’Connor, Leigh and Devonshire, Ian M. and McCann, Sarah K. and Gray, Laura J. and Tanriver-Ayder, Ezgi and {on behalf of the IICARus Collaboration}},
	urldate = {2022-03-22},
	date = {2019-06-12},
	keywords = {Randomised controlled trial, {ARRIVE}, Reporting guidelines},
	file = {Full Text PDF:/Users/tom/Zotero/storage/EQ47QJZY/Hair et al. - 2019 - A randomised controlled trial of an Intervention t.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/3D4QSKX2/s41073-019-0069-3.html:text/html},
}

@article{rouder_what_2016,
	title = {The what, why, and how of born-open data},
	volume = {48},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-015-0630-z},
	doi = {10.3758/s13428-015-0630-z},
	abstract = {Although many researchers agree that scientific data should be open to scrutiny to ferret out poor analyses and outright fraud, most raw data sets are not available on demand. There are many reasons researchers do not open their data, and one is technical. It is often time consuming to prepare and archive data. In response, my laboratory has automated the process such that our data are archived the night they are created without any human approval or action. All data are versioned, logged, time stamped, and uploaded including aborted runs and data from pilot subjects. The archive is {GitHub}, github.com, the world’s largest collection of open-source materials. Data archived in this manner are called born open. In this paper, I discuss the benefits of born-open data and provide a brief technical overview of the process. I also address some of the common concerns about opening data before publication.},
	pages = {1062--1069},
	number = {3},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Rouder, Jeffrey N.},
	urldate = {2022-03-25},
	date = {2016-09-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/tom/Zotero/storage/LQWS7YAY/Rouder - 2016 - The what, why, and how of born-open data.pdf:application/pdf;The what, why, and how of born-open data:/Users/tom/Zotero/storage/98U8HBQ8/rouder2015.pdf.pdf:application/pdf},
}

@article{gernsbacher_writing_2018,
	title = {Writing empirical articles: transparency, reproducibility, clarity, and memorability},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918754485},
	doi = {10.1177/2515245918754485},
	shorttitle = {Writing empirical articles},
	abstract = {This article provides recommendations for writing empirical journal articles that enable transparency, reproducibility, clarity, and memorability. Recommendations for transparency include preregistering methods, hypotheses, and analyses; submitting registered reports; distinguishing confirmation from exploration; and showing your warts. Recommendations for reproducibility include documenting methods and results fully and cohesively, by taking advantage of open-science tools, and citing sources responsibly. Recommendations for clarity include writing short paragraphs, composed of short sentences; writing comprehensive abstracts; and seeking feedback from a naive audience. Recommendations for memorability include writing narratively; embracing the hourglass shape of empirical articles; beginning articles with a hook; and synthesizing, rather than Mad Libbing, previous literature.},
	pages = {403--414},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Gernsbacher, Morton Ann},
	urldate = {2022-03-27},
	date = {2018-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {open science, preregistration, open data, tutorial, open materials, articles, registered reports, writing},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/Z8L5QUVR/Gernsbacher - 2018 - Writing Empirical Articles Transparency, Reproduc.pdf:application/pdf},
}

@article{federer_data_2018,
	title = {Data sharing in {PLOS} {ONE}: An analysis of Data Availability Statements},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194768},
	doi = {10.1371/journal.pone.0194768},
	shorttitle = {Data sharing in {PLOS} {ONE}},
	abstract = {A number of publishers and funders, including {PLOS}, have recently adopted policies requiring researchers to share the data underlying their results and publications. Such policies help increase the reproducibility of the published literature, as well as make a larger body of data available for reuse and re-analysis. In this study, we evaluate the extent to which authors have complied with this policy by analyzing Data Availability Statements from 47,593 papers published in {PLOS} {ONE} between March 2014 (when the policy went into effect) and May 2016. Our analysis shows that compliance with the policy has increased, with a significant decline over time in papers that did not include a Data Availability Statement. However, only about 20\% of statements indicate that data are deposited in a repository, which the {PLOS} policy states is the preferred method. More commonly, authors state that their data are in the paper itself or in the supplemental information, though it is unclear whether these data meet the level of sharing required in the {PLOS} policy. These findings suggest that additional review of Data Availability Statements or more stringent policies may be needed to increase data sharing.},
	pages = {e0194768},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Federer, Lisa M. and Belter, Christopher W. and Joubert, Douglas J. and Livinski, Alicia and Lu, Ya-Ling and Snyders, Lissa N. and Thompson, Holly},
	urldate = {2022-03-28},
	date = {2018-05-02},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open data, Science policy, Reproducibility, Randomized controlled trials, Scientific publishing, Public policy, Biotechnology, Institutional repositories},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HKAVTPCL/Federer et al. - 2018 - Data sharing in PLOS ONE An analysis of Data Avai.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N3AZQJGK/article.html:text/html},
}

@article{besancon_open_2021,
	title = {Open science saves lives: lessons from the {COVID}-19 pandemic},
	volume = {21},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-021-01304-y},
	doi = {10.1186/s12874-021-01304-y},
	shorttitle = {Open science saves lives},
	abstract = {In the last decade Open Science principles have been successfully advocated for and are being slowly adopted in different research communities. In response to the {COVID}-19 pandemic many publishers and researchers have sped up their adoption of Open Science practices, sometimes embracing them fully and sometimes partially or in a sub-optimal manner. In this article, we express concerns about the violation of some of the Open Science principles and its potential impact on the quality of research output. We provide evidence of the misuses of these principles at different stages of the scientific process. We call for a wider adoption of Open Science practices in the hope that this work will encourage a broader endorsement of Open Science principles and serve as a reminder that science should always be a rigorous process, reliable and transparent, especially in the context of a pandemic where research findings are being translated into practice even more rapidly. We provide all data and scripts at https://osf.io/renxy/.},
	pages = {117},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Besançon, Lonni and Peiffer-Smadja, Nathan and Segalas, Corentin and Jiang, Haiting and Masuzzo, Paola and Smout, Cooper and Billy, Eric and Deforet, Maxime and Leyrat, Clémence},
	urldate = {2022-04-01},
	date = {2021-06-05},
	keywords = {Open science, Peer review, Methodology, {COVID}-19},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PEHJDM2J/Besançon et al. - 2021 - Open science saves lives lessons from the COVID-1.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6DSZ29SH/s12874-021-01304-y.html:text/html},
}

@report{alipourfard_systematizing_2021,
	title = {Systematizing Confidence in Open Research and Evidence ({SCORE})},
	url = {https://osf.io/preprints/socarxiv/46mnb/},
	abstract = {Assessing the credibility of research claims is a central, continuous, and laborious part of the scientific process. Credibility assessment strategies range from expert judgment to aggregating existing evidence to systematic replication efforts. Such assessments can require substantial time and effort. Research progress could be accelerated if there were rapid, scalable, accurate credibility indicators to guide attention and resource allocation for further assessment. The {SCORE} program is creating and validating algorithms to provide confidence scores for research claims at scale. To investigate the viability of scalable tools, teams are creating: a database of claims from papers in the social and behavioral sciences; expert and machine generated estimates of credibility; and, evidence of reproducibility, robustness, and replicability to validate the estimates. Beyond the primary research objective, the data and artifacts generated from this program will be openly shared and provide an unprecedented opportunity to examine research credibility and evidence.},
	institution = {{SocArXiv}},
	author = {Alipourfard, Nazanin and Arendt, Beatrix and Benjamin, Daniel M. and Benkler, Noam and Bishop, Michael and Burstein, Mark and Bush, Martin and Caverlee, James and Chen, Yiling and Clark, Chae and Almenberg, Anna Dreber and Errington, Timothy M. and Fidler, Fiona and Fox [{SCORE}, Nicholas and Frank, Aaron and Fraser, Hannah and Friedman, Scott and Gelman, Ben and Gentile, James and Giles, C. Lee and Gordon, Michael B. and Gordon-Sarney, Reed and Griffin, Christopher and Gulden, Timothy and Hahn, Krystal and Hartman, Robert and Holzmeister, Felix and Hu, Xia Ben and Johannesson, Magnus and Kezar, Lee and Struhl, Melissa Kline and Kuter, Ugur and Kwasnica, Anthony M. and Lee, Dong-Ho and Lerman, Kristina and Liu, Yang and Loomas, Zachary and Luis [{SCORE}, Bri and Magnusson, Ian and Miske, Olivia and Mody, Fallon and Morstatter, Fred and Nosek, Brian A. and Parsons, Elan Simon and Pennock, David and Pfeiffer, Thomas and Pujara, Jay and Rajtmajer, Sarah and Ren, Xiang and Salinas, Abel and Selvam, Ravi Kiran and Shipman, Frank and Silverstein, Priya and Sprenger, Amber and Squicciarini, Anna Ms and Stratman, Steve and Sun, Kexuan and Tikoo, Saatvik and Twardy, Charles R. and Tyner, Andrew and Viganola, Domenico and Wang, Juntao and Wilkinson, David Peter and Wintle, Bonnie and Wu, Jian},
	urldate = {2022-04-01},
	date = {2021-05-03},
	langid = {english},
	doi = {10.31235/osf.io/46mnb},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Psychology, replicability, reproducibility, Sociology, Metascience, Economics, Political Science, algorithms, credibility, Leadership Studies, Legal Studies, Organization Development, Public Affairs, Public Policy and Public Administration, social sciences},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7K8U7LBW/Alipourfard et al. - 2021 - Systematizing Confidence in Open Research and Evid.pdf:application/pdf},
}

@report{laurinavichyute_share_2021,
	title = {Share the code, not just the data: A case study of the reproducibility of {JML} articles published under the open data policy},
	url = {https://psyarxiv.com/hf297/},
	shorttitle = {Share the code, not just the data},
	abstract = {In 2019 the Journal of Memory and Language instituted an open data and code policy; this policy requires that, as a rule, code and data be released at the latest upon publication. How effective is this policy? We compared 59 papers published before, and 59 papers published after, the policy took effect. After the policy was in place, the rate of data sharing increased by more than 50\%. We further looked at whether papers published under the open data policy were reproducible, in the sense that the published results should be possible to regenerate given the data, and given the code, when code was provided. For 8 out of the 59 papers, data sets were inaccessible. The reproducibility rate ranged from 34\% to 56\%, depending on the reproducibility criteria. The strongest predictor of whether an attempt to reproduce would be successful is the presence of the analysis code: it increases the probability of reproducing reported results by almost 40\%. We propose two simple steps that can increase the reproducibility of published papers: share the analysis code, and attempt to reproduce one’s own analysis using only the shared materials.},
	institution = {{PsyArXiv}},
	author = {Laurinavichyute, Anna and Yadav, Himanshu and Vasishth, Shravan},
	urldate = {2022-04-02},
	date = {2021-09-30},
	langid = {english},
	doi = {10.31234/osf.io/hf297},
	note = {type: article},
	keywords = {Meta-science, open science, Social and Behavioral Sciences, reproducibility, open data, meta-research, journal policy, Linguistics, Psycholinguistics and Neurolinguistics, reproducible statistical analyses},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3SSD35JV/Laurinavichyute et al. - 2021 - Share the code, not just the data A case study of.pdf:application/pdf},
}

@article{ioannidis_high-cited_2022,
	title = {High-cited favorable studies for {COVID}-19 treatments ineffective in large trials},
	issn = {1878-5921},
	doi = {10.1016/j.jclinepi.2022.04.001},
	abstract = {{OBJECTIVE}: To evaluate for {COVID}-19 treatments without benefits in subsequent large {RCTs} how many of their most-cited clinical studies had declared favorable results.
{STUDY} {DESIGN}: Scopus searches (December 23, 2021) identified articles on lopinavir-ritonavir, hydroxycholoroquine, azithromycin, remdesivir, convalescent plasma, colchicine or interferon (index interventions) that represented clinical trials and had {\textgreater}150 citations. Their conclusions were correlated with study design features. The ten most recent citations for the most-cited article on each index intervention were examined on whether they were critical to the highly-cited study. Altmetric scores were also obtained.
{RESULTS}: 40 eligible articles of clinical studies had received {\textgreater}150 citations. 20/40 (50\%) had favorable conclusions, 4 were equivocal. Highly-cited articles with favorable conclusions were rarely {RCTs} (3/20) while those without favorable conclusions were mostly {RCTs} (15/20, p=0.0003). Only 1 {RCT} with favorable conclusions had {\textgreater}160 patients. Citation counts correlated strongly with Altmetric scores, especially news items. Only 9 (15\%) of 60 recent citations to the most highly-cited studies with favorable or equivocal conclusions were critical.
{CONCLUSION}: Many clinical studies with favorable conclusions for largely ineffective {COVID}-19 treatments are uncritically heavily cited and disseminated. Early observational studies and small randomized trials may cause spurious claims of effectiveness that get perpetuated.},
	pages = {S0895--4356(22)00084--1},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {J Clin Epidemiol},
	author = {Ioannidis, John P. A.},
	date = {2022-04-06},
	pmid = {35398190},
	keywords = {bias, randomized controlled trials, {COVID}-19, non-randomized studies},
	file = {Ioannidis - 2022 - High-cited favorable studies for COVID-19 treatmen.pdf:/Users/tom/Zotero/storage/F53G9FPE/Ioannidis - 2022 - High-cited favorable studies for COVID-19 treatmen.pdf:application/pdf},
}

@article{gabelica_authors_2019,
	title = {Authors of trials from high-ranking anesthesiology journals were not willing to share raw data},
	volume = {109},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(18)30606-1/fulltext},
	doi = {10.1016/j.jclinepi.2019.01.012},
	pages = {111--116},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Gabelica, Mirko and Cavar, Jakica and Puljak, Livia},
	urldate = {2022-04-15},
	date = {2019-05-01},
	pmid = {30738169},
	note = {Publisher: Elsevier},
	keywords = {Randomized controlled trial, Authors, Data sharing, Publication, Anesthesiology, Raw data},
	file = {Gabelica et al. - 2019 - Authors of trials from high-ranking anesthesiology.pdf:/Users/tom/Zotero/storage/PAATVB5Q/Gabelica et al. - 2019 - Authors of trials from high-ranking anesthesiology.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JV44L764/fulltext.html:text/html},
}

@article{smith_biosecurity_2022,
	title = {Biosecurity in an age of open science},
	volume = {20},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001600},
	doi = {10.1371/journal.pbio.3001600},
	abstract = {The risk of accidental or deliberate misuse of biological research is increasing as biotechnology advances. As open science becomes widespread, we must consider its impact on those risks and develop solutions that ensure security while facilitating scientific progress. Here, we examine the interaction between open science practices and biosecurity and biosafety to identify risks and opportunities for risk mitigation. Increasing the availability of computational tools, datasets, and protocols could increase risks from research with misuse potential. For instance, in the context of viral engineering, open code, data, and materials may increase the risk of release of enhanced pathogens. For this dangerous subset of research, both open science and biosecurity goals may be achieved by using access-controlled repositories or application programming interfaces. While preprints accelerate dissemination of findings, their increased use could challenge strategies for risk mitigation at the publication stage. This highlights the importance of oversight earlier in the research lifecycle. Preregistration of research, a practice promoted by the open science community, provides an opportunity for achieving biosecurity risk assessment at the conception of research. Open science and biosecurity experts have an important role to play in enabling responsible research with maximal societal benefit.},
	pages = {e3001600},
	number = {4},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Smith, James Andrew and Sandbrink, Jonas B.},
	urldate = {2022-04-15},
	date = {2022-04-14},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open science, Medical risk factors, Peer review, Research assessment, Pandemics, Antibiotic resistance, Bioengineering, Genetic engineering},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HQXBMNCG/Smith and Sandbrink - 2022 - Biosecurity in an age of open science.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/T5632YWS/article.html:text/html},
}

@article{looyestyn_does_2017,
	title = {Does gamification increase engagement with online programs? A systematic review},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0173403},
	doi = {10.1371/journal.pone.0173403},
	shorttitle = {Does gamification increase engagement with online programs?},
	abstract = {Background Engagement in online programs is difficult to maintain. Gamification is the recent trend that offers to increase engagement through the inclusion of game-like features like points and badges, in non-game contexts. This review will answer the following question, ‘Are gamification strategies effective in increasing engagement in online programs?’ Method Eight databases (Web of Science, {PsycINFO}, Medline, {INSPEC}, {ERIC}, Cochrane Library, Business Source Complete and {ACM} Digital Library) were searched from 2010 to the 28th of October 2015 using a comprehensive search strategy. Eligibility criteria was based on the {PICOS} format, where “population” included adults, “intervention” involved an online program or smart phone application that included at least one gamification feature. “Comparator” was a control group, “outcomes” included engagement and “downstream” outcomes which occurred as a result of engagement; and “study design” included experimental studies from peer-reviewed sources. Effect sizes (Cohens d and 95\% confidence intervals) were also calculated. Results 1017 studies were identified from database searches following the removal of duplicates, of which 15 met the inclusion criteria. The studies involved a total of 10,499 participants, and were commonly undertaken in tertiary education contexts. Engagement metrics included time spent (n = 5), volume of contributions (n = 11) and occasions visited to the software (n = 4); as well as downstream behaviours such as performance (n = 4) and healthy behaviours (n = 1). Effect sizes typically ranged from medium to large in direct engagement and downstream behaviours, with 12 out of 15 studies finding positive significant effects in favour of gamification. Conclusion Gamification is effective in increasing engagement in online programs. Key recommendations for future research into gamification are provided. In particular, rigorous study designs are required to fully examine gamification’s effects and determine how to best achieve sustained engagement.},
	pages = {e0173403},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Looyestyn, Jemma and Kernot, Jocelyn and Boshoff, Kobie and Ryan, Jillian and Edney, Sarah and Maher, Carol},
	urldate = {2022-04-22},
	date = {2017-03-31},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Surveys, Internet, Database searching, Medical risk factors, Peer review, Computer software, Systematic reviews, Physical activity},
	file = {Does gamification increase engagement with online programs? A systematic review:/Users/tom/Zotero/storage/R64K3PSS/looyestyn2017.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/RHPYYC6Y/Looyestyn et al. - 2017 - Does gamification increase engagement with online .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SPZK7H5I/article.html:text/html},
}

@article{ratner_snorkel_2017,
	title = {Snorkel: rapid training data creation with weak supervision},
	volume = {11},
	issn = {2150-8097},
	url = {http://arxiv.org/abs/1711.10160},
	doi = {10.14778/3157794.3157797},
	shorttitle = {Snorkel},
	abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8x faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8x speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
	pages = {269--282},
	number = {3},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
	urldate = {2022-04-22},
	date = {2017-11},
	eprinttype = {arxiv},
	eprint = {1711.10160},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tom/Zotero/storage/UAY6CQTA/Ratner et al. - 2017 - Snorkel Rapid Training Data Creation with Weak Su.pdf:application/pdf;arXiv.org Snapshot:/Users/tom/Zotero/storage/ETWC9F7A/1711.html:text/html},
}

@article{charles_reporting_2009,
	title = {Reporting of sample size calculation in randomised controlled trials: review},
	volume = {338},
	rights = {© Charles et al 2009. This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/338/bmj.b1732},
	doi = {10.1136/bmj.b1732},
	shorttitle = {Reporting of sample size calculation in randomised controlled trials},
	abstract = {Objectives To assess quality of reporting of sample size calculation, ascertain accuracy of calculations, and determine the relevance of assumptions made when calculating sample size in randomised controlled trials.
Design Review.
Data sources We searched {MEDLINE} for all primary reports of two arm parallel group randomised controlled trials of superiority with a single primary outcome published in six high impact factor general medical journals between 1 January 2005 and 31 December 2006. All extra material related to design of trials (other articles, online material, online trial registration) was systematically assessed. Data extracted by use of a standardised form included parameters required for sample size calculation and corresponding data reported in results sections of articles. We checked completeness of reporting of the sample size calculation, systematically replicated the sample size calculation to assess its accuracy, then quantified discrepancies between a priori hypothesised parameters necessary for calculation and a posteriori estimates.
Results Of the 215 selected articles, 10 (5\%) did not report any sample size calculation and 92 (43\%) did not report all the required parameters. The difference between the sample size reported in the article and the replicated sample size calculation was greater than 10\% in 47 (30\%) of the 157 reports that gave enough data to recalculate the sample size. The difference between the assumptions for the control group and the observed data was greater than 30\% in 31\% (n=45) of articles and greater than 50\% in 17\% (n=24). Only 73 trials (34\%) reported all data required to calculate the sample size, had an accurate calculation, and used accurate assumptions for the control group.
Conclusions Sample size calculation is still inadequately reported, often erroneous, and based on assumptions that are frequently inaccurate. Such a situation raises questions about how sample size is calculated in randomised controlled trials.},
	pages = {b1732},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Charles, Pierre and Giraudeau, Bruno and Dechartres, Agnes and Baron, Gabriel and Ravaud, Philippe},
	urldate = {2022-04-25},
	date = {2009-05-12},
	langid = {english},
	pmid = {19435763},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/E5UD4XW2/Charles et al. - 2009 - Reporting of sample size calculation in randomised.pdf:application/pdf;Reporting of sample size calculation in randomised controlled trials\: review:/Users/tom/Zotero/storage/H6NUBX3X/charles2009.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/4IHTGCGR/bmj.html:text/html},
}

@article{jaswetz_no_2022,
	title = {No evidence for disruption of reconsolidation of conditioned threat memories with a cognitively demanding intervention},
	volume = {12},
	rights = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-10184-1},
	doi = {10.1038/s41598-022-10184-1},
	abstract = {Simultaneous execution of memory retrieval and cognitively demanding interventions alter the subjective experience of aversive memories. This principle can be used in treatment to target traumatic memories. An often-used interpretation is that cognitive demand interferes with memory reconsolidation. Laboratory models applying this technique often do not meet some important procedural steps thought necessary to trigger reconsolidation. It remains therefore unclear whether cognitively demanding interventions can alter the reconsolidation process of aversive memories. Here, 78 (41 included) healthy participants completed an established 3-day threat conditioning paradigm. Two conditioned stimuli were paired with a shock ({CS}+ s) and one was not ({CS}-). The next day, one {CS}+ ({CS}+ R), but not the other ({CS}+), was presented as a reminder. After 10 min, participants performed a 2-back working memory task. On day three, we assessed retention. We found successful acquisition of conditioned threat and retention ({CS}+ s {\textgreater} {CS}-). However, {SCRs} to the {CS}+ R and the {CS}+ during retention did not significantly differ. Although threat conditioning was successful, the well-established cognitively demanding intervention did not alter the reconsolidation process of conditioned threat memories. These findings challenge current views on how cognitively demand may enhance psychotherapy-outcome.},
	pages = {6663},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Jaswetz, Lars and de Voogd, Lycia D. and Becker, Eni S. and Roelofs, Karin},
	urldate = {2022-04-26},
	date = {2022-04-22},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Psychology, Cognitive neuroscience, Prefrontal cortex, Human behaviour, Amygdala},
	file = {Full Text PDF:/Users/tom/Zotero/storage/H25Z6HA5/Jaswetz et al. - 2022 - No evidence for disruption of reconsolidation of c.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UJHZP8BD/s41598-022-10184-1.html:text/html},
}

@article{parks_time_2022,
	title = {The time window of reconsolidation: A replication},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-022-02102-3},
	doi = {10.3758/s13423-022-02102-3},
	shorttitle = {The time window of reconsolidation},
	abstract = {Reconsolidation is a process by which a consolidated memory that has been destabilized by reactivation is updated, strengthened, or weakened by the restabilization of the trace. A critical assumption of the reconsolidation theory is that reconsolidation is a time-dependent process. Hupbach, Gomez, Hardt, and Nadel (2007, Learning \& Memory, 14, 47–53) conducted a set of experiments demonstrating that memory updating is only found when the reconsolidation process has time to complete. This finding strengthens reconsolidation theory and poses a challenge to other accounts of memory updating (e.g., context and interference accounts). Because this finding is so critical to the reconsolidation theory, we attempted to directly replicate these experiments, which showed memory updating in a 3-day paradigm (when reconsolidation has time to complete), but not in a 2-day paradigm (when reconsolidation does not have time to complete). We replicated these results, thereby bolstering the reconsolidation theory of memory updating.},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Parks, Colleen M. and Mohawk, Kevin D. and Werner, Laura L. S. and Kiley, Christopher},
	urldate = {2022-04-28},
	date = {2022-04-15},
	langid = {english},
	keywords = {Episodic memory, Reconsolidation, Memory updating},
	file = {Full Text PDF:/Users/tom/Zotero/storage/JY8N3QRL/Parks et al. - 2022 - The time window of reconsolidation A replication.pdf:application/pdf},
}

@article{dennis_privacy_2019,
	title = {Privacy versus open science},
	volume = {51},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-019-01259-5},
	doi = {10.3758/s13428-019-01259-5},
	abstract = {Pervasive internet and sensor technologies promise to revolutionize psychological science. However, the data collected using these technologies are often very personal—indeed, the value of the data is often directly related to how personal they are. At the same time, driven by the replication crisis, there is a sustained push to publish data to open repositories. These movements are in fundamental conflict. In this article, we propose a way to navigate this issue. We argue that there are significant advantages to be gained by ceding the ownership of data to the participants who generate the data. We then provide desiderata for a privacy-preserving platform. In particular, we suggest that researchers should use an interface to perform experiments and run analyses, rather than observing the stimuli themselves. We argue that this method not only improves privacy but will also encourage greater compliance with good research practices than is possible through open repositories.},
	pages = {1839--1848},
	number = {4},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Dennis, Simon and Garrett, Paul and Yim, Hyungwook and Hamm, Jihun and Osth, Adam F. and Sreekumar, Vishnu and Stone, Ben},
	urldate = {2022-04-29},
	date = {2019-08-01},
	langid = {english},
	keywords = {Open science, Privacy, Differential privacy, Open repositories},
	file = {Full Text PDF:/Users/tom/Zotero/storage/BANY326Q/Dennis et al. - 2019 - Privacy versus open science.pdf:application/pdf},
}

@article{wakefield_retracted_1998,
	title = {{RETRACTED}: Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children},
	volume = {351},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140673697110960/fulltext},
	doi = {10.1016/S0140-6736(97)11096-0},
	shorttitle = {{RETRACTED}},
	pages = {637--641},
	number = {9103},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Wakefield, A. J. and Murch, S. H. and Anthony, A. and Linnell, J. and Casson, D. M. and Malik, M. and Berelowitz, M. and Dhillon, A. P. and Thomson, M. A. and Harvey, P. and Valentine, A. and Davies, S. E. and Walker-Smith, J. A.},
	urldate = {2022-05-06},
	date = {1998-02-28},
	pmid = {9500320},
	note = {Publisher: Elsevier},
	file = {Full Text PDF:/Users/tom/Zotero/storage/I6CBWVK7/Wakefield et al. - 1998 - RETRACTED Ileal-lymphoid-nodular hyperplasia, non.pdf:application/pdf;RETRACTED\: Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children:/Users/tom/Zotero/storage/8C6VAQSP/wakefield1998.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/E2RHP4UD/fulltext.html:text/html},
}

@article{bottesini_what_nodate,
	title = {What do participants think of our research practices? An examination of behavioural psychology participants' preferences},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.200048},
	doi = {10.1098/rsos.200048},
	shorttitle = {What do participants think of our research practices?},
	abstract = {What research practices should be considered acceptable? Historically, scientists have set the standards for what constitutes acceptable research practices. However, there is value in considering non-scientists’ perspectives, including research participants'. 1873 participants from {MTurk} and university subject pools were surveyed after their participation in one of eight minimal-risk studies. We asked participants how they would feel if (mostly) common research practices were applied to their data: p-hacking/cherry-picking results, selective reporting of studies, Hypothesizing After Results are Known ({HARKing}), committing fraud, conducting direct replications, sharing data, sharing methods, and open access publishing. An overwhelming majority of psychology research participants think questionable research practices (e.g. p-hacking, {HARKing}) are unacceptable (68.3–81.3\%), and were supportive of practices to increase transparency and replicability (71.4–80.1\%). A surprising number of participants expressed positive or neutral views toward scientific fraud (18.7\%), raising concerns about data quality. We grapple with this concern and interpret our results in light of the limitations of our study. Despite the ambiguity in our results, we argue that there is evidence (from our study and others’) that researchers may be violating participants' expectations and should be transparent with participants about how their data will be used.},
	pages = {200048},
	number = {4},
	journaltitle = {Royal Society Open Science},
	author = {Bottesini, Julia G. and Rhemtulla, Mijke and Vazire, Simine},
	urldate = {2022-05-11},
	note = {Publisher: Royal Society},
	keywords = {open science, informed consent, research practices, scientific integrity},
	file = {Full Text PDF:/Users/tom/Zotero/storage/8B8IGGTA/Bottesini et al. - What do participants think of our research practic.pdf:application/pdf},
}

@report{cook_transparent_2022,
	title = {Transparent research practices in special education},
	url = {https://edarxiv.org/d8unh/},
	abstract = {Open practices such as preregistration, registered reports, open materials, open data, open analytic code, replication, open peer review, open access, and conflict-of-interest and funding statements support the transparency, accessibility, and reproducibility of research and other scholarship. The purpose of this systematic review was to examine the prevalence of these open practices in the special education literature. We reviewed a randomly selected sample of 250 articles published in special education journals in 2020. Results indicated that conflict-of-interest and funding statements were present in most articles; a small but meaningful proportion of articles provided open materials and were open access; and preregistration, registered reports, open data, open analytic code, open peer review, and replication were rarely or never observed. Recommendations for researching and supporting the use of open practices in special education scholarship are provided.},
	institution = {{EdArXiv}},
	author = {Cook, Bryan G. and Dijk, Wilhelmina van and Vargas, Isabel and Fleming, Jesse Irvan and {McDonald}, Sean D. and Richmond, Cassidi L. and Carlisle, Lindsay and {McLucas}, Alan Scott and Johnson, Rachelle M.},
	urldate = {2022-05-14},
	date = {2022-05-10},
	langid = {english},
	doi = {10.35542/osf.io/d8unh},
	note = {type: article},
	keywords = {open science, systematic review, Education, special education, Special Education and Teaching},
	file = {Full Text PDF:/Users/tom/Zotero/storage/Z3ASHBL4/Cook et al. - 2022 - Transparent Research Practices in Special Educatio.pdf:application/pdf},
}

@article{lopez-nicolas_meta-review_2022,
	title = {A meta-review of transparency and reproducibility-related reporting practices in published meta-analyses on clinical psychological interventions (2000–2020)},
	volume = {54},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-021-01644-z},
	doi = {10.3758/s13428-021-01644-z},
	abstract = {Meta-analysis is a powerful and important tool to synthesize the literature about a research topic. Like other kinds of research, meta-analyses must be reproducible to be compliant with the principles of the scientific method. Furthermore, reproducible meta-analyses can be easily updated with new data and reanalysed applying new and more refined analysis techniques. We attempted to empirically assess the prevalence of transparency and reproducibility-related reporting practices in published meta-analyses from clinical psychology by examining a random sample of 100 meta-analyses. Our purpose was to identify the key points that could be improved, with the aim of providing some recommendations for carrying out reproducible meta-analyses. We conducted a meta-review of meta-analyses of psychological interventions published between 2000 and 2020. We searched {PubMed}, {PsycInfo} and Web of Science databases. A structured coding form to assess transparency indicators was created based on previous studies and existing meta-analysis guidelines. We found major issues concerning: completely reproducible search procedures report, specification of the exact method to compute effect sizes, choice of weighting factors and estimators, lack of availability of the raw statistics used to compute the effect size and of interoperability of available data, and practically total absence of analysis script code sharing. Based on our findings, we conclude with recommendations intended to improve the transparency, openness, and reproducibility-related reporting practices of meta-analyses in clinical psychology and related areas.},
	pages = {334--349},
	number = {1},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {López-Nicolás, Rubén and López-López, José Antonio and Rubio-Aparicio, María and Sánchez-Meca, Julio},
	urldate = {2022-05-17},
	date = {2022-02-01},
	langid = {english},
	keywords = {Meta-science, Reproducibility, Meta-analysis, Data sharing, Transparency and openness practices},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ELFQU42Q/López-Nicolás et al. - 2022 - A meta-review of transparency and reproducibility-.pdf:application/pdf},
}

@article{derksen_kinds_2022,
	title = {Kinds of replication: examining the meanings of “conceptual replication” and “direct replication”},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/17456916211041116},
	doi = {10.1177/17456916211041116},
	shorttitle = {Kinds of replication},
	abstract = {Although psychology?s recent crisis has been attributed to various scientific practices, it has come to be called a ?replication crisis,? prompting extensive appraisals of this putatively crucial scientific practice. These have yielded disagreements over what kind of replication is to be preferred and what phenomena are being explored, yet the proposals are all grounded in a conventional philosophy of science. This article proposes another avenue that invites moving beyond a discovery metaphor of science to rethink research as enabling realities and to consider how empirical findings enact or perform a reality. An enactment perspective appreciates multiple, dynamic realities and science as producing different entities, enactments that ever encounter differences, uncertainties, and precariousness. The axioms of an enactment perspective are described and employed to more fully understand the two kinds of replication that predominate in the crisis disputes. Although the enactment perspective described here is a relatively recent development in philosophy of science and science studies, some of its core axioms are not new to psychology, and the article concludes by revisiting psychologists? previous calls to apprehend the dynamism of psychological reality to appreciate how scientific practices actively and unavoidably participate in performativity of reality.},
	pages = {17456916211041116},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Derksen, Maarten and Morawski, Jill},
	urldate = {2022-05-18},
	date = {2022-03-04},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Full Text:/Users/tom/Zotero/storage/F6EWHBJ2/Derksen and Morawski - 2022 - Kinds of Replication Examining the Meanings of “C.pdf:application/pdf},
}

@article{shamseer_preferred_2015,
	title = {Preferred reporting items for systematic review and meta-analysis protocols ({PRISMA}-P) 2015: elaboration and explanation},
	volume = {349},
	rights = {© Shamseer et al 2015.      This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See:   http://creativecommons.org/licenses/by-nc/4.0/},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/349/bmj.g7647},
	doi = {10.1136/bmj.g7647},
	shorttitle = {Preferred reporting items for systematic review and meta-analysis protocols ({PRISMA}-P) 2015},
	abstract = {Protocols of systematic reviews and meta-analyses allow for planning and documentation of review methods, act as a guard against arbitrary decision making during review conduct, enable readers to assess for the presence of selective reporting against completed reviews, and, when made publicly available, reduce duplication of efforts and potentially prompt collaboration. Evidence documenting the existence of selective reporting and excessive duplication of reviews on the same or similar topics is accumulating and many calls have been made in support of the documentation and public availability of review protocols. Several efforts have emerged in recent years to rectify these problems, including development of an international register for prospective reviews ({PROSPERO}) and launch of the first open access journal dedicated to the exclusive publication of systematic review products, including protocols ({BioMed} Central’s Systematic Reviews). Furthering these efforts and building on the {PRISMA} (Preferred Reporting Items for Systematic Reviews and Meta-analyses) guidelines, an international group of experts has created a guideline to improve the transparency, accuracy, completeness, and frequency of documented systematic review and meta-analysis protocols—{PRISMA}-P (for protocols) 2015. The {PRISMA}-P checklist contains 17 items considered to be essential and minimum components of a systematic review or meta-analysis protocol.
This {PRISMA}-P 2015 Explanation and Elaboration paper provides readers with a full understanding of and evidence about the necessity of each item as well as a model example from an existing published protocol. This paper should be read together with the {PRISMA}-P 2015 statement. Systematic review authors and assessors are strongly encouraged to make use of {PRISMA}-P when drafting and appraising review protocols.},
	pages = {g7647},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Shamseer, Larissa and Moher, David and Clarke, Mike and Ghersi, Davina and Liberati, Alessandro and Petticrew, Mark and Shekelle, Paul and Stewart, Lesley A.},
	urldate = {2022-05-19},
	date = {2015-01-02},
	langid = {english},
	pmid = {25555855},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/Q5BVWU36/Shamseer et al. - 2015 - Preferred reporting items for systematic review an.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/YCXTU3RT/bmj.html:text/html},
}

@article{whiting_robis_2016,
	title = {{ROBIS}: A new tool to assess risk of bias in systematic reviews was developed},
	volume = {69},
	issn = {0895-4356},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4687950/},
	doi = {10.1016/j.jclinepi.2015.06.005},
	shorttitle = {{ROBIS}},
	abstract = {Objective
To develop {ROBIS}, a new tool for assessing the risk of bias in systematic reviews (rather than in primary studies).

Study Design and Setting
We used four-stage approach to develop {ROBIS}: define the scope, review the evidence base, hold a face-to-face meeting, and refine the tool through piloting.

Results
{ROBIS} is currently aimed at four broad categories of reviews mainly within health care settings: interventions, diagnosis, prognosis, and etiology. The target audience of {ROBIS} is primarily guideline developers, authors of overviews of systematic reviews (“reviews of reviews”), and review authors who might want to assess or avoid risk of bias in their reviews. The tool is completed in three phases: (1) assess relevance (optional), (2) identify concerns with the review process, and (3) judge risk of bias. Phase 2 covers four domains through which bias may be introduced into a systematic review: study eligibility criteria; identification and selection of studies; data collection and study appraisal; and synthesis and findings. Phase 3 assesses the overall risk of bias in the interpretation of review findings and whether this considered limitations identified in any of the phase 2 domains. Signaling questions are included to help judge concerns with the review process (phase 2) and the overall risk of bias in the review (phase 3); these questions flag aspects of review design related to the potential for bias and aim to help assessors judge risk of bias in the review process, results, and conclusions.

Conclusions
{ROBIS} is the first rigorously developed tool designed specifically to assess the risk of bias in systematic reviews.},
	pages = {225--234},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {J Clin Epidemiol},
	author = {Whiting, Penny and Savović, Jelena and Higgins, Julian P.T. and Caldwell, Deborah M. and Reeves, Barnaby C. and Shea, Beverley and Davies, Philippa and Kleijnen, Jos and Churchill, Rachel},
	urldate = {2022-05-19},
	date = {2016-01},
	pmid = {26092286},
	pmcid = {PMC4687950},
	file = {Full Text:/Users/tom/Zotero/storage/C9L6M4YC/Whiting et al. - 2016 - ROBIS A new tool to assess risk of bias in system.pdf:application/pdf},
}

@article{erdfelder_detecting_2019,
	title = {Detecting evidential value and p-hacking with the p-curve tool},
	volume = {227},
	issn = {2190-8370},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000383},
	doi = {10.1027/2151-2604/a000383},
	abstract = {.Simonsohn, Nelson, and Simmons (2014a) proposed p-curve – the distribution of statistically significant p-values for a set of studies – as a tool to assess the evidential value of these studies. They argued that, whereas right-skewed p-curves indicate true underlying effects, left-skewed p-curves indicate selective reporting of significant results when there is no true effect (“p-hacking”). We first review previous research showing that, in contrast to the first claim, null effects may produce right-skewed p-curves under some conditions. We then question the second claim by showing that not only selective reporting but also selective nonreporting of significant results due to a significant outcome of a more popular alternative test of the same hypothesis may produce left-skewed p-curves, even if all studies reflect true effects. Hence, just as right-skewed p-curves do not necessarily imply evidential value, left-skewed p-curves do not necessarily imply p-hacking and absence of true effects in the studies involved.},
	pages = {249--260},
	number = {4},
	journaltitle = {Zeitschrift für Psychologie},
	author = {Erdfelder, Edgar and Heck, Daniel W.},
	urldate = {2022-05-20},
	date = {2019-10},
	note = {Publisher: Hogrefe Publishing},
	keywords = {p-hacking, publication bias, {ANCOVA}, false-positive results, p-curve},
	file = {Full Text PDF:/Users/tom/Zotero/storage/32E85FUD/Erdfelder and Heck - 2019 - Detecting Evidential Value and p-Hacking With the .pdf:application/pdf},
}

@article{smaldino_how_2020,
	title = {How to translate a verbal theory into a formal model},
	volume = {51},
	issn = {1864-9335},
	url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000425},
	doi = {10.1027/1864-9335/a000425},
	abstract = {. Turning verbal theories into formal models is an essential business of a mature science. Here I elaborate on taxonomies of models, provide ten lessons for translating a verbal theory into a formal model, and discuss the specific challenges involved in collaborations between modelers and non-modelers. It’s a start.},
	pages = {207--218},
	number = {4},
	journaltitle = {Social Psychology},
	author = {Smaldino, Paul E.},
	urldate = {2022-05-20},
	date = {2020-07},
	note = {Publisher: Hogrefe Publishing},
	keywords = {theory, agent-based models, flatten the curve, formal models, {SIR}},
	file = {Full Text PDF:/Users/tom/Zotero/storage/5PT3L4XQ/Smaldino - 2020 - How to Translate a Verbal Theory Into a Formal Mod.pdf:application/pdf},
}

@article{renkewitz_how_2019,
	title = {How to detect publication bias in psychological research},
	volume = {227},
	issn = {2190-8370},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000386},
	doi = {10.1027/2151-2604/a000386},
	abstract = {. Publication biases and questionable research practices are assumed to be two of the main causes of low replication rates. Both of these problems lead to severely inflated effect size estimates in meta-analyses. Methodologists have proposed a number of statistical tools to detect such bias in meta-analytic results. We present an evaluation of the performance of six of these tools. To assess the Type I error rate and the statistical power of these methods, we simulated a large variety of literatures that differed with regard to true effect size, heterogeneity, number of available primary studies, and sample sizes of these primary studies; furthermore, simulated studies were subjected to different degrees of publication bias. Our results show that across all simulated conditions, no method consistently outperformed the others. Additionally, all methods performed poorly when true effect sizes were heterogeneous or primary studies had a small chance of being published, irrespective of their results. This suggests that in many actual meta-analyses in psychology, bias will remain undiscovered no matter which detection method is used.},
	pages = {261--279},
	number = {4},
	journaltitle = {Zeitschrift für Psychologie},
	author = {Renkewitz, Frank and Keiner, Melanie},
	urldate = {2022-05-20},
	date = {2019-10},
	note = {Publisher: Hogrefe Publishing},
	keywords = {publication bias, meta-analysis, heterogeneity, bias detection, optional stopping},
	file = {Full Text PDF:/Users/tom/Zotero/storage/BVE3S7QV/Renkewitz and Keiner - 2019 - How to Detect Publication Bias in Psychological Re.pdf:application/pdf},
}

@article{steiner_causal_2019,
	title = {A causal replication framework for designing and assessing replication efforts},
	volume = {227},
	issn = {2190-8370},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000385},
	doi = {10.1027/2151-2604/a000385},
	abstract = {. Replication has long been a cornerstone for establishing trustworthy scientific results, but there remains considerable disagreement about what constitutes a replication, how results from these studies should be interpreted, and whether direct replication of results is even possible. This article addresses these concerns by presenting the methodological foundations for a replication science. It provides an introduction to the causal replication framework, which defines “replication” as a research design that tests whether two (or more) studies produce the same causal effect within the limits of sampling error. The framework formalizes the conditions under which replication success can be expected, and allows for the causal interpretation of replication failures. Through two applied examples, the article demonstrates how the causal replication framework may be utilized to plan prospective replication designs, as well as to interpret results from existing replication efforts.},
	pages = {280--292},
	number = {4},
	journaltitle = {Zeitschrift für Psychologie},
	author = {Steiner, Peter M. and Wong, Vivian C. and Anglin, Kylie},
	urldate = {2022-05-20},
	date = {2019-10},
	note = {Publisher: Hogrefe Publishing},
	keywords = {replication crisis, causal inference, causal replication framework, potential outcomes, replication assumptions},
	file = {Full Text PDF:/Users/tom/Zotero/storage/E5ZQPPW3/Steiner et al. - 2019 - A Causal Replication Framework for Designing and A.pdf:application/pdf},
}

@article{kastner_what_2013,
	title = {What do letters to the editor publish about randomized controlled trials? A cross-sectional study},
	volume = {6},
	issn = {1756-0500},
	url = {https://doi.org/10.1186/1756-0500-6-414},
	doi = {10.1186/1756-0500-6-414},
	shorttitle = {What do letters to the editor publish about randomized controlled trials?},
	abstract = {To identify published letters to the editor ({LTE}) written in response to randomized controlled trials ({RCTs}), determine the topics addressed in the letters, and to examine if these topics were affected by the characteristics and results of the {RCTs}.},
	pages = {414},
	number = {1},
	journaltitle = {{BMC} Research Notes},
	shortjournal = {{BMC} Res Notes},
	author = {Kastner, Monika and Menon, Anita and Straus, Sharon E. and Laupacis, Andreas},
	urldate = {2022-05-24},
	date = {2013-10-14},
	langid = {english},
	keywords = {Randomized controlled trials, Journalogy, Letters to the editor},
	file = {Full Text PDF:/Users/tom/Zotero/storage/H36RI77H/Kastner et al. - 2013 - What do letters to the editor publish about random.pdf:application/pdf},
}

@report{smaldino_interdisciplinarity_2020,
	title = {Interdisciplinarity can aid the spread of better methods between scientific communities},
	url = {https://osf.io/preprints/metaarxiv/cm5v3/},
	abstract = {Why do bad methods persist in some academic disciplines, even when they have been clearly rejected in others?  What factors allow good methodological advances to spread across disciplines?  In this paper, we investigate some key features determining the success and failure of methodological spread between the sciences.  We introduce a formal model that considers factors like methodological competence and reviewer bias towards one's own methods.  We show how self-preferential biases can protect poor methodology within scientific communities, and lack of reviewer competence can contribute to failures to adopt better methods. We then use a second model to further argue that input from outside disciplines, especially in the form of peer review and other credit assignment mechanisms, can help break down barriers to methodological improvement. This work therefore presents an underappreciated benefit of interdisciplinarity.},
	institution = {{MetaArXiv}},
	author = {Smaldino, Paul E. and O'Connor, Cailin},
	urldate = {2022-05-25},
	date = {2020-11-05},
	langid = {english},
	doi = {10.31222/osf.io/cm5v3},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, metascience, Other Social and Behavioral Sciences, agent-based, cultural evolution, interdisciplinary, model, philosophy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ZF22CJGE/Smaldino and O'Connor - 2020 - Interdisciplinarity Can Aid the Spread of Better M.pdf:application/pdf},
}

@report{tay_reminder-dependent_2022,
	title = {Reminder-dependent alterations in long-term declarative memory expression},
	url = {https://psyarxiv.com/zgkxt/},
	abstract = {The reminder of a previously-learned memory can render that memory vulnerable to disruption or change in expression. Such memory alterations have been viewed as supportive of the framework of memory reconsolidation. However, alternative interpretations and inconsistencies in the replication of fundamental findings have raised questions particularly in the domain of human declarative memory. Here we present a series of related experiments, all of which involve the learning of a declarative memory, followed 1-2 days later by memory reminder. Post-reminder learning of interfering material did result in modulation of subsequent recall at test, but the precise manifestation of that interference effect differed across experiments. With post-reminder performance of a visuospatial task, a quantitative impairment in test recall performance was observed within a visual list-learning paradigm, but not in a foreign vocabulary learning paradigm. These results support the existence of reminder-induced memory processes that can lead to the alteration of subsequent memory performance by interfering tasks. However, it remains unclear whether these effects are reflective of modulation or impairment of the putative memory reconsolidation process.},
	institution = {{PsyArXiv}},
	author = {Tay, Kai Rong and Bolt, Francesca and Wong, Hei Ting and Vasileva, Svetlina and Lee, Jonathan},
	urldate = {2022-05-26},
	date = {2022-05-25},
	langid = {english},
	doi = {10.31234/osf.io/zgkxt},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Cognitive Psychology, Memory},
	file = {Full Text PDF:/Users/tom/Zotero/storage/82LHMYM4/Tay et al. - 2022 - Reminder-dependent alterations in long-term declar.pdf:application/pdf},
}

@article{klingmuller_intrusions_2017,
	title = {Intrusions in episodic memory: reconsolidation or interference?},
	volume = {24},
	issn = {1072-0502},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5397684/},
	doi = {10.1101/lm.045047.117},
	shorttitle = {Intrusions in episodic memory},
	abstract = {It would be profoundly important if reconsolidation research in animals and other memory domains generalized to human episodic memory. A 3-d-list-discrimination procedure, based on free recall of objects, with a contextual reminder cue (the testing room), has been thought to demonstrate reconsolidation of human episodic memory (as noted in a previous study). Our goal was to replicate the central result, a high intrusion rate during recall of the target list, and evaluate the reconsolidation account relative to an alternative account, based on state-dependent learning and interference. First, replication was not straightforward (Experiment 1). Second, using a very unique, highly salient context (Experiment 2), the method produced a qualitative replication, but it was small in magnitude. A critical assumption of the reconsolidation account, that the target list is reactivated and destabilized during re-exposure to the study context, was not supported (Experiment 3). Although troubling for the reconsolidation account, the findings can be easily accommodated by an alternative account that does not assume additional neurobiological processes underlying the destabilization of consolidated memories, instead explaining intrusion rates simply in terms of well-established cognitive effects, such as item-to-context binding and interference during retrieval.},
	pages = {216--224},
	number = {5},
	journaltitle = {Learning \& Memory},
	shortjournal = {Learn Mem},
	author = {Klingmüller, Angela and Caplan, Jeremy B. and Sommer, Tobias},
	urldate = {2022-05-26},
	date = {2017-05},
	pmid = {28416633},
	pmcid = {PMC5397684},
	file = {Full Text:/Users/tom/Zotero/storage/VFHHNXQG/Klingmüller et al. - 2017 - Intrusions in episodic memory reconsolidation or .pdf:application/pdf;Intrusions in episodic memory\: reconsolidation or interference?:/Users/tom/Zotero/storage/A6WVUDZ5/10.1101@lm.045047.117.pdf.pdf:application/pdf},
}

@article{maier_robust_2022,
	title = {Robust Bayesian meta-analysis: Addressing publication bias with model-averaging.},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000405},
	doi = {10.1037/met0000405},
	shorttitle = {Robust Bayesian meta-analysis},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Maier, Maximilian and Bartoš, František and Wagenmakers, Eric-Jan},
	urldate = {2022-05-27},
	date = {2022-05-19},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/CJ6NUWD8/Maier et al. - 2022 - Robust Bayesian meta-analysis Addressing publicat.pdf:application/pdf},
}

@article{bhopal_role_1994,
	title = {The role of letters in reviewing research},
	volume = {308},
	rights = {© 1994 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/308/6944/1582},
	doi = {10.1136/bmj.308.6944.1582},
	abstract = {The publication of medical research is prone to error. Misuse of statistics, selective citation of published work, misquotation of references, and overenthusiasm in the search for positive findings are crimes, often committed unwittingly, that can escape peer review before a paper is published.1 Only after publication can a piece of research be exposed to the sort of critical review, by journal readers, that can either establish its place or consign it to the dustbin. The potential of correspondence as a form of peer review is supported by editors2 - the {BMJ} reserves its letters pages almost exclusively for comment on published material - but it remains underdeveloped and undervalued by clinicians, academics, teachers, and many journals.

In four specialist journals examined by Spodick and Goldberg only 2\% of the space was devoted to letters from readers.3 Four general journals gave 15\% of the space to letters, but fewer than half referred to original papers. Dr Steven Spiro, editor of Thorax, says he receives only about four letters an issue (the journal received only 37 in 1993). He encourages letters referring to original research and publishes most that come in, usually with a response from the authors. Last year the Annals of Rheumatic Diseases, a monthly journal, received only 84 letters and the British Journal of Ophthalmology, also a monthly, received only 37. …},
	pages = {1582--1583},
	number = {6944},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Bhopal, Raj S. and Tonks, Alison},
	urldate = {2022-05-27},
	date = {1994-06-18},
	langid = {english},
	pmid = {8025421},
	note = {Publisher: British Medical Journal Publishing Group
Section: Editorial},
	file = {Full Text:/Users/tom/Zotero/storage/QV5HMGI4/Bhopal and Tonks - 1994 - The role of letters in reviewing research.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5BWMPQS9/1582.html:text/html},
}

@article{thomasson_uncritical_1955,
	title = {Uncritical citation of criticized data},
	volume = {121},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.121.3147.610},
	doi = {10.1126/science.121.3147.610},
	pages = {610--611},
	number = {3147},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Thomasson, Peggy and Stanley, Julian C.},
	urldate = {2022-05-27},
	date = {1955-04-22},
	langid = {english},
	file = {Thomasson and Stanley - 1955 - Uncritical citation of criticized data.pdf:/Users/tom/Zotero/storage/J89MGXJK/Thomasson and Stanley - 1955 - Uncritical citation of criticized data.pdf:application/pdf},
}

@article{heycke_screen_2019,
	title = {Screen recordings as a tool to document computer assisted data collection procedures},
	volume = {59},
	issn = {0033-2879},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6659756/},
	doi = {10.5334/pb.490},
	abstract = {Recently in psychological science and many related fields, a surprisingly large amount of experiments could not be replicated by independent researchers. A non-replication could indicate that a previous finding might have been a false positive statistical result and the effect does not exist. However, it could also mean that a specific detail of the experimental procedure is essential for the effect to emerge, which might not have been included in the replication attempt. Therefore any replication attempt that does not replicate the original effect is most informative when the original procedure is closely adhered to. One proposed solution to facilitate the empirical reproducibility of the experimental procedures in psychology is to upload the experimental script and materials to a public repository. However, we believe that merely providing the materials of an experimental procedure is not sufficient, as many software solutions are not freely available, software solutions might change, and it is time consuming to set up the procedure. We argue that there is a simple solution to these problems when an experiment is conducted using computers: recording an example procedure with a screen capture software and providing the video in an online repository. We therefore provide a brief tutorial on screen recordings using an open source screen recording software. With this information, individual researchers should be able to record their experimental procedures and we hope to facilitate the use of screen recordings in computer assisted data collection procedures.},
	pages = {269--280},
	number = {1},
	journaltitle = {Psychologica Belgica},
	shortjournal = {Psychol Belg},
	author = {Heycke, Tobias and Spitzer, Lisa},
	urldate = {2022-06-03},
	date = {2019},
	pmid = {31367457},
	pmcid = {PMC6659756},
	file = {Full Text:/Users/tom/Zotero/storage/7P7TRUYL/Heycke and Spitzer - Screen Recordings as a Tool to Document Computer A.pdf:application/pdf},
}

@article{bergeat_data_2022,
	title = {Data sharing and reanalyses among randomized clinical trials published in surgical journals before and after adoption of a data availability and reproducibility policy},
	volume = {5},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2022.15209},
	doi = {10.1001/jamanetworkopen.2022.15209},
	abstract = {Clinical trial data sharing holds promise for maximizing the value of clinical research. The International Committee of Medical Journal Editors ({ICMJE}) adopted a policy promoting data sharing in July 2018.To evaluate the association of the {ICMJE} data sharing policy with data availability and reproducibility of main conclusions among leading surgical journals.This cross-sectional study, conducted in October 2021, examined randomized clinical trials ({RCTs}) in 10 leading surgical journals before and after the implementation of the {ICMJE} data sharing policy in July 2018.Implementation of the {ICMJE} data sharing policy.To demonstrate a pre-post increase in data availability from 5\% to 25\% (α = .05; β = 0.1), 65 {RCTs} published before and 65 {RCTs} published after the policy was issued were included, and their data were requested. The primary outcome was data availability (ie, the receipt of sufficient data to enable reanalysis of the primary outcome). When data sharing was available, the primary outcomes reported in the journal articles were reanalyzed to explore reproducibility. The reproducibility features of these studies were detailed.Data were available for 2 of 65 {RCTs} (3.1\%) published before the {ICMJE} policy and for 2 of 65 {RCTs} (3.1\%) published after the policy was issued (odds ratio, 1.00; 95\% {CI}, 0.07-14.19; P \&gt; .99). A data sharing statement was observed in 11 of 65 {RCTs} (16.9\%) published after the policy vs none before the policy (risk ratio, 2.20; 95\% {CI}, 1.81-2.68; P = .001). Data obtained for reanalysis (n = 4) were not from {RCTs} published with a data sharing statement. Of the 4 {RCTs} with available data, all of them had primary outcomes that were fully reproduced. However, discrepancies or inaccuracies that were not associated with study conclusions were identified in 3 {RCTs}. These concerned the number of patients included in 1 {RCT}, the management of missing values in another {RCT}, and discrepant timing for the principal outcome declared in the study registration and reported in the third {RCT}.This cross-sectional study suggests that data sharing practices are rare in surgical journals despite the {ICMJE} policy and that most {RCTs} published in these journals lack transparency. The results of these studies may not be reproducible by external researchers.},
	pages = {e2215209},
	number = {6},
	journaltitle = {{JAMA} Network Open},
	shortjournal = {{JAMA} Network Open},
	author = {Bergeat, Damien and Lombard, Nicolas and Gasmi, Anis and Le Floch, Bastien and Naudet, Florian},
	urldate = {2022-06-03},
	date = {2022-06-02},
	file = {Snapshot:/Users/tom/Zotero/storage/FQARISVB/2792858.html:text/html},
}

@article{barnett_growth_2020,
	title = {The growth of acronyms in the scientific literature},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.60080},
	doi = {10.7554/eLife.60080},
	abstract = {Some acronyms are useful and are widely understood, but many of the acronyms used in scientific papers hinder understanding and contribute to the increasing fragmentation of science. Here we report the results of an analysis of more than 24 million article titles and 18 million article abstracts published between 1950 and 2019. There was at least one acronym in 19\% of the titles and 73\% of the abstracts. Acronym use has also increased over time, but the re-use of acronyms has declined. We found that from more than one million unique acronyms in our data, just over 2,000 (0.2\%) were used regularly, and most acronyms (79\%) appeared fewer than 10 times. Acronyms are not the biggest current problem in science communication, but reducing their use is a simple change that would help readers and potentially increase the value of science.},
	pages = {e60080},
	journaltitle = {{eLife}},
	author = {Barnett, Adrian and Doubleday, Zoe},
	editor = {Rodgers, Peter},
	urldate = {2022-06-05},
	date = {2020-07-23},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {meta-research, scientific publishing, acronyms, communication, knowledge, scientific writing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/U36M4DYJ/Barnett and Doubleday - 2020 - The growth of acronyms in the scientific literatur.pdf:application/pdf},
}

@article{page_data_2022,
	title = {Data and code availability statements in systematic reviews of interventions were often missing or inaccurate: a content analysis},
	volume = {147},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(22)00064-6/fulltext},
	doi = {10.1016/j.jclinepi.2022.03.003},
	shorttitle = {Data and code availability statements in systematic reviews of interventions were often missing or inaccurate},
	pages = {1--10},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Page, Matthew J. and Nguyen, Phi-Yen and Hamilton, Daniel G. and Haddaway, Neal R. and Kanukula, Raju and Moher, David and {McKenzie}, Joanne E.},
	urldate = {2022-06-05},
	date = {2022-07-01},
	pmid = {35278609},
	note = {Publisher: Elsevier},
	keywords = {Open data, Open science, Research integrity, Evidence synthesis, Open synthesis, Reproducibility of research},
	file = {Snapshot:/Users/tom/Zotero/storage/27G48B3J/fulltext.html:text/html},
}

@article{hassell_aggregation_1974,
	title = {Aggregation of predators and insect parasites and its effect on stability},
	volume = {43},
	issn = {0021-8790},
	url = {https://www.jstor.org/stable/3384},
	doi = {10.2307/3384},
	abstract = {(1) Some examples are illustrated where predators have been shown to concentrate their searching activities in regions of high prey density. (2) A model, based on the changes in turning behaviour seen in many predators after feeding (or parasites after parasitism), is developed to show the expected form of aggregative responses resulting from this behaviour alone. The responses tend to be sigmoid in form when expressed as the time spent by a predator in unit areas of different prey density. (3) A further model is discussed in which predators remain in a unit area of prey provided that a prey is encountered within a critical threshold time. This model also generates responses which are more-or-less sigmoid in form. (4) On the basis of these examples (from observation, experiment and models) a general aggregative response may be characterized by three components: an upper plateau where approximately the same (maximum) time is spent in unit areas of relatively high prey densities; similarly, a lower plateau at relatively low prey densities where a constant minimum time is spent in each area and, thirdly, a transitional region where there is a marked increase in time spent per unit area as prey density increases. (5) The stability properties of models including different types of aggregative response are discussed. These responses vary from simple linear relationships between time spent per unit area and prey density, to more complex responses containing the three components of a general response listed in (4) above. In the first place, the prey are assumed to have an effective rate of increase of approximately unity and to have a strongly clumped distribution (k of negative binomial {\textless}{\textless}1). These constraints enable a full analytic treatment to be presented. However, the basic findings are also shown to apply when the prey are not so strongly clumped (k = 1) and for the general case where prey reproductive rate can taken any value. (6) Stability is increased when (a) there is a marked difference between the maximum and minimum time spent per unit area, (b) the average prey density falls in the neighbourhood of the transition region where the response to prey density is most marked, (c) the time spent travelling between unit areas of prey is high, (d) the effective prey reproductive rate is reduced and (e) the prey distribution becomes more clumped.},
	pages = {567--594},
	number = {2},
	journaltitle = {Journal of Animal Ecology},
	author = {Hassell, M. P. and May, R. M.},
	urldate = {2022-06-05},
	date = {1974},
	note = {Publisher: [Wiley, British Ecological Society]},
	file = {Aggregation of Predators and Insect Parasites and its Effect on Stability:/Users/tom/Zotero/storage/9YNACWRK/hassell1974.pdf.pdf:application/pdf},
}

@incollection{macleod_stroop_2005,
	location = {Washington, {DC}, {US}},
	title = {The stroop task: Indirectly measuring concept activation},
	isbn = {978-1-59147-185-1},
	shorttitle = {The stroop task},
	abstract = {Introduces the next two chapters, in which the authors will describe the cognitive and clinical perspectives on the Stroop task, with emphasis on the methods used to carry out these studies. The classic color-word Stroop task has more recently been adapted to incorporate noncolor words in place of the traditional color words. This modification has come to be called the emotional Stroop task because the response to the critical noncolor words, as in the suicide illustration just described, is seen as at least partly having to do with affect. Since the 1980s, there has been a rapidly increasing number of studies in clinical psychology in which this emotional Stroop task has been used to explore aspects of anxieties and clinical disorders. This variation on the task has been useful in diagnosing problems and even in evaluating the efficacy of treatments. ({PsycINFO} Database Record (c) 2017 {APA}, all rights reserved)},
	pages = {13--16},
	booktitle = {Cognitive methods and their application to clinical research},
	publisher = {American Psychological Association},
	author = {{MacLeod}, Colin and {MacLeod}, Colin M.},
	date = {2005},
	doi = {10.1037/10870-001},
	keywords = {Clinical Psychology, Cognitive Psychology, Stroop Color Word Test, Stroop Effect},
	file = {MacLeod and MacLeod - 2005 - The stroop task Indirectly measuring concept acti.pdf:/Users/tom/Zotero/storage/QX32AQXA/MacLeod and MacLeod - 2005 - The stroop task Indirectly measuring concept acti.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/XG8IS6HA/2004-19025-001.html:text/html},
}

@article{young_alternative_1992,
	title = {Alternative Outcomes of Natural and Experimental High Pollen Loads},
	volume = {73},
	issn = {1939-9170},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/1940770},
	doi = {10.2307/1940770},
	abstract = {Seed production is usually assumed to be a positive monotonic function of pollen deposition and/or pollinator visitation. If this assumption were correct, there would be only two outcomes of excess pollen levels: an increase in fruit or seed set, or no increase. However, a substantial minority of the studies reviewed here has found that seed production declines with increased pollen loads, both under experimental and natural conditions. To explain this decrease, we propose the following mechanisms: pollen tube crowding, pollen removal or stigma damage by pollen thieves or pollinators, stigma damage during hand—pollination, application of low—diversity or local pollen, effects of bagging flowers, missed stigma receptivity, and the application of inviable pollen. These mechanisms can be distinguished through more complete and more careful experimental designs and incremental pollen supplementation.},
	pages = {639--647},
	number = {2},
	journaltitle = {Ecology},
	author = {Young, Helen J. and Young, Truman P.},
	urldate = {2022-06-05},
	date = {1992},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2307/1940770},
	file = {Alternative Outcomes of Natural and Experimental High Pollen Loads:/Users/tom/Zotero/storage/PN775PMZ/young1992.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SSCYQMDT/1940770.html:text/html},
}

@article{kupfer_forest_2004,
	title = {Forest fragmentation affects early successional patterns on shifting cultivation fields near Indian Church, Belize},
	volume = {103},
	issn = {0167-8809},
	url = {https://www.sciencedirect.com/science/article/pii/S0167880903004304},
	doi = {10.1016/j.agee.2003.11.011},
	abstract = {This study examines whether vegetation and soils sampled in slash-and-burn fields in an agricultural landscape near Indian Church, Belize varied as a function of cropping status (in use, 1–2 year fallow, 3–10 year fallow) and distance to older forest (adjacent to or {\textgreater}150m from forest). Multi-response permutation procedures ({MRPPs}) indicated that species composition (woody species density, herbaceous species cover) differed significantly among all treatments (woody: P=0.006; herbaceous: P=0.0001) and between distance classes (woody: P=0.03; herbaceous: P=0.002). The frequency of herbaceous life forms also differed between distance classes ({MANOVA}: P=0.04), with lianas (P=0.03) and legumes (P=0.008) being more common at greater distances from forest. Soil macronutrients (P, K) were significantly lower in long-term fallows than in use fields due to sequestration by the regenerating vegetation but ammonium nitrogen was significantly lower far from older forests (two-way {ANOVA}: P{\textless}0.05). Despite these differences in vegetation and soil characteristics, there were no significant differences in species diversity (richness, evenness) or vegetation structure (woody density, frequency, basal area) as a function of distance to forest (two-way {ANOVA}: P{\textgreater}0.05), most likely due to seed inputs from surrounding early successional habitats. This study indicates that changes in landscape structure may influence ecological processes such as succession in fragmented tropical landscapes and underscores the need to reject a conceptualization of the landscape matrix as a featureless, inhospitable habitat in favor of one that recognizes and incorporates the influence of matrix quality and heterogeneity.},
	pages = {509--518},
	number = {3},
	journaltitle = {Agriculture, Ecosystems \& Environment},
	shortjournal = {Agriculture, Ecosystems \& Environment},
	author = {Kupfer, John A and Webbeking, Amy L and Franklin, Scott B},
	urldate = {2022-06-05},
	date = {2004-08-01},
	langid = {english},
	keywords = {Dry tropical forest, Landscape ecology, Landscape structure, Slash-and-burn, Succession},
	file = {Forest fragmentation affects early successional patterns on shifting cultivation fields near Indian Church, Belize:/Users/tom/Zotero/storage/K7QQMR55/kupfer2004.pdf.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/AVRWLM3F/S0167880903004304.html:text/html},
}

@article{lakens_equivalence_2018,
	title = {Equivalence testing for psychological research: a tutorial},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918770963},
	doi = {10.1177/2515245918770963},
	shorttitle = {Equivalence testing for psychological research},
	abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests ({TOST}) procedure to test for equivalence and reject the presence of a smallest effect size of interest ({SESOI}). The {TOST} procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the {SESOI} exists. We explain a range of approaches to determine the {SESOI} in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
	pages = {259--269},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Lakens, Daniël and Scheel, Anne M. and Isager, Peder M.},
	urldate = {2022-06-05},
	date = {2018-06-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Equivalence Testing for Psychological Research\: A Tutorial:/Users/tom/Zotero/storage/GXX6CXYP/lakens2018.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/QHV95FBF/Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf:application/pdf},
}

@article{hetherington_two-_1975,
	title = {Two-, three-, and four-atom exchange effects in bcc \${\textasciicircum}\{3\}{\textbackslash}mathrm\{he\}\$},
	volume = {35},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1442},
	doi = {10.1103/PhysRevLett.35.1442},
	abstract = {We have made mean-field calculations with a Hamiltonian obtained from two-, three-, and four-atom exchange in bcc solid 3He. We are able to fit the high-temperature experiments as well as the phase diagram of Kummer et al. at low temperatures. We find two kinds of antiferromagnetic phases as suggested by Kummer's experiments.},
	pages = {1442--1444},
	number = {21},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Hetherington, J. H. and Willard, F. D. C.},
	urldate = {2022-06-05},
	date = {1975-11-24},
	note = {Publisher: American Physical Society},
	file = {APS Snapshot:/Users/tom/Zotero/storage/FSXAX5NY/PhysRevLett.35.html:text/html;Full Text PDF:/Users/tom/Zotero/storage/2BWUXMI6/Hetherington and Willard - 1975 - Two-, Three-, and Four-Atom Exchange Effects in bc.pdf:application/pdf;Two-, Three-, and Four-Atom Exchange Effects in bcc \$^ 3 \\mathrm He \$:/Users/tom/Zotero/storage/AC7VFVTP/hetherington1975.pdf.pdf:application/pdf},
}

@article{gabelica_many_2022,
	title = {Many researchers were not compliant with their published data sharing statement: mixed-methods study},
	volume = {0},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(22)00141-X/abstract},
	doi = {10.1016/j.jclinepi.2022.05.019},
	shorttitle = {Many researchers were not compliant with their published data sharing statement},
	abstract = {{\textless}h2{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Objectives{\textless}/h3{\textgreater}{\textless}p{\textgreater}To analyse researchers' compliance with their Data Availability Statement ({DAS}) from manuscripts published in open access journals with the mandatory {DAS}.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Study Design and Setting{\textless}/h3{\textgreater}{\textless}p{\textgreater}We analyzed all articles from 333 open-access journals published during January 2019 by {BioMed} Central. We categorized types of {DAS}. We surveyed corresponding authors who wrote in {DAS} that they would share the data. A consent to participate in the study was sought for all included manuscripts. After accessing raw data sets, we checked whether data were available in a way that enabled re-analysis.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater}{\textless}p{\textgreater}Of 3556 analyzed articles, 3416 contained {DAS}. The most frequent {DAS} category (42\%) indicated that the datasets are available on reasonable request. Among 1792 manuscripts in which {DAS} indicated that authors are willing to share their data, 1670 (93\%) authors either did not respond or declined to share their data with us. Among 254 (14\%) of 1792 authors who responded to our query for data sharing, only 122 (6.8\%) provided the requested data.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusion{\textless}/h3{\textgreater}{\textless}p{\textgreater}Even when authors indicate in their manuscript that they will share data upon request, the compliance rate is the same as for authors who do not provide {DAS}, suggesting that {DAS} may not be sufficient to ensure data sharing.{\textless}/p{\textgreater}},
	number = {0},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Gabelica, Mirko and Bojčić, Ružica and Puljak, Livia},
	urldate = {2022-06-05},
	date = {2022-05-29},
	note = {Publisher: Elsevier},
	file = {Snapshot:/Users/tom/Zotero/storage/4YHRV2A3/pdf.html:text/html},
}

@article{avenell_randomized_2022,
	title = {A randomized trial alerting authors, with or without co-authors or editors, that research they cited in systematic reviews and guidelines has been retracted},
	volume = {0},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2022.2082290},
	doi = {10.1080/08989621.2022.2082290},
	abstract = {Retracted clinical trials may be influential in citing systematic reviews and clinical guidelines.We assessed the influence of 27 retracted trials on systematic reviews and clinical guidelines (citing publications), then alerted authors to these retractions. Citing publications were randomized to up to three emails to contact author with/without up to two co-authors, with/without the editor. After one year we assessed corrective action.We included 88 citing publications; 51\% (45/88) had findings likely to change if the retracted trials were removed, 87\% (39/45) likely substantially.51\% (44/86) of contacted citing publications replied. Including three authors rather than the contact author alone was more likely to elicit a reply (P=0.03), but including the editor did not increase replies (P=0.66). Whether findings were judged likely to change, and the size of the likely change, had no effect on response rate or action taken. One year after emails were sent only nine publications had published notifications.Email alerts to authors and editors are inadequate to correct the impact of retracted publications in citing systematic reviews and guidelines. Changes to bibliographic and referencing systems, and submission processes are needed. Citing publications with retracted citations should be marked until authors resolve concerns.},
	pages = {null},
	issue = {ja},
	journaltitle = {Accountability in Research},
	author = {Avenell, Alison and Bolland, Mark J and Gamble, Greg D and Grey, Andrew},
	urldate = {2022-06-07},
	date = {2022-05-30},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2022.2082290},
	keywords = {retraction, systematic review, clinical guideline, impact, Publication integrity},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HMVSTTWN/Avenell et al. - 2022 - A randomized trial alerting authors, with or witho.pdf:application/pdf},
}

@report{bergh_bayesian_2022,
	title = {Bayesian Repeated-Measures {ANOVA}: An Updated Methodology Implemented in {JASP}},
	url = {https://psyarxiv.com/fb8zn/},
	shorttitle = {Bayesian Repeated-Measures {ANOVA}},
	abstract = {Analysis of variance ({ANOVA}) is widely used to assess the influence of one or more (quasi-)experimental manipulations on a continuous outcome. Traditionally, {ANOVA} is carried out in a frequentist manner using p-values, but a Bayesian alternative has been proposed. It seems reasonable to assume that this Bayesian {ANOVA} would be a direct translation of its frequentist analog, but the two approaches can yield radically different conclusions for repeated-measures data. We illustrate such discrepancies with an empirical data set from a two-factorial within-subject experiment, for which the frequentist and Bayesian {ANOVA} strongly disagree about which main effect accounts for variance in the data. We then show that the reason for this discrepancy is that the frequentist and Bayesian repeated-measures {ANOVAs} use different model specifications. To resolve such discrepancies, we recommend that the Bayesian repeated-measures {ANOVA} is adjusted to adopt the standard frequentist model specification. This adjustment, we believe, also reflects more reasonable assumptions about individual differences than the original Bayesian model specification. The paper concludes with a discussion on how this proposed adjustment impacts previously published results of Bayesian repeated-measures {ANOVAs}.},
	institution = {{PsyArXiv}},
	author = {Bergh, Don van den and Wagenmakers, Eric-Jan and Aust, Frederik},
	urldate = {2022-06-09},
	date = {2022-06-08},
	langid = {english},
	doi = {10.31234/osf.io/fb8zn},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, Quantitative Methods, Statistical Methods, Bayesian inference, {ANOVA}, {JASP}, random slopes, Repeated-measures {ANOVA}},
	file = {Full Text PDF:/Users/tom/Zotero/storage/F3WNJZUJ/Bergh et al. - 2022 - Bayesian Repeated-Measures ANOVA An Updated Metho.pdf:application/pdf},
}

@article{holcombe_documenting_2020,
	title = {Documenting contributions to scholarly articles using {CRediT} and tenzing},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0244611},
	doi = {10.1371/journal.pone.0244611},
	abstract = {Scholars traditionally receive career credit for a paper based on where in the author list they appear, but position in an author list often carries little information about what the contribution of each researcher was. “Contributorship” refers to a movement to formally document the nature of each researcher’s contribution to a project. We discuss the emerging {CRediT} standard for documenting contributions and describe a web-based app and R package called
              tenzing
              that is designed to facilitate its use.
              tenzing
              can make it easier for researchers on a project to plan and record their planned contributions and to document those contributions in a journal article.},
	pages = {e0244611},
	number = {12},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Holcombe, Alex O. and Kovacs, Marton and Aust, Frederik and Aczel, Balazs},
	editor = {Sugimoto, Cassidy R.},
	urldate = {2022-06-09},
	date = {2020-12-31},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/3K98UZ9C/Holcombe et al. - 2020 - Documenting contributions to scholarly articles us.pdf:application/pdf},
}

@article{brysbaert_how_nodate,
	title = {How many participants do we have to include in properly powered experiments? A tutorial of power analysis with reference tables},
	volume = {2},
	issn = {2514-4820},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640316/},
	doi = {10.5334/joc.72},
	shorttitle = {How many participants do we have to include in properly powered experiments?},
	abstract = {Given that an effect size of d = .4 is a good first estimate of the smallest effect size of interest in psychological research, we already need over 50 participants for a simple comparison of two within-participants conditions if we want to run a study with 80\% power. This is more than current practice. In addition, as soon as a between-groups variable or an interaction is involved, numbers of 100, 200, and even more participants are needed. As long as we do not accept these facts, we will keep on running underpowered studies with unclear results. Addressing the issue requires a change in the way research is evaluated by supervisors, examiners, reviewers, and editors. The present paper describes reference numbers needed for the designs most often used by psychologists, including single-variable between-groups and repeated-measures designs with two and three levels, two-factor designs involving two repeated-measures variables or one between-groups variable and one repeated-measures variable (split-plot design). The numbers are given for the traditional, frequentist analysis with p {\textless} .05 and Bayesian analysis with {BF} {\textgreater} 10. These numbers provide researchers with a standard to determine (and justify) the sample size of an upcoming study. The article also describes how researchers can improve the power of their study by including multiple observations per condition per participant.},
	pages = {16},
	number = {1},
	journaltitle = {Journal of Cognition},
	shortjournal = {J Cogn},
	author = {Brysbaert, Marc},
	urldate = {2022-06-10},
	pmid = {31517234},
	pmcid = {PMC6640316},
	file = {Full Text:/Users/tom/Zotero/storage/GBENLFC7/Brysbaert - How Many Participants Do We Have to Include in Pro.pdf:application/pdf},
}

@article{nuijten_prevalence_2016,
	title = {The prevalence of statistical reporting errors in psychology (1985–2013)},
	volume = {48},
	issn = {1554-3528},
	url = {http://link.springer.com/10.3758/s13428-015-0664-2},
	doi = {10.3758/s13428-015-0664-2},
	abstract = {This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package Bstatcheck.{\textasciicircum} statcheck retrieved null-hypothesis significance testing ({NHST}) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use {NHST} contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called Bco-pilot model,{\textasciicircum} and to use statcheck to flag possible inconsistencies in one’s own manuscript or during the review process.},
	pages = {1205--1226},
	number = {4},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Nuijten, Michèle B. and Hartgerink, Chris H. J. and van Assen, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
	urldate = {2022-06-13},
	date = {2016-12},
	langid = {english},
	file = {Nuijten et al. - 2016 - The prevalence of statistical reporting errors in .pdf:/Users/tom/Zotero/storage/ZP9URXWF/Nuijten et al. - 2016 - The prevalence of statistical reporting errors in .pdf:application/pdf;The prevalence of statistical reporting errors in psychology (1985–2013):/Users/tom/Zotero/storage/JNE3LCQ2/nuijten2015.pdf.pdf:application/pdf},
}

@book{knuth_literate_1992,
	location = {Stanford, Calif.},
	title = {Literate programming},
	isbn = {978-0-937073-80-3 978-0-937073-81-0},
	series = {{CSLI} lecture notes},
	pagetotal = {368},
	number = {no. 27},
	publisher = {Center for the Study of Language and Information},
	author = {Knuth, Donald Ervin},
	date = {1992},
	keywords = {Computer programming},
}

@article{simmons_21_2012,
	title = {A 21 word solution},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2160588},
	doi = {10.2139/ssrn.2160588},
	journaltitle = {{SSRN} Electronic Journal},
	shortjournal = {{SSRN} Journal},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	urldate = {2022-06-14},
	date = {2012},
	langid = {english},
	file = {A 21 Word Solution:/Users/tom/Zotero/storage/FU2NGAUM/a-21-word-solution.pdf.pdf:application/pdf},
}

@article{elliott_values_2022,
	title = {Values in Science},
	url = {https://www.cambridge.org/core/elements/values-in-science/8C9899A25764AA9A791287325A511C3C},
	doi = {10.1017/9781009052597},
	abstract = {Cambridge Core - Philosophy of Science - Values in Science},
	journaltitle = {Elements in the Philosophy of Science},
	author = {Elliott, Kevin C.},
	urldate = {2022-06-14},
	date = {2022-06},
	langid = {english},
	note = {{ISBN}: 9781009052597 9781009055635
Publisher: Cambridge University Press},
	file = {Snapshot:/Users/tom/Zotero/storage/QXPPYGA9/8C9899A25764AA9A791287325A511C3C.html:text/html},
}

@article{ioannidis_massive_2018-1,
	title = {Massive citations to misleading methods and research tools: Matthew effect, quotation error and citation copying},
	volume = {33},
	issn = {0393-2990},
	url = {https://www.jstor.org/stable/45217247},
	shorttitle = {Massive citations to misleading methods and research tools},
	pages = {1021--1023},
	number = {11},
	journaltitle = {European Journal of Epidemiology},
	author = {Ioannidis, John P. A.},
	urldate = {2022-06-14},
	date = {2018},
	note = {Publisher: Springer},
	file = {Ioannidis - 2018 - Massive citations to misleading methods and resear.pdf:/Users/tom/Zotero/storage/EJDYA97K/Ioannidis - 2018 - Massive citations to misleading methods and resear.pdf:application/pdf},
}

@article{stang_case_2018,
	title = {Case study in major quotation errors: a critical commentary on the Newcastle–Ottawa scale},
	volume = {33},
	issn = {0393-2990, 1573-7284},
	url = {http://link.springer.com/10.1007/s10654-018-0443-3},
	doi = {10.1007/s10654-018-0443-3},
	shorttitle = {Case study in major quotation errors},
	pages = {1025--1031},
	number = {11},
	journaltitle = {European Journal of Epidemiology},
	shortjournal = {Eur J Epidemiol},
	author = {Stang, Andreas and Jonas, Stephan and Poole, Charles},
	urldate = {2022-06-14},
	date = {2018-11},
	langid = {english},
	file = {Case study in major quotation errors\: a critical commentary on the Newcastle–Ottawa scale:/Users/tom/Zotero/storage/MQLT8Z59/stang2018.pdf.pdf:application/pdf},
}

@article{hosseini_doing_2018,
	title = {Doing the Right Thing: A Qualitative Investigation of Retractions Due to Unintentional Error},
	volume = {24},
	issn = {1471-5546},
	url = {https://doi.org/10.1007/s11948-017-9894-2},
	doi = {10.1007/s11948-017-9894-2},
	shorttitle = {Doing the Right Thing},
	abstract = {Retractions solicited by authors following the discovery of an unintentional error—what we henceforth call a “self-retraction”—are a new phenomenon of growing importance, about which very little is known. Here we present results of a small qualitative study aimed at gaining preliminary insights about circumstances, motivations and beliefs that accompanied the experience of a self-retraction. We identified retraction notes that unambiguously reported an honest error and that had been published between the years 2010 and 2015. We limited our sample to retractions with at least one co-author based in the Netherlands, Belgium, United Kingdom, Germany or a Scandinavian country, and we invited these authors to a semi-structured interview. Fourteen authors accepted our invitation. Contrary to our initial assumptions, most of our interviewees had not originally intended to retract their paper. They had contacted the journal to request a correction and the decision to retract had been made by journal editors. All interviewees reported that having to retract their own publication made them concerned for their scientific reputation and career, often causing considerable stress and anxiety. Interviewees also encountered difficulties in communicating with the journal and recalled other procedural issues that had unnecessarily slowed down the process of self-retraction. Intriguingly, however, all interviewees reported how, contrary to their own expectations, the self-retraction had brought no damage to their reputation and in some cases had actually improved it. We also examined the ethical motivations that interviewees ascribed, retrospectively, to their actions and found that such motivations included a combination of moral and prudential (i.e. pragmatic) considerations. These preliminary results suggest that scientists would welcome innovations to facilitate the process of self-retraction.},
	pages = {189--206},
	number = {1},
	journaltitle = {Science and Engineering Ethics},
	shortjournal = {Sci Eng Ethics},
	author = {Hosseini, Mohammad and Hilhorst, Medard and de Beaufort, Inez and Fanelli, Daniele},
	urldate = {2022-06-14},
	date = {2018-02-01},
	langid = {english},
	keywords = {Corrections, Error, Integrity, Misconduct, Moral reasoning, Retractions},
	file = {Accepted Version:/Users/tom/Zotero/storage/XSUXCSYN/Hosseini et al. - 2018 - Doing the Right Thing A Qualitative Investigation.pdf:application/pdf;Doing the Right Thing\: A Qualitative Investigation of Retractions Due to Unintentional Error:/Users/tom/Zotero/storage/GMBJYXFH/hosseini2017.pdf.pdf:application/pdf},
}

@report{strand_error_2021,
	title = {Error tight: exercises for lab groups to prevent research mistakes},
	url = {https://psyarxiv.com/rsn5y/},
	shorttitle = {Error tight},
	abstract = {No one is immune from making mistakes. In research, mistakes might include things like analyzing raw data instead of cleaned data, reversing variable labels, transcribing information incorrectly, or inadvertently saving over a file. The consequences of these kinds of mistakes can range from minor annoyances like wasted time and resources to major issues such as retraction of a paper.  Mistakes can happen under any circumstances, but their occurrence may be amplified by the incentive structure of science which rewards rapid, prolific publication rather than slow, methodological, and systematic work. The purpose of this project is to provide hands-on exercises for lab groups to identify places in their research workflow where errors may occur and pinpoint ways to address them. The appropriate approach for a given lab will vary depending on the kind of research they do, their tools, the nature of the data they work with, and many other factors. Therefore, this project does not provide a set of one-size-fits-all guidelines, but rather is intended to be an exercise in self-reflection for researchers and provide resources for solutions that are well-suited to them.},
	institution = {{PsyArXiv}},
	author = {Strand, Julia},
	urldate = {2022-06-15},
	date = {2021-03-31},
	langid = {english},
	doi = {10.31234/osf.io/rsn5y},
	note = {type: article},
	keywords = {Meta-science, meta-science, error detection, error prevention},
	file = {Full Text PDF:/Users/tom/Zotero/storage/RAUZENHC/Strand - 2021 - Error Tight Exercises for Lab Groups to Prevent R.pdf:application/pdf},
}

@book{zinsser_writing_2006,
	location = {New York},
	edition = {30th anniversary ed., 7th ed., rev. and updated},
	title = {On writing well: the classic guide to writing nonfiction},
	isbn = {978-0-06-089154-1},
	shorttitle = {On writing well},
	pagetotal = {321},
	publisher = {{HarperCollins}},
	author = {Zinsser, William},
	date = {2006},
	note = {{OCLC}: ocm62421288},
	keywords = {English language, Exposition (Rhetoric), Report writing, Rhetoric},
}

@article{smith_classical_2010,
	title = {Classical peer review: an empty gun},
	volume = {12},
	issn = {1465-542X},
	url = {https://breast-cancer-research.biomedcentral.com/articles/10.1186/bcr2742},
	doi = {10.1186/bcr2742},
	shorttitle = {Classical peer review},
	pages = {S13},
	issue = {S4},
	journaltitle = {Breast Cancer Research},
	shortjournal = {Breast Cancer Res},
	author = {Smith, Richard},
	urldate = {2022-06-20},
	date = {2010-12},
	langid = {english},
	file = {Classical peer review\: an empty gun:/Users/tom/Zotero/storage/YJT4SYWE/smith2010.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/Q3XHVQJJ/Smith - 2010 - Classical peer review an empty gun.pdf:application/pdf},
}

@article{godlee_effect_1998,
	title = {Effect on the quality of peer review of blinding reviewers and asking them to sign their reports: a randomized controlled trial},
	volume = {280},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.280.3.237},
	doi = {10.1001/jama.280.3.237},
	abstract = {Context.—Anxiety about bias, lack of accountability, and poor quality of peer review has led to questions about the imbalance in anonymity between reviewers
and authors.Objective.—To evaluate the effect on the quality of peer review of blinding reviewers
to the authors' identities and requiring reviewers to sign their reports.Design.—Randomized controlled trial.Setting.—A general medical journal.Participants.—A total of 420 reviewers from the journal's database.Intervention.—We modified a paper accepted for publication introducing 8 areas of
weakness. Reviewers were randomly allocated to 5 groups. Groups 1 and 2 received
manuscripts from which the authors' names and affiliations had been removed,
while groups 3 and 4 were aware of the authors' identities. Groups 1 and 3
were asked to sign their reports, while groups 2 and 4 were asked to return
their reports unsigned. The fifth group was sent the paper in the usual manner
of the journal, with authors' identities revealed and a request to comment
anonymously. Group 5 differed from group 4 only in that its members were unaware
that they were taking part in a study.Main Outcome Measure.—The number of weaknesses in the paper that were commented on by the
reviewers.Results.—Reports were received from 221 reviewers (53\%). The mean number of weaknesses
commented on was 2 (1.7, 2.1, 1.8, and 1.9 for groups 1, 2, 3, and 4 and 5
combined, respectively). There were no statistically significant differences
between groups in their performance. Reviewers who were blinded to authors'
identities were less likely to recommend rejection than those who were aware
of the authors' identities (odds ratio, 0.5; 95\% confidence interval, 0.3-1.0).Conclusions.—Neither blinding reviewers to the authors and origin of the paper nor
requiring them to sign their reports had any effect on rate of detection of
errors. Such measures are unlikely to improve the quality of peer review reports.},
	pages = {237--240},
	number = {3},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Godlee, Fiona and Gale, Catharine R. and Martyn, Christopher N.},
	urldate = {2022-06-20},
	date = {1998-07-15},
	file = {Effect on the Quality of Peer Review of Blinding Reviewers and Asking Them to Sign Their ReportsA Randomized Controlled Trial:/Users/tom/Zotero/storage/6KZITQHM/godlee1998.pdf.pdf:application/pdf},
}

@article{church_correction_1996,
	title = {Correction of errors in scientific research},
	volume = {28},
	issn = {1532-5970},
	url = {https://doi.org/10.3758/BF03204787},
	doi = {10.3758/BF03204787},
	abstract = {Four ways to reduce scientific errors are by tests of equipment and programs, examination of results, peer review, and replication. This article describes various types of errors that may occur and procedures available for the prevention and correction of both unintentional and intentional errors in experiments that use computer programs to generate the stimuli, record the responses, or analyze the data. We describe a case study of a particular experiment that produced a result that has been found to be erroneous. The case study provides additional evidence of the essential importance of replication for the identification and elimination of scientific error.},
	pages = {305--310},
	number = {2},
	journaltitle = {Behavior Research Methods, Instruments, \& Computers},
	shortjournal = {Behavior Research Methods, Instruments, \& Computers},
	author = {Church, Russell M. and Crystal, Jonathon D. and Collyer, Charles E.},
	urldate = {2022-06-20},
	date = {1996-06-01},
	langid = {english},
	keywords = {Stimulus Duration, Data Analysis Program, Pixel Location, Psychophysical Function, Replication Experiment},
	file = {Correction of errors in scientific research:/Users/tom/Zotero/storage/URAY42GY/church1996.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/W8Z4ZTE2/Church et al. - 1996 - Correction of errors in scientific research.pdf:application/pdf},
}

@article{callaham_reliability_1998,
	title = {Reliability of editors' subjective quality ratings of peer reviews of manuscripts},
	volume = {280},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.280.3.229},
	doi = {10.1001/jama.280.3.229},
	abstract = {Context.—Quality of reviewers is crucial to journal quality, but there are usually
too many for editors to know them all personally. A reliable method of rating
them (for education and monitoring) is needed.Objective.—Whether editors' quality ratings of peer reviewers are reliable and
how they compare with other performance measures.Design.—A 3.5-year prospective observational study.Setting.—Peer-reviewed journal.Participants.—All editors and peer reviewers who reviewed at least 3 manuscripts.Main Outcome Measures.—Reviewer quality ratings, individual reviewer rate of recommendation
for acceptance, congruence between reviewer recommendation and editorial decision
(decision congruence), and accuracy in reporting flaws in a masked test manuscript.Interventions.—Editors rated the quality of each review on a subjective 1 to 5 scale.Results.—A total of 4161 reviews of 973 manuscripts by 395 reviewers were studied.
The within-reviewer intraclass correlation was 0.44 (P\&lt;.001),
indicating that 20\% of the variance seen in the review ratings was attributable
to the reviewer. Intraclass correlations for editor and manuscript were only
0.24 and 0.12, respectively. Reviewer average quality ratings correlated poorly
with the rate of recommendation for acceptance (R=−0.34)
and congruence with editorial decision (R=0.26).
Among 124 reviewers of the fictitious manuscript, the mean quality rating
for each reviewer was modestly correlated with the number of flaws they reported
(R=0.53). Highly rated reviewers reported twice as
many flaws as poorly rated reviewers.Conclusions.—Subjective editor ratings of individual reviewers were moderately reliable
and correlated with reviewer ability to report manuscript flaws. Individual
reviewer rate of recommendation for acceptance and decision congruence might
be thought to be markers of a discriminating (ie, high-quality) reviewer,
but these variables were poorly correlated with editors' ratings of review
quality or the reviewer's ability to detect flaws in a fictitious manuscript.
Therefore, they cannot be substituted for actual quality ratings by editors.},
	pages = {229--231},
	number = {3},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Callaham, Michael L. and Baxt, William G. and Waeckerle, Joseph F. and Wears, Robert L.},
	urldate = {2022-06-20},
	date = {1998-07-15},
	file = {Reliability of Editors' Subjective Quality Ratings of Peer Reviews of Manuscripts:/Users/tom/Zotero/storage/Y39KLZBG/callaham1998.pdf.pdf:application/pdf},
}

@article{baxt_who_1998,
	title = {Who reviews the reviewers? Feasibility of using a fictitious manuscript to evaluate peer reviewer performance},
	volume = {32},
	issn = {0196-0644, 1097-6760},
	url = {https://www.annemergmed.com/article/S0196-0644(98)70006-X/fulltext},
	doi = {10.1016/S0196-0644(98)70006-X},
	shorttitle = {Who reviews the reviewers?},
	pages = {310--317},
	number = {3},
	journaltitle = {Annals of Emergency Medicine},
	shortjournal = {Annals of Emergency Medicine},
	author = {Baxt, William G. and Waeckerle, Joseph F. and Berlin, Jesse A. and Callaham, Michael L.},
	urldate = {2022-06-20},
	date = {1998-09-01},
	note = {Publisher: Elsevier},
	file = {Full Text PDF:/Users/tom/Zotero/storage/Z42XU7U8/Baxt et al. - 1998 - Who Reviews the Reviewers Feasibility of Using a .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/GYSX7AKE/fulltext.html:text/html;Who Reviews the Reviewers? Feasibility of Using a Fictitious Manuscript to Evaluate Peer Reviewer Performance:/Users/tom/Zotero/storage/DAXLFQDP/baxt1998.pdf.pdf:application/pdf},
}

@article{mayo-wilson_evaluating_2021-1,
	title = {Evaluating implementation of the Transparency and Openness Promotion ({TOP}) guidelines: the {TRUST} process for rating journal policies, procedures, and practices},
	volume = {6},
	issn = {2058-8615},
	url = {https://doi.org/10.1186/s41073-021-00112-8},
	doi = {10.1186/s41073-021-00112-8},
	shorttitle = {Evaluating implementation of the Transparency and Openness Promotion ({TOP}) guidelines},
	abstract = {The Transparency and Openness Promotion ({TOP}) Guidelines describe modular standards that journals can adopt to promote open science. The {TOP} Factor is a metric to describe the extent to which journals have adopted the {TOP} Guidelines in their policies. Systematic methods and rating instruments are needed to calculate the {TOP} Factor. Moreover, implementation of these open science policies depends on journal procedures and practices, for which {TOP} provides no standards or rating instruments.},
	pages = {9},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Research Integrity and Peer Review},
	author = {Mayo-Wilson, Evan and Grant, Sean and Supplee, Lauren and Kianersi, Sina and Amin, Afsah and {DeHaven}, Alex and Mellor, David},
	urldate = {2022-06-20},
	date = {2021-06-02},
	keywords = {Open science, Reproducibility, Research transparency, {TOP} factor, {TOP} guidelines},
	file = {Evaluating implementation of the Transparency and Openness Promotion (TOP) guidelines\: the TRUST process for rating journal policies, procedures, and practices:/Users/tom/Zotero/storage/PAKYY9BF/mayo-wilson2021.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/NVL7BTTA/Mayo-Wilson et al. - 2021 - Evaluating implementation of the Transparency and .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/II6LGLWF/s41073-021-00112-8.html:text/html},
}

@article{kaufman_implementing_2020,
	title = {Implementing novel, flexible, and powerful survey designs in R Shiny},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232424},
	doi = {10.1371/journal.pone.0232424},
	abstract = {Survey research is ubiquitous in the social sciences as a cost-effective and time-efficient means of collecting data. However, the available software for implementing and disseminating such surveys lacks flexibility, stifling researcher creativity and severely limiting the scope of questions that survey research can address. In this paper I introduce the use of R Shiny, an open source web application and scripting language, for implementing powerful, innovative, and fully customizable surveys. Through six applications rooted in important questions in political science, I show that R Shiny allows for (1) randomized question selection, (2) programmatic treatments, (3) programmatic survey flow, (4) adaptive question batteries, (5) sequentially block-randomized design, and (6) randomized intracoder reliability tests, expanding the scope, ease, and cost effectiveness of online survey research. I make all replication code available online.},
	pages = {e0232424},
	number = {4},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Kaufman, Aaron R.},
	urldate = {2022-06-20},
	date = {2020-04-30},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Research design, Surveys, Social sciences, Survey research, Computer software, Open source software, Political science, User interfaces},
	file = {Full Text PDF:/Users/tom/Zotero/storage/UHZK8UTY/Kaufman - 2020 - Implementing novel, flexible, and powerful survey .pdf:application/pdf;Implementing novel, flexible, and powerful survey designs in R Shiny:/Users/tom/Zotero/storage/N4I8MAW5/10.1371@journal.pone.0232424.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8T8MRRPJ/article.html:text/html},
}

@article{ince_within-participant_2022,
	title = {Within-participant statistics for cognitive science},
	volume = {0},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(22)00114-0},
	doi = {10.1016/j.tics.2022.05.008},
	abstract = {{\textless}p{\textgreater}Experimental studies in cognitive science typically focus on the population average effect. An alternative is to test each individual participant and then quantify the proportion of the population that would show the effect: the prevalence, or participant replication probability. We argue that this approach has conceptual and practical advantages.{\textless}/p{\textgreater}},
	number = {0},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Ince, Robin A. A. and Kay, Jim W. and Schyns, Philippe G.},
	urldate = {2022-06-20},
	date = {2022-06-13},
	note = {Publisher: Elsevier},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MB68XM6G/Ince et al. - 2022 - Within-participant statistics for cognitive scienc.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6EJXILAQ/S1364-6613(22)00114-0.html:text/html},
}

@article{schulz_is_2022,
	title = {Is the future of peer review automated?},
	volume = {15},
	issn = {1756-0500},
	url = {https://doi.org/10.1186/s13104-022-06080-6},
	doi = {10.1186/s13104-022-06080-6},
	abstract = {The rising rate of preprints and publications, combined with persistent inadequate reporting practices and problems with study design and execution, have strained the traditional peer review system. Automated screening tools could potentially enhance peer review by helping authors, journal editors, and reviewers to identify beneficial practices and common problems in preprints or submitted manuscripts. Tools can screen many papers quickly, and may be particularly helpful in assessing compliance with journal policies and with straightforward items in reporting guidelines. However, existing tools cannot understand or interpret the paper in the context of the scientific literature. Tools cannot yet determine whether the methods used are suitable to answer the research question, or whether the data support the authors’ conclusions. Editors and peer reviewers are essential for assessing journal fit and the overall quality of a paper, including the experimental design, the soundness of the study’s conclusions, potential impact and innovation. Automated screening tools cannot replace peer review, but may aid authors, reviewers, and editors in improving scientific papers. Strategies for responsible use of automated tools in peer review may include setting performance criteria for tools, transparently reporting tool performance and use, and training users to interpret reports.},
	pages = {203},
	number = {1},
	journaltitle = {{BMC} Research Notes},
	shortjournal = {{BMC} Research Notes},
	author = {Schulz, Robert and Barnett, Adrian and Bernard, René and Brown, Nicholas J. L. and Byrne, Jennifer A. and Eckmann, Peter and Gazda, Małgorzata A. and Kilicoglu, Halil and Prager, Eric M. and Salholz-Hillel, Maia and ter Riet, Gerben and Vines, Timothy and Vorland, Colby J. and Zhuang, Han and Bandrowski, Anita and Weissgerber, Tracey L.},
	urldate = {2022-06-20},
	date = {2022-06-11},
	keywords = {Transparency, Reproducibility, Peer review, toread, Automated screening, Rigor},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HUH7HB2I/Schulz et al. - 2022 - Is the future of peer review automated.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/F9LLM6XQ/s13104-022-06080-6.html:text/html},
}

@article{ioannidis_can_1998,
	title = {Can quality of clinical trials and meta-analyses be quantified?},
	volume = {352},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(98)22034-4/fulltext},
	doi = {10.1016/S0140-6736(98)22034-4},
	pages = {590},
	number = {9128},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Ioannidis, John {PA} and Lau, Joseph},
	urldate = {2022-06-20},
	date = {1998-08-22},
	pmid = {9746014},
	note = {Publisher: Elsevier},
	file = {Can quality of clinical trials and meta-analyses be quantified?:/Users/tom/Zotero/storage/SBWTKNA6/ioannidis1998.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/MRPPWZY4/fulltext.html:text/html},
}

@article{von_hippel_is_2022,
	title = {Is psychological science self-correcting? Citations before and after successful and failed replications},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/17456916211072525},
	doi = {10.1177/17456916211072525},
	shorttitle = {Is psychological science self-correcting?},
	abstract = {In principle, successful replications should enhance the credibility of scientific findings, and failed replications should reduce credibility. Yet it is unknown how replication typically affects the influence of research. We analyzed the citation history of 98 articles. Each was published by a selective psychology journal in 2008 and subjected to a replication attempt published in 2015. Relative to successful replications, failed replications reduced citations of replicated studies by only 5\% to 9\% on average, an amount that did not differ significantly from zero. Less than 3\% of articles citing the original studies cited the replication attempt. It does not appear that replication failure much reduced the influence of nonreplicated findings in psychology. To increase the influence of replications, we recommend (a) requiring authors to cite replication studies alongside the individual findings and (b) enhancing reference databases and search engines to give higher priority to replication studies.},
	pages = {17456916211072525},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {von Hippel, Paul T.},
	urldate = {2022-06-23},
	date = {2022-06-17},
	note = {Publisher: {SAGE} Publications Inc},
	file = {von Hippel - 2022 - Is psychological science self-correcting Citation.pdf:/Users/tom/Zotero/storage/6ANYH3AZ/von Hippel - 2022 - Is psychological science self-correcting Citation.pdf:application/pdf},
}

@article{bishop_fallibility_2018,
	title = {Fallibility in science: responding to errors in the work of oneself and others},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918776632},
	doi = {10.1177/2515245918776632},
	shorttitle = {Fallibility in science},
	pages = {432--438},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Bishop, D. V. M.},
	urldate = {2022-06-27},
	date = {2018-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Fallibility in Science\: Responding to Errors in the Work of Oneself and Others:/Users/tom/Zotero/storage/MZW4XGJU/bishop2018.pdf.pdf:application/pdf;Submitted Version:/Users/tom/Zotero/storage/PUPT9BQT/Bishop - 2018 - Fallibility in Science Responding to Errors in th.pdf:application/pdf},
}

@report{norris_awareness_2022,
	title = {Awareness of and engagement with Open Research behaviours: Development of the Brief Open Research Survey ({BORS}) with the {UK} Reproducibility Network},
	url = {https://osf.io/preprints/metaarxiv/w48yh/},
	shorttitle = {Awareness of and engagement with Open Research behaviours},
	abstract = {Objectives: A need for Open Research practices exists, yet there remains a lack of validated questionnaires to assess Open Research practices. The study aimed to develop a brief ({\textless}5 minutes), standardised questionnaire to measure Open Research awareness and engagement across {UK} institutions.
Methods: The Brief Open Research Survey ({BORS}) was developed in six steps: 1) a scoping exercise collated previous questionnaires on Open Research, 2) a brief questionnaire was developed, 3) peer-reviewed, 4) piloted, 5) revised, and 6) the final questionnaire was distributed across {UK} Reproducibility Network ({UKRN}) local networks.
Results: Respondents across thirty-five {UKRN} local networks participated (n = 1,274). Respondents were most aware of Open Access publications (94.1\%) and also used them the most (76.5\%). They were least aware of Registered Reports (38.1\%) and also used them the least (8.3\%). Respondents reported that incentives (51\%), dedicated funding (46.2\%), and recognition in promotion and recruitment criteria (39.6\%) would help them embed Open Research.
Conclusion: Although various Open Research initiatives exist, there remains a disconnect between awareness and implementation. Support from funders and institutions is required to increase Open Research. The Brief Open Research Survey can be used to track uptake over time and adapted to measure Open Research globally.},
	institution = {{MetaArXiv}},
	author = {Norris, Emma and Clark, Kait and Munafo, Marcus and Jay, Caroline and Baldwin, Jessie and Lautarescu, Alexandra and Pedder, Hugo and Page, Mike and Rinke, Eike Mark and Burn, Charlotte and Cawthorn, Will and Ballou, Nick and Glover, Scott and Evans, Samuel and Rossit, Stephanie and Soltanlou, Mojtaba and Wise, Emma and Kelson, Mark and Soliman, Nadia and Jones, Andrew and Costello, Rianne and Smailes, David and Wilkinson, Laura L. and Piccardi, Elena Serena and Partridge, Adam Michael and Hulme, Charlotte and Schultze, Anna and Pennington, Charlotte Rebecca},
	urldate = {2022-06-27},
	date = {2022-06-08},
	langid = {english},
	doi = {10.31222/osf.io/w48yh},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, transparency, Medicine and Health Sciences, Physical Sciences and Mathematics, Sociology, {UK} Reproducibility Network, open research, Other Social and Behavioral Sciences, meta research, responsible research practices, Social Statistics},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NYBFM5V2/Norris et al. - 2022 - Awareness of and engagement with Open Research beh.pdf:application/pdf},
}

@article{bishop_open_nodate,
	title = {Open research practices: unintended consequences and suggestions for averting them. (Commentary on the Peer Reviewers' Openness Initiative)},
	volume = {3},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.160109},
	doi = {10.1098/rsos.160109},
	shorttitle = {Open research practices},
	abstract = {The Peer Reviewers' Openness Initiative ({PROI}) is a move to enlist reviewers in the promotion of data-sharing. In this commentary, I discuss objections that can be raised, first to the specific proposals in the {PROI}, and second to data-sharing in general. I argue that although many objections have strong counter-arguments, others merit more serious consideration. Regarding the {PROI}, I suggest that it could backfire if editors and authors feel coerced into data-sharing and so may not be the most pragmatic way of encouraging greater openness. More generally, while promoting data-sharing, we need to be sensitive to cases where sharing of data from human participants could create ethical problems. Furthermore, those interested in promoting reproducible science need to defend against an increased risk of data-dredging when large, multivariable datasets are shared. I end with some suggestions to avoid these unintended consequences.},
	pages = {160109},
	number = {4},
	journaltitle = {Royal Society Open Science},
	author = {Bishop, D. V. M.},
	urldate = {2022-07-03},
	note = {Publisher: Royal Society},
	keywords = {reproducibility, ethics, data-dredging, data-sharing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7RH66Q6Y/Bishop - Open research practices unintended consequences a.pdf:application/pdf;Open research practices\: unintended consequences and suggestions for averting them. (Commentary on the Peer Reviewers' Openness Initiative):/Users/tom/Zotero/storage/EMNAKDFA/bishop2016.pdf.pdf:application/pdf},
}

@article{mello_clinical_2018,
	title = {Clinical trial participants’ views of the risks and benefits of data sharing},
	volume = {378},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMsa1713258},
	doi = {10.1056/NEJMsa1713258},
	pages = {2202--2211},
	number = {23},
	journaltitle = {New England Journal of Medicine},
	shortjournal = {N Engl J Med},
	author = {Mello, Michelle M. and Lieou, Van and Goodman, Steven N.},
	urldate = {2022-07-04},
	date = {2018-06-07},
	langid = {english},
	file = {Accepted Version:/Users/tom/Zotero/storage/B8CD26KD/Mello et al. - 2018 - Clinical Trial Participants’ Views of the Risks an.pdf:application/pdf;Clinical Trial Participants’ Views of the Risks and Benefits of Data Sharing:/Users/tom/Zotero/storage/2LYCGN8U/mello2018.pdf.pdf:application/pdf},
}

@article{amrhein_inferential_2019,
	title = {Inferential statistics as descriptive statistics: there is no replication crisis if we don’t expect replication},
	volume = {73},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1543137},
	doi = {10.1080/00031305.2018.1543137},
	shorttitle = {Inferential statistics as descriptive statistics},
	pages = {262--270},
	issue = {sup1},
	journaltitle = {The American Statistician},
	shortjournal = {The American Statistician},
	author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
	urldate = {2022-07-04},
	date = {2019-03-29},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/4A4KJGT3/Amrhein et al. - 2019 - Inferential Statistics as Descriptive Statistics .pdf:application/pdf;Inferential Statistics as Descriptive Statistics\: There Is No Replication Crisis if We Don’t Expect Replication:/Users/tom/Zotero/storage/6J5XN48B/10.1080@00031305.2018.1543137.pdf.pdf:application/pdf},
}

@article{collins_reproducibility_2022,
	title = {Reproducibility of {COVID}-19 pre-prints},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-022-04418-2},
	doi = {10.1007/s11192-022-04418-2},
	abstract = {To examine the reproducibility of {COVID}-19 research, we create a dataset of pre-prints posted to {arXiv}, {bioRxiv}, and {medRxiv} between 28 January 2020 and 30 June 2021 that are related to {COVID}-19. We extract the text from these pre-prints and parse them looking for keyword markers signaling the availability of the data and code underpinning the pre-print. For the pre-prints that are in our sample, we are unable to find markers of either open data or open code for 75\% of those on {arXiv}, 67\% of those on {bioRxiv}, and 79\% of those on {medRxiv}.},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Collins, Annie and Alexander, Rohan},
	urldate = {2022-07-05},
	date = {2022-07-04},
	langid = {english},
	keywords = {Open science, Reproducibility, {COVID}-19},
	file = {Full Text PDF:/Users/tom/Zotero/storage/DN79JLPR/Collins and Alexander - 2022 - Reproducibility of COVID-19 pre-prints.pdf:application/pdf},
}

@article{ioannidis_pooling_1999,
	title = {Pooling research results: benefits and limitations of meta-analysis},
	volume = {25},
	issn = {1070-3241},
	url = {https://www.sciencedirect.com/science/article/pii/S1070324116304606},
	doi = {10.1016/S1070-3241(16)30460-6},
	shorttitle = {Pooling research results},
	abstract = {Article-at-a-Glance
Background
Meta-analysis, the systematic and quantitative synthesis of evidence, has developed considerably in the 1990s and is emerging as an important methodology in medical decision making. As a research methodology, meta-analysis has benefits and limitations that must be acknowledged in its application.
Examples of benefits
The benefits of meta-analysis include the ability to improve the power of small or inconclusive studies to answer questions and the ability to identify sources of diversity across various types of studies. Meta-analysis may reveal how heterogeneity among populations affects the effectiveness of medical interventions in different settings and in different patients. It can also help detect biases, such as publication bias and “Tower of Babel” bias, as well as deficiencies in the design, conduct, analysis, and interpretation of research. In this way, it can also stimulate improvements in the quality of the data needed to optimize medical care.
Examples of limitations
Meta-analysis cannot improve the quality or reporting of the original studies. Other limitations come from misapplications of the method, such as when study diversity is ignored or mishandled in the analysis or when the variability of patient populations, the quality of the data, and the potential for underlying biases are not addressed.
Conclusions
Meta-analysis has promoted the sense that obtaining evidence is a global enterprise and that complete information needs to be evaluated and synthesized to obtain the most unbiased results. Analyzing sources of bias and diversity is essential to performing, understanding, and using meta-analyses in medical care.},
	pages = {462--469},
	number = {9},
	journaltitle = {The Joint Commission Journal on Quality Improvement},
	shortjournal = {The Joint Commission Journal on Quality Improvement},
	author = {Ioannidis, John P. A. and Lau, Joseph},
	urldate = {2022-07-05},
	date = {1999-09-01},
	langid = {english},
	file = {Pooling Research Results\: Benefits and Limitations of Meta-Analysis:/Users/tom/Zotero/storage/WRE24BF2/ioannidis1999.pdf.pdf:application/pdf},
}

@article{lau_summing_1998,
	title = {Summing up evidence: one answer is not always enough},
	volume = {351},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673697084687},
	doi = {10.1016/S0140-6736(97)08468-7},
	shorttitle = {Summing up evidence},
	pages = {123--127},
	number = {9096},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Lau, Joseph and Ioannidis, John {PA} and Schmid, Christopher H},
	urldate = {2022-07-05},
	date = {1998-01},
	langid = {english},
	file = {Lau et al. - 1998 - Summing up evidence one answer is not always enou.pdf:/Users/tom/Zotero/storage/4DXTN8CK/Lau et al. - 1998 - Summing up evidence one answer is not always enou.pdf:application/pdf},
}

@article{moher_assessing_1995,
	title = {Assessing the quality of randomized controlled trials: An annotated bibliography of scales and checklists},
	volume = {16},
	issn = {0197-2456},
	url = {https://www.sciencedirect.com/science/article/pii/019724569400031W},
	doi = {10.1016/0197-2456(94)00031-W},
	shorttitle = {Assessing the quality of randomized controlled trials},
	abstract = {Assessing the quality of randomized controlled trials ({RCTs}) is important and relatively new. Quality gives us an estimate of the likelihood that the results are a valid estimate of the truth. We present an annotated bibliography of scales and checklists developed to assess quality. Twenty-five scales and nine checklists have been developed to assess quality. The checklists are most useful in providing investigators with guidelines as to what information should be included in reporting {RCTs}. The scales give readers a quantitative index of the likelihood that the reported methodology and results are free of bias. There are several shortcomings with these scales. Future scale development is likely to be most beneficial if questions common to all trials are assessed, if the scale is easy to use, and if it is developed with sufficient rigor.},
	pages = {62--73},
	number = {1},
	journaltitle = {Controlled Clinical Trials},
	shortjournal = {Controlled Clinical Trials},
	author = {Moher, David and Jadad, Alejandro R and Nichol, Graham and Penman, Marie and Tugwell, Peter and Walsh, Sharon},
	urldate = {2022-07-06},
	date = {1995-02-01},
	langid = {english},
	keywords = {assessment, checklists, Clinimetrics, quality, scales},
	file = {Moher et al. - 1995 - Assessing the quality of randomized controlled tri.pdf:/Users/tom/Zotero/storage/DFHZIQ34/Moher et al. - 1995 - Assessing the quality of randomized controlled tri.pdf:application/pdf},
}

@article{heck_review_2022,
	title = {A review of applications of the Bayes factor in psychological research.},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000454},
	doi = {10.1037/met0000454},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Heck, Daniel W. and Boehm, Udo and Böing-Messing, Florian and Bürkner, Paul-Christian and Derks, Koen and Dienes, Zoltan and Fu, Qianrao and Gu, Xin and Karimova, Diana and Kiers, Henk A. L. and Klugkist, Irene and Kuiper, Rebecca M. and Lee, Michael D. and Leenders, Roger and Leplaa, Hidde J. and Linde, Maximilian and Ly, Alexander and Meijerink-Bosman, Marlyne and Moerbeek, Mirjam and Mulder, Joris and Palfi, Bence and Schönbrodt, Felix D. and Tendeiro, Jorge N. and van den Bergh, Don and Van Lissa, Caspar J. and van Ravenzwaaij, Don and Vanpaemel, Wolf and Wagenmakers, Eric-Jan and Williams, Donald R. and Zondervan-Zwijnenburg, Mariëlle and Hoijtink, Herbert},
	urldate = {2022-07-06},
	date = {2022-03-17},
	langid = {english},
	file = {Heck et al. - 2022 - A review of applications of the Bayes factor in ps.pdf:/Users/tom/Zotero/storage/WPH3YUST/Heck et al. - 2022 - A review of applications of the Bayes factor in ps.pdf:application/pdf},
}

@article{scheibehenne_fixed_2017,
	title = {Fixed or random? A resolution through model averaging: reply to carlsson, schimmack, williams, and bürkner (2017)},
	volume = {28},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797617724426},
	doi = {10.1177/0956797617724426},
	shorttitle = {Fixed or random?},
	pages = {1698--1701},
	number = {11},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Scheibehenne, Benjamin and Gronau, Quentin F. and Jamil, Tahira and Wagenmakers, Eric-Jan},
	urldate = {2022-07-06},
	date = {2017-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Fixed or Random? A Resolution Through Model Averaging\: Reply to Carlsson, Schimmack, Williams, and Bürkner (2017):/Users/tom/Zotero/storage/LQE96N7W/scheibehenne2017.pdf.pdf:application/pdf},
}

@article{carlsson_bayes_2017,
	title = {Bayes factors from pooled data are no substitute for bayesian meta-analysis: commentary on scheibehenne, jamil, and wagenmakers (2016)},
	volume = {28},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797616684682},
	doi = {10.1177/0956797616684682},
	shorttitle = {Bayes factors from pooled data are no substitute for bayesian meta-analysis},
	pages = {1694--1697},
	number = {11},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Carlsson, Rickard and Schimmack, Ulrich and Williams, Donald R. and Bürkner, Paul-Christian},
	urldate = {2022-07-06},
	date = {2017-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Bayes Factors From Pooled Data Are No Substitute for Bayesian Meta-Analysis\: Commentary on Scheibehenne, Jamil, and Wagenmakers (2016):/Users/tom/Zotero/storage/3KYHJ9QD/carlsson2017.pdf.pdf:application/pdf},
}

@article{scheibehenne_bayesian_2016,
	title = {Bayesian evidence synthesis can reconcile seemingly inconsistent results: the case of hotel towel reuse},
	volume = {27},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797616644081},
	doi = {10.1177/0956797616644081},
	shorttitle = {Bayesian evidence synthesis can reconcile seemingly inconsistent results},
	pages = {1043--1046},
	number = {7},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Scheibehenne, Benjamin and Jamil, Tahira and Wagenmakers, Eric-Jan},
	urldate = {2022-07-06},
	date = {2016-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Bayesian Evidence Synthesis Can Reconcile Seemingly Inconsistent Results\: The Case of Hotel Towel Reuse:/Users/tom/Zotero/storage/HH2RSYNR/scheibehenne2016.pdf.pdf:application/pdf},
}

@article{kuiper_combining_2013,
	title = {Combining statistical evidence from several studies: a method using bayesian updating and an example from research on trust problems in social and economic exchange},
	volume = {42},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124112464867},
	doi = {10.1177/0049124112464867},
	shorttitle = {Combining statistical evidence from several studies},
	abstract = {The effect of an independent variable on a dependent variable is often evaluated with hypothesis testing. Sometimes, multiple studies are available that test the same hypothesis. In such studies, the dependent variable and the main predictors might differ, while they do measure the same theoretical concepts. In this article, we present a Bayesian updating method that can be used to quantify the joint evidence in multiple studies regarding the effect of one variable of interest. We apply our method to four studies on how trust in social and economic exchange depends on experience from previous exchange with the same partner. In addition, we examine five hypothetical situations in which the results from the separate studies are less clear-cut than in our trust example.},
	pages = {60--81},
	number = {1},
	journaltitle = {Sociological Methods \& Research},
	shortjournal = {Sociological Methods \& Research},
	author = {Kuiper, Rebecca M. and Buskens, Vincent and Raub, Werner and Hoijtink, Herbert},
	urldate = {2022-07-06},
	date = {2013-02-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {Bayes factor, trust, Bayesian updating, embeddedness, posterior model probabilities},
	file = {Combining Statistical Evidence From Several Studies\: A Method Using Bayesian Updating and an Example From Research on Trust Problems in Social and Economic Exchange:/Users/tom/Zotero/storage/85L89B8K/kuiper2012.pdf.pdf:application/pdf},
}

@article{williams_potentially_nodate,
	title = {Potentially harmful therapies: A meta-scientific review of evidential value},
	volume = {n/a},
	issn = {1468-2850},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cpsp.12331},
	doi = {10.1111/cpsp.12331},
	shorttitle = {Potentially harmful therapies},
	abstract = {Lilienfeld (2007, Psychological treatments that cause harm. Perspectives on Psychological Science, 2, 53) identified a list of potentially harmful therapies ({PHTs}). Given concerns regarding the replicability of scientific findings, we conducted a meta-scientific review of Lilienfeld's {PHTs} to determine the evidential strength for harm. We evaluated the extent to which effects used as evidence of harm were as follows: (a) (in)correctly reported; (b) well-powered; (c) statistically significant at an inflated rate given their power; and (d) stronger compared with null effects of ineffectiveness or evidence of benefit, based on a Bayesian index of evidence. We found evidence of harm from some {PHTs}, though most metrics were ambiguous. To enhance provision of ethical and science-based care, a comprehensive reexamination of what constitutes evidence for claims of harm is necessary.},
	pages = {e12331},
	issue = {n/a},
	journaltitle = {Clinical Psychology: Science and Practice},
	author = {Williams, Alexander J. and Botanov, Yevgeny and Kilshaw, Robyn E. and Wong, Ryan E. and Sakaluk, John Kitchener},
	urldate = {2022-07-06},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cpsp.12331},
	keywords = {replicability, reproducibility, metascience, evidential value, potentially harmful therapies},
	file = {Potentially harmful therapies\: A meta-scientific review of evidential value:/Users/tom/Zotero/storage/CTGK8LS3/10.1111@cpsp.12331.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DQIYKWN8/cpsp.html:text/html},
}

@article{naylor_case_1995,
	title = {The case for failed meta-analyses},
	volume = {1},
	issn = {1365-2753},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2753.1995.tb00018.x},
	doi = {10.1111/j.1365-2753.1995.tb00018.x},
	pages = {127--130},
	number = {2},
	journaltitle = {Journal of Evaluation in Clinical Practice},
	author = {Naylor, C. David},
	urldate = {2022-07-06},
	date = {1995},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2753.1995.tb00018.x},
	keywords = {meta-analysis, heterogenedy, overview, random allocation},
	file = {Snapshot:/Users/tom/Zotero/storage/T8E6DPJV/j.1365-2753.1995.tb00018.html:text/html;The case for failed meta-analyses:/Users/tom/Zotero/storage/QGJ6MWCE/naylor1995.pdf.pdf:application/pdf},
}

@article{hilgard_how_2017,
	title = {How much evidence is p {\textgreater} .05? Stimulus pre-testing and null primary outcomes in violent video games research},
	volume = {6},
	issn = {2160-4142},
	doi = {10.1037/ppm0000102},
	shorttitle = {How much evidence is p {\textgreater} .05?},
	abstract = {[Correction Notice: An Erratum for this article was reported in Vol 6(4) of Psychology of Popular Media Culture (see record 2016-27707-001). In the article, the infinity symbol was incorrectly replaced with a 1 in two locations in the fourth paragraph of the “Bayesian Inference” section. The corrected text follows: “Bayes factor values range from 0 to ∞ and describe how much more probable the data are under one position than another. . . . Infinite support for the null and alternative are obtained when B₀₁ = ∞ and B₀₁ = 0, respectively.”] Research on the effects of violent video games frequently relies on arguments for the null hypothesis. Proponents of the effects argue that there are no meaningful differences save violent content between the violent and nonviolent games played, while critics of the effects argue that their nonsignificant study results constitute evidence for the null hypothesis of no difference. However, neither argument can be supported through the use of traditional null-hypothesis significance testing, as such tests can only ever reject or retain the null, never rejecting the alternative hypothesis in favor of the null. Therefore, to evaluate these claims, we apply a more appropriate Bayesian analysis to measure evidence for or against the null hypothesis relative to reasonable alternative hypotheses. We conclude that current methodological standards cannot rule out substantial confounds between violent and nonviolent video games. Furthermore, we find that studies that claim to find an absence of violent video game effects vary substantially in the strength of evidence, with some strongly supporting the null, others weakly supporting the null, and some others finding evidence of differences between conditions. We recommend the use of Bayesian analyses, larger sample sizes, and the creation of custom-designed games for experimental research. ({PsycINFO} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {361--380},
	number = {4},
	journaltitle = {Psychology of Popular Media Culture},
	author = {Hilgard, Joseph and Engelhardt, Christopher R. and Bartholow, Bruce D. and Rouder, Jeffrey N.},
	date = {2017},
	note = {Place: {US}
Publisher: Educational Publishing Foundation},
	keywords = {Statistical Probability, Null Hypothesis Testing, Statistical Significance, Aggressive Behavior, Bayesian Analysis, Computer Games, Violence},
	file = {How much evidence is p > .05? Stimulus pre-testing and null primary outcomes in violent video games research:/Users/tom/Zotero/storage/LTL5DVCD/hilgard2015.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/29N5JUFA/2015-55821-001.html:text/html},
}

@article{glasziou_assessing_2004,
	title = {Assessing the quality of research},
	volume = {328},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC313908/},
	abstract = {Inflexible use of evidence hierarchies confuses practitioners and irritates researchers. So how can we improve the way we assess research?},
	pages = {39--41},
	number = {7430},
	journaltitle = {{BMJ} : British Medical Journal},
	shortjournal = {{BMJ}},
	author = {Glasziou, Paul and Vandenbroucke, Jan and Chalmers, Iain},
	urldate = {2022-07-06},
	date = {2004-01-03},
	pmid = {14703546},
	pmcid = {PMC313908},
	file = {Glasziou et al. - 2004 - Assessing the quality of research.pdf:/Users/tom/Zotero/storage/LKQS5Y59/Glasziou et al. - 2004 - Assessing the quality of research.pdf:application/pdf},
}

@article{hartling_risk_2009,
	title = {Risk of bias versus quality assessment of randomised controlled trials: cross sectional study},
	volume = {339},
	rights = {©  . This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/339/bmj.b4012},
	doi = {10.1136/bmj.b4012},
	shorttitle = {Risk of bias versus quality assessment of randomised controlled trials},
	abstract = {Objectives To evaluate the risk of bias tool, introduced by the Cochrane Collaboration for assessing the internal validity of randomised trials, for inter-rater agreement, concurrent validity compared with the Jadad scale and Schulz approach to allocation concealment, and the relation between risk of bias and effect estimates.
Design Cross sectional study.
Study sample 163 trials in children.
Main outcome measures Inter-rater agreement between reviewers assessing trials using the risk of bias tool (weighted κ), time to apply the risk of bias tool compared with other approaches to quality assessment (paired t test), degree of correlation for overall risk compared with overall quality scores (Kendall’s τ statistic), and magnitude of effect estimates for studies classified as being at high, unclear, or low risk of bias (metaregression).
Results Inter-rater agreement on individual domains of the risk of bias tool ranged from slight (κ=0.13) to substantial (κ=0.74). The mean time to complete the risk of bias tool was significantly longer than for the Jadad scale and Schulz approach, individually or combined (8.8 minutes ({SD} 2.2) per study v 2.0 ({SD} 0.8), P{\textless}0.001). There was low correlation between risk of bias overall compared with the Jadad scores (P=0.395) and Schulz approach (P=0.064). Effect sizes differed between studies assessed as being at high or unclear risk of bias (0.52) compared with those at low risk (0.23).
Conclusions Inter-rater agreement varied across domains of the risk of bias tool. Generally, agreement was poorer for those items that required more judgment. There was low correlation between assessments of overall risk of bias and two common approaches to quality assessment: the Jadad scale and Schulz approach to allocation concealment. Overall risk of bias as assessed by the risk of bias tool differentiated effect estimates, with more conservative estimates for studies at low risk.},
	pages = {b4012},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Hartling, Lisa and Ospina, Maria and Liang, Yuanyuan and Dryden, Donna M. and Hooton, Nicola and Seida, Jennifer Krebs and Klassen, Terry P.},
	urldate = {2022-07-06},
	date = {2009-10-19},
	langid = {english},
	pmid = {19841007},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/DAPVCPCK/Hartling et al. - 2009 - Risk of bias versus quality assessment of randomis.pdf:application/pdf;Risk of bias versus quality assessment of randomised controlled trials\: cross sectional study:/Users/tom/Zotero/storage/AKL89W4L/hartling2009.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6TGTALZQ/bmj.b4012.html:text/html},
}

@article{lamberink_clinical_2022,
	title = {Clinical trial registration patterns and changes in primary outcomes of randomized clinical trials from 2002 to 2017},
	volume = {182},
	issn = {2168-6106},
	url = {https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2791913},
	doi = {10.1001/jamainternmed.2022.1551},
	pages = {779},
	number = {7},
	journaltitle = {{JAMA} Internal Medicine},
	shortjournal = {{JAMA} Intern Med},
	author = {Lamberink, Herm J. and Vinkers, Christiaan H. and Lancee, Michelle and Damen, Johanna A. A. and Bouter, Lex M. and Otte, Willem M. and Tijdink, Joeri K.},
	urldate = {2022-07-06},
	date = {2022-07-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/BPFZAS5U/Lamberink et al. - 2022 - Clinical Trial Registration Patterns and Changes i.pdf:application/pdf},
}

@article{herbison_adjustment_2006,
	title = {Adjustment of meta-analyses on the basis of quality scores should be abandoned},
	volume = {59},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(06)00128-4/fulltext},
	doi = {10.1016/j.jclinepi.2006.03.008},
	pages = {1249.e1--1249.e11},
	number = {12},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Herbison, Peter and Hay-Smith, Jean and Gillespie, William J.},
	urldate = {2022-07-06},
	date = {2006-12-01},
	note = {Publisher: Elsevier},
	keywords = {Bias, Meta-analysis, Systematic reviews, {RCT}, Adjusting for quality, Quality score},
	file = {Herbison et al. - 2006 - Adjustment of meta-analyses on the basis of qualit.pdf:/Users/tom/Zotero/storage/IV5EI5ZG/Herbison et al. - 2006 - Adjustment of meta-analyses on the basis of qualit.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/HWQPRMMG/fulltext.html:text/html},
}

@article{costa_pedros_2013,
	title = {{PEDro}'s bias: summary quality scores should not be used in meta-analysis},
	volume = {66},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(12)00277-6/fulltext},
	doi = {10.1016/j.jclinepi.2012.08.003},
	shorttitle = {{PEDro}'s bias},
	pages = {75--77},
	number = {1},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Costa, Bruno R. da and Hilfiker, Roger and Egger, Matthias},
	urldate = {2022-07-06},
	date = {2013-01-01},
	pmid = {23177896},
	note = {Publisher: Elsevier},
	file = {Costa et al. - 2013 - PEDro's bias summary quality scores should not be.pdf:/Users/tom/Zotero/storage/NVZTQZC2/Costa et al. - 2013 - PEDro's bias summary quality scores should not be.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N3MQV3RF/fulltext.html:text/html},
}

@article{valentine_systematic_2008,
	title = {A systematic and transparent approach for assessing the methodological quality of intervention effectiveness research: the Study Design and Implementation Assessment Device (Study {DIAD})},
	volume = {13},
	issn = {1082-989X},
	doi = {10.1037/1082-989X.13.2.130},
	shorttitle = {A systematic and transparent approach for assessing the methodological quality of intervention effectiveness research},
	abstract = {Assessments of studies meant to evaluate the effectiveness of interventions, programs, and policies can serve an important role in the interpretation of research results. However, evidence suggests that available quality assessment tools have poor measurement characteristics and can lead to opposing conclusions when applied to the same body of studies. These tools tend to (a) be insufficiently operational, (b) rely on arbitrary post-hoc decision rules, and (c) result in a single number to represent a multidimensional construct. In response to these limitations, a multilevel and hierarchical instrument was developed in consultation with a wide range of methodological and statistical experts. The instrument focuses on the operational details of studies and results in a profile of scores instead of a single score to represent study quality. A pilot test suggested that satisfactory between-judge agreement can be obtained using well-trained raters working in naturalistic conditions. Limitations of the instrument are discussed, but these are inherent in making decisions about study quality given incomplete reporting and in the absence of strong, contextually based information about the effects of design flaws on study outcomes.},
	pages = {130--149},
	number = {2},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychol Methods},
	author = {Valentine, Jeffrey C. and Cooper, Harris},
	date = {2008-06},
	pmid = {18557682},
	keywords = {Psychology, Data Interpretation, Statistical, Humans},
	file = {Valentine and Cooper - 2008 - A systematic and transparent approach for assessin.pdf:/Users/tom/Zotero/storage/B56FVKNP/Valentine and Cooper - 2008 - A systematic and transparent approach for assessin.pdf:application/pdf},
}

@article{polanin_review_2017-1,
	title = {A Review of Meta-Analysis Packages in R},
	volume = {42},
	issn = {1076-9986, 1935-1054},
	url = {http://journals.sagepub.com/doi/10.3102/1076998616674315},
	doi = {10.3102/1076998616674315},
	abstract = {Meta-analysis is a statistical technique that allows an analyst to synthesize effect sizes from multiple primary studies. To estimate meta-analysis models, the open-source statistical environment R is quickly becoming a popular choice. The meta-analytic community has contributed to this growth by developing numerous packages specific to meta-analysis. The purpose of this study is to locate all publicly available meta-analytic R packages. We located 63 packages via a comprehensive online search. To help elucidate these functionalities to the field, we describe each of the packages, recommend applications for researchers interested in using R for meta-analyses, provide a brief tutorial of two meta-analysis packages, and make suggestions for future meta-analytic R package creators.},
	pages = {206--242},
	number = {2},
	journaltitle = {Journal of Educational and Behavioral Statistics},
	shortjournal = {Journal of Educational and Behavioral Statistics},
	author = {Polanin, Joshua R. and Hennessy, Emily A. and Tanner-Smith, Emily E.},
	urldate = {2022-07-07},
	date = {2017-04},
	langid = {english},
	file = {A Review of Meta-Analysis Packages in R:/Users/tom/Zotero/storage/C268ZZX5/polanin2016.pdf.pdf:application/pdf},
}

@article{haddaway_citationchaser_nodate,
	title = {Citationchaser: A tool for transparent and efficient forward and backward citation chasing in systematic searching},
	volume = {n/a},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1563},
	doi = {10.1002/jrsm.1563},
	shorttitle = {Citationchaser},
	abstract = {Systematic searching aims to find all possibly relevant research from multiple sources, the basis for an unbiased and comprehensive evidence base. Along with bibliographic databases, systematic reviewers use a variety of additional methods to minimise procedural bias. Citation chasing exploits connections between research articles to identify relevant records for a review by making use of explicit mentions of one article within another. Citation chasing is a popular supplementary search method because it helps to build on the work of primary research and review authors. It does so by identifying potentially relevant studies that might otherwise not be retrieved by other search methods; for example, because they did not use the review authors' search terms in the specified combinations in their titles, abstracts, or keywords. Here, we briefly provide an overview of citation chasing as a method for systematic reviews. Furthermore, given the challenges and high resource requirements associated with citation chasing, the limited application of citation chasing in otherwise rigorous systematic reviews, and the potential benefit of identifying terminologically disconnected but semantically linked research studies, we have developed and describe a free and open source tool that allows for rapid forward and backward citation chasing. We introduce citationchaser, an R package and Shiny app for conducting forward and backward citation chasing from a starting set of articles. We describe the sources of data, the backend code functionality, and the user interface provided in the Shiny app.},
	issue = {n/a},
	journaltitle = {Research Synthesis Methods},
	author = {Haddaway, Neal R. and Grainger, Matthew J. and Gray, Charles T.},
	urldate = {2022-07-07},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1563},
	keywords = {bibliographic checking, evidence synthesis tools, information retrieval, pearl growing, software development, systematic review tool, systematic searching},
	file = {Full Text PDF:/Users/tom/Zotero/storage/682IZSM8/Haddaway et al. - Citationchaser A tool for transparent and efficie.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/A2GA4DKB/jrsm.html:text/html},
}

@collection{cooper_handbook_2019,
	location = {New York},
	edition = {3rd Edition},
	title = {Handbook of research synthesis and meta-analysis},
	isbn = {978-1-61044-886-4},
	pagetotal = {1},
	publisher = {Russell Sage Foundation},
	editor = {Cooper, Harris M. and Hedges, Larry V. and Valentine, Jeff C.},
	date = {2019},
	langid = {english},
	keywords = {Methodology, Research, Statistical methods, Handbooks, manuals, etc, Information storage and retrieval systems},
	file = {Cooper et al. - 2019 - Handbook of research synthesis and meta-analysis.pdf:/Users/tom/Zotero/storage/8EJA323X/Cooper et al. - 2019 - Handbook of research synthesis and meta-analysis.pdf:application/pdf},
}

@article{ioannidis_reasons_2008-1,
	title = {Reasons or excuses for avoiding meta-analysis in forest plots},
	volume = {336},
	rights = {© {BMJ} Publishing Group Ltd 2008},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/content/336/7658/1413},
	doi = {10.1136/bmj.a117},
	abstract = {{\textless}p{\textgreater}Heterogeneous data are a common problem in meta-analysis. \textbf{John Ioannidis}, \textbf{Nikolaos Patsopoulos}, and \textbf{Hannah Rothstein} show that final synthesis is possible and desirable in most cases {\textless}/p{\textgreater}},
	pages = {1413--1415},
	number = {7658},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Ioannidis, John P. A. and Patsopoulos, Nikolaos A. and Rothstein, Hannah R.},
	urldate = {2022-07-08},
	date = {2008-06-19},
	langid = {english},
	pmid = {18566080},
	note = {Publisher: British Medical Journal Publishing Group
Section: Analysis},
	file = {Full Text:/Users/tom/Zotero/storage/Z4ED639M/Ioannidis et al. - 2008 - Reasons or excuses for avoiding meta-analysis in f.pdf:application/pdf;Reasons or excuses for avoiding meta-analysis in forest plots:/Users/tom/Zotero/storage/9UVAW9GA/ioannidis2008.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/QFD3T7FG/1413.html:text/html},
}

@article{pigott_outcome-reporting_2013,
	title = {Outcome-reporting bias in education research},
	volume = {42},
	issn = {0013-189X},
	url = {https://doi.org/10.3102/0013189X13507104},
	doi = {10.3102/0013189X13507104},
	abstract = {Outcome-reporting bias occurs when primary studies do not include information about all outcomes measured in a study. When studies omit findings on important measures, efforts to synthesize the research using systematic review techniques will be biased and interpretations of individual studies will be incomplete. Outcome-reporting bias has been well documented in medicine and has been shown to lead to inaccurate assessments of the effects of medical treatments and, in some cases, to omission of reports of harms. This study examines outcome-reporting bias in educational research by comparing the reports of educational interventions from dissertations to their published versions. We find that nonsignificant outcomes were 30\% more likely to be omitted from a published study than statistically significant ones.},
	pages = {424--432},
	number = {8},
	journaltitle = {Educational Researcher},
	shortjournal = {Educational Researcher},
	author = {Pigott, Therese D. and Valentine, Jeffrey C. and Polanin, Joshua R. and Williams, Ryan T. and Canada, Dericka D.},
	urldate = {2022-07-08},
	date = {2013-11-01},
	langid = {english},
	note = {Publisher: American Educational Research Association},
	keywords = {meta-analysis, research methodology, communication, experimental research, program evaluation, research utilization},
	file = {Outcome-Reporting Bias in Education Research:/Users/tom/Zotero/storage/T3JH7NUF/pigott2013.pdf.pdf:application/pdf;Submitted Version:/Users/tom/Zotero/storage/92KET2CE/Pigott et al. - 2013 - Outcome-Reporting Bias in Education Research.pdf:application/pdf},
}

@article{torgerson_contamination_2001,
	title = {Contamination in trials: is cluster randomisation the answer?},
	volume = {322},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1119583/},
	shorttitle = {Contamination in trials},
	pages = {355--357},
	number = {7282},
	journaltitle = {{BMJ} : British Medical Journal},
	shortjournal = {{BMJ}},
	author = {Torgerson, David J},
	urldate = {2022-07-08},
	date = {2001-02-10},
	pmid = {11159665},
	pmcid = {PMC1119583},
}

@article{schafer_meaningfulness_2019,
	title = {The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases},
	volume = {10},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813},
	shorttitle = {The Meaningfulness of Effect Sizes in Psychological Research},
	abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes—when is an effect small, medium, or large?—has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
	journaltitle = {Frontiers in Psychology},
	author = {Schäfer, Thomas and Schwarz, Marcus A.},
	urldate = {2022-07-11},
	date = {2019},
	file = {Full Text PDF:/Users/tom/Zotero/storage/H2UIEW5E/Schäfer and Schwarz - 2019 - The Meaningfulness of Effect Sizes in Psychologica.pdf:application/pdf},
}

@report{tiokhin_shifting_2021,
	title = {Shifting the level of selection in science},
	url = {https://osf.io/preprints/metaarxiv/juwck/},
	abstract = {Criteria for recognizing and rewarding scientists typically focus on individual contributions. This creates a conflict between what is best for scientists’ careers and what is best for science. In this paper, we show how principles from the theory of multilevel selection provide a toolkit for modifying incentives to better align individual and collective interests. A core principle is the need to shift the level at which selection operates, from individuals to the groups in which individuals are embedded. This principle is used in several fields to improve collective outcomes, including animal husbandry, professional sports, and professional organizations. Shifting the level of selection has the potential to ameliorate several problems in contemporary science, including accounting for scientists’ indirect contributions, reducing individual-level competition, and promoting specialization. We discuss the difficulties associated with changing the level of selection and outline directions for future development in this domain.},
	institution = {{MetaArXiv}},
	author = {Tiokhin, Leo and Panchanathan, Karthik and Smaldino, Paul E. and Lakens, Daniel},
	urldate = {2022-07-11},
	date = {2021-10-28},
	langid = {english},
	doi = {10.31222/osf.io/juwck},
	note = {type: article},
	keywords = {Social and Behavioral Sciences, cooperation, competition, toread, evolution, indirect contributions, multilevel-selection theory, recognition and rewards, scientific evaluation, specialization, team science},
	file = {Full Text PDF:/Users/tom/Zotero/storage/5KEAWISH/Tiokhin et al. - 2021 - Shifting the level of selection in science.pdf:application/pdf},
}

@article{ross_many_2022,
	title = {Many analysts and few incentives},
	issn = {2153-599X, 2153-5981},
	url = {https://www.tandfonline.com/doi/full/10.1080/2153599X.2022.2070248},
	doi = {10.1080/2153599X.2022.2070248},
	pages = {1--3},
	journaltitle = {Religion, Brain \& Behavior},
	shortjournal = {Religion, Brain \& Behavior},
	author = {Ross, Robert M. and Sulik, Justin and Buczny, Jacek and Schivinski, Bruno},
	urldate = {2022-07-24},
	date = {2022-07-06},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/USVSLRB8/Ross et al. - 2022 - Many analysts and few incentives.pdf:application/pdf},
}

@article{purgar_quantifying_2022,
	title = {Quantifying research waste in ecology},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-022-01820-0},
	doi = {10.1038/s41559-022-01820-0},
	abstract = {Research inefficiencies can generate huge waste: evidence from biomedical research has shown that most research is avoidably wasted and steps have been taken to tackle this costly problem. Although other scientific fields could also benefit from identifying and quantifying waste and acting to reduce it, no other estimates of research waste are available. Given that ecological issues interweave most of the United Nations Sustainable Development Goals, we argue that tackling research waste in ecology should be prioritized. Our study leads the way. We estimate components of waste in ecological research based on a literature review and a meta-analysis. Shockingly, our results suggest only 11–18\% of conducted ecological research reaches its full informative value. All actors within the research system—including academic institutions, policymakers, funders and publishers—have a duty towards science, the environment, study organisms and the public, to urgently act and reduce this considerable yet preventable loss. We discuss potential ways forward and call for two major actions: (1) further research into waste in ecology (and beyond); (2) focused development and implementation of solutions to reduce unused potential of ecological research.},
	pages = {1--8},
	journaltitle = {Nature Ecology \& Evolution},
	shortjournal = {Nat Ecol Evol},
	author = {Purgar, Marija and Klanjscek, Tin and Culina, Antica},
	urldate = {2022-07-26},
	date = {2022-07-21},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Scientific community, Ecology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ZZ25YQ5S/Purgar et al. - 2022 - Quantifying research waste in ecology.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/FNCEJGIL/s41559-022-01820-0.html:text/html},
}

@article{rochios_are_nodate,
	title = {Are we all on the same page? Subfield differences in open science practices in psychology},
	volume = {n/a},
	issn = {1522-7219},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/icd.2361},
	doi = {10.1002/icd.2361},
	shorttitle = {Are we all on the same page?},
	abstract = {Although open science has become a popular tool to combat the replication crisis, it is unclear whether the uptake of open science practices has been consistent across the field of psychology. In this study, we were particularly interested in whether claims that developmental psychology lags behind other subfields in adopting open science practices were valid. To test this, we determined whether data and material sharing differed as a function of psychological subfield at the distinguished journal, Psychological Science. The results showed that open data and open materials scores increased from 2014–2015 to 2019–2020. Of note, articles published in the field of developmental psychology generated lower open data and open materials scores than articles published in cognition; however, scores were similar to articles published in social psychology. Across Psychological Science articles, shared data and materials were seldom accompanied by documentation that is likely to make shared research objects useful. These findings are discussed in the context of the unique challenges faces by developmental psychologists and how journals can more effectively encourage authors to practice open science across psychology.},
	pages = {e2361},
	issue = {n/a},
	journaltitle = {Infant and Child Development},
	author = {Rochios, Christina and Richmond, Jenny L.},
	urldate = {2022-07-26},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/icd.2361},
	keywords = {open data, open materials, developmental psychology, subfield differences},
	file = {Full Text PDF:/Users/tom/Zotero/storage/TJ9ZITZR/Rochios and Richmond - Are we all on the same page Subfield differences .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/C56KUUSE/icd.html:text/html},
}

@article{hohn_empirical_2020,
	title = {An empirical review of research and reporting practices in psychological meta-analyses},
	volume = {24},
	issn = {1089-2680},
	url = {https://doi.org/10.1177/1089268020918844},
	doi = {10.1177/1089268020918844},
	abstract = {As meta-analytic studies have come to occupy a sizable contingent of published work in the psychological sciences, clarity in the research and reporting practices of such work is crucial to the interpretability and reproducibility of research findings. The present study examines the state of research and reporting practices within a random sample of 384 published psychological meta-analyses across several important dimensions (e.g., search methods, exclusion criteria, statistical techniques). In addition, we surveyed the first authors of the meta-analyses in our sample to ask them directly about the research practices employed and reporting decisions made in their studies, including the assessments and procedures they conducted and the guidelines or materials they relied on. Upon cross-validating the first author responses with what was reported in their published meta-analyses, we identified numerous potential gaps in reporting and research practices. In addition to providing a survey of recent reporting practices, our findings suggest that (a) there are several research practices conducted by meta-analysts that are ultimately not reported; (b) some aspects of meta-analysis research appear to be conducted at disappointingly low rates; and (c) the adoption of the reporting standards, including the Meta-Analytic Reporting Standards ({MARS}), has been slow to nonexistent within psychological meta-analytic research.},
	pages = {195--209},
	number = {3},
	journaltitle = {Review of General Psychology},
	shortjournal = {Review of General Psychology},
	author = {Hohn, Richard E. and Slaney, Kathleen L. and Tafreshi, Donna},
	urldate = {2022-07-27},
	date = {2020-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {open science, meta-analysis, Meta-Analysis Reporting Standards, meta-analytic practices},
	file = {An Empirical Review of Research and Reporting Practices in Psychological Meta-Analyses:/Users/tom/Zotero/storage/WLQR868Y/10.1177@1089268020918844.pdf.pdf:application/pdf},
}

@article{schonenberger_meta-research_2022,
	title = {A meta-research study of randomized controlled trials found infrequent and delayed availability of protocols},
	volume = {149},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435622001366},
	doi = {10.1016/j.jclinepi.2022.05.014},
	abstract = {Objectives
Availability of randomized controlled trial ({RCT}) protocols is essential for the interpretation of trial results and research transparency.
Study Design and Setting
In this study, we determined the availability of {RCT} protocols approved in Switzerland, Canada, Germany, and the United Kingdom in 2012. For these {RCTs}, we searched {PubMed}, Google Scholar, Scopus, and trial registries for publicly available protocols and corresponding full-text publications of results. We determined the proportion of {RCTs} with (1) publicly available protocols, (2) publications citing the protocol, and (3) registries providing a link to the protocol. A multivariable logistic regression model explored factors associated with protocol availability.
Results
Three hundred twenty-six {RCTs} were included, of which 118 (36.2\%) made their protocol publicly available; 56 (47.6\% 56 of 118) provided as a peer-reviewed publication and 48 (40.7\%, 48 of 118) provided as supplementary material. A total of 90.9\% (100 of 110) of the protocols were cited in the main publication, and 55.9\% (66 of 118) were linked in the clinical trial registry. Larger sample size ({\textgreater}500; odds ratio [{OR}] = 5.90, 95\% confidence interval [{CI}], 2.75–13.31) and investigator sponsorship ({OR} = 1.99, 95\% {CI}, 1.11–3.59) were associated with increased protocol availability. Most protocols were made available shortly before the publication of the main results.
Conclusion
{RCT} protocols should be made available at an early stage of the trial.},
	pages = {45--52},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Schönenberger, Christof Manuel and Griessbach, Alexandra and Taji Heravi, Ala and Gryaznov, Dmitry and Gloy, Viktoria L. and Lohner, Szimonetta and Klatte, Katharina and Ghosh, Nilabh and Lee, Hopin and Mansouri, Anita and Marian, Ioana R. and Saccilotto, Ramon and Nury, Edris and Busse, Jason W. and von Niederhäusern, Belinda and Mertz, Dominik and Blümle, Anette and Odutayo, Ayodele and Hopewell, Sally and Speich, Benjamin and Briel, Matthias},
	urldate = {2022-08-03},
	date = {2022-09-01},
	langid = {english},
	keywords = {Transparency, Randomized controlled trials, Meta-research, Trial registration, Protocol publication, Trial protocols},
	file = {ScienceDirect Full Text PDF:/Users/tom/Zotero/storage/BYAGFJBP/Schönenberger et al. - 2022 - A meta-research study of randomized controlled tri.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/XSGJEC9T/S0895435622001366.html:text/html},
}

@article{claesen_comparing_2021,
	title = {Comparing dream to reality: an assessment of adherence of the first generation of preregistered studies},
	volume = {8},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.211037},
	doi = {10.1098/rsos.211037},
	shorttitle = {Comparing dream to reality},
	abstract = {Preregistration is a method to increase research transparency by documenting research decisions on a public, third-party repository prior to any influence by data. It is becoming increasingly popular in all subfields of psychology and beyond. Adherence to the preregistration plan may not always be feasible and even is not necessarily desirable, but without disclosure of deviations, readers who do not carefully consult the preregistration plan might get the incorrect impression that the study was exactly conducted and reported as planned. In this paper, we have investigated adherence and disclosure of deviations for all articles published with the Preregistered badge in Psychological Science between February 2015 and November 2017 and shared our findings with the corresponding authors for feedback. Two out of 27 preregistered studies contained no deviations from the preregistration plan. In one study, all deviations were disclosed. Nine studies disclosed none of the deviations. We mainly observed (un)disclosed deviations from the plan regarding the reported sample size, exclusion criteria and statistical analysis. This closer look at preregistrations of the first generation reveals possible hurdles for reporting preregistered studies and provides input for future reporting guidelines. We discuss the results and possible explanations, and provide recommendations for preregistered research.},
	pages = {211037},
	number = {10},
	journaltitle = {Royal Society Open Science},
	author = {Claesen, Aline and Gomes, Sara and Tuerlinckx, Francis and Vanpaemel, Wolf},
	urldate = {2022-08-05},
	date = {2021},
	note = {Publisher: Royal Society},
	keywords = {open science, preregistration, transparency, researcher degrees of freedom, psychological science},
	file = {Full Text PDF:/Users/tom/Zotero/storage/LFQFXXEJ/Claesen et al. - Comparing dream to reality an assessment of adher.pdf:application/pdf},
}

@article{grund_using_2022,
	title = {Using synthetic data to improve the reproducibility of statistical results in psychological research},
	issn = {1939-1463},
	doi = {10.1037/met0000526},
	abstract = {In recent years, psychological research has faced a credibility crisis, and open data are often regarded as an important step toward a more reproducible psychological science. However, privacy concerns are among the main reasons that prevent data sharing. Synthetic data procedures, which are based on the multiple imputation ({MI}) approach to missing data, can be used to replace sensitive data with simulated values, which can be analyzed in place of the original data. One crucial requirement of this approach is that the synthesis model is correctly specified. In this article, we investigated the statistical properties of synthetic data with a particular emphasis on the reproducibility of statistical results. To this end, we compared conventional approaches to synthetic data based on {MI} with a data-augmented approach ({DA}-{MI}) that attempts to combine the advantages of masking methods and synthetic data, thus making the procedure more robust to misspecification. In multiple simulation studies, we found that the good properties of the {MI} approach strongly depend on the correct specification of the synthesis model, whereas the {DA}-{MI} approach can provide useful results even under various types of misspecification. This suggests that the {DA}-{MI} approach to synthetic data can provide an important tool that can be used to facilitate data sharing and improve reproducibility in psychological research. In a working example, we also demonstrate the implementation of these approaches in widely available software, and we provide recommendations for practice. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {No Pagination Specified--No Pagination Specified},
	journaltitle = {Psychological Methods},
	author = {Grund, Simon and Lüdtke, Oliver and Robitzsch, Alexander},
	date = {2022},
	keywords = {Privacy, Computer Simulation, Open Science, Data Sharing, Simulation, Behavioral Sciences, Credibility, Statistical Data},
}

@article{prasad_constructive_2022,
	title = {Constructive and obsessive criticism in science},
	volume = {n/a},
	issn = {1365-2362},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/eci.13839},
	doi = {10.1111/eci.13839},
	abstract = {Social media and new tools for engagement offer democratic platforms for enhancing constructive scientific criticism which had previously been limited. Constructive criticism can now be massive, timely and open. However, new options have also enhanced obsessive criticism. Obsessive criticism tends to focus on one or a handful of individuals and their work, often includes ad hominem aspects, and the critics often lack field-specific skills and technical expertise. Typical behaviours include: repetitive and persistent comments (including sealioning), lengthy commentaries/tweetorials/responses often longer than the original work, strong degree of moralizing, distortion of the underlying work, argumentum ad populum, calls to suspend/censor/retract the work or the author, guilt-by-association, reputational tarnishing, large gains in followers specifically through attacks, finding and positing sensitive personal information, anonymity or pseudonymity, social media campaigning, and unusual ratio of criticism to pursuit of one's research agenda. These behaviours may last months or years. Prevention and treatment options may include awareness, identifying and working around aggravating factors, placing limits on the volume by editors, constructive pairing of commissioned editorials, incorporation of some hot debates from unregulated locations such as social media or {PubPeer} to the pages of scientific journals, preserving decency and focusing on evidence and arguments and avoiding personal statements, or (in some cases) ignoring. We need more research on the role of social media and obsessive criticism on an evolving cancel culture, the social media credibility, the use/misuse of anonymity and pseudonymity, and whether potential interventions from universities may improve or further weaponize scientific criticism.},
	pages = {e13839},
	issue = {n/a},
	journaltitle = {European Journal of Clinical Investigation},
	author = {Prasad, Vinay and Ioannidis, John P. A.},
	urldate = {2022-08-06},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/eci.13839},
	keywords = {criticism, social media, cancel culture, Twitter, peer-review},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MLR4YCGJ/Prasad and Ioannidis - Constructive and obsessive criticism in science.pdf:application/pdf},
}

@report{brodeur_pre-registration_2022,
	location = {Rochester, {NY}},
	title = {Do pre-registration and pre-analysis plans reduce p-hacking and publication bias?},
	url = {https://papers.ssrn.com/abstract=4180594},
	abstract = {Randomized controlled trials ({RCTs}) are increasingly prominent in economics, with pre-registration and pre-analysis plans ({PAPs}) promoted as important in ensuring the credibility of findings. We investigate whether these tools reduce the extent of p-hacking and publication bias by collecting and studying the universe of test statistics, 15,992 in total, from {RCTs} published in 15 leading economics journals from 2018 through 2021. In our primary analysis, we find no meaningful difference in the distribution of test statistics from pre-registered studies, compared to their non-pre-registered counterparts. However, pre-registered studies that have a complete {PAP} are significantly less p-hacked. These results point to the importance of {PAPs}, rather than pre-registration in itself, in ensuring credibility.},
	number = {4180594},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Brodeur, Abel and Cook, Nikolai and Hartley, Jonathan and Heyes, Anthony},
	urldate = {2022-08-06},
	date = {2022-08-03},
	langid = {english},
	keywords = {Pre-registration, Publication Bias, p-Hacking, Pre-analysis Plan, Research Credibility},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NFXEZFLE/Brodeur et al. - 2022 - Do Pre-Registration and Pre-Analysis Plans Reduce .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8S3ID9GX/papers.html:text/html},
}

@article{bianchi_measuring_2022,
	title = {Measuring the effect of reviewers on manuscript change: A study on a sample of submissions to Royal Society journals (2006–2017)},
	volume = {16},
	issn = {1751-1577},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000682},
	doi = {10.1016/j.joi.2022.101316},
	shorttitle = {Measuring the effect of reviewers on manuscript change},
	abstract = {Peer review is key for public trust of academic journals. It ensures that only rigorous research is published but also helps authors to increase the value of their manuscripts through feedback from reviewers. However, measuring the developmental value of peer review is difficult as it requires fine-grained manuscript data on various stages of the editorial process, which are rarely available. To fill this gap, we accessed complete data from Royal Society journals from 2006 to 2017, and measured manuscript changes during peer review from their initial submissions. We then estimated the effect of the number of reviewers and the evaluation of reviewers on manuscript development and their citations after publication. We found that the number of reviewers had an almost linear effect on manuscript change although with decreasing marginal effects whenever more than two reviewers were involved. This effect did not depend on the initial quality of manuscripts. We also found that changes due to reviewers tended to increase a manuscript’s probability of being cited at least once after publication. While our findings show the multiple functions of peer review for manuscript development, research with larger and more representative journal samples is needed to develop more systematic measures that reflect the complexity of peer review.},
	pages = {101316},
	number = {3},
	journaltitle = {Journal of Informetrics},
	shortjournal = {Journal of Informetrics},
	author = {Bianchi, Federico and García-Costa, Daniel and Grimaldo, Francisco and Squazzoni, Flaminio},
	urldate = {2022-08-09},
	date = {2022-08-01},
	langid = {english},
	keywords = {Peer review, Journals, Reviewers, Manuscript changes, Manuscripts},
	file = {Bianchi et al. - 2022 - Measuring the effect of reviewers on manuscript ch.pdf:/Users/tom/Zotero/storage/NS2BJ35J/Bianchi et al. - 2022 - Measuring the effect of reviewers on manuscript ch.pdf:application/pdf},
}

@article{budd_ten_2015,
	title = {Ten simple rules for organizing an unconference},
	volume = {11},
	issn = {1553-734X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4310607/},
	doi = {10.1371/journal.pcbi.1003905},
	pages = {e1003905},
	number = {1},
	journaltitle = {{PLoS} Computational Biology},
	shortjournal = {{PLoS} Comput Biol},
	author = {Budd, Aidan and Dinkel, Holger and Corpas, Manuel and Fuller, Jonathan C. and Rubinat, Laura and Devos, Damien P. and Khoueiry, Pierre H. and Förstner, Konrad U. and Georgatos, Fotis and Rowland, Francis and Sharan, Malvika and Binder, Janos X. and Grace, Tom and Traphagen, Karyn and Gristwood, Adam and Wood, Natasha T.},
	urldate = {2022-08-10},
	date = {2015-01-29},
	pmid = {25633715},
	pmcid = {PMC4310607},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/BDIB2URX/Budd et al. - 2015 - Ten Simple Rules for Organizing an Unconference.pdf:application/pdf;Ten Simple Rules for Organizing an Unconference:/Users/tom/Zotero/storage/XARX2APB/budd2015.pdf.pdf:application/pdf},
}

@article{baldwin_protecting_2022,
	title = {Protecting against researcher bias in secondary data analysis: challenges and potential solutions},
	volume = {37},
	issn = {1573-7284},
	url = {https://doi.org/10.1007/s10654-021-00839-0},
	doi = {10.1007/s10654-021-00839-0},
	shorttitle = {Protecting against researcher bias in secondary data analysis},
	abstract = {Analysis of secondary data sources (such as cohort studies, survey data, and administrative records) has the potential to provide answers to science and society’s most pressing questions. However, researcher biases can lead to questionable research practices in secondary data analysis, which can distort the evidence base. While pre-registration can help to protect against researcher biases, it presents challenges for secondary data analysis. In this article, we describe these challenges and propose novel solutions and alternative approaches. Proposed solutions include approaches to (1) address bias linked to prior knowledge of the data, (2) enable pre-registration of non-hypothesis-driven research, (3) help ensure that pre-registered analyses will be appropriate for the data, and (4) address difficulties arising from reduced analytic flexibility in pre-registration. For each solution, we provide guidance on implementation for researchers and data guardians. The adoption of these practices can help to protect against researcher bias in secondary data analysis, to improve the robustness of research based on existing data.},
	pages = {1--10},
	number = {1},
	journaltitle = {European Journal of Epidemiology},
	shortjournal = {Eur J Epidemiol},
	author = {Baldwin, Jessie R. and Pingault, Jean-Baptiste and Schoeler, Tabea and Sallis, Hannah M. and Munafò, Marcus R.},
	urldate = {2022-08-10},
	date = {2022-01-01},
	langid = {english},
	keywords = {Open science, Pre-registration, Researcher bias, Secondary data analysis},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3A9X9IWL/Baldwin et al. - 2022 - Protecting against researcher bias in secondary da.pdf:application/pdf},
}

@article{heinl_declaration_2022,
	title = {Declaration of common standards for the preregistration of animal research—speeding up the scientific progress},
	volume = {1},
	issn = {2752-6542},
	url = {https://academic.oup.com/pnasnexus/article/doi/10.1093/pnasnexus/pgac016/6549456},
	doi = {10.1093/pnasnexus/pgac016},
	abstract = {Abstract
            Preregistration of studies is a recognized tool in clinical research to improve the quality and reporting of all gained results. In preclinical research, preregistration could boost the translation of published results into clinical breakthroughs. When studies rely on animal testing or form the basis of clinical trials, maximizing the validity and reliability of research outcomes becomes in addition an ethical obligation. Nevertheless, the implementation of preregistration in animal research is still slow. However, research institutions, funders, and publishers start valuing preregistration, and thereby level the way for its broader acceptance in the future. A total of 3 public registries, the {OSF} registry, preclinicaltrials.eu, and animalstudyregistry.org already encourage the preregistration of research involving animals. Here, they jointly declare common standards to make preregistration a valuable tool for better science. Registries should meet the following criteria: public accessibility, transparency in their financial sources, tracking of changes, and warranty and sustainability of data. Furthermore, registration templates should cover a minimum set of mandatory information and studies have to be uniquely identifiable. Finally, preregistered studies should be linked to any published outcome. To ensure that preregistration becomes a powerful instrument, publishers, funders, and institutions should refer to registries that fulfill these minimum standards.},
	pages = {pgac016},
	number = {1},
	journaltitle = {{PNAS} Nexus},
	author = {Heinl, Céline and Scholman-Végh, Anna M D and Mellor, David and Schönfelder, Gilbert and Strech, Daniel and Chamuleau, Steven and Bert, Bettina},
	editor = {Nelson, Karen E},
	urldate = {2022-08-10},
	date = {2022-04-15},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/FXRM4GW4/Heinl et al. - 2022 - Declaration of common standards for the preregistr.pdf:application/pdf},
}

@report{gerasimova_argument-based_2022,
	title = {Argument-Based Approach to Validity: Developing a Living Document and Incorporating Preregistration},
	url = {https://psyarxiv.com/t8h5g/},
	shorttitle = {Argument-Based Approach to Validity},
	abstract = {I propose two practical advances to the argument-based approach to validity: developing a living document and incorporating preregistration. First, I present a potential structure for the living document that includes an up-to-date summary of the validity argument. As the validation process may span across multiple studies, the living document allows future users of the instrument to access the entire validity argument in one place. Second, I describe how preregistration can be incorporated in the argument-based approach to validity. Specifically, I distinguish between two types of preregistration: preregistration of the argument and preregistration of validation studies. Preregistration of the argument is a single preregistration that is specified for the entire validation process. Here, the developer specifies interpretations, uses, and claims before collecting validity evidence. Preregistration of a validation study refers to preregistering a single validation study that aims to evaluate a set of claims. Here, the developer describes study components (e.g., research design, data collection, data analysis, etc.), before collecting data. Both preregistration types have the potential to reduce bias (e.g., hindsight and confirmation biases), as well as to allow others to evaluate the risk of bias and, hence, calibrate confidence, in the developer’s evaluation of the validity argument.},
	institution = {{PsyArXiv}},
	author = {Gerasimova, Daria},
	urldate = {2022-08-10},
	date = {2022-08-09},
	langid = {english},
	doi = {10.31234/osf.io/t8h5g},
	note = {type: article},
	keywords = {preregistration, Social and Behavioral Sciences, Quantitative Methods, validity, measurement, instrument development, living document, validation},
	file = {Full Text PDF:/Users/tom/Zotero/storage/KKWC4C2R/Gerasimova - 2022 - Argument-Based Approach to Validity Developing a .pdf:application/pdf},
}

@article{goodman_dirty_2008,
	title = {A dirty dozen: twelve p-value misconceptions},
	volume = {45},
	issn = {00371963},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0037196308000620},
	doi = {10.1053/j.seminhematol.2008.04.003},
	shorttitle = {A dirty dozen},
	pages = {135--140},
	number = {3},
	journaltitle = {Seminars in Hematology},
	shortjournal = {Seminars in Hematology},
	author = {Goodman, Steven},
	urldate = {2022-08-16},
	date = {2008-07},
	langid = {english},
	file = {A Dirty Dozen\: Twelve P-Value Misconceptions:/Users/tom/Zotero/storage/5D2Q5KX6/goodman2008.pdf.pdf:application/pdf;Goodman - 2008 - A Dirty Dozen Twelve P-Value Misconceptions.pdf:/Users/tom/Zotero/storage/MXBNECQC/Goodman - 2008 - A Dirty Dozen Twelve P-Value Misconceptions.pdf:application/pdf},
}

@article{campbell_access_2022,
	title = {Access to unpublished protocols and statistical analysis plans of randomised trials},
	volume = {23},
	issn = {1745-6215},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9387046/},
	doi = {10.1186/s13063-022-06641-x},
	abstract = {Background
Access to protocols and statistical analysis plans ({SAPs}) increases the transparency of randomised trial by allowing readers to identify and interpret unplanned changes to study methods, however they are often not made publicly available. We sought to determine how often study investigators would share unavailable documents upon request.

Methods
We used trials from two previously identified cohorts (cohort 1: 101 trials published in high impact factor journals between January and April of 2018; cohort 2: 100 trials published in June 2018 in journals indexed in {PubMed}) to determine whether study investigators would share unavailable protocols/{SAPs} upon request. We emailed corresponding authors of trials with no publicly available protocol or {SAP} up to four times.

Results
Overall, 96 of 201 trials (48\%) across the two cohorts had no publicly available protocol or {SAP} (11/101 high-impact cohort, 85/100 {PubMed} cohort). In total, 8/96 authors (8\%) shared some trial documentation (protocol only [n = 5]; protocol and {SAP} [n = 1]; excerpt from protocol [n = 1]; research ethics application form [n = 1]). We received protocols for 6/96 trials (6\%), and a {SAP} for 1/96 trial (1\%). Seventy-three authors (76\%) did not respond, 7 authors responded (7\%) but declined to share a protocol or {SAP}, and eight email addresses were invalid (8\%). A total of 329 emails were sent (an average of 41 emails for every trial which sent documentation). After emailing authors, the total number of trials with an available protocol increased by only 3\%, from 52\% in to 55\%.

Conclusions
Most study investigators did not share their unpublished protocols or {SAPs} upon direct request. Alternative strategies are needed to increase transparency of randomised trials and ensure access to protocols and {SAPs}.

Supplementary Information
The online version contains supplementary material available at 10.1186/s13063-022-06641-x.},
	pages = {674},
	journaltitle = {Trials},
	shortjournal = {Trials},
	author = {Campbell, David and {McDonald}, Cassandra and Cro, Suzie and Jairath, Vipul and Kahan, Brennan C.},
	urldate = {2022-08-21},
	date = {2022-08-17},
	pmid = {35978391},
	pmcid = {PMC9387046},
}

@article{cook_preregistration_2022,
	title = {Preregistration of randomized controlled trials},
	rights = {© The Author(s) 2022},
	url = {https://journals.sagepub.com/eprint/6QFYFKNXI5SZKUWPXSFH/full},
	doi = {10.1177/10497315221121117},
	shorttitle = {Preregistration of randomized controlled trials},
	abstract = {Randomized controlled trials ({RCTs}) are designed to answer causal questions with internal validity. However, threats to internal validity exist for even well-de...},
	journaltitle = {Research on Social Work Practice},
	author = {Cook, Bryan G. and Wong, Vivian C. and Fleming, Jesse I. and Solari, Emily J.},
	urldate = {2022-08-22},
	date = {2022-08-22},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	file = {Snapshot:/Users/tom/Zotero/storage/982RT4GG/full.html:text/html},
}

@article{marinos_verbal_2022,
	title = {Verbal manipulations of learning expectancy do not enhance reconsolidation},
	volume = {17},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0241943},
	doi = {10.1371/journal.pone.0241943},
	abstract = {Objectives Pharmacological studies using propranolol suggest that if reactivation signals that new information will be learned (i.e., there is an expectation for learning) reconsolidation can be enhanced. We examined if the verbal instructions to expect new learning will enhance reconsolidation of fear memories using the post-retrieval extinction paradigm. Methods On day one, participants (n = 48) underwent differential fear conditioning to two images ({CS}+ and {CS}-). On day two, participants were randomly assigned to one of three groups; groups one and two had their memory for the {CS}+ reactivated (i.e., a single presentation of the {CS}+) 10 minutes prior to extinction, whereas group three did not have their memory reactivated but went right to extinction (no reactivation group). One reactivation group was told that they would learn something new about the images (expectation for learning group), and the other group was told that they would not learn anything new (no expectation for learning group). On day three, return of fear was measured following reinstatement (i.e., four shocks). Fear potentiated startle ({FPS}) and skin conductance response ({SCR}) were measured throughout. Results There was evidence of fear acquisition for participants for {SCR} but not {FPS}. With regards to reconsolidation, {SCR} increased for the {CS}+ and {CS}-in all groups from the end of extinction to the beginning of re-extinction (i.e., return of fear). For {FPS}, post-hoc tests conducted on the sub-group of participants showing fear learning showed that {FPS} remained stable in the two reactivation groups, but increased to the {CS}+, but not the {CS}- in the no reactivation group. Implications These findings suggest that a verbal manipulation of the expectation for learning may not be salient enough to enhance reconsolidation. Results are discussed in relation to theories on differences in between {SCR}, as a measure of cognitive awareness, and {FPS}, as a measure of fear.},
	pages = {e0241943},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Marinos, Julia and Simioni, Olivia and Ashbaugh, Andrea R.},
	urldate = {2022-08-23},
	date = {2022-08-18},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Memory, Learning, Human learning, Fear conditioning, Fear, Skin physiology, Conditioned response, Electromyography},
}

@article{federer_long-term_2022,
	title = {Long-term availability of data associated with articles in {PLOS} {ONE}},
	volume = {17},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0272845},
	doi = {10.1371/journal.pone.0272845},
	abstract = {The adoption of journal policies requiring authors to include a Data Availability Statement has helped to increase the availability of research data associated with research articles. However, having a Data Availability Statement is not a guarantee that readers will be able to locate the data; even if provided with an identifier like a uniform resource locator ({URL}) or a digital object identifier ({DOI}), the data may become unavailable due to link rot and content drift. To explore the long-term availability of resources including data, code, and other digital research objects associated with papers, this study extracted 8,503 {URLs} and {DOIs} from a corpus of nearly 50,000 Data Availability Statements from papers published in {PLOS} {ONE} between 2014 and 2016. These {URLs} and {DOIs} were used to attempt to retrieve the data through both automated and manual means. Overall, 80\% of the resources could be retrieved automatically, compared to much lower retrieval rates of 10–40\% found in previous papers that relied on contacting authors to locate data. Because a {URL} or {DOI} might be valid but still not point to the resource, a subset of 350 {URLs} and 350 {DOIs} were manually tested, with 78\% and 98\% of resources, respectively, successfully retrieved. Having a {DOI} and being shared in a repository were both positively associated with availability. Although resources associated with older papers were slightly less likely to be available, this difference was not statistically significant, suggesting that {URLs} and {DOIs} may be an effective means for accessing data over time. These findings point to the value of including {URLs} and {DOIs} in Data Availability Statements to ensure access to data on a long-term basis.},
	pages = {e0272845},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Federer, Lisa M.},
	urldate = {2022-08-26},
	date = {2022-08-24},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Citation analysis, Science policy, Reproducibility, Medical journals, Scientific publishing, Research ethics, Archives, Information retrieval},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MN6FBCYY/Federer - 2022 - Long-term availability of data associated with art.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/C48HURI7/article.html:text/html},
}

@article{von_niederhausern_academic_2018,
	title = {Academic response to improving value and reducing waste: A comprehensive framework for {INcreasing} {QUality} In patient-oriented academic clinical {REsearch} ({INQUIRE})},
	volume = {15},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.1002580},
	shorttitle = {Academic response to improving value and reducing waste},
	abstract = {{BACKGROUND}: Compelling evidence has demonstrated that a large proportion of investment in biomedical research is wasted; this waste is avoidable. Academic institutions have, thus far, shown limited response to recommendations for increasing value and reducing waste. We formulated an academic response by (i) achieving consensus across a wide range of stakeholder groups on a comprehensive framework for quality of patient-oriented clinical research and (ii) highlighting first successful examples of its operationalization to facilitate waste-reducing strategies at academic institutions.
{METHODS} {AND} {FINDINGS}: Based on a systematic review of quality definitions, concepts, and criteria in the medical literature (systematic {MEDLINE} search up to February 15, 2015, with independent and in duplicate article selection) and on stakeholder websites from 13 countries (Australia, Austria, Canada, France, Germany, Italy, Japan, Norway, Spain, Sweden, Switzerland, United Kingdom, and United States), we systematically developed a comprehensive framework for the quality of clinical research. We identified websites through personal contacts with experts in clinical research or public health who also suggested, for each country, websites of the following 7 stakeholder groups: patient organizations; academic research infrastructures; governmental bodies; regulatory agencies; ethics committees; the pharmaceutical industry; and funding agencies. In addition, we searched websites of inter- or supranational bodies involved in clinical research until no further insights emerged. After consolidation of the identified definitions, concepts, and criteria of quality in a basic framework structure, we conducted 4 rounds of an adapted online Delphi process among the same 7 stakeholder groups from 16 countries. The Delphi process ultimately achieved consensus on structure and content. The framework addresses 5 study stages (concept, planning and feasibility, conduct, analysis and interpretation, and reporting and knowledge translation) and includes the following dimensions: (i) protection of patient safety and rights, (ii) relevance/patient centeredness and involvement, (iii) minimization of bias (internal validity), (iv) precision, (v) transparency/access to data, and (vi) generalizability (external validity) of study results. These dimensions interact with 2 promoters-infrastructure and sustainability through education-that include a set of factors that may enhance all listed quality dimensions. Each quality dimension contains specific questions and explanatory items that guide quality assessment at each research stage from conceptualization of the research question through reporting and knowledge translation of study results. In the last survey round, Delphi participants from 9 countries (Austria, Australia, Canada, Germany, Italy, the Netherlands, Switzerland, {UK}, and {US}) agreed on the structure, content, and wording of the research stages, quality dimensions, specific questions, and descriptive examples of the final framework. In Switzerland, {INQUIRE} has resulted in a roadmap that guides initiatives to increase value within the Swiss Clinical Trial Organization network and through affiliated researchers.
{CONCLUSIONS}: We present a framework based on a consensus of different stakeholder groups guiding the practical assessment of clinical research quality at all stages of a research project. Operationalization of this common structure will support the increase of value by guiding academic institutions and researchers in developing quality enhancement initiatives, from posing the right research question to the transparent publication of results.},
	pages = {e1002580},
	number = {6},
	journaltitle = {{PLoS} medicine},
	shortjournal = {{PLoS} Med},
	author = {von Niederhäusern, Belinda and Guyatt, Gordon H. and Briel, Matthias and Pauli-Magnus, Christiane},
	date = {2018-06},
	pmid = {29879117},
	pmcid = {PMC5991651},
	keywords = {Humans, Research Design, Biomedical Research, Delphi Technique},
	file = {Academic response to improving value and reducing waste\: A comprehensive framework for INcreasing QUality In patient-oriented academic clinical REsearch (INQUIRE):/Users/tom/Zotero/storage/ZQCK2JXE/10.1371@journal.pmed.1002580.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/I69VZS83/von Niederhäusern et al. - 2018 - Academic response to improving value and reducing .pdf:application/pdf},
}

@article{hardwicke_post-publication_2022,
	title = {Post-publication critique at top-ranked journals across scientific disciplines: a cross-sectional assessment of policies and practice},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220139},
	doi = {10.1098/rsos.220139},
	shorttitle = {Post-publication critique at top-ranked journals across scientific disciplines},
	abstract = {Journals exert considerable control over letters, commentaries and online comments that criticize prior research (post-publication critique). We assessed policies (Study One) and practice (Study Two) related to post-publication critique at 15 top-ranked journals in each of 22 scientific disciplines (N = 330 journals). Two-hundred and seven (63\%) journals accepted post-publication critique and often imposed limits on length (median 1000, interquartile range ({IQR}) 500–1200 words) and time-to-submit (median 12, {IQR} 4–26 weeks). The most restrictive limits were 175 words and two weeks; some policies imposed no limits. Of 2066 randomly sampled research articles published in 2018 by journals accepting post-publication critique, 39 (1.9\%, 95\% confidence interval [1.4, 2.6]) were linked to at least one post-publication critique (there were 58 post-publication critiques in total). Of the 58 post-publication critiques, 44 received an author reply, of which 41 asserted that original conclusions were unchanged. Clinical Medicine had the most active culture of post-publication critique: all journals accepted post-publication critique and published the most post-publication critique overall, but also imposed the strictest limits on length (median 400, {IQR} 400–550 words) and time-to-submit (median 4, {IQR} 4–6 weeks). Our findings suggest that top-ranked academic journals often pose serious barriers to the cultivation, documentation and dissemination of post-publication critique.},
	pages = {220139},
	number = {8},
	journaltitle = {Royal Society Open Science},
	author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica E. and Tzavella, Loukia and Bendixen, Theiss and Handcock, Sarah A. and Köneke, Vivian E. and Ioannidis, John P. A.},
	urldate = {2022-09-10},
	date = {2022},
	note = {Publisher: Royal Society},
	keywords = {meta-research, peer review, journal policy, letter to the editor, scientific criticism, post-publication critique},
	file = {Full Text PDF:/Users/tom/Zotero/storage/B8KPH5KU/Hardwicke et al. - Post-publication critique at top-ranked journals a.pdf:application/pdf},
}

@article{askarov_significance_2022,
	title = {The Significance of Data-Sharing Policy},
	issn = {1542-4766, 1542-4774},
	url = {https://academic.oup.com/jeea/advance-article/doi/10.1093/jeea/jvac053/6706852},
	doi = {10.1093/jeea/jvac053},
	abstract = {Abstract
            We assess the impact of mandating data-sharing in economics journals on two dimensions of research credibility: statistical significance and excess statistical significance. Excess statistical significance is a necessary condition for publication selection bias. Quasi-experimental difference-in-differences analysis of 20,121 estimates published in 24 general interest and leading field journals shows that data-sharing policies have reduced reported statistical significance and the associated t-values. The magnitude of this reduction is large and of practical significance. We also find suggestive evidence that mandatory data-sharing reduces excess statistical significance and hence decreases publication bias.},
	pages = {jvac053},
	journaltitle = {Journal of the European Economic Association},
	author = {Askarov, Zohid and Doucouliagos, Anthony and Doucouliagos, Hristos and Stanley, T D},
	urldate = {2022-09-28},
	date = {2022-09-20},
	langid = {english},
}

@article{horbach_changing_2018,
	title = {The changing forms and expectations of peer review},
	volume = {3},
	issn = {2058-8615},
	url = {https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-018-0051-5},
	doi = {10.1186/s41073-018-0051-5},
	abstract = {The quality and integrity of the scientific literature have recently become the subject of heated debate. Due to an apparent increase in cases of scientific fraud and irreproducible research, some have claimed science to be in a state of crisis. A key concern in this debate has been the extent to which science is capable of self-regulation. Among various mechanisms, the peer review system in particular is considered an essential gatekeeper of both quality and sometimes even integrity in science. However, the allocation of responsibility for integrity to the peer review system is fairly recent and remains controversial. In addition, peer review currently comes in a wide variety of forms, developed in the expectation they can address specific problems and concerns in science publishing. At present, there is a clear need for a systematic analysis of peer review forms and the concerns underpinning them, especially considering a wave of experimentation fuelled by internet technologies and their promise to improve research integrity and reporting. We describe the emergence of current peer review forms by reviewing the scientific literature on peer review and by adding recent developments based on information from editors and publishers. We analyse the rationale for developing new review forms and discuss how they have been implemented in the current system. Finally, we give a systematisation of the range of discussed peer review forms. We pay detailed attention to the emergence of the expectation that peer review can maintain ‘the integrity of science’s published record’, demonstrating that this leads to tensions in the academic debate about the responsibilities and abilities of the peer review system.},
	pages = {8},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Res Integr Peer Rev},
	author = {Horbach, S. P. J. M. and Halffman, W. ( Willem)},
	urldate = {2022-10-03},
	date = {2018-12},
	langid = {english},
	file = {Horbach and Halffman - 2018 - The changing forms and expectations of peer review.pdf:/Users/tom/Zotero/storage/LNZUC3SS/Horbach and Halffman - 2018 - The changing forms and expectations of peer review.pdf:application/pdf;The changing forms and expectations of peer review:/Users/tom/Zotero/storage/P2Q9C26A/horbach2018.pdf.pdf:application/pdf},
}

@article{burnham_evolution_1990,
	title = {The evolution of editorial peer review},
	volume = {263},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.1990.03440100023003},
	doi = {10.1001/jama.1990.03440100023003},
	pages = {1323},
	number = {10},
	journaltitle = {{JAMA}: The Journal of the American Medical Association},
	shortjournal = {{JAMA}},
	author = {Burnham, John C.},
	urldate = {2022-10-03},
	date = {1990-03-09},
	langid = {english},
	file = {The Evolution of Editorial Peer Review:/Users/tom/Zotero/storage/BAUDJBYH/burnham1990.pdf.pdf:application/pdf},
}

@article{tennant_limitations_2020,
	title = {The limitations to our understanding of peer review},
	volume = {5},
	issn = {2058-8615},
	url = {https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-020-00092-1},
	doi = {10.1186/s41073-020-00092-1},
	abstract = {Peer review is embedded in the core of our knowledge generation systems, perceived as a method for establishing quality or scholarly legitimacy for research, while also often distributing academic prestige and standing on individuals. Despite its critical importance, it curiously remains poorly understood in a number of dimensions. In order to address this, we have analysed peer review to assess where the major gaps in our theoretical and empirical understanding of it lie. We identify core themes including editorial responsibility, the subjectivity and bias of reviewers, the function and quality of peer review, and the social and epistemic implications of peer review. The high-priority gaps are focused around increased accountability and justification in decision-making processes for editors and developing a deeper, empirical understanding of the social impact of peer review. Addressing this at the bare minimum will require the design of a consensus for a minimal set of standards for what constitutes peer review, and the development of a shared data infrastructure to support this. Such a field requires sustained funding and commitment from publishers and research funders, who both have a commitment to uphold the integrity of the published scholarly record. We use this to present a guide for the future of peer review, and the development of a new research discipline based on the study of peer review.},
	pages = {6},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Res Integr Peer Rev},
	author = {Tennant, Jonathan P. and Ross-Hellauer, Tony},
	urldate = {2022-10-03},
	date = {2020-12},
	langid = {english},
	file = {Tennant and Ross-Hellauer - 2020 - The limitations to our understanding of peer revie.pdf:/Users/tom/Zotero/storage/3IWFHKAA/Tennant and Ross-Hellauer - 2020 - The limitations to our understanding of peer revie.pdf:application/pdf;The limitations to our understanding of peer review:/Users/tom/Zotero/storage/H9XH6QRL/10.1186@s41073-020-00092-1.pdf.pdf:application/pdf},
}

@article{ioannidis_systematic_2022,
	title = {Systematic reviews for basic scientists: a different beast},
	issn = {0031-9333, 1522-1210},
	url = {https://journals.physiology.org/doi/10.1152/physrev.00028.2022},
	doi = {10.1152/physrev.00028.2022},
	shorttitle = {Systematic reviews for basic scientists},
	pages = {physrev.00028.2022},
	journaltitle = {Physiological Reviews},
	shortjournal = {Physiological Reviews},
	author = {Ioannidis, John P.A.},
	urldate = {2022-10-07},
	date = {2022-09-01},
	langid = {english},
	file = {Ioannidis - 2022 - Systematic reviews for basic scientists a differe.pdf:/Users/tom/Zotero/storage/4NL5NMB7/Ioannidis - 2022 - Systematic reviews for basic scientists a differe.pdf:application/pdf},
}

@article{kaplan_how_2005,
	title = {How to fix peer review: separating its two functions--improving manuscripts and judging their scientific merit--would help},
	volume = {14},
	issn = {1573-2843},
	doi = {10.1007/s10826-005-6845-3},
	shorttitle = {How to fix peer review},
	abstract = {Despite its importance as the ultimate gatekeeper of scientific publication and funding, peer review is known to engender bias, incompetence, excessive expense, ineffectiveness, and corruption. Peer review subsumes two functions. First, peer reviewers attempt to improve manuscripts by offering constructive criticisms about concrete elements such as the application of a technique, the strength of results, or the cogency of an argument. The second function of peer review is to render a decision about the biological significance of the findings to prioritize the manuscript for publication. I propose reforming peer review so that the two functions are independent. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {321--323},
	journaltitle = {Journal of Child and Family Studies},
	author = {Kaplan, David},
	date = {2005},
	note = {Place: Germany
Publisher: Springer},
	keywords = {Scientific Communication, Peer Evaluation},
	file = {How to Fix Peer Review\: Separating Its Two Functions--Improving Manuscripts and Judging Their Scientific Merit--Would Help:/Users/tom/Zotero/storage/FK4MCRC8/kaplan2005.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/MIQL47G9/2005-10890-002.html:text/html},
}

@article{heroux_quality_2022,
	title = {Quality output checklist and content assessment (quocca): a new tool for assessing research quality and reproducibility},
	volume = {12},
	rights = {© Author(s) (or their employer(s)) 2022. Re-use permitted under {CC} {BY}-{NC}. No commercial re-use. See rights and permissions. Published by {BMJ}.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {2044-6055, 2044-6055},
	url = {https://bmjopen.bmj.com/content/12/9/e060976},
	doi = {10.1136/bmjopen-2022-060976},
	shorttitle = {Quality output checklist and content assessment (quocca)},
	abstract = {Research must be well designed, properly conducted and clearly and transparently reported. Our independent medical research institute wanted a simple, generic tool to assess the quality of the research conducted by its researchers, with the goal of identifying areas that could be improved through targeted educational activities. Unfortunately, none was available, thus we devised our own. Here, we report development of the Quality Output Checklist and Content Assessment ({QuOCCA}), and its application to publications from our institute’s scientists. Following consensus meetings and external review by statistical and methodological experts, 11 items were selected for the final version of the {QuOCCA}: research transparency (items 1–3), research design and analysis (items 4–6) and research reporting practices (items 7–11). Five pairs of raters assessed all 231 articles published in 2017 and 221 in 2018 by researchers at our institute. Overall, the results were similar between years and revealed limited engagement with several recommended practices highlighted in the {QuOCCA}. These results will be useful to guide educational initiatives and their effectiveness. The {QuOCCA} is brief and focuses on broadly applicable and relevant concepts to open, high-quality, reproducible and well-reported science. Thus, the {QuOCCA} could be used by other biomedical institutions and individual researchers to evaluate research publications, assess changes in research practice over time and guide the discussion about high-quality, open science. Given its generic nature, the {QuOCCA} may also be useful in other research disciplines.},
	pages = {e060976},
	number = {9},
	journaltitle = {{BMJ} Open},
	author = {Héroux, Martin E. and Butler, Annie A. and Cashin, Aidan G. and {McCaughey}, Euan J. and Affleck, Andrew J. and Green, Michael A. and Cartwright, Andrew and Jones, Matthew and Kiely, Kim M. and Schooten, Kimberley S. van and Menant, Jasmine C. and Wewege, Michael and Gandevia, Simon C.},
	urldate = {2022-10-10},
	date = {2022-09-01},
	langid = {english},
	pmid = {36167369},
	note = {Publisher: British Medical Journal Publishing Group
Section: Communication},
	keywords = {protocols \& guidelines, statistics \& research methods, education \& training (see medical education \& training), medical education \& training},
	file = {Full Text PDF:/Users/tom/Zotero/storage/AIXS958U/Héroux et al. - 2022 - Quality Output Checklist and Content Assessment (Q.pdf:application/pdf},
}

@article{barnes_impact_2015,
	title = {Impact of an online writing aid tool for writing a randomized trial report: the {COBWEB} (Consort-based {WEB} tool) randomized controlled trial},
	volume = {13},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-015-0460-y},
	doi = {10.1186/s12916-015-0460-y},
	shorttitle = {Impact of an online writing aid tool for writing a randomized trial report},
	abstract = {Incomplete reporting is a frequent waste in research. Our aim was to evaluate the impact of a writing aid tool ({WAT}) based on the {CONSORT} statement and its extension for non-pharmacologic treatments on the completeness of reporting of randomized controlled trials ({RCTs}).},
	pages = {221},
	number = {1},
	journaltitle = {{BMC} Medicine},
	shortjournal = {{BMC} Medicine},
	author = {Barnes, Caroline and Boutron, Isabelle and Giraudeau, Bruno and Porcher, Raphael and Altman, Douglas G. and Ravaud, Philippe},
	urldate = {2022-10-10},
	date = {2015-09-15},
	keywords = {Transparency, Randomized controlled trial, Reporting guidelines, {CONSORT} statement, Clinical epidemiology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/26JV7MTC/Barnes et al. - 2015 - Impact of an online writing aid tool for writing a.pdf:application/pdf;Impact of an online writing aid tool for writing a randomized trial report\: the COBWEB (Consort-based WEB tool) randomized controlled trial:/Users/tom/Zotero/storage/U996MKT7/barnes2015.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/YRRCNBJ7/s12916-015-0460-y.html:text/html},
}

@article{boutron_reporting_2010,
	title = {Reporting and interpretation of randomized controlled trials with statistically nonsignificant results for primary outcomes},
	volume = {303},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2010.651},
	doi = {10.1001/jama.2010.651},
	abstract = {Previous studies indicate that the interpretation of trial results can be distorted by authors of published reports.To identify the nature and frequency of distorted presentation or “spin” (ie, specific reporting strategies, whatever their motive, to highlight that the experimental treatment is beneficial, despite a statistically nonsignificant difference for the primary outcome, or to distract the reader from statistically nonsignificant results) in published reports of randomized controlled trials ({RCTs}) with statistically nonsignificant results for primary outcomes.March 2007 search of {MEDLINE} via {PubMed} using the Cochrane Highly Sensitive Search Strategy to identify reports of {RCTs} published in December 2006.Articles were included if they were parallel-group {RCTs} with a clearly identified primary outcome showing statistically nonsignificant results (ie, P ≥ .05).Two readers appraised each selected article using a pretested, standardized data abstraction form developed in a pilot test.From the 616 published reports of {RCTs} examined, 72 were eligible and appraised. The title was reported with spin in 13 articles (18.0\%; 95\% confidence interval [{CI}], 10.0\%-28.9\%). Spin was identified in the Results and Conclusions sections of the abstracts of 27 (37.5\%; 95\% {CI}, 26.4\%-49.7\%) and 42 (58.3\%; 95\% {CI}, 46.1\%-69.8\%) reports, respectively, with the conclusions of 17 (23.6\%; 95\% {CI}, 14.4\%-35.1\%) focusing only on treatment effectiveness. Spin was identified in the main-text Results, Discussion, and Conclusions sections of 21 (29.2\%; 95\% {CI}, 19.0\%-41.1\%), 31 (43.1\%; 95\% {CI}, 31.4\%-55.3\%), and 36 (50.0\%; 95\% {CI}, 38.0\%-62.0\%) reports, respectively. More than 40\% of the reports had spin in at least 2 of these sections in the main text.In this representative sample of {RCTs} published in 2006 with statistically nonsignificant primary outcomes, the reporting and interpretation of findings was frequently inconsistent with the results.},
	pages = {2058--2064},
	number = {20},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Boutron, Isabelle and Dutton, Susan and Ravaud, Philippe and Altman, Douglas G.},
	urldate = {2022-10-10},
	date = {2010-05-26},
	file = {Full Text:/Users/tom/Zotero/storage/R2DMKN5H/Boutron et al. - 2010 - Reporting and Interpretation of Randomized Control.pdf:application/pdf;Reporting and Interpretation of Randomized Controlled Trials With Statistically Nonsignificant Results for Primary Outcomes:/Users/tom/Zotero/storage/8AGRIFVZ/boutron2010.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6PYHFY3I/185952.html:text/html},
}

@article{boutron_spin_2020,
	title = {Spin in scientific publications: a frequent detrimental research practice},
	volume = {75},
	issn = {0196-0644, 1097-6760},
	url = {https://www.annemergmed.com/article/S0196-0644(19)31358-7/abstract},
	doi = {10.1016/j.annemergmed.2019.11.002},
	shorttitle = {Spin in scientific publications},
	abstract = {{SEE} {RELATED} {ARTICLE}, P. 423.},
	pages = {432--434},
	number = {3},
	journaltitle = {Annals of Emergency Medicine},
	shortjournal = {Annals of Emergency Medicine},
	author = {Boutron, Isabelle},
	urldate = {2022-10-10},
	date = {2020-03-01},
	pmid = {31874770},
	note = {Publisher: Elsevier},
	file = {Snapshot:/Users/tom/Zotero/storage/38LAV84U/pdf.html:text/html;Spin in Scientific Publications\: A Frequent Detrimental Research Practice:/Users/tom/Zotero/storage/TMDUNE5W/10.1016@j.annemergmed.2019.11.002.pdf.pdf:application/pdf},
}

@article{schroter_differences_2006,
	title = {Differences in review quality and recommendations for publication between peer reviewers suggested by authors or by editors},
	volume = {295},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.295.3.314},
	doi = {10.1001/jama.295.3.314},
	pages = {314},
	number = {3},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Schroter, Sara},
	urldate = {2022-10-10},
	date = {2006-01-18},
	langid = {english},
	file = {Differences in Review Quality and Recommendations for Publication Between Peer Reviewers Suggested by Authors or by Editors:/Users/tom/Zotero/storage/D3AI53XR/schroter2006.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/U5CSAC4M/Schroter - 2006 - Differences in Review Quality and Recommendations .pdf:application/pdf},
}

@article{chiu_spin_2017,
	title = {‘Spin’ in published biomedical literature: A methodological systematic review},
	volume = {15},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2002173},
	doi = {10.1371/journal.pbio.2002173},
	shorttitle = {‘Spin’ in published biomedical literature},
	abstract = {In the scientific literature, spin refers to reporting practices that distort the interpretation of results and mislead readers so that results are viewed in a more favourable light. The presence of spin in biomedical research can negatively impact the development of further studies, clinical practice, and health policies. This systematic review aims to explore the nature and prevalence of spin in the biomedical literature. We searched {MEDLINE}, {PreMEDLINE}, Embase, Scopus, and hand searched reference lists for all reports that included the measurement of spin in the biomedical literature for at least 1 outcome. Two independent coders extracted data on the characteristics of reports and their included studies and all spin-related outcomes. Results were grouped inductively into themes by spin-related outcome and are presented as a narrative synthesis. We used meta-analyses to analyse the association of spin with industry sponsorship of research. We included 35 reports, which investigated spin in clinical trials, observational studies, diagnostic accuracy studies, systematic reviews, and meta-analyses. The nature of spin varied according to study design. The highest (but also greatest) variability in the prevalence of spin was present in trials. Some of the common practices used to spin results included detracting from statistically nonsignificant results and inappropriately using causal language. Source of funding was hypothesised by a few authors to be a factor associated with spin; however, results were inconclusive, possibly due to the heterogeneity of the included papers. Further research is needed to assess the impact of spin on readers’ decision-making. Editors and peer reviewers should be familiar with the prevalence and manifestations of spin in their area of research in order to ensure accurate interpretation and dissemination of research.},
	pages = {e2002173},
	number = {9},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Chiu, Kellia and Grundy, Quinn and Bero, Lisa},
	urldate = {2022-10-10},
	date = {2017-09-11},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Language, Randomized controlled trials, Observational studies, Clinical trial reporting, Metaanalysis, Governments, Surgical oncology, Systematic reviews},
	file = {‘Spin’ in published biomedical literature\: A methodological systematic review:/Users/tom/Zotero/storage/DGFN9X9L/chiu2017.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/SZNGP4AN/Chiu et al. - 2017 - ‘Spin’ in published biomedical literature A metho.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/DSP6QKEW/article.html:text/html},
}

@article{iqbal_reproducible_2016,
	title = {Reproducible research practices and transparency across the biomedical literature},
	volume = {14},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002333},
	doi = {10.1371/journal.pbio.1002333},
	abstract = {There is a growing movement to encourage reproducibility and transparency practices in the scientific community, including public access to raw data and protocols, the conduct of replication studies, systematic integration of evidence in systematic reviews, and the documentation of funding and potential conflicts of interest. In this survey, we assessed the current status of reproducibility and transparency addressing these indicators in a random sample of 441 biomedical journal articles published in 2000–2014. Only one study provided a full protocol and none made all raw data directly available. Replication studies were rare (n = 4), and only 16 studies had their data included in a subsequent systematic review or meta-analysis. The majority of studies did not mention anything about funding or conflicts of interest. The percentage of articles with no statement of conflict decreased substantially between 2000 and 2014 (94.4\% in 2000 to 34.6\% in 2014); the percentage of articles reporting statements of conflicts (0\% in 2000, 15.4\% in 2014) or no conflicts (5.6\% in 2000, 50.0\% in 2014) increased. Articles published in journals in the clinical medicine category versus other fields were almost twice as likely to not include any information on funding and to have private funding. This study provides baseline data to compare future progress in improving these indicators in the scientific literature.},
	pages = {e1002333},
	number = {1},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Iqbal, Shareen A. and Wallach, Joshua D. and Khoury, Muin J. and Schully, Sheri D. and Ioannidis, John P. A.},
	urldate = {2022-10-12},
	date = {2016-01-04},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Reproducibility, Scientific publishing, Replication studies, Metaanalysis, Systematic reviews, Conflicts of interest, Case series, Clinical medicine},
	file = {Full Text PDF:/Users/tom/Zotero/storage/J7MNKEHG/Iqbal et al. - 2016 - Reproducible Research Practices and Transparency a.pdf:application/pdf;Reproducible Research Practices and Transparency across the Biomedical Literature:/Users/tom/Zotero/storage/K2FZR7ZQ/iqbal2016.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/K764U3VK/authors.html:text/html},
}

@article{mcdermott_breaking_2022,
	title = {Breaking free: How preregistration hurts scholars and science},
	volume = {41},
	issn = {0730-9384, 1471-5457},
	url = {https://www.cambridge.org/core/product/identifier/S0730938422000041/type/journal_article},
	doi = {10.1017/pls.2022.4},
	shorttitle = {Breaking free},
	abstract = {A
              bstract
            
            Pre-registration has become an increasingly popular proposal to address concerns regarding questionable research practices. Yet preregistration does not necessarily solve these problems. It also causes additional problems, including raising costs for more junior and less resourced scholars. In addition, pre-registration restricts creativity and diminishes the broader scientific enterprise. In this way, pre-registration neither solves the problems it is intended to address, nor does it come without costs. Pre-registration is neither necessary nor sufficient for producing novel or ethical work. In short, pre-registration represents a form of virtue signaling that is more performative than actual.},
	pages = {55--59},
	number = {1},
	journaltitle = {Politics and the Life Sciences},
	shortjournal = {Polit. life sci.},
	author = {{McDermott}, Rose},
	urldate = {2022-10-14},
	date = {2022},
	langid = {english},
	file = {McDermott - 2022 - Breaking free How preregistration hurts scholars .pdf:/Users/tom/Zotero/storage/EG9R8DEP/McDermott - 2022 - Breaking free How preregistration hurts scholars .pdf:application/pdf},
}

@article{batashvili_behavioural_2022,
	title = {Behavioural reconsolidation interference not observed in a within-subjects design},
	volume = {7},
	rights = {2022 The Author(s)},
	issn = {2056-7936},
	url = {https://www.nature.com/articles/s41539-022-00143-w},
	doi = {10.1038/s41539-022-00143-w},
	abstract = {Studies of reconsolidation interference posit that reactivation of a previously consolidated memory via a reminder brings it into an active, labile state, leaving it open for potential manipulation. If interfered with, this may disrupt the original memory trace. While evidence for pharmacological reconsolidation interference is widespread, it remains unclear whether behavioural interference using the presentation of competing information can engender it, especially in declarative memory. Almost all previous studies in this area have employed between-subjects designs, in which there are potential confounds, such as different retrieval strategies for the multiple conditions. In the current studies, within-subjects paradigms were applied to test the effects of reconsolidation interference on associative recognition and free recall. In Experiment 1, participants engaged in pair-associate learning of unrelated object pictures on Day 1, and after a reminder, interference, reminder + interference, or no manipulation (control) on Day 2, were tested on associative recognition of these pairs on Day 3. In Experiments 2 and 3, memoranda were short stories studied on Day 1. On Day 2, stories were assigned to either control, reminder, interference by alternative stories, or reminder + interference conditions. On Day 3 participants recalled the Day 1 stories, and answered yes/no recognition questions. Reminders improved subsequent memory, while interference was effective in reducing retrieval in differing degrees across the experiments. Importantly, the reminder + interference condition was no more effective in impairing retrieval than the interference-alone condition, contrary to the prediction of the behavioural reconsolidation-interference approach.},
	pages = {1--9},
	number = {1},
	journaltitle = {npj Science of Learning},
	shortjournal = {npj Sci. Learn.},
	author = {Batashvili, Michael and Sheaffer, Rona and Katz, Maya and Doron, Yoav and Kempler, Noam and Levy, Daniel A.},
	urldate = {2022-10-15},
	date = {2022-10-11},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Consolidation, Forgetting, Human behaviour},
	file = {Full Text PDF:/Users/tom/Zotero/storage/Y2TYUH3Q/Batashvili et al. - 2022 - Behavioural reconsolidation interference not obser.pdf:application/pdf},
}

@article{doshi_raw_2013,
	title = {Raw data from clinical trials: within reach?},
	volume = {34},
	issn = {0165-6147},
	url = {https://www.cell.com/trends/pharmacological-sciences/abstract/S0165-6147(13)00190-9},
	doi = {10.1016/j.tips.2013.10.006},
	shorttitle = {Raw data from clinical trials},
	pages = {645--647},
	number = {12},
	journaltitle = {Trends in Pharmacological Sciences},
	shortjournal = {Trends in Pharmacological Sciences},
	author = {Doshi, Peter and Goodman, Steven N. and Ioannidis, John P. A.},
	urldate = {2022-10-15},
	date = {2013-12-01},
	pmid = {24295825},
	note = {Publisher: Elsevier},
	file = {Raw data from clinical trials\: within reach?:/Users/tom/Zotero/storage/6BQ5I8EW/doshi2013.pdf.pdf:application/pdf;Raw data from clinical trials\: within reach?:/Users/tom/Zotero/storage/H8BA6SIV/doshi2013.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5LJU57WD/S0165-6147(13)00190-9.html:text/html},
}

@article{ohagan_systematic_2018,
	title = {Systematic reviews of the literature: a better way of addressing basic science controversies},
	volume = {314},
	issn = {1040-0605},
	url = {https://journals.physiology.org/doi/full/10.1152/ajplung.00544.2017},
	doi = {10.1152/ajplung.00544.2017},
	shorttitle = {Systematic reviews of the literature},
	pages = {L439--L442},
	number = {3},
	journaltitle = {American Journal of Physiology-Lung Cellular and Molecular Physiology},
	author = {O’Hagan, Emma C. and Matalon, Sadis and Riesenberg, Lee Ann},
	urldate = {2022-10-16},
	date = {2018-03},
	note = {Publisher: American Physiological Society},
	keywords = {toread},
	file = {Full Text PDF:/Users/tom/Zotero/storage/J24HAHZF/O’Hagan et al. - 2018 - Systematic reviews of the literature a better way.pdf:application/pdf;Systematic reviews of the literature\: a better way of addressing basic science controversies:/Users/tom/Zotero/storage/YQUTTV4X/10.1152@ajplung.00544.2017.pdf.pdf:application/pdf;Systematic reviews of the literature\: a better way of addressing basic science controversies:/Users/tom/Zotero/storage/3293UG5P/10.1152@ajplung.00544.2017.pdf.pdf:application/pdf},
}

@article{oconnor_critical_2014,
	title = {Critical appraisal of studies using laboratory animal models},
	volume = {55},
	issn = {1084-2020},
	url = {https://doi.org/10.1093/ilar/ilu038},
	doi = {10.1093/ilar/ilu038},
	abstract = {In this manuscript we discuss an approach to critically appraising papers based on the results of laboratory animal experiments. The roles of external and internal validity in critically appraising the results of a paper are introduced. The risk of bias domains used by the Cochrane Handbook of Systematic Reviews of Interventions form the basis for assessing internal validity. The bias domains discussed include the selection bias, performance bias, outcome assessment bias, attrition bias, and reporting bias. Further, an approach to considering the role of chance in research findings is discussed.},
	pages = {405--417},
	number = {3},
	journaltitle = {{ILAR} Journal},
	shortjournal = {{ILAR} Journal},
	author = {O'Connor, Annette M. and Sargeant, Jan M.},
	urldate = {2022-10-16},
	date = {2014-12-20},
	file = {Critical Appraisal of Studies Using Laboratory Animal Models:/Users/tom/Zotero/storage/RZDMMMKX/10.1093@ilar@ilu038.pdf.pdf:application/pdf;Critical Appraisal of Studies Using Laboratory Animal Models:/Users/tom/Zotero/storage/ZXQDHT42/10.1093@ilar@ilu038.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/8A73FTEY/O'Connor and Sargeant - 2014 - Critical Appraisal of Studies Using Laboratory Ani.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/FMY8QED5/644697.html:text/html},
}

@article{hooijmans_syrcles_2014,
	title = {{SYRCLE}’s risk of bias tool for animal studies},
	volume = {14},
	issn = {1471-2288},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-43},
	doi = {10.1186/1471-2288-14-43},
	abstract = {Background: Systematic Reviews ({SRs}) of experimental animal studies are not yet common practice, but awareness of the merits of conducting such {SRs} is steadily increasing. As animal intervention studies differ from randomized clinical trials ({RCT}) in many aspects, the methodology for {SRs} of clinical trials needs to be adapted and optimized for animal intervention studies. The Cochrane Collaboration developed a Risk of Bias ({RoB}) tool to establish consistency and avoid discrepancies in assessing the methodological quality of {RCTs}. A similar initiative is warranted in the field of animal experimentation.
Methods: We provide an {RoB} tool for animal intervention studies ({SYRCLE}’s {RoB} tool). This tool is based on the Cochrane {RoB} tool and has been adjusted for aspects of bias that play a specific role in animal intervention studies. To enhance transparency and applicability, we formulated signalling questions to facilitate judgment.
Results: The resulting {RoB} tool for animal studies contains 10 entries. These entries are related to selection bias, performance bias, detection bias, attrition bias, reporting bias and other biases. Half these items are in agreement with the items in the Cochrane {RoB} tool. Most of the variations between the two tools are due to differences in design between {RCTs} and animal studies. Shortcomings in, or unfamiliarity with, specific aspects of experimental design of animal studies compared to clinical studies also play a role.
Conclusions: {SYRCLE}’s {RoB} tool is an adapted version of the Cochrane {RoB} tool. Widespread adoption and implementation of this tool will facilitate and improve critical appraisal of evidence from animal studies. This may subsequently enhance the efficiency of translating animal research into clinical practice and increase awareness of the necessity of improving the methodological quality of animal studies.},
	pages = {43},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Med Res Methodol},
	author = {Hooijmans, Carlijn R and Rovers, Maroeska M and de Vries, Rob {BM} and Leenaars, Marlies and Ritskes-Hoitinga, Merel and Langendam, Miranda W},
	urldate = {2022-10-16},
	date = {2014-12},
	langid = {english},
	file = {Hooijmans et al. - 2014 - SYRCLE’s risk of bias tool for animal studies.pdf:/Users/tom/Zotero/storage/S4WSKKJN/Hooijmans et al. - 2014 - SYRCLE’s risk of bias tool for animal studies.pdf:application/pdf;SYRCLE’s risk of bias tool for animal studies:/Users/tom/Zotero/storage/84PTMMZ6/hooijmans2014.pdf.pdf:application/pdf;SYRCLE’s risk of bias tool for animal studies:/Users/tom/Zotero/storage/TNJNSCSV/hooijmans2014.pdf.pdf:application/pdf},
}

@article{forscher_benefits_2022,
	title = {The Benefits, Barriers, and Risks of Big-Team Science},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/17456916221082970},
	doi = {10.1177/17456916221082970},
	abstract = {Progress in psychology has been frustrated by challenges concerning replicability, generalizability, strategy selection, inferential reproducibility, and computational reproducibility. Although often discussed separately, these five challenges may share a common cause: insufficient investment of intellectual and nonintellectual resources into the typical psychology study. We suggest that the emerging emphasis on big-team science can help address these challenges by allowing researchers to pool their resources together to increase the amount available for a single study. However, the current incentives, infrastructure, and institutions in academic science have all developed under the assumption that science is conducted by solo principal investigators and their dependent trainees, an assumption that creates barriers to sustainable big-team science. We also anticipate that big-team science carries unique risks, such as the potential for big-team-science organizations to be co-opted by unaccountable leaders, become overly conservative, and make mistakes at a grand scale. Big-team-science organizations must also acquire personnel who are properly compensated and have clear roles. Not doing so raises risks related to mismanagement and a lack of financial sustainability. If researchers can manage its unique barriers and risks, big-team science has the potential to spur great progress in psychology and beyond.},
	pages = {174569162210829},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Forscher, Patrick S. and Wagenmakers, Eric-Jan and Coles, Nicholas A. and Silan, Miguel Alejandro and Dutra, Natália and Basnight-Brown, Dana and {IJzerman}, Hans},
	urldate = {2022-10-16},
	date = {2022-10-03},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/9V43E9UL/Forscher et al. - 2022 - The Benefits, Barriers, and Risks of Big-Team Scie.pdf:application/pdf},
}

@article{kraft-todd_practice_2021,
	title = {Practice what you preach: Credibility-enhancing displays and the growth of open science},
	volume = {164},
	issn = {0749-5978},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597820303952},
	doi = {10.1016/j.obhdp.2020.10.009},
	shorttitle = {Practice what you preach},
	abstract = {How can individual scientists most effectively spread the adoption of open science practices? Engaging in open science practices presents a social dilemma because they are individually costly (given the current incentive schemes in academia) but collectively beneficial (due to production of higher quality and more accessible science). Mechanisms for promoting cooperation in social dilemmas typically rely on normativity—but open science practices are still comparatively rare. Further, individuals may be tempted to dishonestly “virtue signal” due to growing support for open science. We formulate a solution based on the theory of credibility-enhancing displays: advocates who are known to themselves practice the behavior they are advocating for (particularly if they are prestigious) are more effective at convincing others—specifically because their actions provide an honest signal of their belief in the behavior’s value. Thus, advocates for open science practices should find ways to engage in those practices visibly and often.},
	pages = {1--10},
	journaltitle = {Organizational Behavior and Human Decision Processes},
	shortjournal = {Organizational Behavior and Human Decision Processes},
	author = {Kraft-Todd, Gordon T. and Rand, David G.},
	urldate = {2022-10-16},
	date = {2021-05-01},
	langid = {english},
	keywords = {Open science, Cooperation, Credibility-enhancing displays, Prestige, Social dilemma, Social norms, Virtue signaling},
	file = {Practice what you preach\: Credibility-enhancing displays and the growth of open science:/Users/tom/Zotero/storage/EAJS6NAB/kraft-todd2021.pdf.pdf:application/pdf},
}

@online{noauthor_self-correction_nodate,
	title = {Self-correction in science at work {\textbar} Science},
	url = {https://www.science.org/doi/10.1126/science.aab3847},
	urldate = {2022-10-16},
}

@article{de_ridder_how_2022,
	title = {How to trust a scientist},
	volume = {93},
	issn = {0039-3681},
	url = {https://www.sciencedirect.com/science/article/pii/S0039368122000346},
	doi = {10.1016/j.shpsa.2022.02.003},
	abstract = {Epistemic trust among scientists is inevitable. There are two questions about this: (1) What is the content of this trust, what do scientists trust each other for? (2) Is such trust epistemically justified? I argue that if we assume a traditional answer to (1), namely that scientists trust each other to be reliable informants, then the answer to question (2) is negative, certainly for the biomedical and social sciences. This motivates a different construal of trust among scientists and therefore a different answer to (1): scientists trust each other to only testify to claims that are backed by evidence gathered in accordance with prevailing methodological standards. On this answer, trust among scientists is epistemically justified.},
	pages = {11--20},
	journaltitle = {Studies in History and Philosophy of Science},
	shortjournal = {Studies in History and Philosophy of Science},
	author = {de Ridder, Jeroen},
	urldate = {2022-10-16},
	date = {2022-06-01},
	langid = {english},
	keywords = {Scientific misconduct, Trust, Reproducibility crisis, Meta-Research, Reliability, Testimony},
}

@misc{ecker_reconsolidation_2021,
	title = {Reconsolidation behavioral updating of human emotional memory: A comprehensive review and unified analysis of successes, replication failures, and clinical translation},
	url = {https://psyarxiv.com/atz3m/},
	doi = {10.31234/osf.io/atz3m},
	shorttitle = {Reconsolidation behavioral updating of human emotional memory},
	abstract = {The annulment of a human emotional memory through reconsolidation behavioral updating has been documented in over twenty laboratory studies since the first such report in 2010.  However, fourteen studies have reported non-replication, the cause(s) of which remain unclear.  This review examines all successful and unsuccessful studies in detail, in an attempt to identify (a) the specific probable causes of non-replication and (b) how clinical translation might optimally be designed. For analyzing non-replications, a set of criteria is defined for principled identification of specific moments of prediction error ({PE}) in experimental procedures, including latent cause transitions, based on a preponderance of empirical evidence. A previously overlooked element of experimental procedure is in that way identified as being potentially decisive, and a unified, testable explanation is proposed for behavioral updating successes and failures in terms of the presence or absence of a {PE} experience. That in turn allows successful studies to be compared for the internal experiences induced in subjects, rather than compared for their external procedures, revealing an invariant set of three experiences shared by all successful updating studies despite their diverse procedures. Clinical translation, defined as replication of those experiences, not any particular procedure, is illustrated by an actual case, one of many published cases that have documented prompt transformational change produced by that specific methodology, suggesting memory reconsolidation as the mechanism of change. Lastly, the core empirical findings of successful reconsolidation updating studies are compared with previously proposed frameworks of memory reconsolidation in psychotherapy, exposing significant departures from scientific fidelity.},
	publisher = {{PsyArXiv}},
	author = {Ecker, Bruce},
	urldate = {2022-10-16},
	date = {2021-11-19},
	langid = {english},
	keywords = {Clinical Psychology, Social and Behavioral Sciences, psychotherapy, Psychotherapy, memory reconsolidation, behavioral updating, retrieval-extinction, Clinical Neuroscience, Neuroscience, prediction error, Behavioral Neuroscience, clinical translation, latent cause, mechanism of change, replication failure, transformational change},
}

@article{ahn_incorporating_2011,
	title = {Incorporating Quality Scores in Meta-Analysis},
	volume = {36},
	issn = {1076-9986},
	url = {https://doi.org/10.3102/1076998610393968},
	doi = {10.3102/1076998610393968},
	abstract = {This paper examines the impact of quality-score weights in meta-analysis. A simulation examines the roles of study characteristics such as population effect size ({ES}) and its variance on the bias and mean square errors ({MSEs}) of the estimators for several patterns of relationship between quality and {ES}, and for specific patterns of systematic deviations related to quality differences. The bias and {MSEs} of the estimators are large when {ESs} from low-quality studies deviate from the population {ES} in specific ways, and bias does not approach zero in these cases. Because meta-analysts can never know whether biases due to quality exist, and because quality weights lead to bias in almost every condition studied, we recommend against the use of quality weights.},
	pages = {555--585},
	number = {5},
	journaltitle = {Journal of Educational and Behavioral Statistics},
	author = {Ahn, Soyeon and Becker, Betsy Jane},
	urldate = {2022-10-16},
	date = {2011-10-01},
	langid = {english},
	file = {Incorporating Quality Scores in Meta-Analysis:/Users/tom/Zotero/storage/ANDNVZTU/ahn2011.pdf.pdf:application/pdf},
}

@article{crowe_review_2011,
	title = {A review of critical appraisal tools show they lack rigor: Alternative tool structure is proposed},
	volume = {64},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(10)00093-4/fulltext},
	doi = {10.1016/j.jclinepi.2010.02.008},
	shorttitle = {A review of critical appraisal tools show they lack rigor},
	pages = {79--89},
	number = {1},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Crowe, Michael and Sheppard, Lorraine},
	urldate = {2022-10-16},
	date = {2011-01-01},
	pmid = {21130354},
	keywords = {Methodology, research, Review, Validation, Critical appraisal, Reliability, Evidence-based practice},
	file = {A review of critical appraisal tools show they lack rigor\: Alternative tool structure is proposed:/Users/tom/Zotero/storage/3KKJVKGN/crowe2011.pdf.pdf:application/pdf},
}

@article{cook_preliminary_2017,
	title = {A Preliminary Investigation of the Empirical Validity of Study Quality Appraisal},
	volume = {50},
	issn = {0022-2194},
	url = {https://doi.org/10.1177/0022219415581178},
	doi = {10.1177/0022219415581178},
	abstract = {When classifying the evidence base of practices, special education scholars typically appraise study quality to identify and exclude from consideration in their reviews unacceptable-quality studies that are likely biased and might bias review findings if included. However, study quality appraisals used in the process of identifying evidence-based practices for students with learning and other disabilities have not been empirically validated (e.g., studies classified as unacceptable quality shown to have different, and presumably more biased, effects than high-quality studies). Using Gersten et al.?s (2005) approach for appraising the quality of group experimental studies in special education, we examined whether (a) studies classified as unacceptable quality and high quality had meaningfully different effects and (b) unacceptable-quality studies were more likely to have outlying effects than high-quality studies among 36 group experimental studies that investigated the effectiveness of instructional practices for students with learning disabilities. Our preliminary analyses found that the effects of unacceptable-quality studies were not meaningfully different from the effects of high-quality studies. We discuss implications of these findings and call for more research to be conducted in this area.},
	pages = {14--22},
	number = {1},
	journaltitle = {Journal of Learning Disabilities},
	shortjournal = {J Learn Disabil},
	author = {Cook, Bryan G. and Dupuis, Danielle N. and Jitendra, Asha K.},
	urldate = {2022-10-16},
	date = {2017-01-01},
	langid = {english},
	file = {A Preliminary Investigation of the Empirical Validity of Study Quality Appraisal:/Users/tom/Zotero/storage/ZJUHKQGA/cook2015.pdf.pdf:application/pdf},
}

@incollection{valentine_chapter_2005,
	location = {Burlington},
	title = {Chapter 5 - Can We Measure the Quality of Causal Research in Education?},
	url = {https://www.sciencedirect.com/science/article/pii/B9780125542579500061},
	series = {Educational Psychology},
	abstract = {This chapter introduces the notions of causality, control, and experimentation. Although, it is true that all dictionary definitions are circular, it is rare for this circularity to reveal itself so quickly and clearly. This implies that the construct “causality” is very abstract. Questions of causality are best addressed through experiments. The chapter reviews the roles of few researchers in shaping the perceptions of what constitutes good experimental research. Given that a study's quality greatly affects the degree of confidence that can be placed in its results, it is not surprising that there have been attempts to develop systematic strategies for assessing how effectively a study's design and implementation permit drawing causal inferences. Most of this work has occurred in medicine. The Study {DIAD} is meant to be a consensual based, multidimensional, transparent instrument for assessing the strength of causal inferences that can be drawn from a study. It is not perfect, partly because some issues in experimental design lack consensual answers regarding their impact on quality and partly because choices need to be made regarding what issues are most important to address. The chapter concludes on the note that greater attention to the issues embodied in the Study {DIAD} will result in improved information becoming available to policy makers, administrators, teachers, and parents as they make evidence-based decisions about the education of children.},
	pages = {85--111},
	booktitle = {Empirical Methods for Evaluating Educational Interventions},
	publisher = {Academic Press},
	author = {Valentine, Jeffrey C. and Cooper, Harris M.},
	editor = {Phye, Gary D. and Robinson, Daniel H. and Levin, Joel R.},
	urldate = {2022-10-16},
	date = {2005-01-01},
	langid = {english},
	doi = {10.1016/B978-012554257-9/50006-1},
}

@article{kent_recommendations_2022,
	title = {Recommendations for empowering early career researchers to improve research culture and practice},
	volume = {20},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001680},
	doi = {10.1371/journal.pbio.3001680},
	abstract = {Early career researchers ({ECRs}) are important stakeholders leading efforts to catalyze systemic change in research culture and practice. Here, we summarize the outputs from a virtual unconventional conference (unconference), which brought together 54 invited experts from 20 countries with extensive experience in {ECR} initiatives designed to improve the culture and practice of science. Together, we drafted 2 sets of recommendations for (1) {ECRs} directly involved in initiatives or activities to change research culture and practice; and (2) stakeholders who wish to support {ECRs} in these efforts. Importantly, these points apply to {ECRs} working to promote change on a systemic level, not only those improving aspects of their own work. In both sets of recommendations, we underline the importance of incentivizing and providing time and resources for systems-level science improvement activities, including {ECRs} in organizational decision-making processes, and working to dismantle structural barriers to participation for marginalized groups. We further highlight obstacles that {ECRs} face when working to promote reform, as well as proposed solutions and examples of current best practices. The abstract and recommendations for stakeholders are available in Dutch, German, Greek (abstract only), Italian, Japanese, Polish, Portuguese, Spanish, and Serbian.},
	pages = {e3001680},
	number = {7},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Kent, Brianne A. and Holman, Constance and Amoako, Emmanuella and Antonietti, Alberto and Azam, James M. and Ballhausen, Hanne and Bediako, Yaw and Belasen, Anat M. and Carneiro, Clarissa F. D. and Chen, Yen-Chung and Compeer, Ewoud B. and Connor, Chelsea A. C. and Crüwell, Sophia and Debat, Humberto and Dorris, Emma and Ebrahimi, Hedyeh and Erlich, Jeffrey C. and Fernández-Chiappe, Florencia and Fischer, Felix and Gazda, Małgorzata Anna and Glatz, Toivo and Grabitz, Peter and Heise, Verena and Kent, David G. and Lo, Hung and {McDowell}, Gary and Mehta, Devang and Neumann, Wolf-Julian and Neves, Kleber and Patterson, Mark and Penfold, Naomi C. and Piper, Sophie K. and Puebla, Iratxe and Quashie, Peter K. and Quezada, Carolina Paz and Riley, Julia L. and Rohmann, Jessica L. and Saladi, Shyam and Schwessinger, Benjamin and Siegerink, Bob and Stehlik, Paulina and Tzilivaki, Alexandra and Umbers, Kate D. L. and Varma, Aalok and Walavalkar, Kaivalya and Winde, Charlotte M. de and Zaza, Cecilia and Weissgerber, Tracey L.},
	urldate = {2022-10-16},
	date = {2022-07-07},
	langid = {english},
	keywords = {Open science, Reproducibility, Scientific publishing, Scientists, Careers, Decision making, Research funding, Careers in research},
}

@article{filmer_over_2022,
	title = {Over the Edge: Extending the duration of a reconsolidation intervention for spider fear},
	volume = {12},
	rights = {2022 The Author(s)},
	issn = {2158-3188},
	url = {https://www.nature.com/articles/s41398-022-02020-x},
	doi = {10.1038/s41398-022-02020-x},
	shorttitle = {Over the Edge},
	abstract = {Pharmacologically disrupting fear memory reconsolidation dramatically reduces fear behaviour. For example, 2–3 min of tarantula exposure followed by 40 mg of propranolol {HCl} (i.e., a reconsolidation intervention) abruptly decreased spider avoidance, an effect that persisted one year later. However, the success of reconsolidation interventions is not guaranteed: Pavlovian fear-conditioning research shows that the window to target memory reconsolidation is small and easy to miss. If exposure is too long to trigger reconsolidation, but too short for extinction learning, an inactive transitional limbo state occurs, rendering the fear memory unchanged and insensitive to amnesic agents. In this pre-registered study, we aimed to find this behaviourally-controlled boundary condition. Spider-fearful participants underwent a {\textasciitilde}3 min (n = 23) or {\textasciitilde}14 min (n = 20) exposure to a tarantula, intended to trigger reconsolidation or the limbo state respectively, followed by 40 mg of propranolol. We expected greater spider fear reduction after 3 than 14 min of exposure. Unexpectedly, there were no group differences on any outcome measures. In both groups, Bayesian analysis revealed a marked reduction in fear behaviour towards a generalisation stimulus (a house spider) accompanied by lower self-reported distress, with a sharp decline in spider fear scores two days after treatment that persisted one year later. Possible explanations include that the boundary conditions of reconsolidation are wider in older and stronger memories than experimentally-induced fears, or that alternative processes caused the treatment effects. Although the mechanism is unclear, these results carry a tentative promising message for the potential of brief reconsolidation-targeting interventions to mitigate irrational fears.},
	pages = {1--10},
	number = {1},
	journaltitle = {Translational Psychiatry},
	shortjournal = {Transl Psychiatry},
	author = {Filmer, Anna I. and Peters, Jacqueline and Bridge, Lara A. and Visser, Renée M. and Kindt, Merel},
	urldate = {2022-10-16},
	date = {2022-06-23},
	langid = {english},
	keywords = {Psychology, Learning and memory},
}

@article{ansell_journal_2016,
	title = {Journal Editors and “Results-Free” Research: A Cautionary Note},
	volume = {49},
	issn = {0010-4140},
	url = {https://doi.org/10.1177/0010414016669369},
	doi = {10.1177/0010414016669369},
	shorttitle = {Journal Editors and “Results-Free” Research},
	abstract = {Should journals review submissions based only on the research question and research design, independent of whether the results are statistically and substantively significant? This special issue is the first effort in political science (and perhaps across the social sciences) to publish articles based on submission of research designs alone. We offer our thoughts on the process.},
	pages = {1809--1815},
	number = {13},
	journaltitle = {Comparative Political Studies},
	author = {Ansell, Ben and Samuels, David},
	urldate = {2022-10-16},
	date = {2016-11-01},
	langid = {english},
}

@misc{lissa_worcs_2020,
	title = {{WORCS}: A Workflow for Open Reproducible Code in Science},
	url = {https://psyarxiv.com/k4wde/},
	doi = {10.31234/osf.io/k4wde},
	shorttitle = {{WORCS}},
	abstract = {Adopting open science principles can be challenging and time-intensive, because doing so requires substantial conceptual education and training in the use of new tools. This paper introduces the Workflow for Open Reproducible Code in Science ({WORCS}): A step-by-step procedure that researchers can follow to make a research project open and reproducible. The purpose of the workflow is to lower the threshold for adoption of open science principles. It is based on established best practices, and can be used either in parallel to, or in absence of, top-down requirements by journals, institutions, and funding bodies. To facilitate widespread adoption, the {WORCS} principles have been implemented in the R package worcs, which offers an {RStudio} project template and utility functions for specific workflow steps. This paper introduces the conceptual workflow, discusses how it meets different standards for open science, and addresses the functionality provided by the R implementation, worcs. This paper is primarily targeted towards scholars conducting research projects in R, conducting research that involves academic prose, analysis code, and (optionally) tabular data. However, the workflow is flexible enough to accommodate other scenarios, and offers a sensible starting point for customized solutions. The source code for the R package and manuscript, and a list of user examples of {WORCS} projects, are available at https://github.com/cjvanlissa/worcs.},
	publisher = {{PsyArXiv}},
	author = {Lissa, Caspar J. van and Brandmaier, Andreas Markus and Brinkman, Loek and Lamprecht, Anna-Lena and Peikert, Aaron and Struiksma, Marijn and Vreede, Barbara},
	urldate = {2022-10-16},
	date = {2020-05-31},
	langid = {english},
	keywords = {Meta-science, open science, reproducibility, dynamic document generation, r, version control},
}

@article{humphreys_payment_2022,
	title = {Payment and progress in peer review},
	volume = {400},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(22)00921-7/fulltext},
	doi = {10.1016/S0140-6736(22)00921-7},
	pages = {159},
	number = {10347},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Humphreys, Hilary},
	urldate = {2022-10-16},
	date = {2022-07-16},
	pmid = {35843240},
}

@article{scheffer_belief_2022,
	title = {Belief traps: Tackling the inertia of harmful beliefs},
	volume = {119},
	url = {https://www.pnas.org/doi/10.1073/pnas.2203149119},
	doi = {10.1073/pnas.2203149119},
	shorttitle = {Belief traps},
	pages = {e2203149119},
	number = {32},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Scheffer, Marten and Borsboom, Denny and Nieuwenhuis, Sander and Westley, Frances},
	urldate = {2022-10-16},
	date = {2022-08-09},
}

@online{noauthor_psyarxiv_nodate,
	title = {{PsyArXiv} Preprints {\textbar} Question Wording and Item Formulation},
	url = {https://psyarxiv.com/e4ktc/},
	urldate = {2022-10-16},
}

@misc{boulesteix_adjust_2022,
	title = {To adjust or not to adjust: It is not the tests you perform that count, but how you report them},
	url = {https://osf.io/preprints/metaarxiv/j986q/},
	doi = {10.31222/osf.io/j986q},
	shorttitle = {To adjust or not to adjust},
	abstract = {Most original articles published in the medical literature report the results of multiple statistical tests. In a few simple cases, there is general agreement on whether one should adjust for multiple testing or not. In most cases encountered in practice, however, there are contradictory and confusing recommendations in the literature. In this article, we suggest a unique criterion to decide whether to adjust for multiple testing or not, which has the advantage of being easily understandable for medical researchers and statisticians and of including most existing rules as special cases while providing answers in less straightforward situations.},
	publisher = {{MetaArXiv}},
	author = {Boulesteix, Anne-Laure and Hoffmann, Sabine},
	urldate = {2022-10-16},
	date = {2022-07-19},
	langid = {english},
	keywords = {Medicine and Health Sciences, Physical Sciences and Mathematics, Epidemiology, Biostatistics, Public Health, Other Medicine and Health Sciences, Statistics and Probability},
}

@article{bringmann_back_2022,
	title = {Back to Basics: The Importance of Conceptual Clarification in Psychological Science},
	volume = {31},
	issn = {0963-7214},
	url = {https://doi.org/10.1177/09637214221096485},
	doi = {10.1177/09637214221096485},
	shorttitle = {Back to Basics},
	abstract = {Although the lack of conceptual clarity has been observed to be a widespread and fundamental problem in psychology, conceptual clarification plays a mostly marginal role in psychological research. In this article, we argue that better conceptualization of psychological phenomena is needed to move psychology forward as a science. We first show how conceptual unclarity seeps through all aspects of psychological research, from everyday concepts to statistical measures. We then turn to recommendations on how to improve conceptual clarity in psychology, emphasizing the importance of seeing research as an iterative process in which it is necessary to revisit the phenomena that are the foundations of theories and models, as well as how they are conceptualized and measured.},
	pages = {340--346},
	number = {4},
	journaltitle = {Current Directions in Psychological Science},
	shortjournal = {Curr Dir Psychol Sci},
	author = {Bringmann, Laura F. and Elmer, Timon and Eronen, Markus I.},
	urldate = {2022-10-16},
	date = {2022-08-01},
	langid = {english},
}

@incollection{higgins_principles_2022,
	title = {Principles of Systematic Reviewing},
	isbn = {978-1-119-09936-9},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119099369.ch2},
	abstract = {The principles and steps of systematic reviews are similar to any other research undertaking: formulation of the problem to be addressed, collection and analysis of the data, and interpretation of the results. A study protocol should be written, which states objectives and eligibility criteria, describes how studies will be identified and selected, explains how any assessment of methodological quality or risk of bias will be undertaken, and provides details of any planned synthesis methods. The results from eligible studies are expressed in a standardized format and are often displayed graphically in forest plots with confidence intervals. Heterogeneity between study results and possible biases may be explored graphically in a forest, funnel, and other plots, and in statistical analyses. If a meta-analysis is deemed appropriate, a typical effect is estimated by combining the data. Most meta-analysis methods follow either fixed-effect(s) or random-effects approaches, which differ in the way they treat between-study heterogeneity. Meta-analyses generally use relative effect measures, while absolute measures are used when applying the findings to clinical or public health situations.},
	pages = {17--35},
	booktitle = {Systematic Reviews in Health Research},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Higgins, Julian P.T. and Davey Smith, George and Altman, Douglas G. and Egger, Matthias},
	urldate = {2022-10-16},
	date = {2022},
	langid = {english},
	doi = {10.1002/9781119099369.ch2},
	keywords = {meta-analysis, systematic review, eligibility criteria, review protocol, risk of bias, selecting studies},
}

@article{contessa_it_nodate,
	title = {It Takes a Village to Trust Science: Towards a (Thoroughly) Social Approach to Public Trust in Science},
	doi = {10.1007/s10670-021-00485-8},
	shorttitle = {It Takes a Village to Trust Science},
	pages = {1--26},
	journaltitle = {Erkenntnis},
	author = {Contessa, Gabriele},
}

@online{noauthor_investigating_nodate,
	title = {Investigating and Dealing with Publication Bias and Other Reporting Biases - Systematic Reviews in Health Research - Wiley Online Library},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119099369.ch5},
	urldate = {2022-10-16},
}

@article{pirosca_tolerating_2022,
	title = {Tolerating bad health research: the continuing scandal},
	volume = {23},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/s13063-022-06415-5},
	doi = {10.1186/s13063-022-06415-5},
	shorttitle = {Tolerating bad health research},
	abstract = {At the 2015 {REWARD}/{EQUATOR} conference on research waste, the late Doug Altman revealed that his only regret about his 1994 {BMJ} paper ‘The scandal of poor medical research’ was that he used the word ‘poor’ rather than ‘bad’. But how much research is bad? And what would improve things?},
	pages = {458},
	number = {1},
	journaltitle = {Trials},
	shortjournal = {Trials},
	author = {Pirosca, Stefania and Shiely, Frances and Clarke, Mike and Treweek, Shaun},
	urldate = {2022-10-16},
	date = {2022-06-02},
	keywords = {Risk of bias, Methodologists, Randomised trials, Research waste, Statisticians},
}

@misc{cobey_establishing_2022,
	title = {Establishing a core set of open science practices in biomedicine: a modified Delphi study},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2022.06.27.22276964v1},
	doi = {10.1101/2022.06.27.22276964},
	shorttitle = {Establishing a core set of open science practices in biomedicine},
	abstract = {Background Mandates and recommendations related to embedding open science practices within the research lifecycle are increasingly common. Few stakeholders, however, are monitoring compliance to their mandates or recommendations. It is necessary to monitor the current state of open science to track changes over time and to identify areas to create interventions to drive improvements.Monitoring open science practices requires that they are defined and operationalized. Involving the biomedical community, we sought to reach consensus on a core set of open science practices to monitor at biomedical research institutions.
Methods and Findings To establish consensus in a structured and systematic fashion, we conducted a modified 3-round Delphi study. Participants in Round 1 were 80 individuals from 20 biomedical research institutions that exhibit interest in or actively support open science. Participants were research administrators, researchers, specialists in dedicated open science roles, and librarians. In Rounds 1 and 2, participants completed an online survey evaluating a set of potential open science practices that could be important and meaningful to monitor in an automated institutional open science dashboard. Participants voted on the inclusion of each item and provided a rationale for their choice. We defined consensus as 80\% agreement. Between rounds, participants received aggregated voting scores for each item and anonymized comments from all participants, and were asked to re-vote on items that did not reach consensus. For Round 3, we hosted two half- day virtual meetings with 21 and 17 participants respectively to discuss and vote on all items that had not reached consensus after Round 2. Ultimately, participants reached consensus to include a 19 open science practices.
Conclusions A group of international stakeholders used a modified Delphi process to agree upon open science practices to monitor in a proposed open science dashboard for biomedical institutions. The core set of 19 open science practices identified by participants will form the foundation for institutional dashboards that display compliance with open science practices. They will now be assessed and tested for automatic inclusion in terms of technical feasibility. Using user-centered design, participating institutions will be involved in creating a dashboard prototype, which can then be implemented to monitor rates of open science practices at biomedical institutions. Our methods and approach may also transfer to other research settings–other disciplines could consider using our consensus list as a starting point for agreement upon a discipline-specific set of open science practices to monitor. The findings may also be of broader value to the development of policy, education, and interventions.},
	publisher = {{medRxiv}},
	author = {Cobey, Kelly D. and Haustein, Stefanie and Brehaut, Jamie and Dirnagl, Ulrich and Franzen, Delwen L. and Hemkens, Lars G. and Presseau, Justin and Riedel, Nico and Strech, Daniel and Alperin, Juan Pablo and Costas, Rodrigo and Sena, Emily S. and Leeuwen, Thed van and Ardern, Clare L. and Bacellar, Isabel O. L. and Camack, Nancy and Correa, Marcos Britto and Buccione, Roberto and Cenci, Maximiliano Sergio and Fergusson, Dean A. and Praag, Cassandra Gould van and Hoffman, Michael M. and Bielemann, Renata Moraes and Moschini, Ugo and Paschetta, Mauro and Pasquale, Valentina and Rac, Valeria E. and Roskams-Edris, Dylan and Schatzl, Hermann M. and Stratton, Jo Anne and Moher, David},
	urldate = {2022-10-16},
	date = {2022-06-28},
	langid = {english},
}

@article{richters_incredible_2021,
	title = {Incredible Utility: The Lost Causes and Causal Debris of Psychological Science},
	volume = {43},
	issn = {0197-3533},
	url = {https://doi.org/10.1080/01973533.2021.1979003},
	doi = {10.1080/01973533.2021.1979003},
	shorttitle = {Incredible Utility},
	abstract = {Variable-oriented, sample-based individual differences research strategies and statistical modeling approaches to causal-theoretical inference depend on their logic, coherence, justification, and presumed heuristic value on the tacit assumption that individuals are qualitatively the same, homogeneous with respect to the psychological structures and processes underlying their overt functioning, and that quantitative differences between them are produced by exactly the same psychological structures functioning in exactly the same way within each individual. This psychological homogeneity assumption, however, is demonstrably false and invalidated by a substantial body of uncontested scientific evidence documenting psychological heterogeneity as a ubiquitous, defining characteristic of human functioning. This irreconcilable mismatch between the psychological homogeneity assumption of the paradigm and the psychologically heterogeneous realities of its phenomena renders the individual differences methodology intrinsically incapable of advancing theoretical knowledge about the causes of psychological and behavioral phenomena. A detailed look at this mismatch reveals also that it holds considerable explanatory power as the root cause of the slow theoretical progress and replication failures of psychological research, as well as the driving force behind psychology's inability to relinquish its controversial reliance on null hypothesis significance testing as a justification standard for evaluating theoretical claims.},
	pages = {366--405},
	number = {6},
	journaltitle = {Basic and Applied Social Psychology},
	author = {Richters, John E.},
	urldate = {2022-10-16},
	date = {2021-11-02},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/01973533.2021.1979003},
	file = {Submitted Version:/Users/tom/Zotero/storage/TX3EH38Y/Richters - 2021 - Incredible Utility The Lost Causes and Causal Deb.pdf:application/pdf},
}

@article{squazzoni_does_2013,
	title = {Does incentive provision increase the quality of peer review? An experimental study},
	volume = {42},
	issn = {0048-7333},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733312001230},
	doi = {10.1016/j.respol.2012.04.014},
	shorttitle = {Does incentive provision increase the quality of peer review?},
	abstract = {Although peer review is crucial for innovation and experimental discoveries in science, it is poorly understood in scientific terms. Discovering its true dynamics and exploring adjustments which improve the commitment of everyone involved could benefit scientific development for all disciplines and consequently increase innovation in the economy and the society. We have reported the results of an innovative experiment developed to model peer review. We demonstrate that offering material rewards to referees tends to decrease the quality and efficiency of the reviewing process. Our findings help to discuss the viability of different options of incentive provision, supporting the idea that journal editors and responsible of research funding agencies should be extremely careful in offering material incentives on reviewing, since these might undermine moral motives which guide referees’ behavior.},
	pages = {287--294},
	number = {1},
	journaltitle = {Research Policy},
	shortjournal = {Research Policy},
	author = {Squazzoni, Flaminio and Bravo, Giangiacomo and Takács, Károly},
	urldate = {2022-10-16},
	date = {2013-02-01},
	langid = {english},
	keywords = {Science policy, Peer review, Trust, Cooperation, Reputation},
	file = {Does incentive provision increase the quality of peer review? An experimental study:/Users/tom/Zotero/storage/8JM8YNK9/squazzoni2013.pdf.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/TKZTYZBA/S0048733312001230.html:text/html},
}

@article{leblanc_scientific_2019,
	title = {Scientific sinkhole: The pernicious price of formatting},
	volume = {14},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223116},
	doi = {10.1371/journal.pone.0223116},
	shorttitle = {Scientific sinkhole},
	abstract = {Objective To conduct a time-cost analysis of formatting in scientific publishing. Design International, cross-sectional study (one-time survey). Setting Internet-based self-report survey, live between September 2018 and January 2019. Participants Anyone working in research, science, or academia and who submitted at least one peer-reviewed manuscript for consideration for publication in 2017. Completed surveys were available for 372 participants from 41 countries (60\% of respondents were from Canada). Main outcome measure Time (hours) and cost (wage per hour x time) associated with formatting a research paper for publication in a peer-reviewed academic journal. Results The median annual income category was {US}\$61,000–80,999, and the median number of publications formatted per year was four. Manuscripts required a median of two attempts before they were accepted for publication. The median formatting time was 14 hours per manuscript, or 52 hours per person, per year. This resulted in a median calculated cost of {US}\$477 per manuscript or {US}\$1,908 per person, per year. Conclusions To our knowledge, this is the first study to analyze the cost of manuscript formatting in scientific publishing. Our results suggest that scientific formatting represents a loss of 52 hours, costing the equivalent of {US}\$1,908 per researcher per year. These results identify the hidden and pernicious price associated with scientific publishing and provide evidence to advocate for the elimination of strict formatting guidelines, at least prior to acceptance.},
	pages = {e0223116},
	number = {9},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {{LeBlanc}, Allana G. and Barnes, Joel D. and Saunders, Travis J. and Tremblay, Mark S. and Chaput, Jean-Philippe},
	urldate = {2022-10-16},
	date = {2019-09-26},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Surveys, Internet, Scientific publishing, Peer review, Scientists, Social media, Professions, Salaries},
	file = {Full Text PDF:/Users/tom/Zotero/storage/UM6YUQ62/LeBlanc et al. - 2019 - Scientific sinkhole The pernicious price of forma.pdf:application/pdf;Scientific sinkhole\: The pernicious price of formatting:/Users/tom/Zotero/storage/D6RLIQKX/10.1371@journal.pone.0223116.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5JK7D69Y/article.html:text/html},
}

@article{slough_external_nodate,
	title = {External validity and meta-analysis},
	abstract = {Meta-analysis is a method that combines estimates from studies conducted on different samples, in different contexts, or at different times. Social scientists increasingly use metaanalyses to aggregate evidence and learn about general substantive phenomena. We develop a framework to examine the theoretical foundations of meta-analysis, with emphasis on clarifying the role of external validity. We identify the conditions under which multiple studies are target-equivalent, meaning they identify the same empirical target. Our main result shows that external validity and harmonization, in comparisons made and how outcomes are measured, are necessary and sufﬁcient for target-equivalence. We examine common formulations of meta-analysis—ﬁxed- and random-effects models—developing the theoretical assumptions that underpin them and providing design-based identiﬁcation results for these models. We then provide practical guidance based on our framework and results. Our results reveal limits to agnostic approaches to the combination of causal evidence from multiple studies.},
	pages = {35},
	author = {Slough, Tara and Tyson, Scott A},
	langid = {english},
	keywords = {toread},
	file = {Slough and Tyson - External Validity and Meta-Analysis.pdf:/Users/tom/Zotero/storage/3TWAS794/Slough and Tyson - External Validity and Meta-Analysis.pdf:application/pdf},
}

@misc{pargent_introduction_2022,
	title = {An Introduction to Machine Learning for Psychologists in R},
	url = {https://psyarxiv.com/89snd/},
	doi = {10.31234/osf.io/89snd},
	abstract = {Supervised machine learning ({ML}) is becoming an influential research method in psychology and other social sciences. However, theoretical {ML} concepts and predictive modeling techniques are not yet widely taught in psychology programs. This tutorial is intended to provide a low-barrier, non-technical entrance to supervised {ML} for psychologists in four consecutive modules. After introducing the basic idea of supervised {ML}, Module I covers performance evaluation of {ML} models with resampling methods (performance measures, bias-variance tradeoff, k-fold cross-validation). Module {II} introduces nonlinear, tree-based algorithms, focusing on random forests and their components, regression and classification trees. Module {III} is about performing empirical benchmark experiments (comparing the performance of several {ML} algorithms on multiple datasets). Finally, Module {IV} discusses the interpretation of {ML} models, including permutation variable importance measures, effect plots (partial dependence plots, individual conditional expectation profiles, accumulated local effect plots), and the concept of model fairness. Throughout the tutorial, intuitive descriptions of theoretical concepts (with as few mathematical formulas as possible) are followed by code examples, using the mlr3 and companion packages in R. Key practical analysis steps are demonstrated on the publicly available {PhoneStudy} dataset (N = 624), which includes over 1800 variables from smartphone sensing to predict Big Five personality trait scores. The manuscript contains a checklist to be used as a reminder on important aspects when performing, reporting, or reviewing {ML} analyses in psychology. Additional examples and more advanced concepts are demonstrated in extensive online materials (https://osf.io/9273g/).},
	publisher = {{PsyArXiv}},
	author = {Pargent, Florian and Schoedel, Ramona and Stachl, Clemens},
	urldate = {2022-10-16},
	date = {2022-04-11},
	langid = {english},
	keywords = {Social and Behavioral Sciences, other, Psychology, Quantitative Methods, Statistical Methods, Social and Personality Psychology, toread, Quantitative Psychology, Individual Differences},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ZX75JYIX/Pargent et al. - 2022 - An Introduction to Machine Learning for Psychologi.pdf:application/pdf},
}

@misc{frankenhuis_strategic_2022,
	title = {Strategic ambiguity in the social sciences},
	url = {https://osf.io/preprints/metaarxiv/kep5b/},
	doi = {10.31222/osf.io/kep5b},
	abstract = {In the wake of the replication crisis, there have been calls to increase the clarity and precision of theory in the social sciences. Here, we argue that the effects of these calls may be limited due to systematic and structural factors, and focus our attention on incentives favoring ambiguous theory. Intentionally or not, scientists can exploit theoretical ambiguities to make support for a claim appear stronger than is warranted. Practices include ‘theory stretching’, interpreting an ambiguous claim more expansively to absorb data outside of the scope of the original claim, and ‘post-hoc precision’, interpreting an ambiguous claim more narrowly so it appears more precisely aligned with the data. These practices lead to the overestimation of evidence for the original claim and create the appearance of consistent support and progressive research programs, which may in turn be rewarded by journals, funding agencies, and hiring committees. Selection for ambiguous research can occur outside of scientists’ awareness and even when scientists act in good faith. We supplement our verbal argument with a simple mathematical model. Although ambiguity might be inevitable or even useful in the early stages of theory construction, scientists should aim for increased clarity as knowledge advances. Science benefits from transparently communicating about known ambiguities. To attain transparency about ambiguity, we provide a set of recommendations for authors, reviewers, and journals. We conclude with suggestions for research on how scientists use strategic ambiguity to advance their careers and on the ways in which norms, incentives, and practices favor strategic ambiguity.},
	publisher = {{MetaArXiv}},
	author = {Frankenhuis, Willem and Panchanathan, Karthik and Smaldino, Paul E.},
	urldate = {2022-10-16},
	date = {2022-07-23},
	langid = {english},
	keywords = {Social and Behavioral Sciences, Psychology, formal modeling, theory development, incentive structures, Other Social and Behavioral Sciences, post-hoc precision, strategic ambiguity, theory stretching},
	file = {Full Text PDF:/Users/tom/Zotero/storage/LLXXRZKU/Frankenhuis et al. - 2022 - Strategic ambiguity in the social sciences.pdf:application/pdf},
}

@misc{labib_how_2022,
	title = {How to combine rules and commitment in fostering research integrity?},
	url = {https://osf.io/preprints/metaarxiv/sx58q/},
	doi = {10.31222/osf.io/sx58q},
	abstract = {Research integrity ({RI}) is crucial for producing research that is trustworthy and of high quality. Rules are important in setting {RI} standards, improving research practice and fostering responsible research practices. At the same time, rules can lead to increased bureaucracy, which without commensurate increased commitment amongst researchers towards {RI} is unlikely to lead to more responsible research behavior. In this paper, we explore the question: How can rules and commitment be combined to foster {RI}?  
There are three ways that research institutions can govern {RI}: markets (governing through incentives), hierarchies or bureaucracies (governing through rules), and network processes (governing through commitment and agreement at group level). Based on Habermas’ Theory of Communicative Action, we argue that network processes focusing on consensus, as part of the lifeworld, are necessary to legitimize and support systems, i.e. market and bureaucratic modes of governance. We analyze the institutional response to a serious {RI} case to illustrate how network processes can create a context in which rules can foster {RI}. Specifically, we analyze how the Science Committee established at Tilburg University in 2012 has navigated and combined different modes of governance to foster {RI}. Based on this case analysis, we formulate recommendations to research institutions on how to combine rules and commitment.},
	publisher = {{MetaArXiv}},
	author = {Labib, Krishma and Tijdink, Joeri K. and Sijtsma, Klaas and Bouter, Lex and Evans, Natalie and Widdershoven, Guy},
	urldate = {2022-10-16},
	date = {2022-07-28},
	langid = {english},
	keywords = {Social and Behavioral Sciences, Medicine and Health Sciences, Physical Sciences and Mathematics, research integrity, responsible conduct of research, bureaucracy, lifeworld, networks, research governance, research misconduct, rules, system},
	file = {Full Text PDF:/Users/tom/Zotero/storage/R2IRKK9W/Labib et al. - 2022 - How to combine rules and commitment in fostering r.pdf:application/pdf},
}

@misc{ruggeri_evaluating_2022,
	title = {Evaluating expectations from social and behavioral science about {COVID}-19 and lessons for the next pandemic},
	url = {https://psyarxiv.com/58udn/},
	doi = {10.31234/osf.io/58udn},
	abstract = {Social and behavioral science research proliferated during the {COVID}-19 pandemic, reflecting the substantial increase in influence of behavioral science in public health and public policy more broadly. This review presents a comprehensive assessment of 742 scientific articles on human behavior during {COVID}-19. Two independent teams evaluated 19 substantive policy recommendations (“claims”) on potentially critical aspects of behaviors during the pandemic drawn from the most widely cited behavioral science papers on {COVID}-19. Teams were made up of original authors and an independent team, all of whom were blinded to other team member reviews throughout. Both teams found evidence in support of 16 of the claims; for two claims, teams found only null evidence; and for no claims did the teams find evidence of effects in the opposite direction. One claim had no evidence available to assess. Seemingly due to the risks of the pandemic, most studies were limited to surveys, highlighting a need for more investment in field research and behavioral validation studies. The strongest findings indicate interventions that combat misinformation and polarization, and to utilize effective forms of messaging that engage trusted leaders and emphasize positive social norms.},
	publisher = {{PsyArXiv}},
	author = {Ruggeri, Kai and Stock, Friederike and Haslam, S. Alexander and Capraro, Valerio and Boggio, Paulo and Ellemers, Naomi and Cichocka, Aleksandra and Douglas, Karen and Rand, David and Cikara, Mina and Finkel, Eli and Linden, Dr Sander van der and Druckman, James and Wohl, Michael and Petty, Richard and Tucker, Joshua A. and Peters, Ellen and Shariff, Azim and Gelfand, Michele and Packer, Dominic and Lange, Paul van and Pennycook, Gordon and Baicker, Katherine and Crum, Alia and Weeden, Kim A. and Napper, Lucy E. and Tabri, Nassim and Zaki, Jamil and Skitka, Linda and Kitayama, Shinobu and Sunstein, Cass R. and Galizzi, Matteo M. and Milkman, Katherine and Petrović, Marija and Todsen, Anna Louise and Hajian, Ali and Verra, Sanne and Buehler, Vanessa and Friedemann, Maja and Hecht, Marlene and Mobarak, Rayyan and Jetten, Jolanda and Karakasheva, Ralitsa and Tünte, Markus R. and Yeung, Siu Kit and Rosenbaum, R. Shayna and Yamada, Yuki and Hudson, Sa-kiera Tiarra Jolynn and Soboleva, Irina and Macchia, Lucía and Dimant, Eugen and Geiger, Sandra J. and Buabang, Eike Kofi and Landman, Marna and Lep, Žan and Jarke, Hannes and Wingen, Tobias and Berkessel, Jana and Mareva, Silvana and {McGill}, Lucy and Papa, Francesca and Većkalov, Bojana and Afif, Zeina and Tavera, Felice and Andrews, Jack and Bursalıoğlu, Aslı and Zupan, Zorana and Wagner, Lisa and Navajas, Joaquin and Vranka, Marek A. and Kasdan, David and Novak, Lindsay and Hudson, Kathleen and Teas, Paul and Rachev, Nikolay R. and Bavel, Jay J. Van and Willer, Robb},
	urldate = {2022-10-16},
	date = {2022-10-10},
	langid = {english},
	keywords = {Social and Behavioral Sciences, misinformation, Social and Personality Psychology, prosociality, behavioral science, public policy, behavioral economics, behavioral policy, collectivism, covid-19, evidence-based policy, identity, messaging, nudging, polarization, vaccination},
	file = {Full Text PDF:/Users/tom/Zotero/storage/WGXQDTBR/Ruggeri et al. - 2022 - Evaluating expectations from social and behavioral.pdf:application/pdf},
}

@article{poeppel_we_2022,
	title = {We don’t know how the brain stores anything, let alone words},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661322002066},
	doi = {10.1016/j.tics.2022.08.010},
	abstract = {Cognitive, computational, and neurobiological approaches have made impressive advances in characterizing the operations that transform linguistic signals into meanings. But our understanding of how words and concepts are retained in the brain remains inadequate. How is the long-term storage of words, or in fact any representations, achieved? This puzzle requires new thinking to stimulate reinvestigation of the storage problem.},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Poeppel, David and Idsardi, William},
	urldate = {2022-10-16},
	date = {2022-09-27},
	langid = {english},
	keywords = {toread},
}

@article{cashin_registration_2021,
	title = {Registration of health and medical research},
	rights = {© Author(s) (or their employer(s)) 2021. No commercial re-use. See rights and permissions. Published by {BMJ}.},
	issn = {2515-446X, 2515-4478},
	url = {https://ebm.bmj.com/content/early/2021/12/21/bmjebm-2021-111836},
	doi = {10.1136/bmjebm-2021-111836},
	abstract = {Registration of health and medical research is an effective way of improving the transparency and credibility of evidence. Registration involves pre-specifying the research objectives, design, methods and analytic plan on a publicly accessible repository before conducting the study. Registration can reduce bias and improve the transparency and credibility of research findings. Registration is mandated for clinical trials, but it is also relevant to systematic reviews, observational and preclinical experimental research. This paper describes how researchers can register their research and outlines possible barriers and challenges in doing so. Widespread adoption of research registration can reduce research waste and improve evidence-informed clinical and policy decision making.},
	journaltitle = {{BMJ} Evidence-Based Medicine},
	author = {Cashin, Aidan G. and Richards, Georgia C. and {DeVito}, Nicholas J. and Mellor, David T. and Lee, Hopin},
	urldate = {2022-10-16},
	date = {2021-12-21},
	langid = {english},
	pmid = {34933926},
	note = {Publisher: Royal Society of Medicine
Section: Research methods and reporting},
	keywords = {evidence-based practice},
}

@article{hatton_computational_2019,
	title = {Computational reproducibility: the elephant in the room},
	volume = {36},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/8648256/},
	doi = {10.1109/MS.2018.2883805},
	shorttitle = {Computational reproducibility},
	pages = {137--144},
	number = {2},
	journaltitle = {{IEEE} Software},
	shortjournal = {{IEEE} Softw.},
	author = {Hatton, Les and van Genuchten, Michiel},
	urldate = {2022-10-16},
	date = {2019-03},
	keywords = {toread},
	file = {Computational reproducibility\: the elephant in the room:/Users/tom/Zotero/storage/8LWQNRPI/hatton2019.pdf.pdf:application/pdf;Computational Reproducibility\: The Elephant in the Room:/Users/tom/Zotero/storage/5AHNM4QI/hatton2019.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/HDQ9WRD3/Hatton and van Genuchten - 2019 - Computational Reproducibility The Elephant in the.pdf:application/pdf},
}

@misc{stantcheva_how_2022,
	location = {Rochester, {NY}},
	title = {How to Run Surveys: A Guide to Creating Your Own Identifying Variation and Revealing the Invisible},
	url = {https://papers.ssrn.com/abstract=4238254},
	shorttitle = {How to Run Surveys},
	abstract = {Surveys are an essential approach for eliciting otherwise invisible factors such as perceptions, knowledge and beliefs, attitudes, and reasoning. These factors are critical determinants of social, economic, and political outcomes. Surveys are not merely a research tool. They are also not only a way of collecting data. Instead, they involve creating the process that will generate the data. This allows the researcher to create their own identifying and controlled variation. Thanks to the rise of mobile technologies and platforms, surveys offer valuable opportunities to study either broadly representative samples or focus on specific groups. This paper offers guidance on the complete survey process, from the design of the questions and experiments to the recruitment of respondents and the collection of data to the analysis of survey responses. It covers issues related to the sampling process, selection and attrition, attention and carelessness, survey question design and measurement, response biases, and survey experiments.Institutional subscribers to the {NBER} working paper series, and residents of developing countries may download this paper without additional charge at www.nber.org.},
	number = {4238254},
	author = {Stantcheva, Stefanie},
	urldate = {2022-10-16},
	date = {2022-09-01},
	langid = {english},
	keywords = {toread, How to Run Surveys: A Guide to Creating Your Own Identifying Variation and Revealing the Invisible, {SSRN}, Stefanie Stantcheva},
	file = {Snapshot:/Users/tom/Zotero/storage/BER68WGQ/papers.html:text/html},
}

@article{otgaar_what_2022,
	title = {What can expert witnesses reliably say about memory in the courtroom?},
	volume = {3},
	issn = {26663538},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666353822000364},
	doi = {10.1016/j.fsiml.2022.100106},
	pages = {100106},
	journaltitle = {Forensic Science International: Mind and Law},
	shortjournal = {Forensic Science International: Mind and Law},
	author = {Otgaar, Henry and Howe, Mark L. and Dodier, Olivier},
	urldate = {2022-10-16},
	date = {2022-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/8YCWWMX5/Otgaar et al. - 2022 - What can expert witnesses reliably say about memor.pdf:application/pdf},
}

@article{baumeister_review_2022,
	title = {A review of multi-site replication projects in social psychology: is it viable to sustain any confidence in social psychology's knowledge base?},
	shorttitle = {A review of multi-site replication projects in social psychology},
	abstract = {Multi-site (multi-lab/many-lab) replications have emerged as a popular way of verifying prior research findings, but their record in social psychology has prompted distrust of the field and a sense of crisis. We review all 36 multi-site social psychology replications (plus three articles reporting multiple mini-studies). We start by assuming both the original and the multi-site replications were conducted in honest and diligent fashion, despite often yielding different conclusions. Four of the 36 (11\%) were clearly successful in terms of providing significant support for the original hypothesis, and five others (14\%) had mixed results. The remaining 27 (75\%) were failures. Multiple explanations for the generally poor record of replications are considered and relevant evidence assessed, including: original hypothesis was wrong; hypothesis not tested because of operational failure; low engagement of participants; and bias toward failure. There was evidence for each of these, with low engagement emerging as a widespread problem (reflected in high rates of discarded data and weak manipulation checks). The few procedures with actual interpersonal interaction fared much better than others. We discuss implications in relation to manipulation checks, effect sizes, and impact on the field and offer recommendations for improving future multi-site projects.},
	author = {Baumeister, Roy and Tice, Dianne and Bushman, Brad},
	date = {2022-08-11},
	keywords = {toread},
}

@article{prosser_when_nodate,
	title = {When open data closes the door: A critical examination of the past, present and the potential future for open data guidelines in journals},
	volume = {n/a},
	issn = {2044-8309},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bjso.12576},
	doi = {10.1111/bjso.12576},
	shorttitle = {When open data closes the door},
	abstract = {Opening data promises to improve research rigour and democratize knowledge production. But it also presents practical, theoretical, and ethical considerations for qualitative researchers in particular. Discussion about open data in qualitative social psychology predates the replication crisis. However, the nuances of this ongoing discussion have not been translated into current journal guidelines on open data. In this article, we summarize ongoing debates about open data from qualitative perspectives, and through a content analysis of 261 journals we establish the state of current journal policies for open data in the domain of social psychology. We critically discuss how current common expectations for open data may not be adequate for establishing qualitative rigour, can introduce ethical challenges, and may place those who wish to use qualitative approaches at a disadvantage in peer review and publication processes. We advise that future open data guidelines should aim to reflect the nuance of arguments surrounding data sharing in qualitative research, and move away from a universal “one-size-fits-all” approach to data sharing. This article outlines the past, present, and the potential future of open data guidelines in social-psychological journals. We conclude by offering recommendations for how journals might more inclusively consider the use of open data in qualitative methods, whilst recognizing and allowing space for the diverse perspectives, needs, and contexts of all forms of social-psychological research.},
	issue = {n/a},
	journaltitle = {British Journal of Social Psychology},
	author = {Prosser, Annayah M. B. and Hamshaw, Richard J. T. and Meyer, Johanna and Bagnall, Ralph and Blackwood, Leda and Huysamen, Monique and Jordan, Abbie and Vasileiou, Konstantina and Walter, Zoe},
	urldate = {2022-10-16},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bjso.12576},
	keywords = {open science, open data, psychology, content analysis, ethics, social psychology, qualitative, social sciences, journal guidelines, journals, qualitative methods, quantitative},
	file = {Prosser et al. - When open data closes the door A critical examination of the past, present and the potential future.pdf:/Users/tom/Zotero/storage/AN5UNLAH/Prosser et al. - When open data closes the door A critical examination of the past, present and the potential future.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/A8PK76ZR/bjso.html:text/html},
}

@article{ward_value-laden_2021,
	title = {On value-laden science},
	volume = {85},
	issn = {00393681},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0039368120301783},
	doi = {10.1016/j.shpsa.2020.09.006},
	pages = {54--62},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Ward, Zina B.},
	urldate = {2022-10-16},
	date = {2021-02},
	langid = {english},
	file = {On value-laden science:/Users/tom/Zotero/storage/EF47D5PP/ward2020.pdf.pdf:application/pdf},
}

@article{douglas_inductive_2000,
	title = {Inductive Risk and Values in Science},
	volume = {67},
	issn = {0031-8248, 1539-767X},
	url = {https://www.cambridge.org/core/product/identifier/S003182480005858X/type/journal_article},
	doi = {10.1086/392855},
	abstract = {Although epistemic values have become widely accepted as part of scientific reasoning, non-epistemic values have been largely relegated to the “external” parts of science (the selection of hypotheses, restrictions on methodologies, and the use of scientific technologies). I argue that because of inductive risk, or the risk of error, non-epistemic values are required in science wherever non-epistemic consequences of error should be considered. I use examples from dioxin studies to illustrate how non-epistemic consequences of error can and should be considered in the internal stages of science: choice of methodology, characterization of data, and interpretation of results.},
	pages = {559--579},
	number = {4},
	journaltitle = {Philosophy of Science},
	shortjournal = {Philos. of Sci.},
	author = {Douglas, Heather},
	urldate = {2022-10-16},
	date = {2000-12},
	langid = {english},
	file = {Inductive Risk and Values in Science:/Users/tom/Zotero/storage/3SDBF6EH/douglas2000.pdf.pdf:application/pdf},
}

@article{ihde_why_1997,
	title = {Why Not Science Critics?:},
	volume = {29},
	issn = {0270-5664},
	url = {http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=intstudphil_1997_0029_0001_0045_0054&svc_id=info:www.pdcnet.org/collection},
	doi = {10.5840/intstudphil19972915},
	shorttitle = {Why Not Science Critics?},
	pages = {45--54},
	number = {1},
	journaltitle = {International Studies in Philosophy},
	author = {Ihde, Don and {Binghamton University State University of New York}},
	urldate = {2022-10-16},
	date = {1997},
}

@incollection{allen_false_2020,
	location = {Cham},
	title = {False Beliefs and the Social Structure of Science: Some Models and Case Studies},
	isbn = {978-3-030-36821-0 978-3-030-36822-7},
	url = {http://link.springer.com/10.1007/978-3-030-36822-7_4},
	shorttitle = {False Beliefs and the Social Structure of Science},
	pages = {37--48},
	booktitle = {Groupthink in Science},
	publisher = {Springer International Publishing},
	author = {O’Connor, Cailin and Weatherall, James Owen},
	editor = {Allen, David M. and Howell, James W.},
	urldate = {2022-10-16},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-36822-7_4},
	file = {O’Connor and Weatherall - 2020 - False Beliefs and the Social Structure of Science.pdf:/Users/tom/Zotero/storage/YWUYJCGR/O’Connor and Weatherall - 2020 - False Beliefs and the Social Structure of Science.pdf:application/pdf},
}

@article{syed_guidelines_nodate,
	title = {Guidelines for establishing reliability when coding narrative data},
	abstract = {The use of quantitative, qualitative, and mixed methods approaches has been foundational to research on emerging adulthood, yet there remain many unresolved methodological issues pertaining to how to handle qualitative data. The purpose of this article is to review best practices for coding and establishing reliability when working with narrative data. In doing so, we highlight how establishing reliability must be seen as an evolving process, rather than simply a focus on the end product. The review is divided into three broad sections. In the first section, we discuss relatively more quantitatively-focused methods of coding and establishing reliability, whereas in the second section we discuss relatively more qualitatively-focused methods. In the final section, we provide recommendations for researchers interested in coding narrative and other types of open-ended data. This article is intended to serve as an essential resource for researchers working on a variety of topics related to emerging adulthood and beyond.},
	pages = {25},
	author = {Syed, Moin and Nelson, Sarah C},
	langid = {english},
	file = {Syed and Nelson - Guidelines for Establishing Reliability when Codin.pdf:/Users/tom/Zotero/storage/Q2FQFNSS/Syed and Nelson - Guidelines for Establishing Reliability when Codin.pdf:application/pdf},
}

@article{lewis_what_2021,
	title = {What counts as good science? How the battle for methodological legitimacy affects public psychology.},
	volume = {76},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000870},
	doi = {10.1037/amp0000870},
	shorttitle = {What counts as good science?},
	abstract = {Part of the “boundary work” (Gieryn, 1983) throughout the history of psychology has been to divide the discipline into camps of ‘basic’ and ‘applied’ researchers who take different methodological approaches to construct knowledge. Each ‘side’ has come up with different processes for conceptualizing, constructing, and evaluating the legitimacy of knowledge claims, processes that have implications for applying research insights to practical issues in society. In this paper I review and synthesize research on the history of knowledge construction in both basic and applied psychology, and the implications of their respective methodological practices for their perceived legitimacy. I then discuss how the lessons learned from the past can be leveraged to address the current crisis of confidence in the “credibility revolution” era (Vazire, 2018), as well as the field’s perceived legitimacy to external stakeholders. Finally, I end with recommendations for structural changes to improve the credibility and legitimacy of our field’s findings as well as their relevance for achieving our public psychology goals.},
	pages = {1323--1333},
	number = {8},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Lewis, Neil A.},
	urldate = {2022-10-17},
	date = {2021-11},
	langid = {english},
	file = {Lewis - 2021 - What counts as good science How the battle for me.pdf:/Users/tom/Zotero/storage/V23H9J8G/Lewis - 2021 - What counts as good science How the battle for me.pdf:application/pdf},
}

@book{heard_scientists_2016,
	location = {Princeton, New Jersey},
	title = {The scientist's guide to writing: how to write more easily and effectively throughout your scientific career},
	isbn = {978-0-691-17021-3 978-0-691-17022-0},
	shorttitle = {The scientist's guide to writing},
	pagetotal = {306},
	publisher = {Princeton University Press},
	author = {Heard, Stephen B.},
	date = {2016},
	langid = {english},
	keywords = {Technical writing},
	file = {Heard - 2016 - The scientist's guide to writing how to write mor.pdf:/Users/tom/Zotero/storage/RGWV4V4X/Heard - 2016 - The scientist's guide to writing how to write mor.pdf:application/pdf},
}

@article{ihde_why_1997-1,
	title = {Why not science critics?},
	volume = {29},
	issn = {0270-5664},
	url = {http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=intstudphil_1997_0029_0001_0045_0054&svc_id=info:www.pdcnet.org/collection},
	doi = {10.5840/intstudphil19972915},
	shorttitle = {Why not science critics?},
	pages = {45--54},
	number = {1},
	journaltitle = {International Studies in Philosophy},
	author = {Ihde, Don and {Binghamton University State University of New York}},
	urldate = {2022-10-17},
	date = {1997},
	file = {Ihde 1997 Why Not Science Critics.pdf:/Users/tom/Zotero/storage/2GLNBYV5/Ihde 1997 Why Not Science Critics.pdf:application/pdf},
}

@incollection{vallacher_models_2017,
	location = {New York : Routledge, 2017. {\textbar} Series: Frontiers of social psychology},
	edition = {1},
	title = {Models are stupid, and we need more of them},
	isbn = {978-1-315-17372-6},
	url = {https://www.taylorfrancis.com/books/9781351701686/chapters/10.4324/9781315173726-14},
	pages = {311--331},
	booktitle = {Computational Social Psychology},
	publisher = {Routledge},
	author = {Smaldino, Paul E.},
	editor = {Vallacher, Robin R. and Read, Stephen J. and Nowak, Andrzej},
	urldate = {2022-10-17},
	date = {2017-05-25},
	langid = {english},
	doi = {10.4324/9781315173726-14},
	file = {Smaldino - 2017 - Models Are Stupid, and We Need More of Them.pdf:/Users/tom/Zotero/storage/TPTGZKYF/Smaldino - 2017 - Models Are Stupid, and We Need More of Them.pdf:application/pdf},
}

@article{oreskes_role_nodate,
	title = {The role of quantitative models in science},
	abstract = {Models in science may be used for various purposes: organizing data, synthesizing information, and making predictions. However, the value of model predictions is undermined by their uncertainty, which arises primarily from the fact that our models of complex natural systems are always open. Models can never fully specify the systems that they describe, and therefore their predictions are always subject to uncertainties that we cannot fully specify. Moreover, the attempt to make models capture the complexities of natural systems leads to a paradox: the more we strive for realism by incorporating as many as possible of the different processes and parameters that we believe to be operating in the system, the more difficult it is for us to know if our tests of the model are meaningful. A complex model may be more realistic, yet it is ironic that as we add more factors to a model, the certainty of its predictions may decrease even as our intuitive faith in the model increases. For this and other reasons, model output should not be viewed as an accurate prediction of the future state of the system. Short timeframe model output can and should be used to evaluate models and suggest avenues for future study. Model output can also generate “what if” scenarios that can help to evaluate alternative courses of action (or inaction), including worst-case and best-case outcomes. But scientists should eschew long-range deterministic predictions, which are likely to be erroneous and may damage the credibility of the communities that generate them.},
	pages = {19},
	author = {Oreskes, Naomi},
	langid = {english},
	file = {Oreskes - The Role of Quantitative Models in Science.pdf:/Users/tom/Zotero/storage/YYQ9HKSR/Oreskes - The Role of Quantitative Models in Science.pdf:application/pdf},
}

@incollection{cretu_perspectival_2020,
	location = {Singapore},
	title = {Perspectival Realism},
	isbn = {978-981-287-532-7},
	url = {https://doi.org/10.1007/978-981-287-532-7_695-1},
	pages = {1--7},
	booktitle = {Encyclopedia of Educational Philosophy and Theory},
	publisher = {Springer},
	author = {Creţu, Ana-Maria},
	editor = {Peters, Michael A.},
	urldate = {2022-10-17},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-981-287-532-7_695-1},
}

@article{boutron_impact_2014,
	title = {Impact of spin in the abstracts of articles reporting results of randomized controlled trials in the field of cancer: the {SPIIN} randomized controlled trial},
	volume = {32},
	issn = {1527-7755},
	doi = {10.1200/JCO.2014.56.7503},
	shorttitle = {Impact of spin in the abstracts of articles reporting results of randomized controlled trials in the field of cancer},
	abstract = {{PURPOSE}: We aimed to assess the impact of spin (ie, reporting to convince readers that the beneficial effect of the experimental treatment is greater than shown by the results) on the interpretation of results of abstracts of randomized controlled trials ({RCTs}) in the field of cancer.
{METHODS}: We performed a two-arm, parallel-group {RCT}. We selected a sample of published {RCTs} with statistically nonsignificant primary outcome and with spin in the abstract conclusion. Two versions of these abstracts were used-the original with spin and a rewritten version without spin. Participants were clinician corresponding authors of articles reporting {RCTs}, investigators of trials, and reviewers of French national grants. The primary outcome was clinicians' interpretation of the beneficial effect of the experimental treatment (0 to 10 scale). Participants were blinded to study hypothesis.
{RESULTS}: Three hundred clinicians were randomly assigned using a Web-based system; 150 clinicians assessed an abstract with spin and 150 assessed an abstract without spin. For abstracts with spin, the experimental treatment was rated as being more beneficial (mean difference, 0.71; 95\% {CI}, 0.07 to 1.35; P = .030), the trial was rated as being less rigorous (mean difference, -0.59; 95\% {CI}, -1.13 to 0.05; P = .034), and clinicians were more interested in reading the full-text article (mean difference, 0.77; 95\% {CI}, 0.08 to 1.47; P = .029). There was no statistically significant difference in the clinicians' rating of the importance of the study or the need to run another trial.
{CONCLUSION}: Spin in abstracts can have an impact on clinicians' interpretation of the trial results.},
	pages = {4120--4126},
	number = {36},
	journaltitle = {Journal of Clinical Oncology: Official Journal of the American Society of Clinical Oncology},
	shortjournal = {J Clin Oncol},
	author = {Boutron, Isabelle and Altman, Douglas G. and Hopewell, Sally and Vera-Badillo, Francisco and Tannock, Ian and Ravaud, Philippe},
	date = {2014-12-20},
	pmid = {25403215},
	keywords = {Humans, Randomized Controlled Trials as Topic, Neoplasms},
	file = {Impact of spin in the abstracts of articles reporting results of randomized controlled trials in the field of cancer\: the SPIIN randomized controlled trial:/Users/tom/Zotero/storage/DGPWGIHY/boutron2014.pdf.pdf:application/pdf},
}

@online{portfolio_standards_2020,
	title = {Standards for evidence in policy decision-making},
	url = {http://socialsciences.nature.com/users/399005-kai-ruggeri/posts/standards-for-evidence-in-policy-decision-making},
	abstract = {A multidisciplinary team of scientists, policymakers, government officials, and academics present a framework for classifying evidence used in policy.},
	titleaddon = {Behavioural and Social Sciences at Nature Portfolio},
	author = {Portfolio, Behavioural \{and\} Social Sciences at Nature},
	urldate = {2022-10-18},
	date = {2020-05-23},
	langid = {english},
	note = {Section: {COVID}-19},
	file = {Snapshot:/Users/tom/Zotero/storage/B7I3QC8X/standards-for-evidence-in-policy-decision-making.html:text/html},
}

@article{haber_causal_2022,
	title = {Causal and associational language in observational health research: a systematic evaluation},
	issn = {0002-9262, 1476-6256},
	url = {https://academic.oup.com/aje/advance-article/doi/10.1093/aje/kwac137/6655746},
	doi = {10.1093/aje/kwac137},
	shorttitle = {Causal and associational language in observational health research},
	abstract = {Abstract
            We estimated the degree to which language used in the high-profile medical/public health/epidemiology literature implied causality using language linking exposures to outcomes and action recommendations; examined disconnects between language and recommendations; identified the most common linking phrases; and estimated how strongly linking phrases imply causality. We searched for and screened 1,170 articles from 18 high-profile journals (65 per journal) published from 2010–2019. Based on written framing and systematic guidance, 3 reviewers rated the degree of causality implied in abstracts and full text for exposure/outcome linking language and action recommendations. Reviewers rated the causal implication of exposure/outcome linking language as none (no causal implication) in 13.8\%, weak in 34.2\%, moderate in 33.2\%, and strong in 18.7\% of abstracts. The implied causality of action recommendations was higher than the implied causality of linking sentences for 44.5\% or commensurate for 40.3\% of articles. The most common linking word in abstracts was “associate” (45.7\%). Reviewers’ ratings of linking word roots were highly heterogeneous; over half of reviewers rated “association” as having at least some causal implication. This research undercuts the assumption that avoiding “causal” words leads to clarity of interpretation in medical research.},
	pages = {kwac137},
	journaltitle = {American Journal of Epidemiology},
	author = {Haber, Noah A and Wieten, Sarah E and Rohrer, Julia M and Arah, Onyebuchi A and Tennant, Peter W G and Stuart, Elizabeth A and Murray, Eleanor J and Pilleron, Sophie and Lam, Sze Tung and Riederer, Emily and Howcutt, Sarah Jane and Simmons, Alison E and Leyrat, Clémence and Schoenegger, Philipp and Booman, Anna and Dufour, Mi-Suk Kang and O’Donoghue, Ashley L and Baglini, Rebekah and Do, Stefanie and Takashima, Mari De La Rosa and Evans, Thomas Rhys and Rodriguez-Molina, Daloha and Alsalti, Taym M and Dunleavy, Daniel J and Meyerowitz-Katz, Gideon and Antonietti, Alberto and Calvache, Jose A and Kelson, Mark J and Salvia, Meg G and Parra, Camila Olarte and Khalatbari-Soltani, Saman and {McLinden}, Taylor and Chatton, Arthur and Seiler, Jessie and Steriu, Andreea and Alshihayb, Talal S and Twardowski, Sarah E and Dabravolskaj, Julia and Au, Eric and Hoopsick, Rachel A and Suresh, Shashank and Judd, Nicholas and Peña, Sebastián and Axfors, Cathrine and Khan, Palwasha and Rivera Aguirre, Ariadne E and Odo, Nnaemeka U and Schmid, Ian and Fox, Matthew P},
	urldate = {2022-10-18},
	date = {2022-08-04},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/KNZLFIHV/Haber et al. - 2022 - Causal and Associational Language in Observational.pdf:application/pdf},
}

@article{brown_belief_2013,
	title = {Belief beyond the evidence: using the proposed effect of breakfast on obesity to show 2 practices that distort scientific evidence},
	volume = {98},
	issn = {0002-9165},
	url = {https://doi.org/10.3945/ajcn.113.064410},
	doi = {10.3945/ajcn.113.064410},
	shorttitle = {Belief beyond the evidence},
	abstract = {Background: Various intentional and unintentional factors influence beliefs beyond what scientific evidence justifies. Two such factors are research lacking probative value ({RLPV}) and biased research reporting ({BRR}).Objective: We investigated the prevalence of {RLPV} and {BRR} in research about the proposition that skipping breakfast causes weight gain, which is called the proposed effect of breakfast on obesity ({PEBO}) in this article.Design: Studies related to the {PEBO} were synthesized by using a cumulative meta-analysis. Abstracts from these studies were also rated for the improper use of causal language and biased interpretations. In separate analyses, articles that cited an observational study about the {PEBO} were rated for the inappropriate use of causal language, and articles that cited a randomized controlled trial ({RCT}) about the {PEBO} were rated for misleadingly citing the {RCT}.Results: The current body of scientific knowledge indicates that the {PEBO} is only presumed true. The observational literature on the {PEBO} has gratuitously established the association, but not the causal relation, between skipping breakfast and obesity (final cumulative meta-analysis P value \&lt;10−42), which is evidence of {RLPV}. Four examples of {BRR} are evident in the {PEBO} literature as follows: 1) biased interpretation of one’s own results, 2) improper use of causal language in describing one’s own results, 3) misleadingly citing others’ results, and 4) improper use of causal language in citing others’ work.Conclusions: The belief in the {PEBO} exceeds the strength of scientific evidence. The scientific record is distorted by {RLPV} and {BRR}. {RLPV} is a suboptimal use of collective scientific resources.},
	pages = {1298--1308},
	number = {5},
	journaltitle = {The American Journal of Clinical Nutrition},
	shortjournal = {The American Journal of Clinical Nutrition},
	author = {Brown, Andrew W and Bohan Brown, Michelle M and Allison, David B},
	urldate = {2022-10-18},
	date = {2013-11-01},
	file = {Belief beyond the evidence\: using the proposed effect of breakfast on obesity to show 2 practices that distort scientific evidence:/Users/tom/Zotero/storage/LRWZKIEE/brown2013.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/BNECALYA/Brown et al. - 2013 - Belief beyond the evidence using the proposed eff.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KXA2A674/4577332.html:text/html},
}

@article{juni_hazards_1999,
	title = {The hazards of scoring the quality of clinical trials for meta-analysis},
	volume = {282},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.282.11.1054},
	doi = {10.1001/jama.282.11.1054},
	abstract = {{ContextAlthough} it is widely recommended that clinical trials undergo some type of quality review, the number and variety of quality assessment scales
that exist make it unclear how to achieve the best assessment.{ObjectiveTo} determine whether the type of quality assessment scale used affects
the conclusions of meta-analytic studies.Design and {SettingMeta}-analysis of 17 trials comparing low-molecular-weight heparin ({LMWH})
with standard heparin for prevention of postoperative thrombosis using 25
different scales to identify high-quality trials. The association between
treatment effect and summary scores and the association with 3 key domains
(concealment of treatment allocation, blinding of outcome assessment, and
handling of withdrawals) were examined in regression models.Main Outcome {MeasurePooled} relative risks of deep vein thrombosis with {LMWH} vs standard
heparin in high-quality vs low-quality trials as determined by 25 quality
scales.{ResultsPooled} relative risks from high-quality trials ranged from 0.63 (95\%
confidence interval [{CI}], 0.44-0.90) to 0.90 (95\% {CI}, 0.67-1.21) vs 0.52 (95\%
{CI}, 0.24-1.09) to 1.13 (95\% {CI}, 0.70-1.82) for low-quality trials. For 6 scales,
relative risks of high-quality trials were close to unity, indicating that
{LMWH} was not significantly superior to standard heparin, whereas low-quality
trials showed better protection with {LMWH} (P\&lt;.05).
Seven scales showed the opposite: high quality trials showed an effect whereas
low quality trials did not. For the remaining 12 scales, effect estimates
were similar in the 2 quality strata. In regression analysis, summary quality
scores were not significantly associated with treatment effects. There was
no significant association of treatment effects with allocation concealment
and handling of withdrawals. Open outcome assessment, however, influenced
effect size with the effect of {LMWH}, on average, being exaggerated by 35\%
(95\% {CI}, 1\%-57\%; P=.046).{ConclusionsOur} data indicate that the use of summary scores to identify trials
of high quality is problematic. Relevant methodological aspects should be
assessed individually and their influence on effect sizes explored.},
	pages = {1054--1060},
	number = {11},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Jüni, Peter and Witschi, Anne and Bloch, Ralph and Egger, Matthias},
	urldate = {2022-10-19},
	date = {1999-09-15},
	file = {Jüni et al. - 1999 - The hazards of scoring the quality of clinical tri.pdf:/Users/tom/Zotero/storage/ICNUSAHY/Jüni et al. - 1999 - The hazards of scoring the quality of clinical tri.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/I6QE4IIH/191652.html:text/html},
}

@article{roediger_double-edged_2022,
	title = {The double-edged sword of memory retrieval},
	rights = {2022 Springer Nature America, Inc.},
	issn = {2731-0574},
	url = {https://www.nature.com/articles/s44159-022-00115-2},
	doi = {10.1038/s44159-022-00115-2},
	abstract = {Accurately retrieving information from memory boosts later retrieval. However, retrieving memories can also open a window to errors when erroneous information is retrieved or when new information is encoded during retrieval. Similarly, the process of retrieval can influence recall of related information, either inhibiting or facilitating it depending upon the situation. In addition, retrieving or attempting to retrieve information can facilitate encoding of new information, regardless of whether the new information is correct or incorrect. In this Review, we provide selective coverage of the influences of memory retrieval in three distinct arenas: effects on the retrieved information itself, effects on retrieval of related information, and effects on information encoded just after an event is retrieved. Consideration of both positive and negative effects of retrieval in these three domains is critically important to understanding the complexity of retrieval processes and their effects. We discuss episodic context as a conceptual umbrella relevant to all these retrieval effects and note key directions for future research.},
	pages = {1--13},
	journaltitle = {Nature Reviews Psychology},
	shortjournal = {Nat Rev Psychol},
	author = {Roediger, Henry L. and Abel, Magdalena},
	urldate = {2022-10-20},
	date = {2022-10-17},
	langid = {english},
	keywords = {Psychology, Long-term memory, Human behaviour, Learning and memory},
}

@article{pallath_paperfetcher_nodate,
	title = {Paperfetcher: A tool to automate handsearching and citation searching for systematic reviews},
	volume = {n/a},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1604},
	doi = {10.1002/jrsm.1604},
	shorttitle = {Paperfetcher},
	abstract = {Systematic reviews are vital instruments for researchers to understand broad trends in a field and synthesize evidence on the effectiveness of interventions in addressing specific issues. The quality of a systematic review depends critically on having comprehensively surveyed all relevant literature on the review topic. In addition to database searching, handsearching is an important supplementary technique that helps increase the likelihood of identifying all relevant studies in a literature search. Traditional handsearching requires reviewers to manually browse through a curated list of field-specific journals and conference proceedings to find articles relevant to the review topic. This manual process is not only time-consuming, laborious, costly, and error-prone due to human fatigue, but it also lacks replicability due to its cumbersome manual nature. To address these issues, this paper presents a free and open-source Python package and an accompanying web-app, Paperfetcher, to automate the retrieval of article metadata for handsearching. With Paperfetcher’s assistance, researchers can retrieve article metadata from designated journals within a specified time frame in just a few clicks. In addition to handsearching, it also incorporates a beta version of citation searching in both forward and backward directions. Paperfetcher has an easy-to-use interface which allows researchers to download the metadata of retrieved studies as a list of {DOIs} or as an {RIS} file to facilitate seamless import into systematic review screening software. To the best of our knowledge, Paperfetcher is the first tool to automate handsearching with high usability and a multi-disciplinary focus. This article is protected by copyright. All rights reserved.},
	issue = {n/a},
	journaltitle = {Research Synthesis Methods},
	author = {Pallath, Akash and Zhang, Qiyang},
	urldate = {2022-10-20},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1604},
	keywords = {Systematic reviews, information retrieval, citation searching, handsearching, literature search, supplementary techniques},
}

@collection{higgins_cochrane_2008,
	location = {Chichester, England ; Hoboken, {NJ}},
	title = {Cochrane handbook for systematic reviews of interventions},
	isbn = {978-0-470-69951-5 978-0-470-05796-4},
	series = {Cochrane book series},
	pagetotal = {649},
	publisher = {Wiley-Blackwell},
	editor = {Higgins, Julian P. T. and Green, Sally and Cochrane Collaboration},
	date = {2008},
	langid = {english},
	note = {{OCLC}: ocn154798512},
	keywords = {Meta-analysis, Methodology, Evidence-Based Medicine, Meta-Analysis as Topic, toread, methods, Evidence-based medicine, Medicine, Outcome and Process Assessment (Health Care), Outcome assessment (Medical care), Research Evaluation, Review Literature as Topic},
	file = {Higgins et al. - 2008 - Cochrane handbook for systematic reviews of interv.pdf:/Users/tom/Zotero/storage/VJHYT9HP/Higgins et al. - 2008 - Cochrane handbook for systematic reviews of interv.pdf:application/pdf},
}

@article{silber_issue_2022,
	title = {The issue of noncompliance in attention check questions: false positives in instructed response items},
	volume = {34},
	issn = {1525-822X},
	url = {https://doi.org/10.1177/1525822X221115830},
	doi = {10.1177/1525822X221115830},
	shorttitle = {The issue of noncompliance in attention check questions},
	abstract = {Attention checks detect inattentiveness by instructing respondents to perform a specific task. However, while respondents may correctly process the task, they may choose to not comply with the instructions. We investigated the issue of noncompliance in attention checks in two web surveys. In Study 1, we measured respondents? attitudes toward attention checks and their self-reported compliance. In Study 2, we experimentally varied the reasons given to respondents for conducting the attention check. Our results showed that while most respondents understand why attention checks are conducted, a nonnegligible proportion of respondents evaluated them as controlling or annoying. Most respondents passed the attention check; however, among those who failed the test, 61\% seem to have failed the task deliberately. These findings reinforce that noncompliance is a serious concern with attention check instruments. The results of our experiment showed that more respondents passed the attention check if a comprehensible reason was given.},
	pages = {346--360},
	number = {4},
	journaltitle = {Field Methods},
	author = {Silber, Henning and Roßmann, Joss and Gummer, Tobias},
	urldate = {2022-10-21},
	date = {2022-11-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Silber et al. - 2022 - The issue of noncompliance in attention check ques.pdf:/Users/tom/Zotero/storage/TA3WBH3L/Silber et al. - 2022 - The issue of noncompliance in attention check ques.pdf:application/pdf},
}

@article{degtiar_review_2023,
	title = {A review of generalizability and transportability},
	volume = {10},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-042522-103837},
	doi = {10.1146/annurev-statistics-042522-103837},
	abstract = {When assessing causal effects, determining the target population to which the results are intended to generalize is a critical decision. Randomized and observational studies each have strengths and limitations for estimating causal effects in a target population. Estimates from randomized data may have internal validity but are often not representative of the target population. Observational data may better reflect the target population, and hence be more likely to have external validity, but are subject to potential bias due to unmeasured confounding. While much of the causal inference literature has focused on addressing internal validity bias, both internal and external validity are necessary for unbiased estimates in a target population. This article presents a framework for addressing external validity bias, including a synthesis of approaches for generalizability and transportability, and the assumptions they require, as well as tests for the heterogeneity of treatment effects and differences between study and target populations.
            Expected final online publication date for the Annual Review of Statistics and Its Application, Volume 10 is March 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
	pages = {annurev--statistics--042522--103837},
	number = {1},
	journaltitle = {Annual Review of Statistics and Its Application},
	shortjournal = {Annu. Rev. Stat. Appl.},
	author = {Degtiar, Irina and Rose, Sherri},
	urldate = {2022-10-24},
	date = {2023-03-07},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/ESLGHWM4/Degtiar and Rose - 2023 - A Review of Generalizability and Transportability.pdf:application/pdf},
}

@article{detsky_incorporating_1992,
	title = {Incorporating variations in the quality of individual randomized trials into meta-analysis},
	volume = {45},
	issn = {0895-4356},
	doi = {10.1016/0895-4356(92)90085-2},
	abstract = {Meta-analysis is a method of synthesizing evidence from multiple sources. It has been increasingly applied to combine results from randomized trials of therapeutic strategies. Unfortunately there is often variation in the quality of the trials that are included in meta-analyses, limiting the value of combining the results in an overview. This variation in quality can lead to both bias and reduction in precision of the estimate of the therapy's effectiveness. There are a number of methods for quantifying the quality of trials including the detailed Chalmers system and simple scales. The nature of the relationship between these quality scores and the true estimate of effectiveness is unknown at this time. We discuss four methods of incorporating quality into meta-analysis: threshold score as inclusion/exclusion criterion, use of quality score as a weight in statistical pooling, visual plot of effect size against quality score and sequential combination of trial results based on quality score. The last method permits an examination of the relation between quality and both bias and precision on the pooled estimates. We conclude that while it is possible to incorporate the effect of variation of quality of individual trials into overviews, this issue requires more study.},
	pages = {255--265},
	number = {3},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {J Clin Epidemiol},
	author = {Detsky, A. S. and Naylor, C. D. and O'Rourke, K. and {McGeer}, A. J. and L'Abbé, K. A.},
	date = {1992-03},
	pmid = {1569422},
	keywords = {Humans, Bias, Randomized Controlled Trials as Topic, Meta-Analysis as Topic, Quality Control, Epidemiologic Methods},
	file = {Incorporating variations in the quality of individual randomized trials into meta-analysis:/Users/tom/Zotero/storage/JFEKVW78/detsky1992.pdf.pdf:application/pdf},
}

@article{boutron_reporting_2010-1,
	title = {Reporting Methodological Items in Randomized Experiments in Political Science},
	volume = {628},
	issn = {0002-7162},
	url = {https://doi.org/10.1177/0002716209351518},
	doi = {10.1177/0002716209351518},
	abstract = {This article discusses the arguments for using the Consolidated Standards of Reporting Trials ({CONSORT}) procedures in political science field experiments, with the aim of improving the clarity and transparency of research work and reducing the possibility of bias. The article reviews the background to {CONSORT}, which is increasingly required for carrying out and reporting trials in healthcare and other disciplines. It sets out the main elements of the scheme and then applies its criteria to evaluate a published Get Out the Vote ({GOTV}) study by John and Brannan (2008). The {CONSORT} checklist shows the methods in this article to be clear and transparent but that {CONSORT} could improve the reporting of turnout experiments, such as details of the numbers going through the trial at each stage. The article argues that applying {CONSORT} to reports of trials in political science journals is a feasible and desirable objective.},
	pages = {112--131},
	number = {1},
	journaltitle = {The {ANNALS} of the American Academy of Political and Social Science},
	author = {Boutron, Isabelle and John, Peter and Torgerson, David J.},
	urldate = {2022-10-25},
	date = {2010-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Reporting Methodological Items in Randomized Experiments in Political Science:/Users/tom/Zotero/storage/AG9R7DZU/boutron2010.pdf.pdf:application/pdf},
}

@article{juni_systematic_2001,
	title = {Systematic reviews in health care: Assessing the quality of controlled clinical trials},
	volume = {323},
	issn = {0959-8138},
	doi = {10.1136/bmj.323.7303.42},
	shorttitle = {Systematic reviews in health care},
	pages = {42--46},
	number = {7303},
	journaltitle = {{BMJ} (Clinical research ed.)},
	shortjournal = {{BMJ}},
	author = {Jüni, P. and Altman, D. G. and Egger, M.},
	date = {2001-07-07},
	pmid = {11440947},
	pmcid = {PMC1120670},
	keywords = {Humans, Bias, Reproducibility of Results, Meta-Analysis as Topic, Controlled Clinical Trials as Topic},
	file = {Full Text:/Users/tom/Zotero/storage/X9NV96KD/Jüni et al. - 2001 - Systematic reviews in health care Assessing the q.pdf:application/pdf;Systematic reviews in health care\: Assessing the quality of controlled clinical trials:/Users/tom/Zotero/storage/LZR3Y62B/juni2001.pdf.pdf:application/pdf},
}

@article{oxman_science_1993-1,
	title = {The science of reviewing research},
	volume = {703},
	issn = {1749-6632},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1993.tb26342.x},
	doi = {10.1111/j.1749-6632.1993.tb26342.x},
	pages = {125--134},
	number = {1},
	journaltitle = {Annals of the New York Academy of Sciences},
	author = {Oxman, Andrew D. and Guyatt, Gordon H.},
	urldate = {2022-10-25},
	date = {1993},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.1993.tb26342.x},
	file = {Snapshot:/Users/tom/Zotero/storage/JIS3RGUX/j.1749-6632.1993.tb26342.html:text/html;The Science of Reviewing Researcha:/Users/tom/Zotero/storage/84Q5KTQB/oxman1993.pdf.pdf:application/pdf},
}

@article{cook_william_2016,
	title = {William Raymond Shadish Jr.: Will Shadish's Intellectual Accomplishments},
	volume = {37},
	issn = {1098-2140, 1557-0878},
	url = {http://journals.sagepub.com/doi/10.1177/1098214016666714},
	doi = {10.1177/1098214016666714},
	shorttitle = {William Raymond Shadish Jr.},
	pages = {589--591},
	number = {4},
	journaltitle = {American Journal of Evaluation},
	shortjournal = {American Journal of Evaluation},
	author = {Cook, Thomas D.},
	urldate = {2022-10-26},
	date = {2016-12},
	langid = {english},
	file = {William Raymond Shadish Jr.\: Will Shadish's Intellectual Accomplishments:/Users/tom/Zotero/storage/HPJTYA3W/cook2016.pdf.pdf:application/pdf},
}

@article{sterne_investigating_2001,
	title = {Investigating and dealing with publication and other biases in meta-analysis},
	volume = {323},
	rights = {© 2001 {BMJ} Publishing Group Ltd.},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/323/7304/101},
	doi = {10.1136/bmj.323.7304.101},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater} {\textless}p{\textgreater}Obesity drives significant changes in adipose tissue that precede development of tissue and systemic insulin resistance. Immune cell infiltration and inflammation are known contributors to these changes but there is limited understanding of their spatial context tissue-wide. We sought to identify spatial patterning in epididymal adipose tissue immune cells in a time course of diet-induced obesity in mice.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater} {\textless}p{\textgreater}Using spatial transcriptomics and single-cell {RNA}-sequencing, we identified dominant cell type signatures preserved in their anatomical context, quantified gene expression patterns at spots throughout adipose tissue, performed cell type network analysis, and investigated ligand-receptor colocalization.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater} {\textless}p{\textgreater}Our data support increased innate immune cells, including macrophages, monocytes, and innate lymphoid cells with tissue-wide interspersion, and dampened adaptive immune cell signatures with obesity. Network analysis identified increased heterogeneity in all major immune cell types, consistent with increased subtypes. To capture tissue dynamics at obesity onset, we draw on mathematical principles from linear algebra and spectral graph theory and provide a framework for better understanding cell cooperation toward emergence of multicellular tissue function.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusion{\textless}/h3{\textgreater} {\textless}p{\textgreater}The culmination of these analyses revealed a widespread paradigm shift in ligand-receptor activity with near-exclusive macrophage-macrophage or monocyte-macrophage interactions at crown-like structures across adipose tissue in early obesity.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Highlights{\textless}/h3{\textgreater} {\textless}p{\textgreater}Spatial transcriptomics shows innate immune cell dominance in obese adipose tissue{\textless}/p{\textgreater}{\textless}p{\textgreater}Increased monocyte signaling accompanies infiltration in obesity{\textless}/p{\textgreater}{\textless}p{\textgreater}Pre-crown-like niches precede crown-like structure formation{\textless}/p{\textgreater}{\textless}p{\textgreater}Influential monocyte-macrophage ligand-receptor pairs emerge at crown-like niches{\textless}/p{\textgreater}},
	pages = {101--105},
	number = {7304},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Sterne, Jonathan A. C. and Egger, Matthias and Smith, George Davey},
	urldate = {2022-10-26},
	date = {2001-07-14},
	langid = {english},
	pmid = {11451790},
	note = {Publisher: British Medical Journal Publishing Group
Section: Education and debate},
	file = {Investigating and dealing with publication and other biases in meta-analysis:/Users/tom/Zotero/storage/3GCDCWJL/sterne2001.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5BRDTA6Q/101.html:text/html},
}

@article{matthay_alternative_2020,
	title = {Alternative causal inference methods in population health research: Evaluating tradeoffs and triangulating evidence},
	volume = {10},
	issn = {23528273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352827319301545},
	doi = {10.1016/j.ssmph.2019.100526},
	shorttitle = {Alternative causal inference methods in population health research},
	pages = {100526},
	journaltitle = {{SSM} - Population Health},
	shortjournal = {{SSM} - Population Health},
	author = {Matthay, Ellicott C. and Hagan, Erin and Gottlieb, Laura M. and Tan, May Lynn and Vlahov, David and Adler, Nancy E. and Glymour, M. Maria},
	urldate = {2022-10-26},
	date = {2020-04},
	langid = {english},
	file = {Alternative causal inference methods in population health research\: Evaluating tradeoffs and triangulating evidence:/Users/tom/Zotero/storage/SS4YZXL6/10.1016@j.ssmph.2019.100526.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/QJQB7VCG/Matthay et al. - 2020 - Alternative causal inference methods in population.pdf:application/pdf},
}

@article{winship_estimation_1999,
	title = {The estimation of causal effects from observational data},
	volume = {25},
	issn = {0360-0572, 1545-2115},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.soc.25.1.659},
	doi = {10.1146/annurev.soc.25.1.659},
	abstract = {▪ Abstract  When experimental designs are infeasible, researchers must resort to the use of observational data from surveys, censuses, and administrative records. Because assignment to the independent variables of observational data is usually nonrandom, the challenge of estimating causal effects with observational data can be formidable. In this chapter, we review the large literature produced primarily by statisticians and econometricians in the past two decades on the estimation of causal effects from observational data. We first review the now widely accepted counterfactual framework for the modeling of causal effects. After examining estimators, both old and new, that can be used to estimate causal effects from cross-sectional data, we present estimators that exploit the additional information furnished by longitudinal data. Because of the size and technical nature of the literature, we cannot offer a fully detailed and comprehensive presentation. Instead, we present only the main features of methods that are accessible and potentially of use to quantitatively oriented sociologists.},
	pages = {659--706},
	number = {1},
	journaltitle = {Annual Review of Sociology},
	shortjournal = {Annu. Rev. Sociol.},
	author = {Winship, Christopher and Morgan, Stephen L.},
	urldate = {2022-10-26},
	date = {1999-08},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/5K9RM7CG/Winship and Morgan - 1999 - THE ESTIMATION OF CAUSAL EFFECTS FROM OBSERVATIONA.pdf:application/pdf;THE ESTIMATION OF CAUSAL EFFECTS FROM OBSERVATIONAL DATA:/Users/tom/Zotero/storage/N4AMBJWB/winship1999.pdf.pdf:application/pdf},
}

@incollection{egger_systematic_2022,
	edition = {3},
	title = {Systematic Reviews in Health Research: Meta-analysis in Context},
	isbn = {978-1-4051-6050-6 978-1-119-09936-9},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781119099369.fmatter},
	booktitle = {Systematic Reviews in Health Research: Meta-analysis in Context},
	publisher = {Wiley},
	editor = {Egger, Matthias and Higgins, Julian P.T. and Davey Smith, George},
	urldate = {2022-10-27},
	date = {2022-04-22},
	langid = {english},
	doi = {10.1002/9781119099369.fmatter},
	file = {Egger et al. - 2022 - Front Matter.pdf:/Users/tom/Zotero/storage/XMKCAD8X/Egger et al. - 2022 - Front Matter.pdf:application/pdf},
}

@article{welton_models_2009,
	title = {Models for potentially biased evidence in meta-analysis using empirically based priors},
	volume = {172},
	issn = {1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-985X.2008.00548.x},
	doi = {10.1111/j.1467-985X.2008.00548.x},
	abstract = {Summary. We present models for the combined analysis of evidence from randomized controlled trials categorized as being at either low or high risk of bias due to a flaw in their conduct. We formulate a bias model that incorporates between-study and between-meta-analysis heterogeneity in bias, and uncertainty in overall mean bias. We obtain algebraic expressions for the posterior distribution of the bias-adjusted treatment effect, which provide limiting values for the information that can be obtained from studies at high risk of bias. The parameters of the bias model can be estimated from collections of previously published meta-analyses. We explore alternative models for such data, and alternative methods for introducing prior information on the bias parameters into a new meta-analysis. Results from an illustrative example show that the bias-adjusted treatment effect estimates are sensitive to the way in which the meta-epidemiological data are modelled, but that using point estimates for bias parameters provides an adequate approximation to using a full joint prior distribution. A sensitivity analysis shows that the gain in precision from including studies at high risk of bias is likely to be low, however numerous or large their size, and that little is gained by incorporating such studies, unless the information from studies at low risk of bias is limited. We discuss approaches that might increase the value of including studies at high risk of bias, and the acceptability of the methods in the evaluation of health care interventions.},
	pages = {119--136},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Welton, N. J. and Ades, A. E. and Carlin, J. B. and Altman, D. G. and Sterne, J. a. C.},
	urldate = {2022-10-27},
	date = {2009},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-985X.2008.00548.x},
	keywords = {Bayesian methods, Bias, Randomized controlled trials, Health technology assessment, Markov chain Monte Carlo methods},
	file = {Models for potentially biased evidence in meta-analysis using empirically based priors:/Users/tom/Zotero/storage/FEY2Z9NX/welton2009.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PXLA4PSX/j.1467-985X.2008.00548.html:text/html},
}

@article{hemming_stepped_2015,
	title = {The stepped wedge cluster randomised trial: rationale, design, analysis, and reporting},
	volume = {350},
	rights = {© Hemming et al 2015.             This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution ({CC} {BY} 4.0) license, which permits others to distribute, remix, adapt, build upon this work, for commercial use, provided the original work is properly cited. See:http://creativecommons.org/licenses/by/4.0/},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/350/bmj.h391},
	doi = {10.1136/bmj.h391},
	shorttitle = {The stepped wedge cluster randomised trial},
	abstract = {{\textless}p{\textgreater}Absent a remission of proteinuria, primary membranous nephropathy ({MN}) can lead to {ESRD} over many years. Therefore, use of an earlier end point could facilitate the conduct of clinical trials. This manuscript evaluates complete remission ({CR}) and partial remission ({PR}) of proteinuria as surrogate end points for a treatment effect on {ESRD} in patients with primary {MN} with heavy proteinuria. {CR} is associated with a low relapse rate and excellent long–term renal survival, and it plausibly reflects remission of the disease process that leads to {ESRD}. Patients who achieve {PR} have better renal outcomes than those who do not but may have elevated relapse rates. How long {PR} must be maintained to yield a benefit on renal outcomes is also unknown. Hence, available data suggest that {CR} could be used as a surrogate end point in primary {MN}, whereas {PR} seems reasonably likely to predict clinical benefit. In the United States, surrogate end points that are reasonably likely to predict clinical benefit can be used as a basis for accelerated approval; treatments approved under this program must verify the clinical benefit in postmarketing trials. Additional analyses of the relationship between treatment effects on {CR} and {PR} and subsequent renal outcomes would inform the design of future clinical trials in primary {MN}.{\textless}/p{\textgreater}},
	pages = {h391},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Hemming, K. and Haines, T. P. and Chilton, P. J. and Girling, A. J. and Lilford, R. J.},
	urldate = {2022-10-28},
	date = {2015-02-06},
	langid = {english},
	pmid = {25662947},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {The stepped wedge cluster randomised trial\: rationale, design, analysis, and reporting:/Users/tom/Zotero/storage/84YWSNBW/8052207d75b6c82de7de06f726a756a6.pdf.pdf:application/pdf},
}

@misc{allum_researchers_2022,
	title = {Researchers on research integrity: a survey of European and American researchers},
	url = {https://osf.io/preprints/metaarxiv/fgy7c/},
	doi = {10.31222/osf.io/fgy7c},
	shorttitle = {Researchers on research integrity},
	abstract = {Reports of questionable or detrimental research practices ({QRPs}) call into question the reliability of scientific evidence. The International Research Integrity Survey ({IRIS}) maps the opinions and behaviors of 2,300 researchers based in the {US} and 45,000 in Europe (including {UK}, Norway, Iceland and Switzerland)., and how they assess their institutions’ support for research integrity ({RI}). In comparison to researchers in the {US}, European researchers admit to more {QRPs} and are less confident in maintaining high {RI} standards. In the {US} and Europe many researchers judge their organization to fall short of best {RI} practice. All researchers recognize the benefits of {RI}, reliable knowledge and the trust of colleagues and the public, and there is support for {RI} training particularly among Europeans. To create and maintain a culture of integrity in scientific research, a collective commitment from researchers, their institutions and funders is needed.},
	publisher = {{MetaArXiv}},
	author = {Allum, Nick and Reid, Abigail and Bidoglia, Miriam and Gaskell, George and Bonn, Noémie Aubert and Buljan, Ivan and Fuglsang, Simon and Horbach, Serge and Kavouras, Panagiotis and Marusic, Ana and Mejlgaard, Niels and Pizzolato, Daniel and Roje, Rea and Tijdink, Joeri K. and Veltri, Giuseppe A.},
	urldate = {2022-10-28},
	date = {2022-10-27},
	langid = {english},
	keywords = {questionable research practices, Social and Behavioral Sciences, meta-research, research integrity, responsible conduct of research, Other Social and Behavioral Sciences, research policy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/3RE78FWS/Allum et al. - 2022 - Researchers on research integrity a survey of Eur.pdf:application/pdf},
}

@article{stanley_beyond_2022,
	title = {Beyond Random Effects: When Small-Study Findings Are More Heterogeneous},
	volume = {5},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/25152459221120427},
	doi = {10.1177/25152459221120427},
	shorttitle = {Beyond Random Effects},
	abstract = {New meta-regression methods are introduced that identify whether the magnitude of heterogeneity across study findings is correlated with their standard errors. Evidence from dozens of meta-analyses finds robust evidence of this correlation and that small-sample studies typically have higher heterogeneity. This correlated heterogeneity violates the random-effects ({RE}) model of additive and independent heterogeneity. When small studies not only have inadequate statistical power but also high heterogeneity, their scientific contribution is even more dubious. When the heterogeneity variance is correlated with the sampling-error variance to the degree we find, simulations show that {RE} is dominated by an alternative weighted average, the unrestricted weighted least squares ({UWLS}). Meta-research evidence combined with simulations establish that {UWLS} should replace {RE} as the conventional meta-analysis summary of psychological research.},
	pages = {25152459221120427},
	number = {4},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Stanley, T. D. and Doucouliagos, Hristos and Ioannidis, John P. A.},
	urldate = {2022-10-29},
	date = {2022-10-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@article{voldal_swcrtdesign_2020,
	title = {{swCRTdesign}: An R Package for Stepped Wedge Trial Design and Analysis},
	volume = {196},
	issn = {0169-2607},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8559260/},
	doi = {10.1016/j.cmpb.2020.105514},
	shorttitle = {{swCRTdesign}},
	abstract = {Background and objective:
Stepped wedge trials ({SWTs}) are a type of cluster-randomized trial that are commonly used to evaluate health care interventions. Most {SWT}-related software packages have restrictive assumptions about the study design and correlation structure of the data. The objective of this paper is to present a package and corresponding web-based graphical user interface ({GUI}) that provide researchers with another, more flexible option for {SWT} design and analysis.

Methods:
We developed an R package {swCRTdesign} (‘stepped wedge Cluster Randomized Trial design’), which uses a random effects model to account for correlation in the data induced by a {SWT} design. Possible sources of correlation include clusters, time within clusters, and treatment within clusters.

Results:
{swCRTdesign} allows a user to calculate power, simulate {SWT} data to streamline simulation studies (e.g. to estimate power), and create descriptive summaries and plots. Additionally, a {GUI}, developed using shiny, is available to calculate power and create power curves and design plots.

Conclusions:
The {swCRTdesign} package accommodates a wide variety of {SWT} designs, and makes it easy to account for some sources of correlation which are not found in other packages. The user-friendly web-based {GUI} makes some {swCRTdesign} features accessible to researchers not familiar with R. These two resources will make appropriately complex {SWT} calculations more accessible to scientists from a wide variety of backgrounds.},
	pages = {105514},
	journaltitle = {Computer methods and programs in biomedicine},
	shortjournal = {Comput Methods Programs Biomed},
	author = {Voldal, Emily C. and Hakhu, Navneet R. and Xia, Fan and Heagerty, Patrick J. and Hughes, James P.},
	urldate = {2022-10-30},
	date = {2020-11},
	pmid = {32554025},
	pmcid = {PMC8559260},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/3LS96XKT/Voldal et al. - 2020 - swCRTdesign An R Package for Stepped Wedge Trial .pdf:application/pdf;swCRTdesign\: An R Package for Stepped Wedge Trial Design and Analysis:/Users/tom/Zotero/storage/PQLBRA8Y/voldal2020.pdf.pdf:application/pdf},
}

@article{angus_fusing_2015,
	title = {Fusing randomized trials with big data: the key to self-learning health care systems?},
	volume = {314},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2015.7762},
	doi = {10.1001/jama.2015.7762},
	shorttitle = {Fusing randomized trials with big data},
	abstract = {Randomized clinical trials ({RCTs}) have revolutionized medicine by providing evidence on the efficacy and safety of drugs, devices, and procedures. Today, more than 40 000 {RCTs} are reported annually, their quality continues to increase, and oversight mechanisms ensure adequate protection of participants. However, {RCTs} have at least 4 related problems: (1) they are too expensive and difficult; (2) their findings are too broad (average treatment effect not representative of benefit for any given individual) and too narrow (trial population and setting not representative of general practice); (3) randomizing patients can make patients and physicians uncomfortable, especially when comparing different types of existing care; and (4) there are often long delays before {RCT} results diffuse into practice.},
	pages = {767--768},
	number = {8},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Angus, Derek C.},
	urldate = {2022-10-30},
	date = {2015-08-25},
	file = {Fusing Randomized Trials With Big Data\: The Key to Self-learning Health Care Systems?:/Users/tom/Zotero/storage/YU2UAS8J/angus2015.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/5ZR5QXSB/2429723.html:text/html},
}

@article{dunn_application_1929,
	title = {Application of statistical methods in physiology},
	url = {https://journals.physiology.org/doi/10.1152/physrev.1929.9.2.275},
	doi = {10.1152/physrev.1929.9.2.275},
	journaltitle = {Physiological Reviews},
	author = {Dunn, Halbert L.},
	urldate = {2022-11-01},
	date = {1929-04-01},
	langid = {english},
	file = {APPLICATION OF STATISTICAL METHODS IN PHYSIOLOGY:/Users/tom/Zotero/storage/IKV282EW/dunn1929.pdf.pdf:application/pdf},
}

@article{jinha_article_2010,
	title = {Article 50 million: an estimate of the number of scholarly articles in existence},
	volume = {23},
	issn = {1741-4857},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1087/20100308},
	doi = {10.1087/20100308},
	shorttitle = {Article 50 million},
	abstract = {How many scholarly research articles are there in existence? Journal articles first appeared in 1665, and the cumulative total is estimated here to have passed 50 million in 2009. This sum was arrived at based on published figures for global annual output for 2006, and analyses of annual output and growth rates published in the last decade.},
	pages = {258--263},
	number = {3},
	journaltitle = {Learned Publishing},
	author = {Jinha, Arif E.},
	urldate = {2022-11-01},
	date = {2010},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1087/20100308},
	file = {Article 50 million\: an estimate of the number of scholarly articles in existence:/Users/tom/Zotero/storage/EZYF79FY/jinha2010.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/IWR4FJHA/Jinha - 2010 - Article 50 million an estimate of the number of s.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/P5QBWFT3/20100308.html:text/html},
}

@article{cristea_post-retrieval_2018,
	title = {Post-retrieval Tetris should not be likened to a ‘cognitive vaccine’},
	volume = {23},
	rights = {2018 Springer Nature Limited},
	issn = {1476-5578},
	url = {https://www.nature.com/articles/mp2017222},
	doi = {10.1038/mp.2017.222},
	pages = {1972--1973},
	number = {10},
	journaltitle = {Molecular Psychiatry},
	shortjournal = {Mol Psychiatry},
	author = {Cristea, I. A. and Naudet, F. and Shanks, D. R. and Hardwicke, T. E.},
	urldate = {2022-11-01},
	date = {2018-10},
	langid = {english},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Behavioral Sciences, Biological Psychology, general, Medicine/Public Health, Neurosciences, Pharmacotherapy, Psychiatry},
	file = {Post-retrieval Tetris should not be likened to a ‘cognitive vaccine’:/Users/tom/Zotero/storage/5BVKQCRG/cristea2017.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6RZDQ2F8/mp2017222.html:text/html;Submitted Version:/Users/tom/Zotero/storage/BH7927I6/Cristea et al. - 2018 - Post-retrieval Tetris should not be likened to a ‘.pdf:application/pdf},
}

@article{schiavone_reckoning_2022,
	title = {Reckoning with our crisis: an agenda for the field of social and personality psychology},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/17456916221101060},
	doi = {10.1177/17456916221101060},
	shorttitle = {Reckoning with our crisis},
	abstract = {The replication crisis and credibility revolution in the 2010s brought a wave of doubts about the credibility of social and personality psychology. We argue that as a field, we must reckon with the concerns brought to light during this critical decade. How the field responds to this crisis will reveal our commitment to self-correction. If we do not take the steps necessary to address our problems and simply declare the crisis to be over or the problems to be fixed without evidence, we risk further undermining our credibility. To fully reckon with this crisis, we must empirically assess the state of the field to take stock of how credible our science actually is and whether it is improving. We propose an agenda for metascientific research, and we review approaches to empirically evaluate and track where we are as a field (e.g., analyzing the published literature, surveying researchers). We describe one such project (Surveying the Past and Present State of Published Studies in Social and Personality Psychology) underway in our research group. Empirical evidence about the state of our field is necessary if we are to take self-correction seriously and if we hope to avert future crises.},
	pages = {17456916221101060},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Schiavone, Sarah R. and Vazire, Simine},
	urldate = {2022-11-01},
	date = {2022-10-27},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@article{gusenbauer_what_2021,
	title = {What every researcher should know about searching – clarified concepts, search advice, and an agenda to improve finding in academia},
	volume = {12},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1457},
	doi = {10.1002/jrsm.1457},
	abstract = {We researchers have taken searching for information for granted for far too long. The {COVID}-19 pandemic shows us the boundaries of academic searching capabilities, both in terms of our know-how and of the systems we have. With hundreds of studies published daily on {COVID}-19, for example, we struggle to find, stay up-to-date, and synthesize information—all hampering evidence-informed decision making. This {COVID}-19 information crisis is indicative of the broader problem of information overloaded academic research. To improve our finding capabilities, we urgently need to improve how we search and the systems we use. We respond to Klopfenstein and Dampier (Res Syn Meth. 2020) who commented on our 2020 paper and proposed a way of improving {PubMed}'s and Google Scholar's search functionalities. Our response puts their commentary in a larger frame and suggests how we can improve academic searching altogether. We urge that researchers need to understand that search skills require dedicated education and training. Better and more efficient searching requires an initial understanding of the different goals that define the way searching needs to be conducted. We explain the main types of searching that we academics routinely engage in; distinguishing lookup, exploratory, and systematic searching. These three types must be conducted using different search methods (heuristics) and using search systems with specific capabilities. To improve academic searching, we introduce the “Search Triangle” model emphasizing the importance of matching goals, heuristics, and systems. Further, we suggest an urgently needed agenda toward search literacy as the norm in academic research and fit-for-purpose search systems.},
	pages = {136--147},
	number = {2},
	journaltitle = {Research Synthesis Methods},
	author = {Gusenbauer, Michael and Haddaway, Neal R.},
	urldate = {2022-11-01},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1457},
	file = {Full Text PDF:/Users/tom/Zotero/storage/U3LWXDPY/Gusenbauer and Haddaway - 2021 - What every researcher should know about searching .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/S2ALWDHC/jrsm.html:text/html;What every researcher should know about searching – clarified concepts, search advice, and an agenda to improve finding in academia:/Users/tom/Zotero/storage/TMJ8VIUB/10.1002@jrsm.1457.pdf.pdf:application/pdf},
}

@article{noauthor_tell_2020,
	title = {Tell it like it is [Editorial]},
	volume = {4},
	rights = {2020 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-0818-9},
	doi = {10.1038/s41562-020-0818-9},
	abstract = {Every research paper tells a story, but the pressure to provide ‘clean’ narratives is harmful for the scientific endeavour.},
	pages = {1--1},
	number = {1},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	urldate = {2022-11-02},
	date = {2020-01},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Life Sciences, Experimental Psychology, Behavioral Sciences, general, Neurosciences, Microeconomics, Personality and Social Psychology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/6ZV95QP7/2020 - Tell it like it is.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/RH5T76AA/s41562-020-0818-9.html:text/html;Tell it like it is:/Users/tom/Zotero/storage/DU7KLKL3/10.1038@s41562-020-0818-9.pdf.pdf:application/pdf},
}

@article{giofre_influence_2022,
	title = {The influence of journal submission guidelines on authors’ reporting of statistics and use of open research practices: Five years later},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-022-01993-3},
	doi = {10.3758/s13428-022-01993-3},
	shorttitle = {The influence of journal submission guidelines on authors’ reporting of statistics and use of open research practices},
	abstract = {Changes in statistical practices and reporting have been documented by Giofrè et al. {PLOS} {ONE} 12(4), e0175583 (2017), who investigated ten statistical and open practices in two high-ranking journals (Psychological Science [{PS}] and Journal of Experimental Psychology-General [{JEPG}]): null hypothesis significance testing; confidence or credible intervals; meta-analysis of the results of multiple experiments; confidence interval interpretation; effect size interpretation; sample size determination; data exclusion; data availability; materials availability; and preregistered design and analysis plan. The investigation was based on an analysis of all papers published in these journals between 2013 and 2015. The aim of the present study was to follow up changes in both {PS} and {JEPG} in subsequent years, from 2016 to 2020, adding code availability as a further open practice. We found improvement in most practices, with some exceptions (i.e., confidence interval interpretation and meta-analysis). Despite these positive changes, our results indicate a need for further improvements in statistical practices and adoption of open practices.},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Giofrè, David and Boedker, Ingrid and Cumming, Geoff and Rivella, Carlotta and Tressoldi, Patrizio},
	urldate = {2022-11-02},
	date = {2022-10-17},
	langid = {english},
	keywords = {Author submission guidelines, Open science practices, Statistical practices},
	file = {Full Text PDF:/Users/tom/Zotero/storage/X8UALLAG/Giofrè et al. - 2022 - The influence of journal submission guidelines on .pdf:application/pdf},
}

@article{hemming_how_2017,
	title = {How to design efficient cluster randomised trials},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.j3064},
	doi = {10.1136/bmj.j3064},
	pages = {j3064},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Hemming, K and Eldridge, S and Forbes, G and Weijer, C and Taljaard, M},
	urldate = {2022-11-02},
	date = {2017-07-14},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/ERLWRTD6/Hemming et al. - 2017 - How to design efficient cluster randomised trials.pdf:application/pdf;How to design efficient cluster randomised trials:/Users/tom/Zotero/storage/5U6YD774/hemming2017.pdf.pdf:application/pdf},
}

@article{jones_peer_2019,
	title = {Peer reviewed evaluation of registered end-points of randomised trials (the {PRE}-{REPORT} study): protocol for a stepped-wedge, cluster-randomised trial},
	volume = {9},
	issn = {2044-6055, 2044-6055},
	url = {https://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2018-028694},
	doi = {10.1136/bmjopen-2018-028694},
	shorttitle = {Peer reviewed evaluation of registered end-points of randomised trials (the {PRE}-{REPORT} study)},
	abstract = {Introduction
              Clinical trials are critical to the advancement of medical knowledge. However, the reliability of trial conclusions depends in part on consistency between pre-planned and reported study outcomes. Unfortunately, selective outcome reporting, in which outcomes reported in published manuscripts differ from pre-specified study outcomes, is common. Trial registries such as {ClinicalTrials}.gov have the potential to help identify and stop selective outcome reporting during peer review by allowing peer reviewers to compare outcomes between registry entries and submitted manuscripts. However, the persistently high rate of selective outcome reporting among published clinical trials indicates that the current peer review process at most journals does not effectively address the problem of selective outcome reporting.
            
            
              Methods and analysis
              {PRE}-{REPORT} is a stepped-wedge cluster-randomised trial that will test whether providing peer reviewers with a summary of registered, pre-specified primary trial outcomes decreases inconsistencies between prospectively registered and published primary outcomes. Peer reviewed manuscripts describing clinical trial results will be included. Eligible manuscripts submitted to each participating journal during the study period will comprise each cluster. After an initial control phase, journals will transition to the intervention phase in random order, after which peer reviewers will be emailed registry information consisting of the date of registration and any prospectively defined primary outcomes. Blinded outcome assessors will compare registered and published primary outcomes for all included trials. The primary {PRE}-{REPORT} outcome is the presence of a published primary outcome that is consistent with a prospectively defined primary outcome in the study’s trial registry. The primary outcome will be analysed using a mixed effect logistical regression model to compare results between the intervention and control phases.
            
            
              Ethics and dissemination
              The Cooper Health System Institutional Review Board determined that this study does not meet criteria for human subject research. Findings will be published in peer-reviewed journals.
            
            
              Trial registration number
              
                {ISRCTN}41225307
                ; Pre-results.},
	pages = {e028694},
	number = {5},
	journaltitle = {{BMJ} Open},
	shortjournal = {{BMJ} Open},
	author = {Jones, Christopher W and Adams, Amanda and Weaver, Mark A and Schroter, Sara and Misemer, Benjamin S and Schriger, David and Platts-Mills, Timothy F},
	urldate = {2022-11-02},
	date = {2019-05},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/83HRG4MD/Jones et al. - 2019 - Peer reviewed evaluation of registered end-points .pdf:application/pdf;Peer reviewed evaluation of registered end-points of randomised trials (the PRE-REPORT study)\: protocol for a stepped-wedge, cluster-randomised trial:/Users/tom/Zotero/storage/J6366C7N/jones2019.pdf.pdf:application/pdf},
}

@article{walker_dissociable_2003,
	title = {Dissociable stages of human memory consolidation and reconsolidation},
	volume = {425},
	rights = {2003 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature01930},
	doi = {10.1038/nature01930},
	abstract = {Historically, the term ‘memory consolidation’ refers to a process whereby a memory becomes increasingly resistant to interference from competing or disrupting factors with the continued passage of time1. Recent findings regarding the learning of skilled sensory and motor tasks (‘procedural learning’) have refined this definition, suggesting that consolidation can be more strictly determined by time spent in specific brain states such as wake, sleep or certain stages of sleep2,3,4,5,6,7,8. There is also renewed interest9 in the possibility that recalling or ‘reactivating’ a previously consolidated memory renders it once again fragile and susceptible to interference10,11,12, therefore requiring periods of reconsolidation13,14,15. Using a motor skill finger-tapping task, here we provide evidence for at least three different stages of human motor memory processing after initial acquisition. We describe the unique contributions of wake and sleep in the development of different forms of consolidation, and show that waking reactivation can turn a previously consolidated memory back into a labile state requiring subsequent reconsolidation.},
	pages = {616--620},
	number = {6958},
	journaltitle = {Nature},
	author = {Walker, Matthew P. and Brakefield, Tiffany and Allan Hobson, J. and Stickgold, Robert},
	urldate = {2022-11-03},
	date = {2003-10},
	langid = {english},
	keywords = {Science, Humanities and Social Sciences, multidisciplinary},
	file = {Dissociable stages of human memory consolidation and reconsolidation:/Users/tom/Zotero/storage/TXRF7IAV/walker2003.pdf.pdf:application/pdf},
}

@misc{mcaleer_embedding_2022,
	title = {Embedding Data Skills in Research Methods Education: Preparing Students for Reproducible Research},
	url = {https://psyarxiv.com/hq68s/},
	doi = {10.31234/osf.io/hq68s},
	shorttitle = {Embedding Data Skills in Research Methods Education},
	abstract = {Many initiatives to improve reproducibility incentivise replication and encourage greater transparency without directly addressing the underlying skills needed for transparent and reproducible data preparation and analysis. In this paper, we argue that training in data processing and transformation should be embedded in field-specific research methods curricula. Promoting reproducibility and open science requires not only teaching relevant values and practices, but also providing the skills needed for reproducible data analysis. Improving students’ data skills will also enhance their employability within and beyond the academic context. To demonstrate the necessity of these skills, we walk through the analysis of realistic data from a classic paradigm in experimental psychology that is often used in teaching: the Stroop Interference Task. When starting from realistic raw data, nearly 80\% of the data analytic effort for this task involves skills not commonly taught—namely, importing, manipulating, and transforming tabular data. Data processing and transformation is a large and inescapable part of data analysis, and so education should strive to make the work associated with it as efficient, transparent, and reproducible as possible. We conclude by considering the challenges of embedding computational data skills training in undergraduate programmes and offer some solutions.},
	publisher = {{PsyArXiv}},
	author = {{McAleer}, Phil and Stack, Niamh and Woods, Heather and {DeBruine}, Lisa and Paterson, Helena and Nordmann, Emily and Kuepper-Tetzel, Carolina E. and Barr, Dale J.},
	urldate = {2022-11-05},
	date = {2022-11-03},
	langid = {english},
	keywords = {Meta-science, open science, Social and Behavioral Sciences, Quantitative Methods, reproducibility, research methods, Statistical Methods, statistics, pedagogy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/2QKWKSY4/McAleer et al. - 2022 - Embedding Data Skills in Research Methods Educatio.pdf:application/pdf},
}

@article{macnamara_growth_2022,
	title = {Do growth mindset interventions impact students’ academic achievement? A systematic review and meta-analysis with recommendations for best practices.},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000352},
	doi = {10.1037/bul0000352},
	shorttitle = {Do growth mindset interventions impact students’ academic achievement?},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Macnamara, Brooke N. and Burgoyne, Alexander P.},
	urldate = {2022-11-05},
	date = {2022-11-03},
	langid = {english},
	keywords = {toread},
}

@article{spake_improving_2022,
	title = {Improving quantitative synthesis to achieve generality in ecology},
	rights = {2022 Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-022-01891-z},
	doi = {10.1038/s41559-022-01891-z},
	abstract = {Synthesis of primary ecological data is often assumed to achieve a notion of ‘generality’, through the quantification of overall effect sizes and consistency among studies, and has become a dominant research approach in ecology. Unfortunately, ecologists rarely define either the generality of their findings, their estimand (the target of estimation) or the population of interest. Given that generality is fundamental to science, and the urgent need for scientific understanding to curb global scale ecological breakdown, loose usage of the term ‘generality’ is problematic. In other disciplines, generality is defined as comprising both generalizability—extending an inference about an estimand from the sample to the population—and transferability—the validity of estimand predictions in a different sampling unit or population. We review current practice in ecological synthesis and demonstrate that, when researchers fail to define the assumptions underpinning generalizations and transfers of effect sizes, generality often misses its target. We provide guidance for communicating nuanced inferences and maximizing the impact of syntheses both within and beyond academia. We propose pathways to generality applicable to ecological syntheses, including the development of quantitative and qualitative criteria with which to license the transfer of estimands from both primary and synthetic studies.},
	pages = {1--11},
	journaltitle = {Nature Ecology \& Evolution},
	shortjournal = {Nat Ecol Evol},
	author = {Spake, Rebecca and O’Dea, Rose E. and Nakagawa, Shinichi and Doncaster, C. Patrick and Ryo, Masahiro and Callaghan, Corey T. and Bullock, James M.},
	urldate = {2022-11-05},
	date = {2022-11-03},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {toread, Biodiversity, Conservation biology, Ecological modelling},
	file = {Snapshot:/Users/tom/Zotero/storage/KBKEHFQR/s41559-022-01891-z.html:text/html},
}

@article{christie_quantifying_2020,
	title = {Quantifying and addressing the prevalence and bias of study designs in the environmental and social sciences},
	volume = {11},
	rights = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-20142-y},
	doi = {10.1038/s41467-020-20142-y},
	abstract = {Building trust in science and evidence-based decision-making depends heavily on the credibility of studies and their findings. Researchers employ many different study designs that vary in their risk of bias to evaluate the true effect of interventions or impacts. Here, we empirically quantify, on a large scale, the prevalence of different study designs and the magnitude of bias in their estimates. Randomised designs and controlled observational designs with pre-intervention sampling were used by just 23\% of intervention studies in biodiversity conservation, and 36\% of intervention studies in social science. We demonstrate, through pairwise within-study comparisons across 49 environmental datasets, that these types of designs usually give less biased estimates than simpler observational designs. We propose a model-based approach to combine study estimates that may suffer from different levels of study design bias, discuss the implications for evidence synthesis, and how to facilitate the use of more credible study designs.},
	pages = {6377},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Christie, Alec P. and Abecasis, David and Adjeroud, Mehdi and Alonso, Juan C. and Amano, Tatsuya and Anton, Alvaro and Baldigo, Barry P. and Barrientos, Rafael and Bicknell, Jake E. and Buhl, Deborah A. and Cebrian, Just and Ceia, Ricardo S. and Cibils-Martina, Luciana and Clarke, Sarah and Claudet, Joachim and Craig, Michael D. and Davoult, Dominique and De Backer, Annelies and Donovan, Mary K. and Eddy, Tyler D. and França, Filipe M. and Gardner, Jonathan P. A. and Harris, Bradley P. and Huusko, Ari and Jones, Ian L. and Kelaher, Brendan P. and Kotiaho, Janne S. and López-Baucells, Adrià and Major, Heather L. and Mäki-Petäys, Aki and Martín, Beatriz and Martín, Carlos A. and Martin, Philip A. and Mateos-Molina, Daniel and {McConnaughey}, Robert A. and Meroni, Michele and Meyer, Christoph F. J. and Mills, Kade and Montefalcone, Monica and Noreika, Norbertas and Palacín, Carlos and Pande, Anjali and Pitcher, C. Roland and Ponce, Carlos and Rinella, Matt and Rocha, Ricardo and Ruiz-Delgado, María C. and Schmitter-Soto, Juan J. and Shaffer, Jill A. and Sharma, Shailesh and Sher, Anna A. and Stagnol, Doriane and Stanley, Thomas R. and Stokesbury, Kevin D. E. and Torres, Aurora and Tully, Oliver and Vehanen, Teppo and Watts, Corinne and Zhao, Qingyuan and Sutherland, William J.},
	urldate = {2022-11-05},
	date = {2020-12-11},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Social sciences, Scientific community, Ecology, Environmental impact},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XWMUMA6U/Christie et al. - 2020 - Quantifying and addressing the prevalence and bias.pdf:application/pdf;Quantifying and addressing the prevalence and bias of study designs in the environmental and social sciences:/Users/tom/Zotero/storage/EYWV5R48/christie2020.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/M89Z42NU/s41467-020-20142-y.html:text/html},
}

@article{duo_using_nodate,
	title = {Using Randomization in Development Economics Research: A Toolkit},
	abstract = {This paper is a practical guide (a toolkit) for researchers, students and practitioners wishing to introduce randomization as part of a research design in the ﬁeld. It ﬁrst covers the rationale for the use of randomization, as a solution to selection bias and a partial solution to publication biases. Second, it discusses various ways in which randomization can be practically introduced in a ﬁeld settings. Third, it discusses designs issues such as sample size requirements, stratiﬁcation, level of randomization and data collection methods. Fourth, it discusses how to analyze data from randomized evaluations when there are departures from the basic framework. It reviews in particular how to handle imperfect compliance and externalities. Finally, it discusses some of the issues involved in drawing general conclusions from randomized evaluations, including the necessary use of theory as a guide when designing evaluations and interpreting results. {JEL} Classiﬁcation: I0; J0; O0; C93. Keywords: Randomized evaluations; Experiments; Development; Program evaluation.},
	pages = {88},
	author = {Duﬂo, Esther and Glennerster, Rachel and Kremer, Michael},
	langid = {english},
	file = {Duﬂo et al. - Using Randomization in Development Economics Resea.pdf:/Users/tom/Zotero/storage/45GWELYJ/Duﬂo et al. - Using Randomization in Development Economics Resea.pdf:application/pdf},
}

@article{wicherts_how_2022,
	title = {How to protect privacy in open data},
	rights = {2022 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-022-01481-w.epdf?sharing_token=lchZo1bvKFwRKySb0hukldRgN0jAjWel9jnR3ZoTv0O7YZmn3igdPUs9cWqJh3b21d9fx5FMsuKgeBqG0rwpxiwWS5lyDGINLwlkEfb8Wn-yZo6xwAkWUPu3b3chdA7xkSXY1dY-3XVxvvYGN40JL8HVb6KllfjAdflKjIaI51s%3D},
	doi = {10.1038/s41562-022-01481-w},
	abstract = {When sharing research data for verification and reuse, behavioural researchers should protect participants’ privacy, particularly when studying sensitive topics. Because personally identifying data remain present in many open psychology datasets, we urge researchers to mend privacy via checks of re-identification risk before sharing data. We offer guidance for sharing responsibly.},
	pages = {1--3},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Wicherts, Jelte M. and Klein, Richard A. and Swaans, Sofie H. F. and Maassen, Esther and Stoevenbelt, Andrea H. and Peeters, Victor H. B. T. G. and de Jonge, Myrthe and Rüffer, Franziska},
	urldate = {2022-11-08},
	date = {2022-11-04},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Databases},
	file = {Snapshot:/Users/tom/Zotero/storage/F5PJYIQC/s41562-022-01481-w.html:text/html},
}

@article{webb_too_2022,
	title = {Too Good to Be True: Bots and Bad Data From Mechanical Turk},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/17456916221120027},
	doi = {10.1177/17456916221120027},
	shorttitle = {Too Good to Be True},
	abstract = {Psychology is moving increasingly toward digital sources of data, with Amazon?s Mechanical Turk ({MTurk}) at the forefront of that charge. In 2015, up to an estimated 45\% of articles published in the top behavioral and social science journals included at least one study conducted on {MTurk}. In this article, I summarize my own experience with {MTurk} and how I deduced that my sample was?at best?only 2.6\% valid, by my estimate. I share these results as a warning and call for caution. Recently, I conducted an online study via Amazon?s {MTurk}, eager and excited to collect my own data for the first time as a doctoral student. What resulted has prompted me to write this as a warning: it is indeed too good to be true. This is a summary of how I determined that, at best, I had gathered valid data from 14 human beings?2.6\% of my participant sample (N = 529).},
	pages = {17456916221120027},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Webb, Margaret A. and Tangney, June P.},
	urldate = {2022-11-10},
	date = {2022-11-07},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@article{ehlers_data_2022,
	title = {Data sharing in experimental fear and anxiety research: From challenges to a dynamically growing database in 10 simple steps},
	volume = {143},
	issn = {0149-7634},
	url = {https://www.sciencedirect.com/science/article/pii/S014976342200447X},
	doi = {10.1016/j.neubiorev.2022.104958},
	shorttitle = {Data sharing in experimental fear and anxiety research},
	abstract = {Data sharing holds promise for advancing and accelerating science by facilitating and fostering collaboration, reproducibility and optimal use of sparse resources. We argue that despite the existence of general data sharing guidelines (e.g, {FAIR}-principles), their translation and implementation requires field-specific considerations. Here, we addressed this timely question for the field of experimental research on fear and anxiety and showcase the enormous prospects by illustrating the wealth and richness of a curated data collection of publicly available datasets using the fear conditioning paradigm based on 103 studies and 8839 participants. We highlight challenges encountered when aiming to reuse the available data corpus and derive 10 simple steps for making data sharing in the field more efficient and sustainable and hence facilitating collaboration, cumulative knowledge generation and large scale mega-, meta- and psychometric analyses. We share our vision and first steps towards transforming such curated data collections into a homogenized and dynamically growing database allowing for easy contributions and for living analysis tools for the collective benefit of the research community.},
	pages = {104958},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	shortjournal = {Neuroscience \& Biobehavioral Reviews},
	author = {Ehlers, Mana R. and Lonsdorf, Tina B.},
	urldate = {2022-11-15},
	date = {2022-12-01},
	langid = {english},
	keywords = {Open data, Fear conditioning, Database, {FAIR}},
	file = {Ehlers and Lonsdorf - 2022 - Data sharing in experimental fear and anxiety rese.pdf:/Users/tom/Zotero/storage/KLXEJIX4/Ehlers and Lonsdorf - 2022 - Data sharing in experimental fear and anxiety rese.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/FZN4AWQY/S014976342200447X.html:text/html},
}

@article{murray_global_2022,
	title = {The global burden of disease study at 30 years},
	volume = {28},
	rights = {2022 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-022-01990-1},
	doi = {10.1038/s41591-022-01990-1},
	abstract = {The Global Burden of Disease Study ({GBD}) began 30 years ago with the goal of providing timely, valid and relevant assessments of critical health outcomes. Over this period, the {GBD} has become progressively more granular. The latest iteration provides assessments of thousands of outcomes for diseases, injuries and risk factors in more than 200 countries and territories and at the subnational level in more than 20 countries. The {GBD} is now produced by an active collaboration of over 8,000 scientists and analysts from more than 150 countries. With each {GBD} iteration, the data, data processing and methods used for data synthesis have evolved, with the goal of enhancing transparency and comparability of measurements and communicating various sources of uncertainty. The {GBD} has many limitations, but it remains a dynamic, iterative and rigorous attempt to provide meaningful health measurement to a wide range of stakeholders.},
	pages = {2019--2026},
	number = {10},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Murray, Christopher J. L.},
	urldate = {2022-11-21},
	date = {2022-10},
	langid = {english},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Research data, Computational biology and bioinformatics},
	file = {Full Text PDF:/Users/tom/Zotero/storage/4Q2FK6VC/Murray - 2022 - The Global Burden of Disease Study at 30 years.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B9VHHMU9/s41591-022-01990-1.html:text/html},
}

@article{nusser_role_2023,
	title = {The role of statistics in promoting data reusability and research transparency},
	volume = {10},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-033121-105114},
	doi = {10.1146/annurev-statistics-033121-105114},
	abstract = {The value of research data has grown as the emphasis on research transparency and data-intensive research has increased. Data sharing is now required by funders and publishers and is becoming a disciplinary expectation in many fields. However, practices promoting data reusability and research transparency are poorly understood, making it difficult for statisticians and other researchers to reframe study methods to facilitate data sharing. This article reviews the larger landscape of open research and describes contextual information that data reusers need to understand, evaluate, and appropriately analyze shared data. The article connects data reusability to statistical thinking by considering the impact of the type and quality of shared research artifacts on the capacity to reproduce or replicate studies and examining quality evaluation frameworks to understand the nature of data errors and how they can be mitigated prior to sharing. Actions statisticians can take to update their research approaches for their own and collaborative investigations are suggested.
            Expected final online publication date for the Annual Review of Statistics and Its Application, Volume 10 is March 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
	pages = {annurev--statistics--033121--105114},
	number = {1},
	journaltitle = {Annual Review of Statistics and Its Application},
	shortjournal = {Annu. Rev. Stat. Appl.},
	author = {Nusser, Sarah M.},
	urldate = {2022-11-22},
	date = {2023-03-07},
	langid = {english},
	keywords = {toread},
}

@article{gomes_why_2022,
	title = {Why don't we share data and code? Perceived barriers and benefits to public archiving practices},
	volume = {289},
	issn = {0962-8452, 1471-2954},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2022.1113},
	doi = {10.1098/rspb.2022.1113},
	shorttitle = {Why don't we share data and code?},
	abstract = {The biological sciences community is increasingly recognizing the value of open, reproducible and transparent research practices for science and society at large. Despite this recognition, many researchers fail to share their data and code publicly. This pattern may arise from knowledge barriers about how to archive data and code, concerns about its reuse, and misaligned career incentives. Here, we define, categorize and discuss barriers to data and code sharing that are relevant to many research fields. We explore how real and perceived barriers might be overcome or reframed in the light of the benefits relative to costs. By elucidating these barriers and the contexts in which they arise, we can take steps to mitigate them and align our actions with the goals of open science, both as individual scientists and as a scientific community.},
	pages = {20221113},
	number = {1987},
	journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
	shortjournal = {Proc. R. Soc. B.},
	author = {Gomes, Dylan G. E. and Pottier, Patrice and Crystal-Ornelas, Robert and Hudgins, Emma J. and Foroughirad, Vivienne and Sánchez-Reyes, Luna L. and Turba, Rachel and Martinez, Paula Andrea and Moreau, David and Bertram, Michael G. and Smout, Cooper A. and Gaynor, Kaitlyn M.},
	urldate = {2022-11-24},
	date = {2022-11-30},
	langid = {english},
	keywords = {toread},
}

@article{pontika_indicators_2022,
	title = {Indicators of research quality, quantity, openness and responsibility in institutional review, promotion and tenure policies across seven countries},
	issn = {2641-3337},
	url = {https://doi.org/10.1162/qss_a_00224},
	doi = {10.1162/qss_a_00224},
	abstract = {The need to reform research assessment processes related to career advancement at research institutions has become increasingly recognised in recent years, especially to better foster open and responsible research practices. Current assessment criteria are believed to focus too heavily on inappropriate criteria related to productivity and quantity as opposed to quality, collaborative open research practices, and the socio-economic impact of research. Evidence of the extent of these issues is urgently needed to inform actions for reform, however. We analyse current practices as revealed by documentation on institutional review, promotion and tenure processes in seven countries (Austria, Brazil, Germany, India, Portugal, United Kingdom and United States of America). Through systematic coding and analysis of 143 {RPT} policy documents from 107 institutions for the prevalence of 17 criteria (including those related to qualitative or quantitative assessment of research, service to the institution or profession, and open and responsible research practices), we compare assessment practices across a range of international institutions to significantly broaden this evidence-base. Although prevalence of indicators varies considerably between countries, overall we find that currently open and responsible research practices are minimally rewarded and problematic practices of quantification continue to dominate.https://publons.com/publon/10.1162/qss\_a\_00224},
	pages = {1--49},
	journaltitle = {Quantitative Science Studies},
	shortjournal = {Quantitative Science Studies},
	author = {Pontika, Nancy and Klebel, Thomas and Correia, Antonia and Metzler, Hannah and Knoth, Petr and Ross-Hellauer, Tony},
	urldate = {2022-11-24},
	date = {2022-11-15},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PRMNNTRJ/Pontika et al. - 2022 - Indicators of research quality, quantity, openness.pdf:application/pdf},
}

@misc{carneiro_mapping_2022,
	title = {Mapping the content of comments on {bioRxiv} and {medRxiv} preprints},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.11.23.517621v1},
	doi = {10.1101/2022.11.23.517621},
	abstract = {Introduction: Preprints have been increasingly used in biomedical sciences, providing the opportunity for research to be publicly assessed before journal publication. With the increase in attention over preprints during the {COVID}-19 pandemic, we decided to assess the content of comments left on preprint platforms. Methods: Preprints posted on {bioRxiv} and {medRxiv} in 2020 were accessed through each platform's {API}, and a random sample of preprints that had received between 1 and 20 comments was analyzed. Comments were evaluated in triplicate by independent evaluators using an instrument that assessed their features and general content. Results: 7.3\% of preprints received at least 1 comment during a mean follow-up of 7.5 months. Analyzed comments had a median size of 43 words. Criticisms, corrections or suggestions were the most prevalent type of content, followed by compliments or positive appraisals and questions. Most critical comments regarded interpretation, data collection and methodological design, while compliments were usually about relevance and implications. Conclusions: Only a small percentage of preprints posted in 2020 in {bioRxiv} and {medRxiv} received comments in these platforms. When present, however, these comments address content that is similar to that analyzed by traditional peer review. A more precise taxonomy of peer review functions would be desirable to describe whether post-publication peer review fulfills these roles.},
	publisher = {{bioRxiv}},
	author = {Carneiro, Clarissa F. D. and Costa, Gabriel and Neves, Kleber and Abreu, Mariana B. and Tan, Pedro B. and Rayêe, Danielle and Boos, Flávia and Andrejew, Roberta and Lubiana, Tiago and Malički, Mario and Amaral, Olavo B.},
	urldate = {2022-12-02},
	date = {2022-11-24},
	langid = {english},
	note = {Pages: 2022.11.23.517621
Section: New Results},
	file = {Full Text PDF:/Users/tom/Zotero/storage/A3NSSN3X/Carneiro et al. - 2022 - Mapping the content of comments on bioRxiv and med.pdf:application/pdf},
}

@article{dale_fundamental_2022,
	title = {The fundamental importance of method to theory},
	issn = {2731-0574},
	url = {https://www.nature.com/articles/s44159-022-00120-5},
	doi = {10.1038/s44159-022-00120-5},
	journaltitle = {Nature Reviews Psychology},
	shortjournal = {Nat Rev Psychol},
	author = {Dale, Rick and Warlaumont, Anne S. and Johnson, Kerri L.},
	urldate = {2022-12-07},
	date = {2022-11-29},
	langid = {english},
	keywords = {toread},
}

@article{damen_indicators_2022,
	title = {Indicators of questionable research practices were identified in 163,129 randomized controlled trials},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435622003079},
	doi = {10.1016/j.jclinepi.2022.11.020},
	pages = {S0895435622003079},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Damen, Johanna A. and Heus, Pauline and Lamberink, Herm J. and Tijdink, Joeri K. and Bouter, Lex and Glasziou, Paul and Moher, David and Otte, Willem M. and Vinkers, Christiaan H. and Hooft, Lotty},
	urldate = {2022-12-07},
	date = {2022-12},
	langid = {english},
	keywords = {toread},
}

@article{patarcic_adoption_2022,
	title = {Adoption of Transparency and Openness Promotion ({TOP}) Guidelines across Journals},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2304-6775},
	url = {https://www.mdpi.com/2304-6775/10/4/46},
	doi = {10.3390/publications10040046},
	abstract = {Journal policies continuously evolve to enable knowledge sharing and support reproducible science. However, that change happens within a certain framework. Eight modular standards with three levels of increasing stringency make Transparency and Openness Promotion ({TOP}) guidelines which can be used to evaluate to what extent and with which stringency journals promote open science. Guidelines define standards for data citation, transparency of data, material, code and design and analysis, replication, plan and study pre-registration, and two effective interventions: “Registered reports” and “Open science badges”, and levels of adoption summed up across standards define journal’s {TOP} Factor. In this paper, we analysed the status of adoption of {TOP} guidelines across two thousand journals reported in the {TOP} Factor metrics. We show that the majority of the journals’ policies align with at least one of the {TOP}’s standards, most likely “Data citation” (70\%) followed by “Data transparency” (19\%). Two-thirds of adoptions of {TOP} standard are of the stringency Level 1 (less stringent), whereas only 9\% is of the stringency Level 3. Adoption of {TOP} standards differs across science disciplines and multidisciplinary journals (N = 1505) and journals from social sciences (N = 1077) show the greatest number of adoptions. Improvement of the measures that journals take to implement open science practices could be done: (1) discipline-specific, (2) journals that have not yet adopted {TOP} guidelines could do so, (3) the stringency of adoptions could be increased.},
	pages = {46},
	number = {4},
	journaltitle = {Publications},
	author = {Patarčić, Inga and Stojanovski, Jadranka},
	urldate = {2022-12-07},
	date = {2022-12},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {open science, {TOP} guidelines, publishing policies, {TOP} Factor, transparency and openness promotion},
	file = {Full Text PDF:/Users/tom/Zotero/storage/YFLYSXSA/Patarčić and Stojanovski - 2022 - Adoption of Transparency and Openness Promotion (T.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B8MGH248/46.html:text/html},
}

@article{waldron_not_2022,
	title = {Not all pre-registrations are equal},
	volume = {47},
	rights = {2022 The Author(s)},
	issn = {1740-634X},
	url = {https://www.nature.com/articles/s41386-022-01418-x},
	doi = {10.1038/s41386-022-01418-x},
	pages = {2181--2183},
	number = {13},
	journaltitle = {Neuropsychopharmacology},
	shortjournal = {Neuropsychopharmacol.},
	author = {Waldron, Sophie and Allen, Christopher},
	urldate = {2022-12-07},
	date = {2022-12},
	langid = {english},
	note = {Number: 13
Publisher: Nature Publishing Group},
	keywords = {Psychology, toread, Neuroscience},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HFFVGPDR/Waldron and Allen - 2022 - Not all pre-registrations are equal.pdf:application/pdf},
}

@incollection{armitage_bias_2005,
	location = {Chichester, {UK}},
	title = {Bias, Overview},
	isbn = {978-0-470-84907-1 978-0-470-01181-2},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0470011815.b2a00001},
	pages = {b2a00001},
	booktitle = {Encyclopedia of Biostatistics},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Choi, Bernard C. K. and Pak, Anita W. P.},
	editor = {Armitage, Peter and Colton, Theodore},
	urldate = {2022-12-07},
	date = {2005-07-15},
	langid = {english},
	doi = {10.1002/0470011815.b2a00001},
	file = {Choi and Pak - 2005 - Bias, Overview.pdf:/Users/tom/Zotero/storage/N3SZS3NC/Choi and Pak - 2005 - Bias, Overview.pdf:application/pdf},
}

@incollection{armitage_data_2005,
	location = {Chichester, {UK}},
	title = {Data Mining},
	isbn = {978-0-470-84907-1 978-0-470-01181-2},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0470011815.b2a00007},
	pages = {b2a00007},
	booktitle = {Encyclopedia of Biostatistics},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Hand, David J.},
	editor = {Armitage, Peter and Colton, Theodore},
	urldate = {2022-12-07},
	date = {2005-07-15},
	langid = {english},
	doi = {10.1002/0470011815.b2a00007},
	file = {Hand - 2005 - Data Mining.pdf:/Users/tom/Zotero/storage/6JB6KA4D/Hand - 2005 - Data Mining.pdf:application/pdf},
}

@incollection{armitage_statistics_2005,
	location = {Chichester, {UK}},
	title = {Statistics, Overview},
	isbn = {978-0-470-84907-1 978-0-470-01181-2},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0470011815.b2a00006},
	pages = {b2a00006},
	booktitle = {Encyclopedia of Biostatistics},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Cox, D. R.},
	editor = {Armitage, Peter and Colton, Theodore},
	urldate = {2022-12-07},
	date = {2005-07-15},
	langid = {english},
	doi = {10.1002/0470011815.b2a00006},
	file = {Cox - 2005 - Statistics, Overview.pdf:/Users/tom/Zotero/storage/ZGI3G8R8/Cox - 2005 - Statistics, Overview.pdf:application/pdf},
}

@book{cartwright_tangle_2022,
	location = {New York},
	title = {The tangle of science: reliability beyond method, rigour, and objectivity},
	isbn = {978-0-19-886634-3},
	shorttitle = {The tangle of science},
	abstract = {"Science is remarkably reliable. It puts people on the moon, performs laser eye surgery, tells us about ancient civilisations and species, and predicts the future of our climate. What underwrites this reliability? This book argues that the standard answers-the scientific method, rigour, and objectivity-are insufficient for the job. Here we propose a new model of science that places its products front and centre. This is the 'Tangle of Science'. In this book we show how any reliable piece of science is underpinned by a vast, diverse, and thick network of other scientific products. In doing so we bring back into focus areas of science that have been long neglected, emphasising how every product, from the screws that hold the space shuttle together to ways of measuring the consumer price index to Einstein's theory of general relativity, work together to support results we can trust. (146)"--},
	publisher = {Oxford University Press},
	author = {Cartwright, Nancy and Hardie, Jeremy and Montuschi, Eleonora and Soleiman, Matthew and Thresher, Ann C.},
	date = {2022},
	keywords = {toread},
	file = {Nancy Cartwright, Jeremy Hardie, Eleonora Montuschi, Matthew Soleiman, Ann C. Thresher - The Tangle of Science_ Reliability Beyond Method, Rigour, and Objectivity-Oxford University Press (2023).epub:/Users/tom/Zotero/storage/Z9HID3EV/Nancy Cartwright, Jeremy Hardie, Eleonora Montuschi, Matthew Soleiman, Ann C. Thresher - The Tangle of Science_ Reliability Beyond Method, Rigour, and Objectivity-Oxford University Press (2023).epub:application/epub+zip},
}

@misc{laflamme_survey_2022,
	title = {A survey of researchers' methods sharing practices and priorities},
	url = {https://osf.io/preprints/metaarxiv/7jxav/},
	doi = {10.31222/osf.io/7jxav},
	abstract = {Missing or inaccessible information about the methods used in scientific research slows the pace of discovery and hampers reproducibility. Yet little is known about how, why, and under what conditions researchers share detailed methods information, or about how such practices vary across social categories like career stage, field, and region. We surveyed 997 active researchers about their attitudes and behaviors with respect to methods sharing. The most common approach reported by respondents was private sharing upon request, but a substantial minority (33\%) had publicly shared detailed methods information independently of their research findings. The most widely used channels for public sharing were connected to peer-reviewed publications, while the most significant barriers to public sharing were found to be lack of time and lack of awareness about how or where to share. Insofar as respondents were moderately satisfied with their ability to accomplish various goals associated with methods sharing, we conclude that efforts to promote public sharing may wish to focus on enhancing and building awareness of existing solutions—even as future research should seek to understand the needs of methods users and the extent to which they align with prevailing practices of sharing.},
	publisher = {{MetaArXiv}},
	author = {{LaFlamme}, Marcel and Harney, James and Hrynaszkiewicz, Iain},
	urldate = {2022-12-07},
	date = {2022-10-14},
	langid = {english},
	keywords = {open science, Social and Behavioral Sciences, research methods, toread, Library and Information Science, methods sharing, protocol sharing, Scholarly Communication, Scholarly Publishing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/DPLGUS9Q/LaFlamme et al. - 2022 - A survey of researchers' methods sharing practices.pdf:application/pdf},
}

@article{whitcomb_intellectual_2017,
	title = {Intellectual humility: owning our limitations},
	volume = {94},
	issn = {1933-1592},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/phpr.12228},
	doi = {10.1111/phpr.12228},
	shorttitle = {Intellectual humility},
	pages = {509--539},
	number = {3},
	journaltitle = {Philosophy and Phenomenological Research},
	author = {Whitcomb, Dennis and Battaly, Heather and Baehr, Jason and Howard-Snyder, Daniel},
	urldate = {2022-12-09},
	date = {2017},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/phpr.12228},
	file = {Snapshot:/Users/tom/Zotero/storage/TBRJ8GNU/phpr.html:text/html;Whitcomb et al. - 2017 - Intellectual humility owning our limitations.pdf:/Users/tom/Zotero/storage/SDDNDK5E/Whitcomb et al. - 2017 - Intellectual humility owning our limitations.pdf:application/pdf},
}

@article{alfano_development_2017,
	title = {Development and validation of a multi-dimensional measure of intellectual humility},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0182950},
	doi = {10.1371/journal.pone.0182950},
	abstract = {This paper presents five studies on the development and validation of a scale of intellectual humility. This scale captures cognitive, affective, behavioral, and motivational components of the construct that have been identified by various philosophers in their conceptual analyses of intellectual humility. We find that intellectual humility has four core dimensions: Open-mindedness (versus Arrogance), Intellectual Modesty (versus Vanity), Corrigibility (versus Fragility), and Engagement (versus Boredom). These dimensions display adequate self-informant agreement, and adequate convergent, divergent, and discriminant validity. In particular, Open-mindedness adds predictive power beyond the Big Six for an objective behavioral measure of intellectual humility, and Intellectual Modesty is uniquely related to Narcissism. We find that a similar factor structure emerges in Germanophone participants, giving initial evidence for the model’s cross-cultural generalizability.},
	pages = {e0182950},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Alfano, Mark and Iurino, Kathryn and Stey, Paul and Robinson, Brian and Christen, Markus and Yu, Feng and Lapsley, Daniel},
	urldate = {2022-12-09},
	date = {2017-08-16},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Personality, Language, Human learning, Emotions, Human intelligence, Factor analysis, Personality traits, Hispanic people},
	file = {Alfano et al. - 2017 - Development and validation of a multi-dimensional .pdf:/Users/tom/Zotero/storage/7WVGNNQY/Alfano et al. - 2017 - Development and validation of a multi-dimensional .pdf:application/pdf},
}

@article{leary_cognitive_2017,
	title = {Cognitive and Interpersonal Features of Intellectual Humility},
	volume = {43},
	issn = {1552-7433},
	doi = {10.1177/0146167217697695},
	abstract = {Four studies examined intellectual humility-the degree to which people recognize that their beliefs might be wrong. Using a new Intellectual Humility ({IH}) Scale, Study 1 showed that intellectual humility was associated with variables related to openness, curiosity, tolerance of ambiguity, and low dogmatism. Study 2 revealed that participants high in intellectual humility were less certain that their beliefs about religion were correct and judged people less on the basis of their religious opinions. In Study 3, participants high in intellectual humility were less inclined to think that politicians who changed their attitudes were "flip-flopping," and Study 4 showed that people high in intellectual humility were more attuned to the strength of persuasive arguments than those who were low. In addition to extending our understanding of intellectual humility, this research demonstrates that the {IH} Scale is a valid measure of the degree to which people recognize that their beliefs are fallible.},
	pages = {793--813},
	number = {6},
	journaltitle = {Personality \& Social Psychology Bulletin},
	shortjournal = {Pers Soc Psychol Bull},
	author = {Leary, Mark R. and Diebels, Kate J. and Davisson, Erin K. and Jongman-Sereno, Katrina P. and Isherwood, Jennifer C. and Raimi, Kaitlin T. and Deffler, Samantha A. and Hoyle, Rick H.},
	date = {2017-06},
	pmid = {28903672},
	keywords = {Humans, Personality, Adult, Female, Male, Thinking, Cognition, Adolescent, Aged, Aged, 80 and over, arrogance, belief certainty, humility, intellectual humility, Interpersonal Relations, Middle Aged, openness to ideas, Personality Inventory, Young Adult},
	file = {Leary et al. - 2017 - Cognitive and Interpersonal Features of Intellectu.pdf:/Users/tom/Zotero/storage/C7FG63XE/Leary et al. - 2017 - Cognitive and Interpersonal Features of Intellectu.pdf:application/pdf},
}

@article{hoyle_holding_2016,
	title = {Holding specific views with humility: Conceptualization and measurement of specific intellectual humility},
	volume = {97},
	issn = {0191-8869},
	url = {https://www.sciencedirect.com/science/article/pii/S0191886916301970},
	doi = {10.1016/j.paid.2016.03.043},
	shorttitle = {Holding specific views with humility},
	abstract = {Although significant progress has been made in the conceptualization and measurement of intellectual humility, little is known about intellectual humility with respect to specific opinions, beliefs, and positions. We offer a conceptualization of specific intellectual humility and present three studies that examine its key tenets. Study 1 developed the Specific Intellectual Humility Scale and showed that its psychometric properties are excellent and invariant across a range of specific views. Study 2 considered additional specific views, further establishing measurement invariance and providing evidence of convergent and discriminant validity. Study 3 broadened the range of specific views and revealed that intellectual humility with respect to a specific view is a complex function of dispositional intellectual humility, the extremity of the view, and the basis for the view. These findings demonstrate the value of investigating intellectual humility with respect to specific views and the usefulness of the Specific Intellectual Humility Scale.},
	pages = {165--172},
	journaltitle = {Personality and Individual Differences},
	shortjournal = {Personality and Individual Differences},
	author = {Hoyle, Rick H. and Davisson, Erin K. and Diebels, Kate J. and Leary, Mark R.},
	urldate = {2022-12-09},
	date = {2016-07-01},
	langid = {english},
	keywords = {Measurement, Validation, Humility, Intellectual virtues},
	file = {Hoyle et al. - 2016 - Holding specific views with humility Conceptualiz.pdf:/Users/tom/Zotero/storage/VVKWUW8G/Hoyle et al. - 2016 - Holding specific views with humility Conceptualiz.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/W6RRRCVM/S0191886916301970.html:text/html},
}

@article{zachry_situation-based_2018,
	title = {Situation-based contingencies underlying wisdom-content manifestations: examining intellectual humility in daily life},
	volume = {73},
	issn = {1079-5014, 1758-5368},
	url = {https://academic.oup.com/psychsocgerontology/article/73/8/1404/4883184},
	doi = {10.1093/geronb/gby016},
	shorttitle = {Situation-based contingencies underlying wisdom-content manifestations},
	pages = {1404--1415},
	number = {8},
	journaltitle = {The Journals of Gerontology: Series B},
	author = {Zachry, Corinne E and Phan, Le Vy and Blackie, Laura E R and Jayawickreme, Eranda},
	urldate = {2022-12-09},
	date = {2018-10-10},
	langid = {english},
	file = {Zachry et al. - 2018 - Situation-based contingencies underlying wisdom-co.pdf:/Users/tom/Zotero/storage/CPSERU8M/Zachry et al. - 2018 - Situation-based contingencies underlying wisdom-co.pdf:application/pdf},
}

@article{mcelroy-heltzel_embarrassment_2019,
	title = {Embarrassment of riches in the measurement of humility: A critical review of 22 measures},
	volume = {14},
	issn = {1743-9760},
	url = {https://doi.org/10.1080/17439760.2018.1460686},
	doi = {10.1080/17439760.2018.1460686},
	shorttitle = {Embarrassment of riches in the measurement of humility},
	abstract = {Less than ten years ago, humility science seemed stuck with intractable measurement problems. Due to theoretical innovations, measures have proliferated in recent years. Humility science now faces a critical task of reconciling definitions and measures. We reviewed 22 measures of humility, including (a) survey measures of general humility, (b) survey measures of humility subdomains, (c) indirect measures of humility, and (d) state measures of humility. We coded each item of each measure into a humility content domain and compared the various content areas covered by each measure. Then, we described the scale structure and evidence pertaining to reliability and validity. Finally, we identified the relatively stronger measures of humility and recommended a consolidated definition of humility.},
	pages = {393--404},
	number = {3},
	journaltitle = {The Journal of Positive Psychology},
	author = {{McElroy}-Heltzel, Stacey E. and Davis, Don E. and {DeBlaere}, Cirleen and Worthington, Everett L. and Hook, Joshua N.},
	urldate = {2022-12-09},
	date = {2019-05-04},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/17439760.2018.1460686},
	keywords = {reliability, validity, measurement, Humility},
	file = {Embarrassment of riches in the measurement of humility\: A critical review of 22 measures:/Users/tom/Zotero/storage/GUME9GLR/mcelroy-heltzel2018.pdf.pdf:application/pdf},
}

@book{bowles_moral_2016,
	location = {New Haven ; London},
	title = {The moral economy: why good incentives are no substitute for good citizens},
	isbn = {978-0-300-16380-3 978-0-300-23051-2},
	series = {The castle lectures in ethics, politics, and economics},
	shorttitle = {The moral economy},
	abstract = {Should the idea of economic man-the amoral and self-interested Homo economicus-determine how we expect people to respond to monetary rewards, punishments, and other incentives? Samuel Bowles answers with a resounding "no." Policies that follow from this paradigm, he shows, may "crowd out" ethical and generous motives and thus backfire. But incentives per se are not really the culprit. Bowles shows that crowding out occurs when the message conveyed by fines and rewards is that self-interest is expected, that the employer thinks the workforce is lazy, or that the citizen cannot otherwise be trusted to contribute to the public good. Using historical and recent case studies as well as behavioral experiments, Bowles shows how well-designed incentives can crowd in the civic motives on which good governance depends},
	pagetotal = {272},
	publisher = {Yale University Press},
	author = {Bowles, Samuel},
	date = {2016},
	langid = {english},
	note = {{OCLC}: ocn930798081},
	keywords = {Economics, 08.38 ethics, Anreiz, Arbeitsmotivation, Gemeinwohl, Homo oeconomicus, Law and economics, Moral and ethical aspects, Wirtschaftliches Verhalten},
	file = {Bowles - 2016 - The moral economy why good incentives are no subs.pdf:/Users/tom/Zotero/storage/F3QJPTWB/Bowles - 2016 - The moral economy why good incentives are no subs.pdf:application/pdf},
}

@article{schmidt_moderator_1978,
	title = {Moderator research and the law of small numbers},
	volume = {31},
	issn = {1744-6570},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1744-6570.1978.tb00441.x},
	doi = {10.1111/j.1744-6570.1978.tb00441.x},
	abstract = {The thesis of this paper is that many proposed moderators in personnel psychology are probably illusory, having been created solely by belief in the law of small numbers. Evidence is presented that race as a moderator of test validity is one such illusory moderator. In addition, a model for validity generalization is described which, in addition to eliminating the need for criterion-related validity studies under certain circumstances, strongly calls into question the idea that situations moderate test validity, i.e., the traditional doctrine of situational specificity of test validities. Calculations are presented which show that adequate statistical power in moderator research requires much larger sample sizes than have typically been employed. This requirement is illustrated empirically using validity data for the Army Classification Battery for 35 jobs and 21,000 individuals. These analyses show that (1) even when a moderator is generally assumed to be large, large samples are required to gauge its effect reliably and (2) large sample research may show that moderators that appear plausible and important a priori are nonexistent or trivial in magnitude. The practice of pooling across numerous small sample studies to obtain statistical power equivalent to that of large sample studies is recommended. In light of the evidence that many proposed moderators may not exist, the authors hypothesize that the true structure of underlying relationships in personnel psychology is considerably simpler than personnel psychologists have generally imagined it to be.},
	pages = {215--232},
	number = {2},
	journaltitle = {Personnel Psychology},
	author = {Schmidt, Frank L. and Hunter, John E.},
	urldate = {2022-12-09},
	date = {1978},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1744-6570.1978.tb00441.x},
	file = {Moderator Research and the Law of Small Numbers1:/Users/tom/Zotero/storage/AXAUG26W/schmidt1978.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/667KY5J2/j.1744-6570.1978.tb00441.html:text/html},
}

@article{goodman_evidence_1988,
	title = {Evidence and scientific research},
	volume = {78},
	issn = {0090-0036},
	doi = {10.2105/ajph.78.12.1568},
	abstract = {This commentary reviews the arguments for and against the use of p-values put forward in the Journal and other forums, and shows that they are all missing both a measure and concept of "evidence." The mathematics and logic of evidential theory are presented, with the log-likelihood ratio used as the measure of evidence. The profoundly different philosophy behind evidential methods (as compared to traditional ones) is presented, as well as a comparative example showing the difference between the two approaches. The reasons why we mistakenly ascribe evidential meaning to p-values and related measures are discussed. Unfamiliarity with the technology and philosophy of evidence is seen as the main reason why certain arguments about p-values persist, and why they are frequently contradictory and confusing.},
	pages = {1568--1574},
	number = {12},
	journaltitle = {American Journal of Public Health},
	shortjournal = {Am J Public Health},
	author = {Goodman, S. N. and Royall, R.},
	date = {1988-12},
	pmid = {3189634},
	pmcid = {PMC1349737},
	keywords = {Data Interpretation, Statistical, Humans, Philosophy, Research, Probability, Bayes Theorem, Blood Pressure, Logic},
	file = {Evidence and scientific research:/Users/tom/Zotero/storage/APIWWLLE/goodman1988.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/IEPJG7A4/Goodman and Royall - 1988 - Evidence and scientific research.pdf:application/pdf},
}

@article{swazey_ethical_1993,
	title = {Ethical problems in academic research},
	volume = {81},
	issn = {0003-0996},
	url = {https://www.jstor.org/stable/29775057},
	pages = {542--553},
	number = {6},
	journaltitle = {American Scientist},
	author = {Swazey, Judith P. and Anderson, Melissa S. and Lewis, Karen Seashore and Louis, Karen Seashore},
	urldate = {2022-12-09},
	date = {1993},
	note = {Publisher: Sigma Xi, The Scientific Research Society},
	file = {JSTOR Full Text PDF:/Users/tom/Zotero/storage/PCXE5K8U/Swazey et al. - 1993 - Ethical Problems in Academic Research.pdf:application/pdf},
}

@article{baldwin_scientific_2018,
	title = {Scientific autonomy, public accountability, and the rise of “peer review” in the cold war united states},
	volume = {109},
	issn = {0021-1753, 1545-6994},
	url = {https://www.journals.uchicago.edu/doi/10.1086/700070},
	doi = {10.1086/700070},
	pages = {538--558},
	number = {3},
	journaltitle = {Isis},
	shortjournal = {Isis},
	author = {Baldwin, Melinda},
	urldate = {2022-12-09},
	date = {2018-09},
	langid = {english},
	file = {Scientific Autonomy, Public Accountability, and the Rise of “Peer Review” in the Cold War United States:/Users/tom/Zotero/storage/GY8ECKIE/baldwin2018.pdf.pdf:application/pdf},
}

@article{porter_clarifying_2022,
	title = {Clarifying the content of intellectual humility: a systematic review and integrative framework},
	volume = {104},
	issn = {0022-3891, 1532-7752},
	url = {https://www.tandfonline.com/doi/full/10.1080/00223891.2021.1975725},
	doi = {10.1080/00223891.2021.1975725},
	shorttitle = {Clarifying the content of intellectual humility},
	pages = {573--585},
	number = {5},
	journaltitle = {Journal of Personality Assessment},
	shortjournal = {Journal of Personality Assessment},
	author = {Porter, Tenelle and Baldwin, Chayce R. and Warren, Michael T. and Murray, Elise D. and Cotton Bronk, Kendall and Forgeard, Marie J.C and Snow, Nancy E. and Jayawickreme, Eranda},
	urldate = {2022-12-13},
	date = {2022-09-03},
	langid = {english},
	file = {Porter et al. - 2022 - Clarifying the content of intellectual humility a.pdf:/Users/tom/Zotero/storage/UQEYZYPK/Porter et al. - 2022 - Clarifying the content of intellectual humility a.pdf:application/pdf},
}

@article{porter_predictors_2022,
	title = {Predictors and consequences of intellectual humility},
	volume = {1},
	rights = {2022 Springer Nature America, Inc.},
	issn = {2731-0574},
	url = {https://www.nature.com/articles/s44159-022-00081-9},
	doi = {10.1038/s44159-022-00081-9},
	abstract = {In a time of societal acrimony, psychological scientists have turned to a possible antidote — intellectual humility. Interest in intellectual humility comes from diverse research areas, including researchers studying leadership and organizational behaviour, personality science, positive psychology, judgement and decision-making, education, culture, and intergroup and interpersonal relationships. In this Review, we synthesize empirical approaches to the study of intellectual humility. We critically examine diverse approaches to defining and measuring intellectual humility and identify the common element: a meta-cognitive ability to recognize the limitations of one’s beliefs and knowledge. After reviewing the validity of different measurement approaches, we highlight factors that influence intellectual humility, from relationship security to social coordination. Furthermore, we review empirical evidence concerning the benefits and drawbacks of intellectual humility for personal decision-making, interpersonal relationships, scientific enterprise and society writ large. We conclude by outlining initial attempts to boost intellectual humility, foreshadowing possible scalable interventions that can turn intellectual humility into a core interpersonal, institutional and cultural value.},
	pages = {524--536},
	number = {9},
	journaltitle = {Nature Reviews Psychology},
	shortjournal = {Nat Rev Psychol},
	author = {Porter, Tenelle and Elnakouri, Abdo and Meyers, Ethan A. and Shibayama, Takuya and Jayawickreme, Eranda and Grossmann, Igor},
	urldate = {2022-12-13},
	date = {2022-09},
	langid = {english},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Psychology, Human behaviour, Social behaviour},
	file = {Porter et al. - 2022 - Predictors and consequences of intellectual humili.pdf:/Users/tom/Zotero/storage/3J2N7ZL5/Porter et al. - 2022 - Predictors and consequences of intellectual humili.pdf:application/pdf},
}

@article{van_tongeren_behavioral_2022,
	title = {Behavioral measures of humility: Part 1. Theoretical and methodological review},
	volume = {0},
	issn = {1743-9760},
	url = {https://doi.org/10.1080/17439760.2022.2109202},
	doi = {10.1080/17439760.2022.2109202},
	shorttitle = {Behavioral measures of humility},
	abstract = {Research on humility has burgeoned. However, behavioral assessments of humility that do not rely on self-reports have developed much more slowly. The purpose of this paper is to take stock of existing approaches to conceptualize and measure humility. Specifically, we provide a conceptual overview of humility, including the limitations of current methodological approaches to studying humility and the need for behavioral assessments. In addition, we argue that behavioral assessments of humility may inform broader measures of virtues by considering both the relevance of and the degree to which actual behaviors pertaining to that virtue are expressed. Understanding the current conceptual and methodological limitations of approaches to humility will better situate research efforts aimed at catalyzing behavioral measures of humility.},
	pages = {1--11},
	number = {0},
	journaltitle = {The Journal of Positive Psychology},
	author = {Van Tongeren, Daryl R. and Ng, Vincent and Hickman, Louis and Tay, Louis},
	urldate = {2022-12-13},
	date = {2022-08-07},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/17439760.2022.2109202},
	keywords = {measurement, Humility, behavior, humble},
	file = {Van Tongeren et al. - 2022 - Behavioral measures of humility Part 1. Theoretic.pdf:/Users/tom/Zotero/storage/WNTTBNCV/Van Tongeren et al. - 2022 - Behavioral measures of humility Part 1. Theoretic.pdf:application/pdf},
}

@article{van_tongeren_behavioral_2022-1,
	title = {Behavioral measures of humility: Part 1. Theoretical and methodological review},
	volume = {0},
	issn = {1743-9760},
	url = {https://doi.org/10.1080/17439760.2022.2109202},
	doi = {10.1080/17439760.2022.2109202},
	shorttitle = {Behavioral measures of humility},
	abstract = {Research on humility has burgeoned. However, behavioral assessments of humility that do not rely on self-reports have developed much more slowly. The purpose of this paper is to take stock of existing approaches to conceptualize and measure humility. Specifically, we provide a conceptual overview of humility, including the limitations of current methodological approaches to studying humility and the need for behavioral assessments. In addition, we argue that behavioral assessments of humility may inform broader measures of virtues by considering both the relevance of and the degree to which actual behaviors pertaining to that virtue are expressed. Understanding the current conceptual and methodological limitations of approaches to humility will better situate research efforts aimed at catalyzing behavioral measures of humility.},
	pages = {1--11},
	number = {0},
	journaltitle = {The Journal of Positive Psychology},
	author = {Van Tongeren, Daryl R. and Ng, Vincent and Hickman, Louis and Tay, Louis},
	urldate = {2022-12-13},
	date = {2022-08-07},
	keywords = {measurement, Humility, behavior, humble},
}

@article{mcelroy-heltzel_embarrassment_2019-1,
	title = {Embarrassment of riches in the measurement of humility: A critical review of 22 measures},
	volume = {14},
	issn = {1743-9760, 1743-9779},
	url = {https://www.tandfonline.com/doi/full/10.1080/17439760.2018.1460686},
	doi = {10.1080/17439760.2018.1460686},
	shorttitle = {Embarrassment of riches in the measurement of humility},
	pages = {393--404},
	number = {3},
	journaltitle = {The Journal of Positive Psychology},
	shortjournal = {The Journal of Positive Psychology},
	author = {{McElroy}-Heltzel, Stacey E. and Davis, Don E. and {DeBlaere}, Cirleen and Worthington, Everett L. and Hook, Joshua N.},
	urldate = {2022-12-13},
	date = {2019-05-04},
	langid = {english},
	file = {Embarrassment of riches in the measurement of humility\: A critical review of 22 measures:/Users/tom/Zotero/storage/KUU4UELA/mcelroy-heltzel2018.pdf.pdf:application/pdf},
}

@article{chetty_what_2014,
	title = {What policies increase prosocial behavior? An experiment with referees at the \textit{journal of public economics}},
	volume = {28},
	issn = {0895-3309},
	url = {https://pubs.aeaweb.org/doi/10.1257/jep.28.3.169},
	doi = {10.1257/jep.28.3.169},
	shorttitle = {What policies increase prosocial behavior?},
	abstract = {We evaluate policies to increase prosocial behavior using a field experiment with 1,500 referees at the Journal of Public Economics. We randomly assign referees to four groups: a control group with a six-week deadline to submit a referee report; a group with a four-week deadline; a cash incentive group rewarded with \$100 for meeting the four-week deadline; and a social incentive group in which referees were told that their turnaround times would be publicly posted. We obtain four sets of results. First, shorter deadlines reduce the time referees take to submit reports substantially. Second, cash incentives significantly improve speed, especially in the week before the deadline. Cash payments do not crowd out intrinsic motivation: after the cash treatment ends, referees who received cash incentives are no slower than those in the four-week deadline group. Third, social incentives have smaller but significant effects on review times and are especially effective among tenured professors, who are less sensitive to deadlines and cash incentives. Fourth, all the treatments have little or no effect on rates of agreement to review, quality of reports, or review times at other journals. We conclude that small changes in journals' policies could substantially expedite peer review at little cost. More generally, price incentives, nudges, and social pressure are effective and complementary methods of increasing prosocial behavior.},
	pages = {169--188},
	number = {3},
	journaltitle = {Journal of Economic Perspectives},
	shortjournal = {Journal of Economic Perspectives},
	author = {Chetty, Raj and Saez, Emmanuel and Sándor, László},
	urldate = {2022-12-14},
	date = {2014-08-01},
	langid = {english},
	file = {Chetty et al. - 2014 - What Policies Increase Prosocial Behavior An Expe.pdf:/Users/tom/Zotero/storage/9GM5LEMD/Chetty et al. - 2014 - What Policies Increase Prosocial Behavior An Expe.pdf:application/pdf},
}

@article{leblanc_scientific_2019-1,
	title = {Scientific sinkhole: The pernicious price of formatting},
	volume = {14},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223116},
	doi = {10.1371/journal.pone.0223116},
	shorttitle = {Scientific sinkhole},
	abstract = {Objective To conduct a time-cost analysis of formatting in scientific publishing. Design International, cross-sectional study (one-time survey). Setting Internet-based self-report survey, live between September 2018 and January 2019. Participants Anyone working in research, science, or academia and who submitted at least one peer-reviewed manuscript for consideration for publication in 2017. Completed surveys were available for 372 participants from 41 countries (60\% of respondents were from Canada). Main outcome measure Time (hours) and cost (wage per hour x time) associated with formatting a research paper for publication in a peer-reviewed academic journal. Results The median annual income category was {US}\$61,000–80,999, and the median number of publications formatted per year was four. Manuscripts required a median of two attempts before they were accepted for publication. The median formatting time was 14 hours per manuscript, or 52 hours per person, per year. This resulted in a median calculated cost of {US}\$477 per manuscript or {US}\$1,908 per person, per year. Conclusions To our knowledge, this is the first study to analyze the cost of manuscript formatting in scientific publishing. Our results suggest that scientific formatting represents a loss of 52 hours, costing the equivalent of {US}\$1,908 per researcher per year. These results identify the hidden and pernicious price associated with scientific publishing and provide evidence to advocate for the elimination of strict formatting guidelines, at least prior to acceptance.},
	pages = {e0223116},
	number = {9},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {{LeBlanc}, Allana G. and Barnes, Joel D. and Saunders, Travis J. and Tremblay, Mark S. and Chaput, Jean-Philippe},
	urldate = {2022-12-16},
	date = {2019-09-26},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Surveys, Internet, Scientific publishing, Peer review, Scientists, Social media, Professions, Salaries},
	file = {Full Text PDF:/Users/tom/Zotero/storage/L8XS27SG/LeBlanc et al. - 2019 - Scientific sinkhole The pernicious price of forma.pdf:application/pdf;Scientific sinkhole\: The pernicious price of formatting:/Users/tom/Zotero/storage/3YH4Z3VW/10.1371@journal.pone.0223116.pdf.pdf:application/pdf},
}

@article{gardener_open_2022,
	title = {Open science and conflict of interest policies of medical and health sciences journals before and during the {COVID}-19 pandemic: A repeat cross-sectional study: Open science policies of medical journals},
	volume = {13},
	issn = {2054-2704},
	url = {https://doi.org/10.1177/20542704221132139},
	doi = {10.1177/20542704221132139},
	shorttitle = {Open science and conflict of interest policies of medical and health sciences journals before and during the {COVID}-19 pandemic},
	abstract = {{ObjectivesTo} audit the transparent and open science standards of health and medical sciences journal policies and explore the impact of the {COVID}-19 pandemic.{DesignRepeat} cross-sectional study.Setting19 journals listed in Google Scholar's Top Publications for health and medical sciences.{ParticipantsBlood}, Cell, Circulation, European Heart Journal, Gastroenterology, Journal of Clinical Oncology, Journal of the American College of Cardiology, Nature Genetics, Nature Medicine, Nature Neuroscience, Neuron, {PLoS} {ONE}, Proceedings of the National Academy of Sciences, Science Translational Medicine, The British Medical Journal, The Journal of the American Medical Association, The Lancet, The Lancet Oncology, and The New England Journal of Medicine.Main outcome {measuresWe} used the Transparency and Openness Promotion ({TOP}) guideline and the International Committee of Medical Journal Editors ({ICMJE}) requirements for disclosing conflicts of interest ({COIs}) to evaluate journals standards.{ResultsTOP} scores slightly improved during the {COVID}-19 pandemic, from a median of 5 ({IQR}: 2?12.5) out of a possible 24 points in February 2020 to 7 ({IQR}: 4?12) in May 2021, but overall, scores were very low at both time points. Journal policies scored highest for their adherence to data transparency and scored lowest for preregistration of study protocols and analysis plans and the submission of replication studies. Most journals fulfilled all {ICMJE} provisions for reporting {COIs} before (84\%; n??=??16) and during (95\%; n??=??18) the {COVID}-19 pandemic.{ConclusionsThe} {COVID}-19 pandemic has highlighted the importance of practising open science. However, requirements for open science practices in audited policies were overall low, which may impede progress in health and medical research. As key stakeholders in disseminating research, journals should promote a research culture of greater transparency and more robust open science practices.},
	pages = {20542704221132139},
	number = {11},
	journaltitle = {{JRSM} Open},
	author = {Gardener, Antoni D. and Hick, Ellen J. and Jacklin, Chloe and Tan, Gifford and Cashin, Aidan G. and Lee, Hopin and Nunan, David and Toomey, Elaine C. and Richards, Georgia C.},
	urldate = {2022-12-18},
	date = {2022-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
	keywords = {toread},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/WZH7NG55/Gardener et al. - 2022 - Open science and conflict of interest policies of .pdf:application/pdf},
}

@online{noauthor_metaarxiv_nodate,
	title = {{MetaArXiv} Preprints {\textbar} ‘Trust Us’: Open Data and Preregistration in Political Science and International Relations},
	url = {https://osf.io/preprints/metaarxiv/8h2bp/},
	urldate = {2022-12-18},
	keywords = {toread},
	file = {MetaArXiv Preprints | ‘Trust Us’\: Open Data and Preregistration in Political Science and International Relations:/Users/tom/Zotero/storage/JZL7FCKJ/8h2bp.html:text/html},
}

@misc{grainger_why_2022,
	title = {Why “vote-counting” is never acceptable in evidence synthesis},
	url = {https://osf.io/c49uh/},
	doi = {10.31219/osf.io/c49uh},
	abstract = {Despite many publications  identifying the limitations of vote counting as a method of evidence synthesis this flawed method is still used. Here we highlight the main issues with using vote-counting and what alternatives there are for synthesising evidence.},
	publisher = {{OSF} Preprints},
	author = {Grainger, Matthew and Stewart, Gavin and Haddaway, Neal Robert},
	urldate = {2022-12-18},
	date = {2022-11-30},
	langid = {english},
	keywords = {Social and Behavioral Sciences, Medicine and Health Sciences, Life Sciences, Education, Evidence synthesis, Vote counting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/YC9CSLN4/Grainger et al. - 2022 - Why “vote-counting” is never acceptable in evidenc.pdf:application/pdf},
}

@article{schweinsberg_research_nodate,
	title = {Research Problem Validity in Primary Research:},
	abstract = {Four validity types evaluate the approximate truth of inferences communicated by primary research. However, current validity frameworks ignore the truthfulness of empirical inferences that are central to research problem statements. Problem statements contrast a review of past research with other knowledge that extend, contradict, or call into question specific features of past research. Authors communicate empirical inferences, or quantitative judgments about the frequency (e.g., “few,” “most”) and variability (e.g., “on the one hand, on the other hand”) in their reviews of existing theories, measures, samples, or results. We code a random sample of primary research articles and show that 83\% of quantitative judgments in our sample are both vague and their origin non-transparent, making it difficult to assess their validity. We review validity threats of current practices. We propose that documenting the literature search, how the search was coded, along with quantification facilitates more precise judgments and makes their origin transparent. This practice enables research questions that are more closely tied to the existing body of knowledge and allows for more informed evaluations of the contribution of primary research articles, their design choices, and how they advance knowledge. We discuss potential limitations of our proposed framework.},
	author = {Schweinsberg, Martin},
	langid = {english},
	keywords = {toread},
	file = {Schweinsberg - Research Problem Validity in Primary Research.pdf:/Users/tom/Zotero/storage/ASI5WLR3/Schweinsberg - Research Problem Validity in Primary Research.pdf:application/pdf},
}

@article{assel_statistical_2018-1,
	title = {Statistical code for clinical research papers in a high-impact specialist medical journal},
	volume = {168},
	issn = {0003-4819},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6705117/},
	doi = {10.7326/M17-2863},
	pages = {832--833},
	number = {11},
	journaltitle = {Annals of internal medicine},
	shortjournal = {Ann Intern Med},
	author = {Assel, Melissa and Vickers, Andrew J.},
	urldate = {2022-12-19},
	date = {2018-06-05},
	pmid = {29404569},
	pmcid = {PMC6705117},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/5DLJICA6/Assel and Vickers - 2018 - Statistical code for clinical research papers in a.pdf:application/pdf;Statistical code for clinical research papers in a high-impact specialist medical journal:/Users/tom/Zotero/storage/K8T99EDU/assel2018.pdf.pdf:application/pdf},
}

@article{anderson_normative_2007,
	title = {Normative dissonance in science: results from a national survey of U.S. scientists},
	volume = {2},
	issn = {1556-2646},
	url = {https://www.jstor.org/stable/10.1525/jer.2007.2.4.3},
	doi = {10.1525/jer.2007.2.4.3},
	shorttitle = {Normative dissonance in science},
	abstract = {{NORMS} {OF} {BEHAVIOR} {IN} {SCIENTIFIC} {RESEARCH} represent ideals to which most scientists subscribe. Our analysis of the extent of dissonance between these widely espoused ideals and scientists' perceptions of their own and others' behavior is based on survey responses from 3,247 mid- and early-career scientists who had research funding from the U.S. National Institutes of Health. We found substantial normative dissonance, particularly between espoused ideals and respondents' perceptions of other scientists' typical behavior. Also, respondents on average saw other scientists' behavior as more counternormative than normative. Scientists' views of their fields as cooperative or competitive were associated with their normative perspectives, with competitive fields showing more counternormative behavior. The high levels of normative dissonance documented here represent a persistent source of stress in science.},
	pages = {3--14},
	number = {4},
	journaltitle = {Journal of Empirical Research on Human Research Ethics: An International Journal},
	author = {Anderson, Melissa S. and Martinson, Brian C. and De Vries, Raymond},
	urldate = {2023-01-08},
	date = {2007},
	note = {Publisher: Sage Publications, Inc.},
	file = {Normative Dissonance in Science\: Results from a National Survey of U.S. Scientists:/Users/tom/Zotero/storage/MD9CW6QW/anderson2007.pdf.pdf:application/pdf},
}

@article{de_vries_normal_2006,
	title = {Normal misbehavior: scientists talk about the ethics of research},
	volume = {1},
	issn = {1556-2646},
	url = {https://doi.org/10.1525/jer.2006.1.1.43},
	doi = {10.1525/jer.2006.1.1.43},
	shorttitle = {Normal misbehavior},
	abstract = {Those concerned with protecting the integrity of science generally focus on the serious but rare infractions of falsification, fabrication, and plagiarism ({FFP}). While the violations of {FFP} are clear threats to the quality of scientific work and public trust in science, are they the behaviors that researchers themselves find most troubling? Noticing that scientists seldom are asked to report their perceptions of the behaviors that pose problems for the enterprise of science, we conducted six focus groups with researchers from major research universities. A total of 51 scientists participated in our focus-group discussions, which lasted from 1.5 to 2 hours each. We found that while researchers were aware of the problems of {FFP}, in their eyes misconduct generally is associated with more mundane, everyday problems in the work environment. These more common problems fall into four categories: The meaning of data, the rules of science, life with colleagues, and the pressures of production in science. Focus on the ?normal misbehaviors? that are part of the ordinary life of researchers allows us to see the way the organization of science generates both compliance and deviance from ethical norms.},
	pages = {43--50},
	number = {1},
	journaltitle = {Journal of Empirical Research on Human Research Ethics},
	author = {De Vries, Raymond and Anderson, Melissa S. and Martinson, Brian C.},
	urldate = {2023-01-08},
	date = {2006-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Normal Misbehavior\: Scientists Talk about the Ethics of Research:/Users/tom/Zotero/storage/CDN3T3GB/devries2006.pdf.pdf:application/pdf;SAGE PDF Full Text:/Users/tom/Zotero/storage/ZY9AFEZS/De Vries et al. - 2006 - Normal Misbehavior Scientists Talk about the Ethi.pdf:application/pdf},
}

@article{anderson_extending_2010,
	title = {Extending the mertonian norms: scientists’ subscription to norms of research},
	volume = {81},
	issn = {0022-1546},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2995462/},
	doi = {10.1353/jhe.0.0095},
	shorttitle = {Extending the mertonian norms},
	abstract = {This analysis, based on focus groups and a national survey, assesses scientists’ subscription to the Mertonian norms of science and associated counternorms. It also supports extension of these norms to governance (as opposed to administration), as a norm of decision-making, and quality (as opposed to quantity), as a evaluative norm.},
	pages = {366--393},
	number = {3},
	journaltitle = {The Journal of higher education},
	shortjournal = {J Higher Educ},
	author = {Anderson, Melissa S. and Ronning, Emily A. and {DeVries}, Raymond and Martinson, Brian C.},
	urldate = {2023-01-08},
	date = {2010-05-01},
	pmid = {21132074},
	pmcid = {PMC2995462},
	file = {Extending the Mertonian Norms\: Scientists’ Subscription to Norms of Research:/Users/tom/Zotero/storage/AM96J483/10.1353@jhe.0.0095.pdf.pdf:application/pdf;PubMed Central Full Text PDF:/Users/tom/Zotero/storage/GJVG48TS/Anderson et al. - 2010 - Extending the Mertonian Norms Scientists’ Subscri.pdf:application/pdf},
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	url = {https://www.science.org/doi/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	pages = {aac4716},
	number = {6251},
	journaltitle = {Science},
	author = {{Open Science Collaboration}},
	urldate = {2023-01-09},
	date = {2015-08-28},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Accepted Version:/Users/tom/Zotero/storage/LWCNDH2Y/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf;Estimating the reproducibility of psychological science:/Users/tom/Zotero/storage/6SH7V8LS/estimating-the-reproducibility-of-psychological-science-2015.pdf.pdf:application/pdf},
}

@article{munn_dark_2022,
	title = {The dark side of rapid reviews: a retreat from systematic approaches and the need for clear expectations and reporting},
	url = {https://www.acpjournals.org/doi/10.7326/M22-2603},
	doi = {10.7326/M22-2603},
	shorttitle = {The dark side of rapid reviews},
	abstract = {Rapid reviews proliferated during the {COVID}-19 pandemic. In this commentary, the authors highlight some risks and concerns with rapid reviews and make suggestions for reviewers and proponents of ev...},
	journaltitle = {Annals of Internal Medicine},
	author = {Munn, Zachary and Pollock, Danielle and Barker, Timothy Hugh and Stone, Jennifer and Stern, Cindy and Aromataris, Edoardo and Pearson, Alan and Straus, Sharon and Khalil, Hanan and Mustafa, Reem A. and Tricco, Andrea C. and Schünemann, Holger J.},
	urldate = {2023-01-09},
	date = {2022-12-27},
	langid = {english},
	note = {Publisher: American College of Physicians},
}

@article{horton_rhetoric_1995,
	title = {The rhetoric of research},
	volume = {310},
	issn = {0959-8138},
	url = {https://www.jstor.org/stable/29726978},
	pages = {985--988},
	number = {6985},
	journaltitle = {{BMJ}: British Medical Journal},
	author = {Horton, Richard and Greenhalgh, Trisha},
	urldate = {2023-01-10},
	date = {1995},
	note = {Publisher: {BMJ}},
	file = {JSTOR Full Text PDF:/Users/tom/Zotero/storage/9A944X86/Horton and Greenhalgh - 1995 - The Rhetoric Of Research.pdf:application/pdf},
}

@article{kinder_presence_2019,
	title = {Presence of ‘spin’ in the abstracts and titles of anaesthesiology randomised controlled trials},
	volume = {122},
	issn = {00070912},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0007091218308043},
	doi = {10.1016/j.bja.2018.10.023},
	pages = {e13--e14},
	number = {1},
	journaltitle = {British Journal of Anaesthesia},
	shortjournal = {British Journal of Anaesthesia},
	author = {Kinder, N.C. and Weaver, M.D. and Wayant, C. and Vassar, M.},
	urldate = {2023-01-10},
	date = {2019-01},
	langid = {english},
	file = {Kinder et al. - 2019 - Presence of ‘spin’ in the abstracts and titles of .pdf:/Users/tom/Zotero/storage/8XP3M6XU/Kinder et al. - 2019 - Presence of ‘spin’ in the abstracts and titles of .pdf:application/pdf},
}

@article{shaqman_reporting_2020,
	title = {Reporting quality and spin in abstracts of randomized clinical trials of periodontal therapy and cardiovascular disease outcomes},
	volume = {15},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7164582/},
	doi = {10.1371/journal.pone.0230843},
	abstract = {Objective
Poor reporting in randomized clinical trial ({RCT}) abstracts reduces quality and misinforms readers. Spin, a biased presentation of findings, could frequently mislead clinicians to accept a clinical intervention despite non-significant primary outcome. Therefore, good reporting practices and absence of spin enhances research quality. We aim to assess the reporting quality and spin in abstracts of {RCTs} evaluating the effect of periodontal therapy on cardiovascular ({CVD}) outcomes.

Methods
{PubMed}, Scopus, the Cochrane Central Register of Controlled Trials ({CENTRAL}), and 17 trial registration platforms were searched. Cohort, non-randomized, non-English studies, and pediatric studies were excluded. {RCT} abstracts were reviewed by 2 authors using the {CONSORT} for abstracts and spin checklists for data extraction. Cohen’s Kappa statistic was used to assess inter-rater agreement. Data on the selected {RCT} publication metrics were collected. Descriptive analysis was performed with non-parametric methods. Correlation analysis between quality, spin and bibliometric parameters was conducted.

Results
24 {RCTs} were selected for {CONSORT} analysis and 14 fulfilled the criteria for spin analysis. Several important {RCT} elements per {CONSORT} were neglected in the abstract including description of the study population (100\%), explicitly stated primary outcome (87\%), methods of randomization and blinding (100\%), trial registration (87\%). No {RCT} examined true outcomes ({CVD} events). A significant fraction of the abstracts appeared with at least one form of spin in the results and conclusions (86\%) and claimed some treatment benefit in spite of non-significant primary outcome (64\%). High-quality reporting had a significant positive correlation with reporting of trial registration (p = 0.04) and funding (p = 0.009). Spinning showed marginal negative correlation with reporting quality (p = 0.059).

Conclusion
Poor adherence to the {CONSORT} guidelines and high levels of data spin were found in abstracts of {RCTs} exploring the effects of periodontal therapy on {CVD} outcomes. Our findings indicate that journal editors and reviewers should consider strict adherence to proper reporting guidelines to improve reporting quality and reduce waste.},
	pages = {e0230843},
	number = {4},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} One},
	author = {Shaqman, Murad and Al-Abedalla, Khadijeh and Wagner, Julie and Swede, Helen and Gunsolley, John Cart and Ioannidou, Effie},
	urldate = {2023-01-10},
	date = {2020-04-17},
	pmid = {32302309},
	pmcid = {PMC7164582},
	file = {PubMed Central Full Text PDF:/Users/tom/Zotero/storage/SNYJ8E2C/Shaqman et al. - 2020 - Reporting quality and spin in abstracts of randomi.pdf:application/pdf},
}

@article{reddy_evaluation_2020,
	title = {Evaluation of spin in abstracts of systematic reviews and meta-analyses focused on treatments of erectile dysfunction: a cross-sectional analysis},
	volume = {9},
	issn = {2050-1161},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7930867/},
	doi = {10.1016/j.esxm.2020.10.012},
	shorttitle = {Evaluation of spin in abstracts of systematic reviews and meta-analyses focused on treatments of erectile dysfunction},
	abstract = {Introduction
It is predicted that erectile dysfunction will affect around 322 million men worldwide by 2025. Because of the large volume of literature on the topic, physicians often turn to systematic reviews and meta-analyses—and particularly abstracts of such articles—for clinical guidance. Thus, it is crucial that findings are not misrepresented in abstracts. In this study, we evaluated the use of spin (ie, the misreporting of study findings by overstating or selectively reporting efficacy results, minimizing harms, or making unwarranted clinical recommendations) in the abstracts of systematic reviews on erectile dysfunction.

Methods
A search strategy was developed using the {MEDLINE} and Embase databases to retrieve systematic reviews focused on treatments for erectile dysfunction. 2 investigators independently screened the titles and abstracts from the reviews for study inclusion. Investigators analyzed the included systematic reviews for 9 of the most severe types of spin using a previously developed classification scheme and rated them for methodological quality using the revised A {MeaSurement} Tool to Assess systematic Reviews ({AMSTAR}) in a masked, duplicate manner. Study characteristics for each review were also extracted in duplicate.

Results
Our search returned 2,224 articles, of which 102 systematic reviews and meta-analyses were included in the final analysis. A total of 31.4\% (32/102) of systematic reviews contained spin. 8 types of spin were identified in our sample. Type 3 (selective reporting of or overemphasis on efficacy outcomes) and type 5 (conclusion claims beneficial effect despite high risk of bias) were the most common types of spin, each occurring in 10.8\% (11/102) of abstracts. There was no significant association between the presence of spin and the extracted study characteristics or methodological quality.

Conclusion
Spin was present in systematic reviews and meta-analyses covering erectile dysfunction treatments. Steps should be taken to improve the reporting quality of abstracts on erectile dysfunction treatment., 
            Reddy A
            K
            , Lulkovich K, Ottwell R, et al. Evaluation of Spin in Abstracts of Systematic Reviews and Meta-analyses Focused on Treatments of Erectile Dysfunction: A Cross-sectional Analysis. Sex Med 2021;9:100284.},
	pages = {100284},
	number = {1},
	journaltitle = {Sexual Medicine},
	shortjournal = {Sex Med},
	author = {Reddy, Arjun K. and Lulkovich, Kaley and Ottwell, Ryan and Arthur, Wade and Bowers, Aaron and Al-Rifai, Shafiq and Cook, Katherine and Wright, Drew N. and Hartwell, Micah and Vassar, Matt},
	urldate = {2023-01-10},
	date = {2020-12-05},
	pmid = {33291041},
	pmcid = {PMC7930867},
	file = {Evaluation of Spin in Abstracts of Systematic Reviews and Meta-analyses Focused on Treatments of Erectile Dysfunction\: A Cross-sectional Analysis:/Users/tom/Zotero/storage/Z7ZD9R2N/reddy2021.pdf.pdf:application/pdf;PubMed Central Full Text PDF:/Users/tom/Zotero/storage/8ADQGW5P/Reddy et al. - 2020 - Evaluation of Spin in Abstracts of Systematic Revi.pdf:application/pdf},
}

@article{davis_peer-review_2018-1,
	title = {Peer-review guidelines promoting replicability and transparency in psychological science},
	volume = {1},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/2515245918806489},
	doi = {10.1177/2515245918806489},
	abstract = {More and more psychological researchers have come to appreciate the perils of common but poorly justified research practices and are rethinking commonly held standards for evaluating research. As this methodological reform expresses itself in psychological research, peer reviewers of such work must also adapt their practices to remain relevant. Reviewers of journal submissions wield considerable power to promote methodological reform, and thereby contribute to the advancement of a more robust psychological literature. We describe concrete practices that reviewers can use to encourage transparency, intellectual humility, and more valid assessments of the methods and statistics reported in articles.},
	pages = {556--573},
	number = {4},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Davis, William E. and Giner-Sorolla, Roger and Lindsay, D. Stephen and Lougheed, Jessica P. and Makel, Matthew C. and Meier, Matt E. and Sun, Jessie and Vaughn, Leigh Ann and Zelenski, John M.},
	urldate = {2023-01-11},
	date = {2018-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/BB5UCAFC/Davis et al. - 2018 - Peer-Review Guidelines Promoting Replicability and.pdf:application/pdf;Peer-Review Guidelines Promoting Replicability and Transparency in Psychological Science:/Users/tom/Zotero/storage/F5JR92Z8/623d5185cbec18d4037430e15e9b1250.pdf.pdf:application/pdf},
}

@article{kruse_state_2017,
	title = {State humility: Measurement, conceptual validation, and intrapersonal processes},
	volume = {16},
	issn = {1529-8868},
	url = {https://doi.org/10.1080/15298868.2016.1267662},
	doi = {10.1080/15298868.2016.1267662},
	shorttitle = {State humility},
	abstract = {Humility is a core psychological process theoretically marked by low self-focus, secure identity, and balanced awareness of strengths and weaknesses. We began with a consensual definition of humility before theoretically unpacking it. First, using 25 samples and 2622 adults, we developed the Brief State Humility Scale, which demonstrates strong construct validity and good reliability, is sensitive to experimental manipulation, and is uncorrelated with social desirability. Second, using this measure, we replicated previously reported relationships involving interpersonal processes; revealed links between state humility and intrapersonal processes (e.g. affect, creativity, and personality); and demonstrated key theoretical differences between state humility and modesty. This framework highlights new avenues for humility research and suggests how humility plays a critical role in emotional experience.},
	pages = {399--438},
	number = {4},
	journaltitle = {Self and Identity},
	author = {Kruse, Elliott and Chancellor, Joseph and Lyubomirsky, Sonja},
	urldate = {2023-01-11},
	date = {2017-07-04},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15298868.2016.1267662},
	keywords = {personality, emotions, Humility, measure validation},
	file = {Full Text:/Users/tom/Zotero/storage/ZZW443F6/Kruse et al. - 2017 - State humility Measurement, conceptual validation.pdf:application/pdf;State humility\: Measurement, conceptual validation, and intrapersonal processes:/Users/tom/Zotero/storage/A5LVMD8Y/kruse2017.pdf.pdf:application/pdf},
}

@article{besancon_correction_2022,
	title = {Correction of scientific literature: Too little, too late!},
	volume = {20},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001572},
	doi = {10.1371/journal.pbio.3001572},
	shorttitle = {Correction of scientific literature},
	abstract = {The Coronavirus Disease 2019 ({COVID}-19) pandemic has highlighted the limitations of the current scientific publication system, in which serious post-publication concerns are often addressed too slowly to be effective. In this Perspective, we offer suggestions to improve academia’s willingness and ability to correct errors in an appropriate time frame.},
	pages = {e3001572},
	number = {3},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Besançon, Lonni and Bik, Elisabeth and Heathers, James and Meyerowitz-Katz, Gideon},
	urldate = {2023-01-11},
	date = {2022-03-03},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Scientific misconduct, Scientific publishing, Peer review, Scientists, Careers, {COVID} 19, Pandemics, Quality control},
	file = {Full Text PDF:/Users/tom/Zotero/storage/STRIN8EZ/Besançon et al. - 2022 - Correction of scientific literature Too little, t.pdf:application/pdf},
}

@online{noauthor_elifes_2022,
	title = {{eLife}'s New Model: What is an {eLife} assessment?},
	rights = {© 2022 {eLife} Sciences Publications Limited. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	url = {https://elifesciences.org/inside-elife/db24dd46/elife-s-new-model-what-is-an-elife-assessment},
	shorttitle = {{eLife}'s New Model},
	abstract = {{eLife} is changing its editorial process to eliminate accept/reject decisions after peer review and instead provide readers with richer and more nuanced assessments of articles.},
	titleaddon = {{eLife}},
	urldate = {2023-01-24},
	date = {2022-10-20},
	langid = {english},
	note = {Publisher: {eLife} Sciences Publications Limited},
	file = {Snapshot:/Users/tom/Zotero/storage/MPADGF7Z/elife-s-new-model-what-is-an-elife-assessment.html:text/html},
}

@online{noauthor_elife_2022,
	title = {{eLife} ends accept/reject decisions following peer review},
	rights = {© 2022 {eLife} Sciences Publications Limited. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	url = {https://elifesciences.org/for-the-press/b2329859/elife-ends-accept-reject-decisions-following-peer-review},
	abstract = {{eLife} will emphasise the public peer review of preprints, restoring author autonomy and promoting the assessment of scientists based on what, not where, they publish.},
	titleaddon = {{eLife}},
	urldate = {2023-01-24},
	date = {2022-10-20},
	langid = {english},
	note = {Publisher: {eLife} Sciences Publications Limited},
	file = {Snapshot:/Users/tom/Zotero/storage/R7AAZY9W/elife-ends-accept-reject-decisions-following-peer-review.html:text/html},
}

@article{eisen_peer_2022,
	title = {Peer review without gatekeeping [editorial]},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.83889},
	doi = {10.7554/eLife.83889},
	abstract = {{eLife} is changing its editorial process to emphasize public reviews and assessments of preprints by eliminating accept/reject decisions after peer review.},
	pages = {e83889},
	journaltitle = {{eLife}},
	author = {Eisen, Michael B and Akhmanova, Anna and Behrens, Timothy E and Diedrichsen, Jörn and Harper, Diane M and Iordanova, Mihaela D and Weigel, Detlef and Zaidi, Mone},
	urldate = {2023-01-24},
	date = {2022-10-20},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {peer review, scientific publishing, preprints, research assessment, research communication},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7F2ZPFJ6/Eisen et al. - 2022 - Peer review without gatekeeping.pdf:application/pdf},
}

@article{krosnick_question_nodate,
	title = {Question and Questionnaire Design},
	author = {Krosnick, Jon A},
	langid = {english},
	file = {Krosnick - Question and Questionnaire Design.pdf:/Users/tom/Zotero/storage/ZZ7T7IZJ/Krosnick - Question and Questionnaire Design.pdf:application/pdf},
}

@article{flake_measurement_2020,
	title = {Measurement schmeasurement: questionable measurement practices and how to avoid them},
	volume = {3},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245920952393},
	doi = {10.1177/2515245920952393},
	shorttitle = {Measurement schmeasurement},
	abstract = {In this article, we define questionable measurement practices ({QMPs}) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: {QMPs} are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid {QMPs}. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study?s inferences, and are necessary for meaningful replication studies.},
	pages = {456--465},
	number = {4},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Flake, Jessica Kay and Fried, Eiko I.},
	urldate = {2023-01-25},
	date = {2020-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Flake and Fried - 2020 - Measurement schmeasurement questionable measureme.pdf:/Users/tom/Zotero/storage/67PDF7NN/Flake and Fried - 2020 - Measurement schmeasurement questionable measureme.pdf:application/pdf},
}

@article{roche_public_2015,
	title = {Public data archiving in ecology and evolution: how well are we doing?},
	volume = {13},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002295},
	doi = {10.1371/journal.pbio.1002295},
	shorttitle = {Public data archiving in ecology and evolution},
	abstract = {Policies that mandate public data archiving ({PDA}) successfully increase accessibility to data underlying scientific publications. However, is the data quality sufficient to allow reuse and reanalysis? We surveyed 100 datasets associated with nonmolecular studies in journals that commonly publish ecological and evolutionary research and have a strong {PDA} policy. Out of these datasets, 56\% were incomplete, and 64\% were archived in a way that partially or entirely prevented reuse. We suggest that cultural shifts facilitating clearer benefits to authors are necessary to achieve high-quality {PDA} and highlight key guidelines to help authors increase their data’s reuse potential and compliance with journal data policies.},
	pages = {e1002295},
	number = {11},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Roche, Dominique G. and Kruuk, Loeske E. B. and Lanfear, Robert and Binning, Sandra A.},
	urldate = {2023-01-27},
	date = {2015-11-10},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Science policy, Reproducibility, Scientific publishing, Computer software, Evolutionary biology, Metadata, Archives, Public policy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/RBDPER29/Roche et al. - 2015 - Public Data Archiving in Ecology and Evolution Ho.pdf:application/pdf},
}

@article{targ_meta-research_group_discrepancy_2022,
	title = {Discrepancy review: a feasibility study of a novel peer review intervention to reduce undisclosed discrepancies between registrations and publications},
	volume = {9},
	rights = {© 2022 The Authors.},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220142},
	doi = {10.1098/rsos.220142},
	shorttitle = {Discrepancy review},
	abstract = {Undisclosed discrepancies often exist between study registrations and their associated
publications. Discrepancies can increase risk of bias, and when undisclosed, they
disguise this increased risk of bias from readers. To remedy this issue, we developed
...},
	pages = {20142},
	journaltitle = {Royal Society Open Science},
	author = {{TARG} Meta-Research Group},
	urldate = {2023-01-27},
	date = {2022-07-27},
	note = {Publisher: The Royal Society},
	file = {Full Text:/Users/tom/Zotero/storage/ECAPVUFI/Collaborators - 2022 - Discrepancy review a feasibility study of a novel.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/B7CVZPI4/rsos.html:text/html},
}

@misc{score_collaboration_systematizing_2021,
	title = {Systematizing Confidence in Open Research and Evidence ({SCORE})},
	url = {https://osf.io/preprints/socarxiv/46mnb/},
	doi = {10.31235/osf.io/46mnb},
	abstract = {Assessing the credibility of research claims is a central, continuous, and laborious part of the scientific process. Credibility assessment strategies range from expert judgment to aggregating existing evidence to systematic replication efforts. Such assessments can require substantial time and effort. Research progress could be accelerated if there were rapid, scalable, accurate credibility indicators to guide attention and resource allocation for further assessment. The {SCORE} program is creating and validating algorithms to provide confidence scores for research claims at scale. To investigate the viability of scalable tools, teams are creating: a database of claims from papers in the social and behavioral sciences; expert and machine generated estimates of credibility; and, evidence of reproducibility, robustness, and replicability to validate the estimates. Beyond the primary research objective, the data and artifacts generated from this program will be openly shared and provide an unprecedented opportunity to examine research credibility and evidence.},
	publisher = {{SocArXiv}},
	author = {{SCORE} Collaboration and Alipourfard, Nazanin and Arendt, Beatrix and Benjamin, Daniel M. and Benkler, Noam and Bishop, Michael and Burstein, Mark and Bush, Martin and Caverlee, James and Chen, Yiling and Clark, Chae and Almenberg, Anna Dreber and Errington, Timothy M. and Fidler, Fiona and Fox [{SCORE}, Nicholas and Frank, Aaron and Fraser, Hannah and Friedman, Scott and Gelman, Ben and Gentile, James and Giles, C. Lee and Gordon, Michael B. and Gordon-Sarney, Reed and Griffin, Christopher and Gulden, Timothy and Hahn, Krystal and Hartman, Robert and Holzmeister, Felix and Hu, Xia Ben and Johannesson, Magnus and Kezar, Lee and Struhl, Melissa Kline and Kuter, Ugur and Kwasnica, Anthony M. and Lee, Dong-Ho and Lerman, Kristina and Liu, Yang and Loomas, Zachary and Luis [{SCORE}, Bri and Magnusson, Ian and Miske, Olivia and Mody, Fallon and Morstatter, Fred and Nosek, Brian A. and Parsons, Elan Simon and Pennock, David and Pfeiffer, Thomas and Pujara, Jay and Rajtmajer, Sarah and Ren, Xiang and Salinas, Abel and Selvam, Ravi Kiran and Shipman, Frank and Silverstein, Priya and Sprenger, Amber and Squicciarini, Anna Ms and Stratman, Steve and Sun, Kexuan and Tikoo, Saatvik and Twardy, Charles R. and Tyner, Andrew and Viganola, Domenico and Wang, Juntao and Wilkinson, David Peter and Wintle, Bonnie and Wu, Jian},
	urldate = {2023-01-27},
	date = {2021-05-03},
	langid = {english},
	keywords = {Social and Behavioral Sciences, Psychology, replicability, reproducibility, Sociology, Metascience, Economics, Political Science, algorithms, credibility, Leadership Studies, Legal Studies, Organization Development, Public Affairs, Public Policy and Public Administration, social sciences},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XTENNDPN/Alipourfard et al. - 2021 - Systematizing Confidence in Open Research and Evid.pdf:application/pdf},
}

@article{hardwicke_estimating_2022,
	title = {Estimating the prevalence of transparency and reproducibility-related research practices in psychology (2014–2017)},
	volume = {17},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620979806},
	doi = {10.1177/1745691620979806},
	abstract = {Psychologists are navigating an unprecedented period of introspection about the credibility and utility of their discipline. Reform initiatives emphasize the benefits of transparency and reproducibility-related research practices; however, adoption across the psychology literature is unknown. Estimating the prevalence of such practices will help to gauge the collective impact of reform initiatives, track progress over time, and calibrate future efforts. To this end, we manually examined a random sample of 250 psychology articles published between 2014 and 2017. Over half of the articles were publicly available (154/237, 65\%, 95\% confidence interval [{CI}] = [59\%, 71\%]); however, sharing of research materials (26/183; 14\%, 95\% {CI} = [10\%, 19\%]), study protocols (0/188; 0\%, 95\% {CI} = [0\%, 1\%]), raw data (4/188; 2\%, 95\% {CI} = [1\%, 4\%]), and analysis scripts (1/188; 1\%, 95\% {CI} = [0\%, 1\%]) was rare. Preregistration was also uncommon (5/188; 3\%, 95\% {CI} = [1\%, 5\%]). Many articles included a funding disclosure statement (142/228; 62\%, 95\% {CI} = [56\%, 69\%]), but conflict-of-interest statements were less common (88/228; 39\%, 95\% {CI} = [32\%, 45\%]). Replication studies were rare (10/188; 5\%, 95\% {CI} = [3\%, 8\%]), and few studies were included in systematic reviews (21/183; 11\%, 95\% {CI} = [8\%, 16\%]) or meta-analyses (12/183; 7\%, 95\% {CI} = [4\%, 10\%]). Overall, the results suggest that transparency and reproducibility-related research practices were far from routine. These findings establish baseline prevalence estimates against which future progress toward increasing the credibility and utility of psychology research can be compared.},
	pages = {239--251},
	number = {1},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica E. and Wallach, Joshua D. and Kidwell, Mallory C. and Ioannidis, John P. A.},
	urldate = {2023-01-27},
	date = {2022-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/HYBIZR8J/Hardwicke et al. - 2022 - Estimating the Prevalence of Transparency and Repr.pdf:application/pdf},
}

@article{hardwicke_reducing_2023,
	title = {Reducing bias, increasing transparency and calibrating confidence with preregistration},
	volume = {7},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-022-01497-2},
	doi = {10.1038/s41562-022-01497-2},
	pages = {15--26},
	number = {1},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Hardwicke, Tom E. and Wagenmakers, Eric-Jan},
	urldate = {2023-01-29},
	date = {2023-01-26},
	langid = {english},
	file = {Hardwicke and Wagenmakers - 2023 - Reducing bias, increasing transparency and calibra.pdf:/Users/tom/Zotero/storage/GZC2GVDQ/Hardwicke and Wagenmakers - 2023 - Reducing bias, increasing transparency and calibra.pdf:application/pdf},
}

@article{chin_transparency_nodate,
	title = {The transparency of quantitative empirical legal research (2018–2020)},
	author = {Chin, Jason and Zeiler, Kathryn and Dilevski, Natali and Holcombe, Alexander and Jeffries, Rosemary Gatfield- and Bishop, Ruby and Vazire, Simine and Schiavone, Sarah},
	langid = {english},
	file = {Chin et al. - The Transparency of Quantitative Empirical Legal R.pdf:/Users/tom/Zotero/storage/34RL8ITY/Chin et al. - The Transparency of Quantitative Empirical Legal R.pdf:application/pdf},
}

@article{visschers_probability_2009,
	title = {Probability information in risk communication: a review of the research literature},
	volume = {29},
	issn = {1539-6924},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.2008.01137.x},
	doi = {10.1111/j.1539-6924.2008.01137.x},
	shorttitle = {Probability information in risk communication},
	abstract = {Communicating probability information about risks to the public is more difficult than might be expected. Many studies have examined this subject, so that their resulting recommendations are scattere...},
	pages = {267--287},
	number = {2},
	journaltitle = {Risk Analysis},
	author = {Visschers, Vivianne H. M. and Meertens, Ree M. and Passchier, Wim W. F. and Vries, Nanne N. K. De},
	urldate = {2023-01-31},
	date = {2009-02-01},
	langid = {english},
	note = {Publisher: John Wiley \& Sons, Ltd},
	file = {Visschers et al. - 2009 - Probability information in risk communication a r.pdf:/Users/tom/Zotero/storage/TBY99QR3/Visschers et al. - 2009 - Probability information in risk communication a r.pdf:application/pdf},
}

@article{theil_role_2002,
	title = {The role of translations of verbal into numerical probability expressions in risk management: a meta-analysis},
	volume = {5},
	issn = {1366-9877, 1466-4461},
	url = {http://www.tandfonline.com/doi/abs/10.1080/13669870110038179},
	doi = {10.1080/13669870110038179},
	shorttitle = {The role of translations of verbal into numerical probability expressions in risk management},
	pages = {177--186},
	number = {2},
	journaltitle = {Journal of Risk Research},
	shortjournal = {Journal of Risk Research},
	author = {Theil, Michael},
	urldate = {2023-01-31},
	date = {2002-04},
	langid = {english},
	file = {Theil - 2002 - The role of translations of verbal into numerical .pdf:/Users/tom/Zotero/storage/274PM7AA/Theil - 2002 - The role of translations of verbal into numerical .pdf:application/pdf},
}

@article{lichtenstein_empirical_1967,
	title = {Empirical scaling of common verbal phrases associated with numerical probabilities},
	volume = {9},
	pages = {563--564},
	journaltitle = {Psychonomic Science},
	author = {Lichtenstein, Sarah and Newman, J Robert},
	date = {1967},
	langid = {english},
	file = {Lichtenstein and Newman - Empirical scaling of common verbal phrases associa.pdf:/Users/tom/Zotero/storage/RA8DGYEZ/Lichtenstein and Newman - Empirical scaling of common verbal phrases associa.pdf:application/pdf},
}

@article{serra-garcia_nonreplicable_2021,
	title = {Nonreplicable publications are cited more than replicable ones},
	volume = {7},
	url = {https://www.science.org/doi/10.1126/sciadv.abd1705},
	doi = {10.1126/sciadv.abd1705},
	abstract = {We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more “interesting,” they apply lower standards regarding their reproducibility.},
	pages = {eabd1705},
	number = {21},
	journaltitle = {Science Advances},
	author = {Serra-Garcia, Marta and Gneezy, Uri},
	urldate = {2023-02-01},
	date = {2021-05-21},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Full Text PDF:/Users/tom/Zotero/storage/JMIEMZLW/Serra-Garcia and Gneezy - 2021 - Nonreplicable publications are cited more than rep.pdf:application/pdf},
}

@article{hanel_using_2023,
	title = {Using self-affirmation to increase intellectual humility in debate},
	volume = {10},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220958},
	doi = {10.1098/rsos.220958},
	abstract = {Intellectual humility, which entails openness to other views and a willingness to listen and engage with them, is crucial for facilitating civil dialogue and progress in debate between opposing sides. In the present research, we tested whether intellectual humility can be reliably detected in discourse and experimentally increased by a prior self-affirmation task. Three hundred and three participants took part in 116 audio- and video-recorded group discussions. Blind to condition, linguists coded participants' discourse to create an intellectual humility score. As expected, the self-affirmation task increased the coded intellectual humility, as well as participants’ self-rated prosocial affect (e.g. empathy). Unexpectedly, the effect on prosocial affect did not mediate the link between experimental condition and intellectual humility in debate. Self-reported intellectual humility and other personality variables were uncorrelated with expert-coded intellectual humility. Implications of these findings for understanding the social psychological mechanisms underpinning intellectual humility are considered.},
	pages = {220958},
	number = {2},
	journaltitle = {Royal Society Open Science},
	shortjournal = {R. Soc. open sci.},
	author = {Hanel, Paul H. P. and Roy, Deborah and Taylor, Samuel and Franjieh, Michael and Heffer, Chris and Tanesini, Alessandra and Maio, Gregory R.},
	urldate = {2023-02-05},
	date = {2023-02},
	langid = {english},
	file = {Hanel et al. - 2023 - Using self-affirmation to increase intellectual hu.pdf:/Users/tom/Zotero/storage/FMIXJNGQ/Hanel et al. - 2023 - Using self-affirmation to increase intellectual hu.pdf:application/pdf},
}

@article{budescu_consistency_1985,
	title = {Consistency in interpretation of probabilistic phrases},
	volume = {36},
	issn = {07495978},
	url = {https://linkinghub.elsevier.com/retrieve/pii/074959788590007X},
	doi = {10.1016/0749-5978(85)90007-X},
	pages = {391--405},
	number = {3},
	journaltitle = {Organizational Behavior and Human Decision Processes},
	shortjournal = {Organizational Behavior and Human Decision Processes},
	author = {Budescu, David V and Wallsten, Thomas S},
	urldate = {2023-02-07},
	date = {1985-12},
	langid = {english},
	file = {Budescu and Wallsten - 1985 - Consistency in interpretation of probabilistic phr.pdf:/Users/tom/Zotero/storage/78NL3SS5/Budescu and Wallsten - 1985 - Consistency in interpretation of probabilistic phr.pdf:application/pdf},
}

@article{willems_variability_2020,
	title = {Variability in the interpretation of probability phrases used in Dutch news articles — a risk for miscommunication},
	volume = {19},
	issn = {1824-2049},
	url = {https://jcom.sissa.it/archive/19/02/JCOM_1902_2020_A03},
	doi = {10.22323/2.19020203},
	abstract = {Verbal probability phrases are often used in science communication to express estimated risks in words instead of numbers. In this study we look at how laypeople and statisticians interpret Dutch probability phrases that are regularly used in news articles. We found that there is a large variability in interpretations, even if the phrases are given in a neutral context. Also, statisticians do not agree on the interpretation of the phrases. We conclude that science communicators should be careful in using verbal probability expressions.},
	pages = {A03},
	number = {2},
	journaltitle = {Journal of Science Communication},
	shortjournal = {{JCOM}},
	author = {Willems, Sanne and Albers, Casper and Smeets, Ionica},
	urldate = {2023-02-13},
	date = {2020-04-06},
	file = {Willems et al. - 2020 - Variability in the interpretation of probability p.pdf:/Users/tom/Zotero/storage/5JWMLDPL/Willems et al. - 2020 - Variability in the interpretation of probability p.pdf:application/pdf},
}

@article{reagan_quantitative_1989,
	title = {Quantitative meanings of verbal probability expressions.},
	volume = {74},
	issn = {1939-1854, 0021-9010},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0021-9010.74.3.433},
	doi = {10.1037/0021-9010.74.3.433},
	pages = {433--442},
	number = {3},
	journaltitle = {Journal of Applied Psychology},
	shortjournal = {Journal of Applied Psychology},
	author = {Reagan, Robert T. and Mosteller, Frederick and Youtz, Cleo},
	urldate = {2023-02-13},
	date = {1989},
	langid = {english},
	file = {Reagan et al. - 1989 - Quantitative meanings of verbal probability expres.pdf:/Users/tom/Zotero/storage/HZ7GNBIC/Reagan et al. - 1989 - Quantitative meanings of verbal probability expres.pdf:application/pdf},
}

@article{wallsten_measuring_1986,
	title = {Measuring the vague meanings of probability terms.},
	volume = {115},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-3445.115.4.348},
	doi = {10.1037/0096-3445.115.4.348},
	pages = {348--365},
	number = {4},
	journaltitle = {Journal of Experimental Psychology: General},
	shortjournal = {Journal of Experimental Psychology: General},
	author = {Wallsten, Thomas S. and Budescu, David V. and Rapoport, Amnon and Zwick, Rami and Forsyth, Barbara},
	urldate = {2023-02-13},
	date = {1986},
	langid = {english},
	file = {Wallsten et al. - 1986 - Measuring the vague meanings of probability terms..pdf:/Users/tom/Zotero/storage/FN8Y6LZY/Wallsten et al. - 1986 - Measuring the vague meanings of probability terms..pdf:application/pdf},
}

@article{budescu_interpretation_2014,
	title = {The interpretation of {IPCC} probabilistic statements around the world},
	volume = {4},
	rights = {2014 Nature Publishing Group},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/nclimate2194},
	doi = {10.1038/nclimate2194},
	abstract = {The Intergovernmental Panel on Climate Change ({IPCC}) uses verbal descriptions of uncertainty (for example, Unlikely) to convey imprecision in its forecasts and conclusions. Previous studies showed that the American public misinterprets these probabilistic statements. We report results from a multi-national study involving 25 samples in 24 countries and 17 languages. As predicted, laypeople interpret {IPCC} statements as conveying probabilities closer to 50\% than intended by the {IPCC} authors. We show that an alternative presentation format supplementing the verbal terms with numerical ranges increases the correspondence between the public’s interpretations and the {IPCC} guidelines, and the terms are better differentiated. These qualitative patterns are remarkably stable across all samples and languages. In fact, interpretations of the terms in various languages are more similar under the new presentation format. These results suggest changing the way the {IPCC} communicates uncertainty.},
	pages = {508--512},
	number = {6},
	journaltitle = {Nature Climate Change},
	shortjournal = {Nature Clim Change},
	author = {Budescu, David V. and Por, Han-Hui and Broomell, Stephen B. and Smithson, Michael},
	urldate = {2023-02-13},
	date = {2014-06},
	langid = {english},
	keywords = {Psychology, Communication},
	file = {Budescu et al. - 2014 - The interpretation of IPCC probabilistic statement.pdf:/Users/tom/Zotero/storage/2ZWSIT34/Budescu et al. - 2014 - The interpretation of IPCC probabilistic statement.pdf:application/pdf},
}

@book{wilcox_modern_2017,
	title = {Modern Statistics for the Social and Behavioral Sciences},
	author = {Wilcox, Rand},
	date = {2017},
	langid = {english},
	file = {Wilcox - Modern Statistics for the Social and Behavioral Sc.pdf:/Users/tom/Zotero/storage/GALQFLPQ/Wilcox - Modern Statistics for the Social and Behavioral Sc.pdf:application/pdf},
}

@article{sikorski_epistemic_2023,
	title = {Epistemic functions of replicability in experimental sciences: defending the orthodox view},
	issn = {1572-8471},
	url = {https://doi.org/10.1007/s10699-023-09901-4},
	doi = {10.1007/s10699-023-09901-4},
	shorttitle = {Epistemic functions of replicability in experimental sciences},
	abstract = {Replicability is widely regarded as one of the defining features of science and its pursuit is one of the main postulates of meta-research, a discipline emerging in response to the replicability crisis. At the same time, replicability is typically treated with caution by philosophers of science. In this paper, we reassess the value of replicability from an epistemic perspective. We defend the orthodox view, according to which replications are always epistemically useful, against the more prudent view that claims that it is useful in very limited circumstances. Additionally, we argue that we can learn more about the original experiment and the limits of the discovered effect from replications at different levels. We hold that replicability is a crucial feature of experimental results and scientists should continue to strive to secure it.},
	journaltitle = {Foundations of Science},
	shortjournal = {Found Sci},
	author = {Sikorski, Michał and Andreoletti, Mattia},
	urldate = {2023-02-22},
	date = {2023-02-18},
	langid = {english},
	keywords = {Replication, Replicability, Experimental Science},
	file = {Sikorski and Andreoletti - 2023 - Epistemic functions of replicability in experiment.pdf:/Users/tom/Zotero/storage/E4B26PTQ/Sikorski and Andreoletti - 2023 - Epistemic functions of replicability in experiment.pdf:application/pdf},
}

@article{finch_colloquium_2001,
	title = {Colloquium on Effect Sizes: the Roles of Editors, Textbook Authors, and the Publication Manual: Reporting of Statistical Inference in the Journal of Applied Psychology: Little Evidence of Reform},
	volume = {61},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/0013164401612001},
	doi = {10.1177/0013164401612001},
	shorttitle = {Colloquium on Effect Sizes},
	abstract = {Reformers have long argued that misuse of Null Hypothesis Significance Testing ({NHST}) is widespread and damaging. The authors analyzed 150 articles from the Journal of Applied Psychology ({JAP}) covering 1940 to 1999. They examined statistical reporting practices related to misconceptions about {NHST}, American Psychological Association ({APA}) guidelines, and reform recommendations. The analysis reveals (a) inconsistency in reporting alpha and p values, (b) the use of ambiguous language in describing {NHST}, (c) frequent acceptance of null hypotheses without consideration of power, (d) that power estimates are rarely reported, and (e) that confidence intervals were virtually never used. {APA} guidelines have been followed only selectively. Research methodology reported in {JAP} has increased greatly in sophistication over 60 years, but inference practices have shown remarkable stability. There is little sign that decades of cogent critiques by reformers had by 1999 led to changes in statistical reporting practices in {JAP}.},
	pages = {181--210},
	number = {2},
	journaltitle = {Educational and Psychological Measurement},
	author = {Finch, Sue and Cumming, Geoff and Thomason, Neil},
	urldate = {2023-02-27},
	date = {2001-04-01},
	langid = {english},
	file = {Colloquium on Effect Sizes\: the Roles of Editors, Textbook Authors, and the Publication Manual\: Reporting of Statistical Inference in the Journal of Applied Psychology\: Little Evidence of Reform:/Users/tom/Zotero/storage/Y3FDBF5D/finch2001.pdf.pdf:application/pdf},
}

@article{finch_reform_2004-1,
	title = {Reform of statistical inference in psychology: The case {ofMemory} \& Cognition},
	volume = {36},
	issn = {1532-5970},
	url = {https://doi.org/10.3758/BF03195577},
	doi = {10.3758/BF03195577},
	shorttitle = {Reform of statistical inference in psychology},
	abstract = {Geoffrey Loftus, Editor {ofMemory} \& Cognition from 1994 to 1997, strongly encouraged presentation of figures with error bars and avoidance of null hypothesis significance testing ({NHST}). The authors examined 696Memory \& Cognition articles published before, during, and after the Loftus editorship. Use of figures with bars increased to 47\% under Loftus’s editorship and then declined. Bars were rarely used for interpretation, and {NHST} remained almost universal. Analysis of 309 articles in other psychology journals confirmed that Loftus’s influence was most evident in the articles he accepted for publication, but was otherwise limited. An e-mail survey of authors of papers accepted by Loftus revealed some support for his policy, but allegiance to traditional practices as well. Reform of psychologists’ statistical practices would require more than editorial encouragement.},
	pages = {312--324},
	number = {2},
	journaltitle = {Behavior Research Methods, Instruments, \& Computers},
	shortjournal = {Behavior Research Methods, Instruments, \& Computers},
	author = {Finch, Sue and Cumming, Geoff and Williams, Jennifer and Palmer, Lee and Griffith, Elvira and Alders, Chris and Anderson, James and Goodman, Olivia},
	urldate = {2023-02-27},
	date = {2004-05-01},
	langid = {english},
	keywords = {American Psychological Association, Lead Author, Psychology Journal, Reporting Practice, Statistical Inference},
	file = {Finch et al. - 2004 - Reform of statistical inference in psychology The.pdf:/Users/tom/Zotero/storage/D7QCKDP2/Finch et al. - 2004 - Reform of statistical inference in psychology The.pdf:application/pdf},
}

@article{sarafoglou_survey_2022,
	title = {A survey on how preregistration affects the research workflow: better science but more work},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.211997},
	doi = {10.1098/rsos.211997},
	shorttitle = {A survey on how preregistration affects the research workflow},
	abstract = {The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioural sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the perceived benefits and challenges of preregistration from the researcher’s perspective. To this end, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. While the benefits outweighed the challenges for the majority of researchers with preregistration experience, this was not the case for the majority of researchers without preregistration experience. The experienced advantages and disadvantages identified in our survey could inform future efforts to improve preregistration and thus help the methodology gain greater acceptance in the scientific community.},
	pages = {211997},
	number = {7},
	journaltitle = {Royal Society Open Science},
	author = {Sarafoglou, Alexandra and Kovacs, Marton and Bakos, Bence and Wagenmakers, Eric-Jan and Aczel, Balazs},
	urldate = {2023-02-27},
	date = {2022-07-06},
	note = {Publisher: Royal Society},
	keywords = {open science, replication crisis, meta-science},
	file = {Full Text PDF:/Users/tom/Zotero/storage/I9B9V6CG/Sarafoglou et al. - 2022 - A survey on how preregistration affects the resear.pdf:application/pdf},
}

@article{bland_statistical_1986,
	title = {Statistical methods for assessing agreement between two methods of clinical measurement},
	volume = {327},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(86)90837-8/fulltext},
	doi = {10.1016/S0140-6736(86)90837-8},
	pages = {307--310},
	number = {8476},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Bland, J. Martin and Altman, {DouglasG}},
	urldate = {2023-02-27},
	date = {1986-02-08},
	note = {Publisher: Elsevier},
	file = {Bland and Altman - 1986 - Statistical methods for assessing agreement betwee.pdf:/Users/tom/Zotero/storage/E9BQZQRR/Bland and Altman - 1986 - Statistical methods for assessing agreement betwee.pdf:application/pdf},
}

@article{zou_sample_2012,
	title = {Sample size formulas for estimating intraclass correlation coefficients with precision and assurance},
	volume = {31},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.5466},
	doi = {10.1002/sim.5466},
	abstract = {The number of subjects required to estimate the intraclass correlation coefficient in a reliability study has usually been determined on the basis of the expected width of a confidence interval. However, this approach fails to explicitly consider the probability of achieving the desired interval width and may thus provide sample sizes that are too small to have adequate chance of achieving the desired precision. In this paper, we present a method that explicitly incorporates a prespecified probability of achieving the prespecified width or lower limit of a confidence interval. The resultant closed-form formulas are shown to be very accurate. Copyright © 2012 John Wiley \& Sons, Ltd.},
	pages = {3972--3981},
	number = {29},
	journaltitle = {Statistics in Medicine},
	author = {Zou, G. Y.},
	urldate = {2023-02-27},
	date = {2012},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5466},
	keywords = {reproducibility, reliability, agreement, confidence interval, interrater, intrarater, measurements},
	file = {Sample size formulas for estimating intraclass correlation coefficients with precision and assurance:/Users/tom/Zotero/storage/9MFLGGZK/zou2012.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/JUVYJSF4/sim.html:text/html},
}

@article{mcgraw_forming_1996,
	title = {Forming inferences about some intraclass correlation coefficients},
	volume = {1},
	issn = {1939-1463},
	doi = {10.1037/1082-989X.1.1.30},
	abstract = {{AIthough} intraclass correlation coefficients ({lCCs}) are {commonIy} used in behavioral measurement, pychometrics, and behavioral genetics, procodures available for forming inferences about {ICC} are not widely known. Following a review of the distinction between various forms of the {ICC}, this article presents procedures available for calculating confidence intervals and conducting tests on {ICCs} developed using data from one-way and two-way random and mixed-{efFect} analysis of variance models. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {30--46},
	journaltitle = {Psychological Methods},
	author = {{McGraw}, Kenneth O. and Wong, S. P.},
	date = {1996},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Statistical Significance, Confidence Limits (Statistics), Statistical Correlation, Variability Measurement},
	file = {Forming inferences about some intraclass correlation coefficients:/Users/tom/Zotero/storage/7RVSBDUN/mcgraw1996.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PNDXXFRA/1996-03170-003.html:text/html},
}

@article{boring_mathematical_1919,
	title = {Mathematical vs. scientific significance},
	volume = {16},
	issn = {1939-1455},
	doi = {10.1037/h0074554},
	abstract = {Differentiates between mathematical and scientific methods. The differences between scientific intuition and mathematical results have been attributed to the fact that scientific generalization is broader than mathematical description. While scientific methods deal with samples which are representative of the total whole, the mathematical methods measure the differences between the particular samples observed. Science begins with description but ends in generalization. Mathematical measures are too high and may need to be discounted in arriving at a scientific conclusion. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {335--338},
	journaltitle = {Psychological Bulletin},
	author = {Boring, Edwin G.},
	date = {1919},
	note = {Place: {US}
Publisher: Psychological Review Company},
	keywords = {Sciences, Intuition, Experimental Methods, Mathematics},
	file = {Mathematical vs. scientific significance:/Users/tom/Zotero/storage/V99P85IP/boring1919.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ZJYPH2NG/1926-02603-001.html:text/html;Submitted Version:/Users/tom/Zotero/storage/LXN7MFVI/Boring - 1919 - Mathematical vs. scientific significance.pdf:application/pdf},
}

@article{budescu_improving_2009,
	title = {Improving Communication of Uncertainty in the Reports of the Intergovernmental Panel on Climate Change},
	volume = {20},
	issn = {0956-7976},
	url = {https://doi.org/10.1111/j.1467-9280.2009.02284.x},
	doi = {10.1111/j.1467-9280.2009.02284.x},
	abstract = {The Intergovernmental Panel on Climate Change ({IPCC}) assesses information relevant to the understanding of climate change and explores options for adaptation and mitigation. The {IPCC} reports communicate uncertainty by using a set of probability terms accompanied by global interpretational guidelines. The judgment literature indicates that there are large differences in the way people understand such phrases, and that their use may lead to confusion and errors in communication. We conducted an experiment in which subjects read sentences from the 2007 {IPCC} report and assigned numerical values to the probability terms. The respondents' judgments deviated significantly from the {IPCC} guidelines, even when the respondents had access to these guidelines. These results suggest that the method used by the {IPCC} is likely to convey levels of imprecision that are too high. We propose an alternative form of communicating uncertainty, illustrate its effectiveness, and suggest several additional ways to improve the communication of uncertainty.},
	pages = {299--308},
	number = {3},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Budescu, David V. and Broomell, Stephen and Por, Han-Hui},
	urldate = {2023-02-28},
	date = {2009-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Budescu et al. - 2009 - Improving Communication of Uncertainty in the Repo.pdf:/Users/tom/Zotero/storage/ES7MG6PC/Budescu et al. - 2009 - Improving Communication of Uncertainty in the Repo.pdf:application/pdf},
}

@article{kekecs_raising_2023,
	title = {Raising the value of research studies in psychological science by increasing the credibility of research reports: the transparent Psi project},
	volume = {10},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.191375},
	doi = {10.1098/rsos.191375},
	shorttitle = {Raising the value of research studies in psychological science by increasing the credibility of research reports},
	abstract = {The low reproducibility rate in social sciences has produced hesitation among researchers in accepting published findings at their face value. Despite the advent of initiatives to increase transparency in research reporting, the field is still lacking tools to verify the credibility of research reports. In the present paper, we describe methodologies that let researchers craft highly credible research and allow their peers to verify this credibility. We demonstrate the application of these methods in a multi-laboratory replication of Bem's Experiment 1 (Bem 2011 J. Pers. Soc. Psychol.100, 407–425. (doi:10.1037/a0021524)) on extrasensory perception ({ESP}), which was co-designed by a consensus panel including both proponents and opponents of Bem's original hypothesis. In the study we applied direct data deposition in combination with born-open data and real-time research reports to extend transparency to protocol delivery and data collection. We also used piloting, checklists, laboratory logs and video-documented trial sessions to ascertain as-intended protocol delivery, and external research auditors to monitor research integrity. We found 49.89\% successful guesses, while Bem reported 53.07\% success rate, with the chance level being 50\%. Thus, Bem's findings were not replicated in our study. In the paper, we discuss the implementation, feasibility and perceived usefulness of the credibility-enhancing methodologies used throughout the project.},
	pages = {191375},
	number = {2},
	journaltitle = {Royal Society Open Science},
	author = {Kekecs, Zoltan and Palfi, Bence and Szaszi, Barnabas and Szecsi, Peter and Zrubka, Mark and Kovacs, Marton and Bakos, Bence E. and Cousineau, Denis and Tressoldi, Patrizio and Schmidt, Kathleen and Grassi, Massimo and Evans, Thomas Rhys and Yamada, Yuki and Miller, Jeremy K. and Liu, Huanxu and Yonemitsu, Fumiya and Dubrov, Dmitrii and Röer, Jan Philipp and Becker, Marvin and Schnepper, Roxane and Ariga, Atsunori and Arriaga, Patrícia and Oliveira, Raquel and Põldver, Nele and Kreegipuu, Kairi and Hall, Braeden and Wiechert, Sera and Verschuere, Bruno and Girán, Kyra and Aczel, Balazs},
	urldate = {2023-03-01},
	date = {2023-02},
	note = {Publisher: Royal Society},
	keywords = {transparency, replication, metascience, research methods, credibility, real-time procedures},
	file = {Full Text PDF:/Users/tom/Zotero/storage/KNRVPW7Y/Kekecs et al. - 2023 - Raising the value of research studies in psycholog.pdf:application/pdf},
}

@article{trisovic_large-scale_2022,
	title = {A large-scale study on research code quality and execution},
	volume = {9},
	rights = {2022 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01143-6},
	doi = {10.1038/s41597-022-01143-6},
	abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74\% of R files failed to complete without error in the initial execution, while 56\% failed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals’ collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
	pages = {60},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Mercè},
	urldate = {2023-03-01},
	date = {2022-02-21},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Software, Research data, Information technology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/8Q5V33U8/Trisovic et al. - 2022 - A large-scale study on research code quality and e.pdf:application/pdf},
}

@article{fleiss_equivalence_1973,
	title = {The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability},
	volume = {33},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/001316447303300309},
	doi = {10.1177/001316447303300309},
	pages = {613--619},
	number = {3},
	journaltitle = {Educational and Psychological Measurement},
	author = {Fleiss, Joseph L. and Cohen, Jacob},
	urldate = {2023-03-02},
	date = {1973-10-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {The Equivalence of Weighted Kappa and the Intraclass Correlation Coefficient as Measures of Reliability:/Users/tom/Zotero/storage/HEGK86E9/fleiss1973.pdf.pdf:application/pdf},
}

@article{nelson_measures_2015,
	title = {Measures of agreement between many raters for ordinal classifications},
	volume = {34},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6546},
	doi = {10.1002/sim.6546},
	abstract = {Screening and diagnostic procedures often require a physician's subjective interpretation of a patient's test result using an ordered categorical scale to define the patient's disease severity. Because of wide variability observed between physicians' ratings, many large-scale studies have been conducted to quantify agreement between multiple experts' ordinal classifications in common diagnostic procedures such as mammography. However, very few statistical approaches are available to assess agreement in these large-scale settings. Many existing summary measures of agreement rely on extensions of Cohen's kappa. These are prone to prevalence and marginal distribution issues, become increasingly complex for more than three experts, or are not easily implemented. Here we propose a model-based approach to assess agreement in large-scale studies based upon a framework of ordinal generalized linear mixed models. A summary measure of agreement is proposed for multiple experts assessing the same sample of patients' test results according to an ordered categorical scale. This measure avoids some of the key flaws associated with Cohen's kappa and its extensions. Simulation studies are conducted to demonstrate the validity of the approach with comparison with commonly used agreement measures. The proposed methods are easily implemented using the software package R and are applied to two large-scale cancer agreement studies. Copyright © 2015 John Wiley \& Sons, Ltd.},
	pages = {3116--3132},
	number = {23},
	journaltitle = {Statistics in Medicine},
	author = {Nelson, Kerrie P. and Edwards, Don},
	urldate = {2023-03-02},
	date = {2015},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6546},
	keywords = {Cohen's kappa, Fleiss' kappa, generalized linear mixed model, inter-rater agreement, ordinal categorical data},
	file = {Accepted Version:/Users/tom/Zotero/storage/VL88FGVL/Nelson and Edwards - 2015 - Measures of agreement between many raters for ordi.pdf:application/pdf},
}

@article{svensson_different_2012,
	title = {Different ranking approaches defining association and agreement measures of paired ordinal data},
	volume = {31},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.5382},
	doi = {10.1002/sim.5382},
	abstract = {Rating scales are common for self-assessments of qualitative variables and also for expert-rating of the severity of disability, outcomes, etc. Scale assessments and other ordered classifications generate ordinal data having rank-invariant properties only. Hence, statistical methods are often based on ranks. The aim is to focus at the differences in ranking approaches between measures of association and of disagreement in paired ordinal data. The Spearman correlation coefficient is a measure of association between two variables, when each data set is transformed to ranks. The augmented ranking approach to evaluate disagreement takes account of the information given by the pairs of data, and provides identification and measures of systematic disagreement, when present, separately from measures of additional individual variability in assessments. The two approaches were applied to empirical data regarding relationship between perceived pain and physical health and reliability in pain assessments made by patients. The art of disagreement between the patients' perceived levels of outcome after treatment and the doctor's criterion-based scoring was also evaluated. The comprehensive evaluation of observed disagreement in terms of systematic and individual disagreement provides valuable interpretable information of their sources. The presence of systematic disagreement can be adjusted for and/or understood. Large individual variability could be a sign of poor quality of a scale or heterogeneity among raters. It was also demonstrated that a measure of association must not be used as a measure of agreement, even though such misuse of correlation coefficients is common. Copyright © 2012 John Wiley \& Sons, Ltd.},
	pages = {3104--3117},
	number = {26},
	journaltitle = {Statistics in Medicine},
	author = {Svensson, Elisabeth},
	urldate = {2023-03-02},
	date = {2012},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5382},
	keywords = {agreement, association, disagreement, ordinal data, ranks},
	file = {Svensson - 2012 - Different ranking approaches defining association .pdf:/Users/tom/Zotero/storage/L3S733P7/Svensson - 2012 - Different ranking approaches defining association .pdf:application/pdf},
}

@article{vargha_critique_2000,
	title = {A critique and improvement of the \textit{cl} common language effect size statistics of mcgraw and wong},
	volume = {25},
	issn = {1076-9986, 1935-1054},
	url = {http://journals.sagepub.com/doi/10.3102/10769986025002101},
	doi = {10.3102/10769986025002101},
	abstract = {{McGraw} and Wong (1992) described an appealing index of effect size, called {CL}, which measures the difference between two populations in terms of the probability that a score sampled at random from the first population will be greater than a score sampled at random from the second. {McGraw} and Wong introduced this "common language effect size statistic" for normal distributions and then proposed an approximate estimation for any continuous distribution. In addition, they generalized {CL} to the n-group case, the correlated samples case, and the discrete values case.
            In the current paper a different generalization of {CL}, called the A measure of stochastic superiority, is proposed, which may be directly applied for any discrete or continuous variable that is at least ordinally scaled. Exact methods for point and interval estimation as well as the significance tests of the A = .5 hypothesis are provided. New generalizations {ofCL} are provided for the multi-group and correlated samples cases.},
	pages = {101--132},
	number = {2},
	journaltitle = {Journal of Educational and Behavioral Statistics},
	shortjournal = {Journal of Educational and Behavioral Statistics},
	author = {Vargha, András and Delaney, Harold D.},
	urldate = {2023-03-09},
	date = {2000-06},
	langid = {english},
	file = {A Critique and Improvement of the <i>CL</i> Common Language Effect Size Statistics of McGraw and Wong:/Users/tom/Zotero/storage/IBXSHJFJ/vargha2000.pdf.pdf:application/pdf},
}

@book{cumming_understanding_2012,
	location = {New York},
	title = {Understanding the new statistics: effect sizes, confidence intervals, and meta-analysis},
	isbn = {978-0-415-87967-5 978-0-415-87968-2 978-0-203-80700-2},
	series = {Multivariate applications series},
	shorttitle = {Understanding the new statistics},
	pagetotal = {519},
	publisher = {Routledge, Taylor \& Francis Group},
	author = {Cumming, Geoff},
	date = {2012},
	note = {{OCLC}: ocn751455367},
	keywords = {Confidence intervals, Statistical methods, Confidence Intervals, Multivariate analysis, Multivariate Analysis, Psychology, Experimental, Sozialwissenschaften, Statistik},
}

@article{newcombe_two-sided_1998,
	title = {Two-sided confidence intervals for the single proportion: comparison of seven methods},
	volume = {17},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819980430%2917%3A8%3C857%3A%3AAID-SIM777%3E3.0.CO%3B2-E},
	doi = {10.1002/(SICI)1097-0258(19980430)17:8<857::AID-SIM777>3.0.CO;2-E},
	shorttitle = {Two-sided confidence intervals for the single proportion},
	abstract = {Simple interval estimate methods for proportions exhibit poor coverage and can produce evidently inappropriate intervals. Criteria appropriate to the evaluation of various proposed methods include: closeness of the achieved coverage probability to its nominal value; whether intervals are located too close to or too distant from the middle of the scale; expected interval width; avoidance of aberrations such as limits outside [0,1] or zero width intervals; and ease of use, whether by tables, software or formulae. Seven methods for the single proportion are evaluated on 96,000 parameter space points. Intervals based on tail areas and the simpler score methods are recommended for use. In each case, methods are available that aim to align either the minimum or the mean coverage with the nominal 1−α. © 1998 John Wiley \& Sons, Ltd.},
	pages = {857--872},
	number = {8},
	journaltitle = {Statistics in Medicine},
	author = {Newcombe, Robert G.},
	urldate = {2023-03-14},
	date = {1998},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819980430\%2917\%3A8\%3C857\%3A\%3AAID-{SIM}777\%3E3.0.{CO}\%3B2-E},
	file = {Snapshot:/Users/tom/Zotero/storage/WFVYSIRI/(SICI)1097-0258(19980430)178857AID-SIM7773.0.html:text/html;Two-sided confidence intervals for the single proportion\: comparison of seven methods:/Users/tom/Zotero/storage/6D48ZAY2/newcombe1998.pdf.pdf:application/pdf},
}

@article{gelman_discussion_2014,
	title = {Discussion: Difficulties in making inferences about scientific truth from distributions of published p-values},
	volume = {15},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxt034},
	doi = {10.1093/biostatistics/kxt034},
	shorttitle = {Discussion},
	pages = {18--23},
	number = {1},
	journaltitle = {Biostatistics},
	shortjournal = {Biostatistics},
	author = {Gelman, A. and O'Rourke, K.},
	urldate = {2023-03-14},
	date = {2014-01-01},
	langid = {english},
	file = {Discussion\: Difficulties in making inferences about scientific truth from distributions of published p-values:/Users/tom/Zotero/storage/FP2GMMNK/gelman2013.pdf.pdf:application/pdf},
}

@article{erdfelder_detecting_2019-1,
	title = {Detecting Evidential Value and \textit{p} -Hacking With the \textit{p} -Curve Tool: A Word of Caution},
	volume = {227},
	issn = {2190-8370, 2151-2604},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000383},
	doi = {10.1027/2151-2604/a000383},
	shorttitle = {Detecting Evidential Value and \textit{p} -Hacking With the \textit{p} -Curve Tool},
	abstract = {Abstract. Simonsohn, Nelson, and Simmons (2014a) proposed p-curve – the distribution of statistically significant p-values for a set of studies – as a tool to assess the evidential value of these studies. They argued that, whereas right-skewed p-curves indicate true underlying effects, left-skewed p-curves indicate selective reporting of significant results when there is no true effect (“ p-hacking”). We first review previous research showing that, in contrast to the first claim, null effects may produce right-skewed p-curves under some conditions. We then question the second claim by showing that not only selective reporting but also selective nonreporting of significant results due to a significant outcome of a more popular alternative test of the same hypothesis may produce left-skewed p-curves, even if all studies reflect true effects. Hence, just as right-skewed p-curves do not necessarily imply evidential value, left-skewed p-curves do not necessarily imply p-hacking and absence of true effects in the studies involved.},
	pages = {249--260},
	number = {4},
	journaltitle = {Zeitschrift für Psychologie},
	shortjournal = {Zeitschrift für Psychologie},
	author = {Erdfelder, Edgar and Heck, Daniel W.},
	urldate = {2023-03-16},
	date = {2019-10},
	langid = {english},
	file = {Erdfelder and Heck - 2019 - Detecting Evidential Value and p -Hacking W.pdf:/Users/tom/Zotero/storage/IZSAV8GN/Erdfelder and Heck - 2019 - Detecting Evidential Value and p -Hacking W.pdf:application/pdf},
}

@article{goodman_mammography_2002,
	title = {The Mammography Dilemma: A Crisis for Evidence-Based Medicine?},
	volume = {137},
	issn = {0003-4819},
	url = {http://annals.org/article.aspx?doi=10.7326/0003-4819-137-5_Part_1-200209030-00015},
	doi = {10.7326/0003-4819-137-5_Part_1-200209030-00015},
	shorttitle = {The Mammography Dilemma},
	pages = {363},
	number = {5},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Goodman, Steven N.},
	urldate = {2023-03-16},
	date = {2002-09-03},
	langid = {english},
	file = {The Mammography Dilemma\: A Crisis for Evidence-Based Medicine?:/Users/tom/Zotero/storage/SVFJ7NI5/goodman2002.pdf.pdf:application/pdf},
}

@book{rothman_modern_2008,
	location = {Philadelphia},
	edition = {3rd ed., thoroughly rev. and updated},
	title = {Modern epidemiology},
	isbn = {978-0-7817-5564-1},
	pagetotal = {758},
	publisher = {Wolters Kluwer Health/Lippincott Williams \& Wilkins},
	author = {Rothman, Kenneth J. and Greenland, Sander and Lash, Timothy L.},
	date = {2008},
	note = {{OCLC}: 169455558},
	keywords = {Epidemiology, Statistical methods, Research Methodology, Epidemiologic Methods},
	file = {Modern Epidemiology.pdf:/Users/tom/Zotero/storage/DQLIGEK3/Modern Epidemiology.pdf:application/pdf},
}

@article{altmann_reconstructing_2003,
	title = {Reconstructing the serial order of events: a case study of September 11, 2001},
	volume = {17},
	issn = {0888-4080, 1099-0720},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/acp.986},
	doi = {10.1002/acp.986},
	shorttitle = {Reconstructing the serial order of events},
	pages = {1067--1080},
	number = {9},
	journaltitle = {Applied Cognitive Psychology},
	shortjournal = {Appl. Cognit. Psychol.},
	author = {Altmann, Erik M.},
	urldate = {2023-03-20},
	date = {2003-11},
	langid = {english},
	file = {Altmann - 2003 - Reconstructing the serial order of events a case .pdf:/Users/tom/Zotero/storage/X4ADDDGV/Altmann - 2003 - Reconstructing the serial order of events a case .pdf:application/pdf},
}

@article{qian_weighted_2019,
	title = {Weighted distance-based models for ranking data using the R package rankdist},
	volume = {90},
	rights = {Copyright (c) 2019 Zhaozhi Qian, Philip L. H. Yu},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v090.i05},
	doi = {10.18637/jss.v090.i05},
	abstract = {rankdist is a recently developed R package which implements various distance-based ranking models. These models capture the occurring probability of rankings based on the distances between them. The package provides a framework for fitting and evaluating finite mixture of distance-based models. This paper also presents a new probability model for ranking data based on a new notion of weighted Kendall distance. The new model is flexible and more interpretable than the existing models. We show that the new model has an analytic form of the probability mass function and the maximum likelihood estimates of the model parameters can be obtained efficiently even for ranking involving a large number of objects.},
	pages = {1--31},
	journaltitle = {Journal of Statistical Software},
	author = {Qian, Zhaozhi and Yu, Philip L. H.},
	urldate = {2023-03-21},
	date = {2019-07-31},
	langid = {english},
	keywords = {R, distance-based models, Kendall distance, mixtures models, rank aggregation, ranking data},
	file = {Qian and Yu - 2019 - Weighted distance-based models for ranking data us.pdf:/Users/tom/Zotero/storage/X37E34JH/Qian and Yu - 2019 - Weighted distance-based models for ranking data us.pdf:application/pdf},
}

@article{kendall_new_1938,
	title = {A new measure of rank correlation},
	volume = {30},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/30.1-2.81},
	doi = {10.1093/biomet/30.1-2.81},
	pages = {81--93},
	number = {1},
	journaltitle = {Biometrika},
	shortjournal = {Biometrika},
	author = {Kendall, M. G.},
	urldate = {2023-03-21},
	date = {1938-06-01},
	file = {Full Text PDF:/Users/tom/Zotero/storage/J48N5HIQ/KENDALL - 1938 - A NEW MEASURE OF RANK CORRELATION.pdf:application/pdf;Kendall - 1938 - A new measure of rank correlation.pdf:/Users/tom/Zotero/storage/R9SB8KJY/Kendall - 1938 - A new measure of rank correlation.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/UZKN345R/176907.html:text/html},
}

@book{baguley_serious_2012,
	location = {Houndmills, Basingstoke, Hampshire [England] ; New York},
	title = {Serious stats: a guide to advanced statistics for the behavioral sciences},
	isbn = {978-0-230-57717-6 978-0-230-57718-3},
	shorttitle = {Serious stats},
	pagetotal = {830},
	publisher = {Palgrave Macmillan},
	author = {Baguley, Thomas},
	date = {2012},
	keywords = {Psychology, Social sciences, Psychometrics, Statistical methods},
}

@article{blaker_confidence_2000,
	title = {Confidence curves and improved exact confidence intervals for discrete distributions},
	volume = {28},
	issn = {0319-5724},
	url = {https://www.jstor.org/stable/3315916},
	doi = {10.2307/3315916},
	abstract = {The author describes a method for improving standard "exact" confidence intervals in discrete distributions with respect to size while retaining correct level. The binomial, negative binomial, hypergeometric, and Poisson distributions are considered explicitly. Contrary to other existing methods, the author's solution possesses a natural nesting condition: if α {\textless} α′, the 1 - α′ confidence interval is included in the 1 - α interval. Nonparametric confidence intervals for a quantile are also considered. /// L'auteur décrit une méthode permettant d'obtenir des intervalles de confiance d'un niveau fixé qui soient plus courts que les intervalles "exacts" classiques pour des lois discrètes. Les lois binomiale, négative binomiale, hypergéométrique et de Poisson sont traitées comme cas particuliers. Contrairement à d'autres méthodes, la solution de l'auteur respecte une condition d'emboîtement naturelle: si α {\textless} α′, l'intervalle de niveau 1 - α′ est toujours compris dans l'intervalle de niveau 1 - α. La construction d'intervalles de confiance non paramétriques pour les quantiles est également abordée.},
	pages = {783--798},
	number = {4},
	journaltitle = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	author = {Blaker, Helge},
	urldate = {2023-03-22},
	date = {2000},
	file = {Blaker - 2000 - Confidence curves and improved exact confidence in.pdf:/Users/tom/Zotero/storage/W2LJY8ZW/Blaker - 2000 - Confidence curves and improved exact confidence in.pdf:application/pdf},
}

@article{franzen_institutional_2023,
	title = {Institutional dashboards on clinical trial transparency for University Medical Centers: A case study},
	volume = {20},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1004175},
	doi = {10.1371/journal.pmed.1004175},
	shorttitle = {Institutional dashboards on clinical trial transparency for University Medical Centers},
	abstract = {Background University Medical Centers ({UMCs}) must do their part for clinical trial transparency by fostering practices such as prospective registration, timely results reporting, and open access. However, research institutions are often unaware of their performance on these practices. Baseline assessments of these practices would highlight where there is room for change and empower {UMCs} to support improvement. We performed a status quo analysis of established clinical trial registration and reporting practices at German {UMCs} and developed a dashboard to communicate these baseline assessments with {UMC} leadership and the wider research community. Methods and findings We developed and applied a semiautomated approach to assess adherence to established transparency practices in a cohort of interventional trials and associated results publications. Trials were registered in {ClinicalTrials}.gov or the German Clinical Trials Register ({DRKS}), led by a German {UMC}, and reported as complete between 2009 and 2017. To assess adherence to transparency practices, we identified results publications associated to trials and applied automated methods at the level of registry data (e.g., prospective registration) and publications (e.g., open access). We also obtained summary results reporting rates of due trials registered in the {EU} Clinical Trials Register ({EUCTR}) and conducted at German {UMCs} from the {EU} Trials Tracker. We developed an interactive dashboard to display these results across all {UMCs} and at the level of single {UMCs}. Our study included and assessed 2,895 interventional trials led by 35 German {UMCs}. Across all {UMCs}, prospective registration increased from 33\% (n = 58/178) to 75\% (n = 144/193) for trials registered in {ClinicalTrials}.gov and from 0\% (n = 0/44) to 79\% (n = 19/24) for trials registered in {DRKS} over the period considered. Of trials with a results publication, 38\% (n = 714/1,895) reported the trial registration number in the publication abstract. In turn, 58\% (n = 861/1,493) of trials registered in {ClinicalTrials}.gov and 23\% (n = 111/474) of trials registered in {DRKS} linked the publication in the registration. In contrast to recent increases in summary results reporting of drug trials in the {EUCTR}, 8\% (n = 191/2,253) and 3\% (n = 20/642) of due trials registered in {ClinicalTrials}.gov and {DRKS}, respectively, had summary results in the registry. Across trial completion years, timely results reporting (within 2 years of trial completion) as a manuscript publication or as summary results was 41\% (n = 1,198/2,892). The proportion of openly accessible trial publications steadily increased from 42\% (n = 16/38) to 74\% (n = 72/97) over the period considered. A limitation of this study is that some of the methods used to assess the transparency practices in this dashboard rely on registry data being accurate and up-to-date. Conclusions In this study, we observed that it is feasible to assess and inform individual {UMCs} on their performance on clinical trial transparency in a reproducible and publicly accessible way. Beyond helping institutions assess how they perform in relation to mandates or their institutional policy, the dashboard may inform interventions to increase the uptake of clinical transparency practices and serve to evaluate the impact of these interventions.},
	pages = {e1004175},
	number = {3},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Franzen, Delwen L. and Carlisle, Benjamin Gregory and Salholz-Hillel, Maia and Riedel, Nico and Strech, Daniel},
	urldate = {2023-03-27},
	date = {2023-03-21},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open science, Medicine and health sciences, Scientific publishing, Clinical trial reporting, Clinical trials, Research assessment, European Union, Open access publishing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/4T7DV9SL/Franzen et al. - 2023 - Institutional dashboards on clinical trial transpa.pdf:application/pdf},
}

@article{feest_why_2019,
	title = {Why replication is overrated},
	volume = {86},
	issn = {0031-8248, 1539-767X},
	url = {https://www.cambridge.org/core/product/identifier/S0031824800015324/type/journal_article},
	doi = {10.1086/705451},
	abstract = {Current debates about the replication crisis in psychology take it for granted that direct replication is valuable, largely focusing on its role in uncovering questionable statistical practices. This article takes a broader look at the notion of replication in psychological experiments. It is argued that all experimentation/replication involves individuation judgments and that research in experimental psychology frequently turns on probing the adequacy of such judgments. In this vein, I highlight the ubiquity of conceptual and material questions in research, arguing that replication has its place but is not as central to psychological research as it is sometimes taken to be.},
	pages = {895--905},
	number = {5},
	journaltitle = {Philosophy of Science},
	shortjournal = {Philos. of Sci.},
	author = {Feest, Uljana},
	urldate = {2023-03-29},
	date = {2019-12},
	langid = {english},
	file = {Why Replication Is Overrated:/Users/tom/Zotero/storage/YB7GQPT5/feest2019.pdf.pdf:application/pdf},
}

@article{macnamara_growth_2022-1,
	title = {Do growth mindset interventions impact students’ academic achievement? A systematic review and meta-analysis with recommendations for best practices},
	issn = {1939-1455},
	doi = {10.1037/bul0000352},
	shorttitle = {Do growth mindset interventions impact students’ academic achievement?},
	abstract = {According to mindset theory, students who believe their personal characteristics can change—that is, those who hold a growth mindset—will achieve more than students who believe their characteristics are fixed. Proponents of the theory have developed interventions to influence students’ mindsets, claiming that these interventions lead to large gains in academic achievement. Despite their popularity, the evidence for growth mindset intervention benefits has not been systematically evaluated considering both the quantity and quality of the evidence. Here, we provide such a review by (a) evaluating empirical studies’ adherence to a set of best practices essential for drawing causal conclusions and (b) conducting three meta-analyses. When examining all studies (63 studies, N = 97,672), we found major shortcomings in study design, analysis, and reporting, and suggestions of researcher and publication bias: Authors with a financial incentive to report positive findings published significantly larger effects than authors without this incentive. Across all studies, we observed a small overall effect: d¯ = 0.05, 95\% {CI} = [0.02, 0.09], which was nonsignificant after correcting for potential publication bias. No theoretically meaningful moderators were significant. When examining only studies demonstrating the intervention influenced students’ mindsets as intended (13 studies, N = 18,355), the effect was nonsignificant: d¯ = 0.04, 95\% {CI} = [−0.01, 0.10]. When examining the highest-quality evidence (6 studies, N = 13,571), the effect was nonsignificant: d¯ = 0.02, 95\% {CI} = [−0.06, 0.10]. We conclude that apparent effects of growth mindset interventions on academic achievement are likely attributable to inadequate study design, reporting flaws, and bias. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {No Pagination Specified--No Pagination Specified},
	journaltitle = {Psychological Bulletin},
	author = {Macnamara, Brooke N. and Burgoyne, Alexander P.},
	date = {2022},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Best Practices, Academic Achievement, Experimenter Bias, Monetary Incentives, Popularity, School Based Intervention, Student Characteristics, Theories},
	file = {Snapshot:/Users/tom/Zotero/storage/28HGWBKC/2023-14088-001.html:text/html;Submitted Version:/Users/tom/Zotero/storage/T8UXIARB/Macnamara and Burgoyne - 2022 - Do growth mindset interventions impact students’ a.pdf:application/pdf},
}

@article{lachin_power_1992,
	title = {Power and sample size evaluation for the mcnemar test with application to matched case-control studies},
	volume = {11},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780110909},
	doi = {10.1002/sim.4780110909},
	abstract = {Various expressions have appeared for sample size calculation based on the power function of {McNemar}'s test for paired or matched proportions, especially with reference to a matched case-control study. These differ principally with respect to the expression for the variance of the statistic under the alternative hypothesis. In addition to the conditional power function, I identify and compare four distinct unconditional expressions. I show that the unconditional calculation of Schlesselman for the matched case-control study can be expressed as a first-order unconditional calculation as described by Miettinen. Corrections to Schlesselman's unconditional expression presented by Fleiss and Levin and by Dupont, which use different models to describe exposure association among matched cases and controls, are also equivalent to a first-order unconditional calculation. I present a simplification of these corrections that directly provides the underlying table of cell probabilities, from which one can perform any of the alternative sample size calculations. Also, I compare the four unconditional sample size expressions relative to the exact power function. The conclusion is that Miettinen's first-order expression tends to underestimate sample size, while his second-order expression is usually fairly accurate, though possibly slightly anti-conservative. A multinomial-based expression presented by Connor, among others, is also fairly accurate and is usually slightly conservative. Finally, a local unconditional expression of Mitra, among others, tends to be excessively conservative.},
	pages = {1239--1251},
	number = {9},
	journaltitle = {Statistics in Medicine},
	author = {Lachin, John M.},
	urldate = {2023-04-03},
	date = {1992},
	langid = {english},
}

@article{fagerland_recommended_2014,
	title = {Recommended tests and confidence intervals for paired binomial proportions},
	volume = {33},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6148},
	doi = {10.1002/sim.6148},
	abstract = {We describe, evaluate, and recommend statistical methods for the analysis of paired binomial proportions. A total of 24 methods are considered. The best tests for association include the asymptotic {McNemar} test and the {McNemar} mid- p test. For the difference between proportions, we recommend two simple confidence intervals with closed-form expressions and the asymptotic score interval. The asymptotic score interval is also recommended for the ratio of proportions, as is an interval with closed-form expression based on combining two Wilson score intervals for the single proportion. For the odds ratio, we recommend a transformation of the Wilson score interval and a transformation of the Clopper–Pearson mid- p interval. We illustrate the practical application of the methods using data from a recently published study of airway reactivity in children before and after stem cell transplantation and a matched case–control study of the association between floppy eyelid syndrome and obstructive sleep apnea-hypopnea syndrome. Copyright © 2014 John Wiley \& Sons, Ltd.},
	pages = {2850--2875},
	number = {16},
	journaltitle = {Statistics in Medicine},
	author = {Fagerland, Morten W. and Lydersen, Stian and Laake, Petter},
	urldate = {2023-04-04},
	date = {2014},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6148},
	keywords = {dependent data, matched data, {McNemar}, mid-p, paired data},
	file = {Fagerland et al. - 2014 - Recommended tests and confidence intervals for pai.pdf:/Users/tom/Zotero/storage/S3XDZPGD/Fagerland et al. - 2014 - Recommended tests and confidence intervals for pai.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/58XUXYKK/sim.html:text/html},
}

@article{fay_confidence_2021,
	title = {Confidence intervals for difference in proportions for matched pairs compatible with exact {McNemar}'s or sign tests},
	volume = {40},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8829},
	doi = {10.1002/sim.8829},
	abstract = {For testing with paired data (eg, twins randomized between two treatments), a simple test is the sign test, where we test if the distribution of the sign of the differences in responses between the two treatments within pairs is more often positive (favoring one treatment) or negative (favoring the other). When the responses are binary, this reduces to a {McNemar}-type test, and the calculations are the same. Although it is easy to calculate an exact P-value by conditioning on the total number of discordant pairs, the accompanying confidence interval on a parameter of interest (proportion positive minus proportion negative) is not straightforward. Effect estimates and confidence intervals are important for interpretation because it is possible that the treatment helps a very small proportion of the population yet gives a highly significant effect. We construct a confidence interval that is compatible with an exact sign test, meaning the 100(1−α)\% interval excludes the null hypothesis of equality of proportions if and only if the associated exact sign test rejects at level α. We conjecture that the proposed confidence intervals guarantee nominal coverage, and we support that conjecture with extensive numerical calculations, but we have no mathematical proof to show guaranteed coverage. We have written and made available the function {mcnemarExactDP} in the exact2x2 R package and the function {signTest} in the asht R package to perform the methods described in this article.},
	pages = {1147--1159},
	number = {5},
	journaltitle = {Statistics in Medicine},
	author = {Fay, Michael P. and Lumbard, Keith},
	urldate = {2023-04-04},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8829},
	keywords = {confidence distribution, exact inference, Melded confidence interval},
	file = {Accepted Version:/Users/tom/Zotero/storage/A7RZWA7X/Fay and Lumbard - 2021 - Confidence intervals for difference in proportions.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SSR4TG67/sim.html:text/html},
}

@article{schafmeister_effect_2021,
	title = {The effect of replications on citation patterns: evidence from a large-scale reproducibility project},
	volume = {32},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/09567976211005767},
	doi = {10.1177/09567976211005767},
	shorttitle = {The effect of replications on citation patterns},
	abstract = {Replication of existing research is often referred to as one of the cornerstones of modern science. In this study, I tested whether the publication of independent replication attempts affects the citation patterns of the original studies. Investigating 95 replications conducted in the context of the Reproducibility Project: Psychology, I found little evidence for an adjustment of citation patterns in response to the publication of these independent replication attempts. This finding was robust to the choice of replication criterion, various model specifications, and the composition of the contrast group. I further present some suggestive evidence that shifts in the underlying composition of supporting and disputing citations have likely been small. I conclude with a review of the evidence in favor of the remaining explanations and discuss the potential consequences of these findings for the workings of the scientific process.},
	pages = {1537--1548},
	number = {10},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Schafmeister, Felix},
	urldate = {2023-04-05},
	date = {2021-10},
	langid = {english},
	file = {Schafmeister - 2021 - The effect of replications on citation patterns e.pdf:/Users/tom/Zotero/storage/9C9QBPWX/Schafmeister - 2021 - The effect of replications on citation patterns e.pdf:application/pdf},
}

@article{anderson_questionable_2023,
	title = {Questionable research practices and cumulative science: The consequences of selective reporting on effect size bias and heterogeneity},
	issn = {1939-1463},
	doi = {10.1037/met0000572},
	shorttitle = {Questionable research practices and cumulative science},
	abstract = {Despite increased attention to open science and transparency, questionable research practices ({QRPs}) remain common, and studies published using {QRPs} will remain a part of the published record for some time. A particularly common type of {QRP} involves multiple testing, and in some forms of this, researchers report only a selection of the tests conducted. Methodological investigations of multiple testing and {QRPs} have often focused on implications for a single study, as well as how these practices can increase the likelihood of false positive results. However, it is illuminating to consider the role of these {QRPs} from a broader, literature-wide perspective, focusing on consequences that affect the interpretability of results across the literature. In this article, we use a Monte Carlo simulation study to explore the consequences of two {QRPs} involving multiple testing, cherry picking and question trolling, on effect size bias and heterogeneity among effect sizes. Importantly, we explicitly consider the role of real-world conditions, including sample size, effect size, and publication bias, that amend the influence of these {QRPs}. Results demonstrated that {QRPs} can substantially affect both bias and heterogeneity, although there were many nuances, particularly relating to the influence of publication bias, among other factors. The present study adds a new perspective to how {QRPs} may influence researchers’ ability to evaluate a literature accurately and cumulatively, and points toward yet another reason to continue to advocate for initiatives that reduce {QRPs}. ({PsycInfo} Database Record (c) 2023 {APA}, all rights reserved)},
	pages = {No Pagination Specified--No Pagination Specified},
	journaltitle = {Psychological Methods},
	author = {Anderson, Samantha F. and Liu, Xinran},
	date = {2023},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Experimentation, Sciences, Experimental Replication, Effect Size (Statistical), Test Bias, Testing},
	file = {Anderson and Liu - 2023 - Questionable research practices and cumulative sci.pdf:/Users/tom/Zotero/storage/HV72LHIM/Anderson and Liu - 2023 - Questionable research practices and cumulative sci.pdf:application/pdf},
}

@article{goodman_use_1994,
	title = {The use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results},
	volume = {121},
	issn = {0003-4819},
	url = {https://www.acpjournals.org/doi/10.7326/0003-4819-121-3-199408010-00008},
	doi = {10.7326/0003-4819-121-3-199408010-00008},
	pages = {200--206},
	number = {3},
	journaltitle = {Annals of Internal Medicine},
	shortjournal = {Ann Intern Med},
	author = {Goodman, Steven N. and Berlin, Jesse A.},
	urldate = {2023-04-06},
	date = {1994-08},
	note = {Publisher: American College of Physicians},
	file = {Goodman and Berlin - 1994 - The use of predicted confidence intervals when pla.pdf:/Users/tom/Zotero/storage/MA8W8AMW/Goodman and Berlin - 1994 - The use of predicted confidence intervals when pla.pdf:application/pdf},
}

@article{rendueles_competition_2023,
	title = {Competition between lysogenic and sensitive bacteria is determined by the fitness costs of the different emerging phage-resistance strategies},
	volume = {12},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.83479},
	doi = {10.7554/eLife.83479},
	abstract = {Many bacterial genomes carry prophages whose induction can eliminate competitors. In response, bacteria may become resistant by modifying surface receptors, by lysogenization, or by other poorly known processes. All these mechanisms affect bacterial fitness and population dynamics. To understand the evolution of phage resistance, we co-cultivated a phage-sensitive strain ({BJ}1) and a polylysogenic Klebsiella pneumoniae strain ({ST}14) under different phage pressures. The population yield remained stable after 30 days. Surprisingly, the initially sensitive strain remained in all populations and its frequency was highest when phage pressure was strongest. Resistance to phages in these populations emerged initially through mutations preventing capsule biosynthesis. Protection through lysogeny was rarely observed because the lysogens have increased death rates due to prophage induction. Unexpectedly, the adaptation process changed at longer time scales: the frequency of capsulated cells in {BJ}1 populations increased again because the production of the capsule was fine-tuned, reducing the ability of phage to absorb. Contrary to the lysogens, these capsulated-resistant clones are pan-resistant to a large panel of phages. Intriguingly, some clones exhibited transient non-genetic resistance to phages, suggesting an important role of phenotypic resistance in coevolving populations. Our results show that interactions between lysogens and sensitive strains are shaped by antagonistic co-evolution between phages and bacteria. These processes may involve key physiological traits, such as the capsule, and depend on the time frame of the evolutionary process. At short time scales, simple and costly inactivating mutations are adaptive, but in the long term, changes drawing more favorable trade-offs between resistance to phages and cell fitness become prevalent.},
	pages = {e83479},
	journaltitle = {{eLife}},
	author = {Rendueles, Olaya and de Sousa, Jorge {AM} and Rocha, Eduardo {PC}},
	editor = {Díaz-Muñoz, Samuel L and Landry, Christian R and Díaz-Muñoz, Samuel L},
	urldate = {2023-04-06},
	date = {2023-03-28},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {coevolution, K pneumoniae, phage resistance, polylysogen},
	file = {Full Text PDF:/Users/tom/Zotero/storage/P4QERQ4E/Rendueles et al. - 2023 - Competition between lysogenic and sensitive bacter.pdf:application/pdf},
}

@article{van_doorn_using_2021,
	title = {Using the weighted Kendall Distance to analyze rank data in psychology},
	volume = {17},
	issn = {2292-1354},
	url = {http://www.tqmp.org/RegularArticles/vol17-2/p154},
	doi = {10.20982/tqmp.17.2.p154},
	pages = {154--165},
	number = {2},
	journaltitle = {The Quantitative Methods for Psychology},
	shortjournal = {{TQMP}},
	author = {van Doorn, Johnny and Westfall, Holly A. and Lee, Michael D.},
	urldate = {2023-04-09},
	date = {2021-06-01},
	file = {van Doorn et al. - 2021 - Using the weighted Kendall Distance to analyze ran.pdf:/Users/tom/Zotero/storage/7ZZ3FN58/van Doorn et al. - 2021 - Using the weighted Kendall Distance to analyze ran.pdf:application/pdf},
}

@misc{rousselet_introduction_2019,
	title = {An introduction to the bootstrap: a versatile method to make inferences by using data-driven simulations},
	url = {https://psyarxiv.com/h8ft7/},
	doi = {10.31234/osf.io/h8ft7},
	shorttitle = {An introduction to the bootstrap},
	abstract = {The bootstrap is a versatile technique that relies on data-driven simulations to make statistical inferences. When combined with robust estimators, the bootstrap can afford much more powerful and flexible inferences than is possible with standard approaches such as t-tests on means. In this tutorial, we use detailed illustrations of bootstrap simulations to give readers an intuition of what the bootstrap does and how it can be applied to solve many practical problems, such as building confidence intervals for many aspects of the data. In particular, we illustrate how to build confidence intervals for measures of location, including measures of central tendency, in the one-sample case, for two independent and two dependent groups. We also demonstrate how to compare correlation coefficients using the bootstrap and to perform simulations to determine if the bootstrap is fit for purpose for a particular application. Our approach is to suggest and motivate what could be done in a situation, with an understanding that various options are valid, though they may help answer different questions about a dataset. The tutorial also addresses two widespread misconceptions about the bootstrap: that it makes no assumptions about the data, and that it leads to robust inferences on its own. The tutorial focuses on detailed graphical descriptions, with data and code available online to reproduce the figures and analyses in the article ({OSF}: https://osf.io/8b4t5/; {GitHub}: https://github.com/{GRousselet}/bootstrap).},
	publisher = {{PsyArXiv}},
	author = {Rousselet, Guillaume and Pernet, Dr Cyril and Wilcox, Rand R.},
	urldate = {2023-04-10},
	date = {2019-05-27},
	langid = {english},
	keywords = {Social and Behavioral Sciences, Quantitative Methods, Statistical Methods, reaction time, correlation, confidence interval, bootstrap-t, group comparison, Harrell-Davis estimator, median, percentile bootstrap, skewness, trimmed mean},
	file = {Full Text PDF:/Users/tom/Zotero/storage/KLBXSQ82/Rousselet et al. - 2019 - An introduction to the bootstrap a versatile meth.pdf:application/pdf},
}

@article{rousselet_percentile_2021,
	title = {The percentile bootstrap: a primer with step-by-step instructions in R},
	volume = {4},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245920911881},
	doi = {10.1177/2515245920911881},
	shorttitle = {The percentile bootstrap},
	abstract = {The percentile bootstrap is the Swiss Army knife of statistics: It is a nonparametric method based on data-driven simulations. It can be applied to many statistical problems, as a substitute to standard parametric approaches, or in situations for which parametric methods do not exist. In this Tutorial, we cover R code to implement the percentile bootstrap to make inferences about central tendency (e.g., means and trimmed means) and spread in a one-sample example and in an example comparing two independent groups. For each example, we explain how to derive a bootstrap distribution and how to get a confidence interval and a p value from that distribution. We also demonstrate how to run a simulation to assess the behavior of the bootstrap. For some purposes, such as making inferences about the mean, the bootstrap performs poorly. But for other purposes, it is the only known method that works well over a broad range of situations. More broadly, combining the percentile bootstrap with robust estimators (i.e., estimators that are not overly sensitive to outliers) can help users gain a deeper understanding of their data than they would using conventional methods.},
	pages = {2515245920911881},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Rousselet, Guillaume A. and Pernet, Cyril R. and Wilcox, Rand R.},
	urldate = {2023-04-10},
	date = {2021-01-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/3ZVWVQS9/Rousselet et al. - 2021 - The Percentile Bootstrap A Primer With Step-by-St.pdf:application/pdf},
}

@article{fagerland_mcnemar_2013,
	title = {The {McNemar} test for binary matched-pairs data: mid-p and asymptotic are better than exact conditional},
	volume = {13},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-13-91},
	doi = {10.1186/1471-2288-13-91},
	shorttitle = {The {McNemar} test for binary matched-pairs data},
	abstract = {Statistical methods that use the mid-p approach are useful tools to analyze categorical data, particularly for small and moderate sample sizes. Mid-p tests strike a balance between overly conservative exact methods and asymptotic methods that frequently violate the nominal level. Here, we examine a mid-p version of the {McNemar} exact conditional test for the analysis of paired binomial proportions.},
	pages = {91},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Fagerland, Morten W. and Lydersen, Stian and Laake, Petter},
	urldate = {2023-04-11},
	date = {2013-07-13},
	keywords = {Dependent proportions, Matched pairs, Paired proportions, Quasi-exact},
	file = {Full Text PDF:/Users/tom/Zotero/storage/UNN497CB/Fagerland et al. - 2013 - The McNemar test for binary matched-pairs data mi.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/Z4JY8MRS/1471-2288-13-91.html:text/html},
}

@article{lenth_practical_2001,
	title = {Some practical guidelines for effective sample size determination},
	volume = {55},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313001317098149},
	doi = {10.1198/000313001317098149},
	abstract = {Sample size determination is often an important step in planning a statistical study—and it is usually a difficult one. Among the important hurdles to be surpassed, one must obtain an estimate of one or more error variances and specify an effect size of importance. There is the temptation to take some shortcuts. This article offers some suggestions for successful and meaningful sample size determination. Also discussed is the possibility that sample size may not be the main issue, that the real goal is to design a high-quality study. Finally, criticism is made of some ill-advised shortcuts relating to power and sample size.},
	pages = {187--193},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Lenth, Russell V},
	urldate = {2023-04-11},
	date = {2001-08-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/000313001317098149},
	keywords = {Study design, Cohen's effect measures, Equivalence testing, Observed power, Power, Retrospective power},
	file = {Lenth - 2001 - Some Practical Guidelines for Effective Sample Siz.pdf:/Users/tom/Zotero/storage/QHHWHVGC/Lenth - 2001 - Some Practical Guidelines for Effective Sample Siz.pdf:application/pdf},
}

@book{wilke_fundamentals_2019,
	location = {Sebastopol, {CA}},
	edition = {First edition},
	title = {Fundamentals of data visualization: a primer on making informative and compelling figures},
	isbn = {978-1-4920-3108-6},
	shorttitle = {Fundamentals of data visualization},
	abstract = {"Effective visualization is the best way to communicate information from the increasingly large and complex datasets in the natural and social sciences. But with the increasing power of visualization software today, scientists, engineers, and business analysts often have to navigate a bewildering array of visualization choices and options. This practical book takes you through many commonly encountered visualization problems, and it provides guidelines on how to turn large datasets into clear and compelling figures. What visualization type is best for the story you want to tell? How do you make informative figures that are visually pleasing? Author Claus O. Wilke teaches you the elements most critical to successful data visualization."--Provided by publisher},
	pagetotal = {370},
	publisher = {O'Reilly Media},
	author = {Wilke, C.},
	date = {2019},
	note = {{OCLC}: on1033902406},
	keywords = {Data processing, Data Visualization, Datenverarbeitung, Handbooks and manuals, Information visualization, Informationsgrafik, Visual analytics, Visualisierung, Visualization},
}

@misc{rodd_moving_2023,
	title = {Moving Experimental Psychology Online: How to Maintain Data Quality When We Can’t See Our Participants},
	url = {https://psyarxiv.com/2fhcb/},
	doi = {10.31234/osf.io/2fhcb},
	shorttitle = {Moving Experimental Psychology Online},
	abstract = {The past 10 years have seen rapid growth of online (web-based) data collection across the behavioural sciences. Many researchers have well-founded concerns about the lack of experimental control that arises when research moves outside of laboratory conditions. This paper provides a broad perspective on how to safeguard data quality in online experiments. I caution researchers against viewing online experiments as being necessarily cheaper, easier or more efficient than in-person testing. Ensuring appropriate data quality for online experiments requires significant effort prior to data collection to maintain the credibility of our rapidly expanding evidence base. I present a set of recommendations for researchers based on three key aspects of online data collection: technology, recruitment, and participant performance. With such safeguards in place online experiments will continue to provide important, paradigm-changing opportunities.},
	publisher = {{PsyArXiv}},
	author = {Rodd, Jennifer M.},
	urldate = {2023-04-14},
	date = {2023-04-13},
	langid = {english},
	keywords = {preregistration, Social and Behavioral Sciences, Cognitive Psychology, mturk, Online Experiments, web-based experiments},
}

@article{sigurdson_homeopathy_2023,
	title = {Homeopathy can offer empirical insights on treatment effects in a null field},
	volume = {155},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(23)00010-0/fulltext},
	doi = {10.1016/j.jclinepi.2023.01.010},
	pages = {64--72},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Sigurdson, Matthew K. and Sainani, Kristin L. and Ioannidis, John P. A.},
	urldate = {2023-04-16},
	date = {2023-03-01},
	pmid = {36736709},
	keywords = {Bias, Research integrity, Replication crisis, Meta-Research, Homeopathy, Null field, Treatment effects},
	file = {Sigurdson et al. - 2023 - Homeopathy can offer empirical insights on treatme.pdf:/Users/tom/Zotero/storage/BNH6PQZT/Sigurdson et al. - 2023 - Homeopathy can offer empirical insights on treatme.pdf:application/pdf},
}

@article{orben_crud_2020,
	title = {Crud (Re)Defined},
	volume = {3},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245920917961},
	doi = {10.1177/2515245920917961},
	abstract = {The idea that in behavioral research everything correlates with everything else was a niche area of the scientific literature for more than half a century. With the increasing availability of large data sets in psychology, the ?crud? factor has, however, become more relevant than ever before. When referenced in empirical work, it is often used by researchers to discount minute?but statistically significant?effects that are deemed too small to be considered meaningful. This review tracks the history of the crud factor and examines how its use in the psychological- and behavioral-science literature has developed to this day. We highlight a common and deep-seated lack of understanding about what the crud factor is and discuss whether it can be proven to exist or estimated and how it should be interpreted. This lack of understanding makes the crud factor a convenient tool for psychologists to use to disregard unwanted results, even though the presence of a crud factor should be a large inconvenience for the discipline. To inspire a concerted effort to take the crud factor more seriously, we clarify the definitions of important concepts, highlight current pitfalls, and pose questions that need to be addressed to ultimately improve understanding of the crud factor. Such work will be necessary to develop the crud factor into a useful concept encouraging improved psychological research.},
	pages = {238--247},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Orben, Amy and Lakens, Daniël},
	urldate = {2023-04-17},
	date = {2020-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Orben and Lakens - 2020 - Crud (Re)Defined.pdf:/Users/tom/Zotero/storage/ITQ9I8T8/Orben and Lakens - 2020 - Crud (Re)Defined.pdf:application/pdf},
}

@article{cardena_experimental_2018,
	title = {The experimental evidence for parapsychological phenomena: A review},
	volume = {73},
	issn = {1935-990X},
	doi = {10.1037/amp0000236},
	shorttitle = {The experimental evidence for parapsychological phenomena},
	abstract = {This article presents a comprehensive integration of current experimental evidence and theories about so-called parapsychological (psi) phenomena. Throughout history, people have reported events that seem to violate the common sense view of space and time. Some psychologists have been at the forefront of investigating these phenomena with sophisticated research protocols and theory, while others have devoted much of their careers to criticizing the field. Both stances can be explained by psychologists' expertise on relevant processes such as perception, memory, belief, and conscious and nonconscious processes. This article clarifies the domain of psi, summarizes recent theories from physics and psychology that present psi phenomena as at least plausible, and then provides an overview of recent/updated meta-analyses. The evidence provides cumulative support for the reality of psi, which cannot be readily explained away by the quality of the studies, fraud, selective reporting, experimental or analytical incompetence, or other frequent criticisms. The evidence for psi is comparable to that for established phenomena in psychology and other disciplines, although there is no consensual understanding of them. The article concludes with recommendations for further progress in the field including the use of project and data repositories, conducting multidisciplinary studies with enough power, developing further nonconscious measures of psi and falsifiable theories, analyzing the characteristics of successful sessions and participants, improving the ecological validity of studies, testing how to increase effect sizes, recruiting more researchers at least open to the possibility of psi, and situating psi phenomena within larger domains such as the study of consciousness. ({PsycINFO} Database Record},
	pages = {663--677},
	number = {5},
	journaltitle = {The American Psychologist},
	shortjournal = {Am Psychol},
	author = {Cardeña, Etzel},
	date = {2018},
	pmid = {29792448},
	keywords = {Humans, Research, Parapsychology},
	file = {Cardeña - 2018 - The experimental evidence for parapsychological ph.pdf:/Users/tom/Zotero/storage/EZFUJLS9/Cardeña - 2018 - The experimental evidence for parapsychological ph.pdf:application/pdf},
}

@article{bem_does_1994,
	title = {Does psi exist? Replicable evidence for an anomalous process of information transfer},
	volume = {115},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.115.1.4},
	shorttitle = {Does psi exist?},
	abstract = {Most academic psychologists do not yet accept the existence of psi, anomalous processes of information or energy transfer (e.g., telepathy or other forms of extrasensory perception) that are currently unexplained in terms of known physical or biological mechanisms. It is believed that the replication rates and effect sizes achieved by 1 particular experimental method, the ganzfeld procedure, are now sufficient to warrant bringing this body of data to the attention of the wider psychological community. Competing meta-analysis of the ganzfeld database are reviewed, one by R. Hyman (see record 1986-05166-001), a skeptical critic of psi research, and the other by C. Honorton (see record 1986-05165-001), a parapsychologist and major contributor to the ganzfeld database. Next the results of 11 new ganzfeld studies that comply with guidelines jointly authored by R. Hyman and C. Honorton (see record 1987-12537-001) are summarized. Issues of replication and theoretical explanation are discussed. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {4--18},
	journaltitle = {Psychological Bulletin},
	author = {Bem, Daryl J. and Honorton, Charles},
	date = {1994},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Parapsychological Phenomena},
	file = {Does psi exist? Replicable evidence for an anomalous process of information transfer:/Users/tom/Zotero/storage/2ZZ5JMXA/bem1994.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/PWHF7AMF/1994-20287-001.html:text/html},
}

@article{milton_does_1999,
	title = {Does psi exist? Lack of replication of an anomalous process of information transfer},
	volume = {125},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.125.4.387},
	shorttitle = {Does psi exist?},
	abstract = {D. J. Bern and C. Honorton (1994) recently presented in this journal a set of ganzfeld extrasensory perception ({ESP}) experiments conducted by C. Honorton that appeared to support the existence of a communication anomaly. In this article, the authors present a meta analysis of 30 ganzfeld {ESP} studies from 7 independent laboratories adhering to the same stringent methodological guidelines that C. Honorton followed. The studies failed to confirm his main effect of participants scoring above change on the {ESP} task, Souffer z = 0.70, p = .24, one-tailed; M effect size (z/N1/2) = 0.013, {SD} = 0.23. The new studies included replication attempts of 3 out of 5 internal effects reported as statistically significant by D. J. Bern and C. Honorton. Only 1 was confirmed, and the authors found the D. J. Bern and C. Honorton were mistaken in describing the original effect as being statistically significant. The authors conclude that the ganzfeld technique does not at present offer a replicable method for producing {ESP} in the laboratory. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {387--391},
	journaltitle = {Psychological Bulletin},
	author = {Milton, Julie and Wiseman, Richard},
	date = {1999},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Methodology, Communication, Parapsychological Phenomena, Extrasensory Perception},
	file = {Does psi exist? Lack of replication of an anomalous process of information transfer:/Users/tom/Zotero/storage/AN4H5H6Z/milton1999.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/QLI3UNFU/1999-05876-001.html:text/html},
}

@article{storm_does_2001,
	title = {Does psi exist? Comments on Milton and Wiseman's (1999) meta-analysis of Ganzfield research},
	volume = {127},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.127.3.424},
	shorttitle = {Does psi exist?},
	abstract = {J. Milton and R. Wiseman (1999) attempted to replicate D. Bern and C. Honorton's (1994) meta-analysis, which yielded evidence that the ganzfeld is a suitable method for demonstrating anomalous communication. Using a database of 30 ganzfeld and autoganzfeld studies, Milton and Wiseman's meta-analysis yielded an effect size ({ES}) of only 0.013 (Stouffer Z = 0.70, p = .24, one-tailed). Thus they failed to replicate Bem and Honorton's finding ({ES} = 0.162, Stouffer Z = 2.52, p = 5.90 × 10-3, one-tailed). The authors conducted stepwise performance comparisons between all available databases of ganzfeld research. Larger aggregates of such studies were formed, including a database comprising 79 ganzfeld-autoganzfeld studies ({ES} = 0.138, Stouffer Z = 5.66, p = 7.78 × 10-9). Thus Bern and Honorton's positive conclusion was confirmed. More accurate population parameters for the ganzfeld and autoganzfeld domains were calculated. Significant bidirectional psi effects were also found in all databases. The ganzfeld appears to be a replicable technique for producing psi effects in the laboratory. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {424--433},
	journaltitle = {Psychological Bulletin},
	author = {Storm, Lance and Ertel, Suitbert},
	date = {2001},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Methodology, Communication, Parapsychological Phenomena, Extrasensory Perception},
	file = {Does psi exist? Comments on Milton and Wiseman's (1999) meta-analysis of Ganzfield research:/Users/tom/Zotero/storage/FBJVJZ5K/storm2001.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/VKS6BGNE/doiLanding.html:text/html},
}

@article{storm_meta-analysis_2010,
	title = {A meta-analysis with nothing to hide: Reply to Hyman (2010)},
	volume = {136},
	issn = {1939-1455},
	doi = {10.1037/a0019840},
	shorttitle = {A meta-analysis with nothing to hide},
	abstract = {In our article (Storm, Tressoldi, \& Di Risio, 2010), we claimed that the ganzfeld experimental design has proved to be consistent and reliable. However, Hyman (2010) argues that the overall evidence for psi is, in fact, contradictory and elusive. We present a case for psi research that undermines Hyman's argument. First, we give examples from parapsychologists who do not outrightly dismiss psi, despite appearances, but actually support it. Second, we claim that Hyman does not tell the full story about the ganzfeld meta-analytic findings and thus presents a one-sided account. Third, we argue that our meta-analysis has followed standard procedures, that we have not broken any rules but have found a communications anomaly, often referred to as psi. Though we may be in agreement that the evidence is largely statistical, the evidence suggests that concealed targets are actually identified rather than guessed. We argue that further research is necessary. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {491--494},
	journaltitle = {Psychological Bulletin},
	author = {Storm, Lance and Tressoldi, Patrizio E. and Risio, Lorenzo Di},
	date = {2010},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Meta Analysis, Parapsychology, Extrasensory Perception, Auditory Stimulation, Free Association, Models},
	file = {A meta-analysis with nothing to hide\: Reply to Hyman (2010):/Users/tom/Zotero/storage/KY89BPKK/storm2010.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/N2WBYTPE/doiLanding.html:text/html},
}

@online{quote_investigator_quote_2014,
	title = {Quote Investigator},
	url = {https://quoteinvestigator.com/2014/04/13/open-mind/},
	titleaddon = {Do not be so open-minded that your brains fall out},
	author = {Quote Investigator},
	urldate = {2023-04-21},
	date = {2014-04-13},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/6V3Y8ABI/open-mind.html:text/html},
}

@incollection{cautin_science_2015,
	location = {Hoboken, {NJ}, {USA}},
	title = {Science Versus Pseudoscience},
	isbn = {978-1-118-62539-2},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118625392.wbecp572},
	pages = {1--7},
	booktitle = {The Encyclopedia of Clinical Psychology},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Lilienfeld, Scott O. and Lynn, Steven Jay and Ammirati, Rachel J.},
	editor = {Cautin, Robin L. and Lilienfeld, Scott O.},
	urldate = {2023-04-21},
	date = {2015-01-23},
	langid = {english},
	doi = {10.1002/9781118625392.wbecp572},
	file = {Lilienfeld et al. - 2015 - Science Versus Pseudoscience.pdf:/Users/tom/Zotero/storage/ZTAFC5BY/Lilienfeld et al. - 2015 - Science Versus Pseudoscience.pdf:application/pdf},
}

@book{sagan_demon-haunted_1997,
	location = {New York, {NY}},
	edition = {1. Ballantine Books ed},
	title = {The demon-haunted world: science as a candle in the dark},
	isbn = {978-0-345-40946-1},
	series = {The New York Times bestseller},
	shorttitle = {The demon-haunted world},
	pagetotal = {457},
	publisher = {Ballantine Books},
	author = {Sagan, Carl},
	date = {1997},
	file = {Sagan - 1997 - The demon-haunted world science as a candle in th.pdf:/Users/tom/Zotero/storage/EP82BD97/Sagan - 1997 - The demon-haunted world science as a candle in th.pdf:application/pdf},
}

@online{lilienfeld_10_nodate,
	title = {The 10 Commandments of Helping Students Distinguish Science from Pseudoscience in Psychology},
	url = {https://www.psychologicalscience.org/observer/the-10-commandments-of-helping-students-distinguish-science-from-pseudoscience-in-psychology},
	titleaddon = {{APS} Observer},
	author = {Lilienfeld, Scott O},
	urldate = {2023-04-21},
	file = {reCAPTCHA:/Users/tom/Zotero/storage/FAML68Y6/the-10-commandments-of-helping-students-distinguish-science-from-pseudoscience-in-psychology.html:text/html},
}

@collection{pigliucci_philosophy_2013,
	location = {Chicago},
	title = {Philosophy of pseudoscience: reconsidering the demarcation problem},
	isbn = {978-0-226-05179-6 978-0-226-05196-3},
	shorttitle = {Philosophy of pseudoscience},
	pagetotal = {469},
	publisher = {The University of Chicago Press},
	editor = {Pigliucci, Massimo and Boudry, Maarten},
	date = {2013},
	langid = {english},
	keywords = {Science, Pseudoscience},
	file = {Pigliucci and Boudry - 2013 - Philosophy of pseudoscience reconsidering the dem.pdf:/Users/tom/Zotero/storage/T47F8XM2/Pigliucci and Boudry - 2013 - Philosophy of pseudoscience reconsidering the dem.pdf:application/pdf},
}

@article{maddox_when_1988,
	title = {When to believe the unbelievable},
	volume = {333},
	rights = {1988 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/333787a0},
	doi = {10.1038/333787a0},
	abstract = {An article in this week's issue describes observations for which there is no present physical basis. There are good and particular reasons why prudent people should, for the time being, suspend judgement.},
	pages = {787--787},
	number = {6176},
	journaltitle = {Nature},
	author = {Maddox, John},
	urldate = {2023-04-24},
	date = {1988-06},
	langid = {english},
	note = {Number: 6176
Publisher: Nature Publishing Group},
	keywords = {Science, Humanities and Social Sciences, multidisciplinary},
	file = {Full Text PDF:/Users/tom/Zotero/storage/LJZIIVQR/1988 - When to believe the unbelievable.pdf:application/pdf;When to believe the unbelievable:/Users/tom/Zotero/storage/3GHVP8X5/when-to-believe-the-unbelievable-1988.pdf.pdf:application/pdf},
}

@article{chalmers_how_2014,
	title = {How to increase value and reduce waste when research priorities are set},
	volume = {383},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673613622291},
	doi = {10.1016/S0140-6736(13)62229-1},
	pages = {156--165},
	number = {9912},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Chalmers, Iain and Bracken, Michael B and Djulbegovic, Ben and Garattini, Silvio and Grant, Jonathan and Gülmezoglu, A Metin and Howells, David W and Ioannidis, John P A and Oliver, Sandy},
	urldate = {2023-04-24},
	date = {2014-01},
	langid = {english},
	file = {How to increase value and reduce waste when research priorities are set:/Users/tom/Zotero/storage/E9CXR4T4/chalmers2014.pdf.pdf:application/pdf},
}

@article{rubin_questionable_2023,
	title = {Questionable Metascience Practices},
	url = {https://journal.trialanderror.org/pub/questionable-metascience-practices/release/2},
	doi = {10.36850/mr4},
	abstract = {Questionable research practices may reduce the public’s trust in science. The present article considers some questionable metascience practices ({QMPs}) that may threaten scientists’ trust in metascience. A {QMP} is a research practice, assumption, or perspective that has been questioned by several commentators as being potentially problematic for the credibility of metascience and/or the science reform movement. The present article reviews 10 {QMPs} that relate to criticism, replication, bias, generalization, and the characterization of science. Specifically, the following {QMPs} are considered: (1) rejecting or ignoring self-criticism; (2) a fast ‘n’ bropen scientific criticism style; (3) overplaying the role of replication in science; (4) assuming a replication rate is “too low” without specifying an “acceptable” rate; (5) an unacknowledged metabias towards explaining the replication crisis in terms of researcher bias; (6) assuming that researcher bias can be reduced; (7) devaluing exploratory results as more “tentative” than confirmatory results; (8) presuming that {QRPs} are problematic research practices; (9) focusing on knowledge accumulation as an index of scientific progress; and (10) focusing on specific scientific methods. It is stressed that only some metascientists engage in some {QMPs} some of the time, and that these {QMPs} may not always be problematic. Research is required to estimate the prevalence and impact of {QMPs}. In the meantime, {QMPs} should be viewed as invitations to ask “questions” about how we go about doing metascience rather than as grounds for mistrusting the credibility of metascience.},
	journaltitle = {Journal of Trial \& Error},
	author = {Rubin, Mark},
	urldate = {2023-04-25},
	date = {2023-04-24},
	langid = {english},
	note = {Publisher: {JOTE} Publishers},
	file = {Rubin - 2023 - Questionable Metascience Practices.pdf:/Users/tom/Zotero/storage/VLRMWW22/Rubin - 2023 - Questionable Metascience Practices.pdf:application/pdf},
}

@article{maul_rethinking_2017,
	title = {Rethinking Traditional Methods of Survey Validation},
	volume = {15},
	issn = {1536-6367},
	url = {https://doi.org/10.1080/15366367.2017.1348108},
	doi = {10.1080/15366367.2017.1348108},
	abstract = {It is commonly believed that self-report, survey-based instruments can be used to measure a wide range of psychological attributes, such as self-control, growth mindsets, and grit. Increasingly, such instruments are being used not only for basic research but also for supporting decisions regarding educational policy and accountability. The validity of such instruments is typically investigated using a classic set of methods, including the examination of reliability coefficients, factor or principal components analyses, and correlations between scores on the instrument and other variables. However, these techniques may fall short of providing the kinds of rigorous, potentially falsifying tests of relevant hypotheses commonly expected in scientific research. This point is illustrated via a series of studies in which respondents were presented with survey items deliberately constructed to be uninterpretable, but the application of the aforementioned validation procedures nonetheless returned favorable-appearing results. In part, this disconnect may be traceable to the way in which operationalist modes of thinking in the social sciences have reinforced the perception that attributes do not need to be defined independently of particular sets of testing operations. It is argued that affairs might be improved via greater attention to the manner in which definitions of psychological attributes are articulated and greater openness to treating beliefs about the existence and measurability of psychological attributes as hypotheses rather than assumptions—in other words, as beliefs potentially subject to revision.},
	pages = {51--69},
	number = {2},
	journaltitle = {Measurement: Interdisciplinary Research and Perspectives},
	author = {Maul, Andrew},
	urldate = {2023-04-30},
	date = {2017-04-03},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15366367.2017.1348108},
	keywords = {survey research, validity, validation, noncognitive measurement, philosophy of measurement},
	file = {Rethinking Traditional Methods of Survey Validation:/Users/tom/Zotero/storage/2WB32A63/maul2017.pdf.pdf:application/pdf},
}

@article{vazire_opening_2021,
	title = {Opening the Black Box of Peer Review},
	volume = {14},
	rights = {©2021 by the American Physical Society. All rights reserved.},
	url = {https://physics.aps.org/articles/v14/169},
	abstract = {More transparency in the peer review process will help researchers to study peer review and improve its quality and fairness.},
	pages = {169},
	journaltitle = {Physics},
	author = {Vazire, Simine},
	urldate = {2023-05-01},
	date = {2021-12-02},
	langid = {english},
	note = {Publisher: American Physical Society},
	file = {Snapshot:/Users/tom/Zotero/storage/6YLI4SLS/169.html:text/html},
}

@report{ross-hellauer_what_2017,
	title = {What is open peer review? A systematic review},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	url = {https://f1000research.com/articles/6-588},
	shorttitle = {What is open peer review?},
	abstract = {Background : “Open peer review” ({OPR}), despite being a major pillar of Open Science, has neither a standardized definition nor an agreed schema of its features and implementations. The literature reflects this, with numerous overlapping and contradictory definitions. While for some the term refers to peer review where the identities of both author and reviewer are disclosed to each other, for others it signifies systems where reviewer reports are published alongside articles. For others it signifies both of these conditions, and for yet others it describes systems where not only “invited experts” are able to comment. For still others, it includes a variety of combinations of these and other novel methods. Methods : Recognising the absence of a consensus view on what open peer review is, this article undertakes a systematic review of definitions of “open peer review” or “open review”, to create a corpus of 122 definitions. These definitions are systematically analysed to build a coherent typology of the various innovations in peer review signified by the term, and hence provide the precise technical definition currently lacking. Results : This quantifiable data yields rich information on the range and extent of differing definitions over time and by broad subject area. Quantifying definitions in this way allows us to accurately portray exactly how ambiguously the phrase “open peer review” has been used thus far, for the literature offers 22 distinct configurations of seven traits, effectively meaning that there are 22 different definitions of {OPR} in the literature reviewed. Conclusions : I propose a pragmatic definition of open peer review as an umbrella term for a number of overlapping ways that peer review models can be adapted in line with the aims of Open Science, including making reviewer and author identities open, publishing review reports and enabling greater participation in the peer review process.},
	number = {6:588},
	institution = {F1000Research},
	author = {Ross-Hellauer, Tony},
	urldate = {2023-05-01},
	date = {2017-08-31},
	langid = {english},
	doi = {10.12688/f1000research.11369.2},
	note = {Type: article},
	keywords = {Open Science, publishing, scholarly communication, open peer review, research evaluation},
	file = {Full Text PDF:/Users/tom/Zotero/storage/RYQIXW8S/Ross-Hellauer - 2017 - What is open peer review A systematic review.pdf:application/pdf},
}

@article{wolfram_open_2020,
	title = {Open peer review: promoting transparency in open science},
	volume = {125},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-020-03488-4},
	doi = {10.1007/s11192-020-03488-4},
	shorttitle = {Open peer review},
	abstract = {Open peer review ({OPR}), where review reports and reviewers’ identities are published alongside the articles, represents one of the last aspects of the open science movement to be widely embraced, although its adoption has been growing since the turn of the century. This study provides the first comprehensive investigation of {OPR} adoption, its early adopters and the implementation approaches used. Current bibliographic databases do not systematically index {OPR} journals, nor do the {OPR} journals clearly state their policies on open identities and open reports. Using various methods, we identified 617 {OPR} journals that published at least one article with open identities or open reports as of 2019 and analyzed their wide-ranging implementations to derive emerging {OPR} practices. The findings suggest that: (1) there has been a steady growth in {OPR} adoption since 2001, when 38 journals initially adopted {OPR}, with more rapid growth since 2017; (2) {OPR} adoption is most prevalent in medical and scientific disciplines (79.9\%); (3) five publishers are responsible for 81\% of the identified {OPR} journals; (4) early adopter publishers have implemented {OPR} in different ways, resulting in different levels of transparency. Across the variations in {OPR} implementations, two important factors define the degree of transparency: open identities and open reports. Open identities may include reviewer names and affiliation as well as credentials; open reports may include timestamped review histories consisting of referee reports and author rebuttals or a letter from the editor integrating reviewers’ comments. When and where open reports can be accessed are also important factors indicating the {OPR} transparency level. Publishers of optional {OPR} journals should add metric data in their annual status reports.},
	pages = {1033--1051},
	number = {2},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Wolfram, Dietmar and Wang, Peiling and Hembree, Adam and Park, Hyoungjoo},
	urldate = {2023-05-01},
	date = {2020-11-01},
	langid = {english},
	keywords = {62P99, C80, Journal editorial policies, Open access, Open peer review, Peer review transparency, Scholarly communication, Transparent review models},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MXKEAUPU/Wolfram et al. - 2020 - Open peer review promoting transparency in open s.pdf:application/pdf;Open peer review\: promoting transparency in open science:/Users/tom/Zotero/storage/73CIH7N3/wolfram2020.pdf.pdf:application/pdf},
}

@article{bem_feeling_2011,
	title = {Feeling the future: experimental evidence for anomalous retroactive influences on cognition and affect},
	volume = {100},
	issn = {1939-1315},
	doi = {10.1037/a0021524},
	shorttitle = {Feeling the future},
	abstract = {The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by "time-reversing" well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.},
	pages = {407--425},
	number = {3},
	journaltitle = {Journal of Personality and Social Psychology},
	shortjournal = {J Pers Soc Psychol},
	author = {Bem, Daryl J.},
	date = {2011-03},
	pmid = {21280961},
	keywords = {Humans, Female, Male, Cognition, Affect, Erotica, Awareness, Parapsychology, Boredom, Escape Reaction, Habituation, Psychophysiologic, Mental Recall, Subliminal Stimulation, Time Factors},
	file = {Feeling the future\: experimental evidence for anomalous retroactive influences on cognition and affect:/Users/tom/Zotero/storage/XFIR32I6/bem2011.pdf.pdf:application/pdf},
}

@online{noauthor_elife_2023,
	title = {{eLife} review process {FAQs}},
	url = {https://elife-cdn.s3.amazonaws.com/documents/eLife_review_process_FAQs.pdf},
	urldate = {2023-05-02},
	date = {2023-05-02},
	file = {eLife_review_process_FAQs.pdf:/Users/tom/Zotero/storage/ZHF3699A/eLife_review_process_FAQs.pdf:application/pdf},
}

@article{galak_correcting_2012,
	title = {Correcting the past: failures to replicate ψ},
	volume = {103},
	issn = {1939-1315},
	doi = {10.1037/a0029709},
	shorttitle = {Correcting the past},
	abstract = {Across 7 experiments (N = 3,289), we replicate the procedure of Experiments 8 and 9 from Bem (2011), which had originally demonstrated retroactive facilitation of recall. We failed to replicate that finding. We further conduct a meta-analysis of all replication attempts of these experiments and find that the average effect size (d = 0.04) is no different from 0. We discuss some reasons for differences between the results in this article and those presented in Bem (2011).},
	pages = {933--948},
	number = {6},
	journaltitle = {Journal of Personality and Social Psychology},
	shortjournal = {J Pers Soc Psychol},
	author = {Galak, Jeff and {LeBoeuf}, Robyn A. and Nelson, Leif D. and Simmons, Joseph P.},
	date = {2012-12},
	pmid = {22924750},
	keywords = {Humans, Adult, Female, Male, Cognition, Perception, Young Adult, Parapsychology, Mental Recall, Time Factors, Anticipation, Psychological, Neuropsychological Tests, Psycholinguistics},
	file = {Correcting the past\: failures to replicate ψ:/Users/tom/Zotero/storage/6ANX8RC4/galak2012.pdf.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/2DLVFWIE/Galak et al. - 2012 - Correcting the past failures to replicate ψ.pdf:application/pdf},
}

@online{wiseman_heads_2010,
	title = {'Heads I Win, Tails You Lose': How Parapsychologists Nullify Null Results {\textbar} Skeptical Inquirer},
	url = {https://skepticalinquirer.org/2010/01/heads-i-win-tails-you-lose-how-parapsychologists-nullify-null-results/},
	shorttitle = {'Heads I Win, Tails You Lose'},
	author = {Wiseman, Richard},
	urldate = {2023-05-02},
	date = {2010-01-01},
	langid = {american},
	file = {2010 - 'Heads I Win, Tails You Lose' How Parapsychologis.pdf:/Users/tom/Zotero/storage/N5GNVE5H/2010 - 'Heads I Win, Tails You Lose' How Parapsychologis.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/7KEWNMEQ/heads-i-win-tails-you-lose-how-parapsychologists-nullify-null-results.html:text/html},
}

@article{schooler_entertaining_2018,
	title = {Entertaining without endorsing: The case for the scientific investigation of anomalous cognition.},
	volume = {5},
	issn = {2326-5531, 2326-5523},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/cns0000151},
	doi = {10.1037/cns0000151},
	shorttitle = {Entertaining without endorsing},
	abstract = {Empirical reports in mainstream journals that human cognition extends in ways that challenge the current boundaries of science (anomalous cognition) has been viewed with dismay by many who see it as evidence that science is broken. Here the authors make the case for the value of conducting and publishing well-designed studies investigating anomalous cognition. They distinguish between the criteria that justify entertaining the possibility of anomalous cognition from those required to endorse it as a bona ﬁde phenomenon. In evaluating these 2 distinct thresholds, the authors draw on Bayes’s theorem to argue that scientists may reasonably differ in their appraisals of the likelihood that anomalous cognition is possible. Although individual scientists may usefully vary in the criteria that they hold both for entertaining and endorsing anomalous cognition, we provide arguments for why researchers should consider adopting a liberal criterion for entertaining anomalous cognition while maintaining a very strict criterion for the outright endorsement of its existence. Grounded in an understanding of the justiﬁability of disparate views on the topic, the authors encourage humility on both the part of those who present evidence in support of anomalous cognition and those who dispute the merit of its investigation.},
	pages = {63--77},
	number = {1},
	journaltitle = {Psychology of Consciousness: Theory, Research, and Practice},
	shortjournal = {Psychology of Consciousness: Theory, Research, and Practice},
	author = {Schooler, Jonathan W. and Baumgart, Stephen and Franklin, Michael},
	urldate = {2023-05-04},
	date = {2018-03},
	langid = {english},
	file = {Schooler et al. - 2018 - Entertaining without endorsing The case for the s.pdf:/Users/tom/Zotero/storage/79R6PJH4/Schooler et al. - 2018 - Entertaining without endorsing The case for the s.pdf:application/pdf},
}

@online{noauthor_it_nodate,
	title = {It matters who does science},
	url = {https://www.science.org/content/blog-post/it-matters-who-does-science},
	urldate = {2023-05-14},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/3EKGIWM4/it-matters-who-does-science.html:text/html},
}

@article{clotworthy_saving_2023,
	title = {Saving time and money in biomedical publishing: the case for free-format submissions with minimal requirements},
	volume = {21},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-023-02882-y},
	doi = {10.1186/s12916-023-02882-y},
	shorttitle = {Saving time and money in biomedical publishing},
	abstract = {Manuscript preparation and the (re)submission of articles can create a significant workload in academic jobs. In this exploratory analysis, we estimate the time and costs needed to meet the diverse formatting requirements for manuscript submissions in biomedical publishing.},
	pages = {172},
	number = {1},
	journaltitle = {{BMC} Medicine},
	shortjournal = {{BMC} Medicine},
	author = {Clotworthy, Amy and Davies, Megan and Cadman, Timothy J. and Bengtsson, Jessica and Andersen, Thea O. and Kadawathagedara, Manik and Vinther, Johan L. and Nguyen, Tri-Long and Varga, Tibor V.},
	urldate = {2023-05-21},
	date = {2023-05-10},
	keywords = {Publishing, Policy, Formatting, Journal article, Submission guidelines},
	file = {Full Text PDF:/Users/tom/Zotero/storage/I6JXIHLH/Clotworthy et al. - 2023 - Saving time and money in biomedical publishing th.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ERAV9YVR/s12916-023-02882-y.html:text/html},
}

@article{van_doorn_strong_2021,
	title = {Strong public claims may not reflect researchers' private convictions},
	volume = {18},
	issn = {1740-9713},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1740-9713.01493},
	doi = {10.1111/1740-9713.01493},
	abstract = {A survey indicates that some researchers are more modest about their findings than their published articles would suggest. Johnny van Doorn, Eric-Jan Wagenmakers and colleagues argue that authors should express this uncertainty openly},
	pages = {44--45},
	number = {1},
	journaltitle = {Significance},
	author = {van Doorn, Johnny and Wagenmakers, Eric-Jan},
	urldate = {2023-05-25},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1740-9713.01493},
	file = {Full Text PDF:/Users/tom/Zotero/storage/B5J7PBFR/van Doorn and Wagenmakers - 2021 - Strong public claims may not reflect researchers' .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/F7TVDDBA/1740-9713.html:text/html;Strong public claims may not reflect researchers' private convictions:/Users/tom/Zotero/storage/TKRLQZIL/doorn2021.pdf.pdf:application/pdf},
}

@article{naaman_exploring_2023,
	title = {Exploring enablers and barriers to implementing the Transparency and Openness Promotion Guidelines: a theory-based survey of journal editors},
	volume = {10},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.221093},
	doi = {10.1098/rsos.221093},
	shorttitle = {Exploring enablers and barriers to implementing the Transparency and Openness Promotion Guidelines},
	abstract = {The Transparency and Openness Promotion ({TOP}) Guidelines provide a framework to help journals develop open science policies. Theories of behaviour change can guide understanding of why journals do (not) implement open science policies and the development of interventions to improve these policies. In this study, we used the Theoretical Domains Framework to survey 88 journal editors on their capability, opportunity and motivation to implement {TOP}. Likert-scale questions assessed editor support for {TOP}, and enablers and barriers to implementing {TOP}. A qualitative question asked editors to provide reflections on their ratings. Most participating editors supported adopting {TOP} at their journal (71\%) and perceived other editors in their discipline to support adopting {TOP} (57\%). Most editors (93\%) agreed their roles include maintaining policies that reflect current best practices. However, most editors (74\%) did not see implementing {TOP} as a high priority compared with other editorial responsibilities. Qualitative responses expressed structural barriers to implementing {TOP} (e.g. lack of time, resources and authority to implement changes) and varying support for {TOP} depending on study type, open science standard, and level of implementation. We discuss how these findings could inform the development of theoretically guided interventions to increase open science policies, procedures and practices.},
	pages = {221093},
	number = {2},
	journaltitle = {Royal Society Open Science},
	author = {Naaman, Kevin and Grant, Sean and Kianersi, Sina and Supplee, Lauren and Henschel, Beate and Mayo-Wilson, Evan},
	urldate = {2023-05-26},
	date = {2023-02},
	keywords = {open science, transparency, reproducibility, behaviour change, journal editors, Top Guidelines},
}

@article{anthony_published_2023,
	title = {Published registered reports are rare, limited to one journal group, and inadequate for randomized controlled trials in the clinical field},
	volume = {0},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(23)00129-4/abstract},
	doi = {10.1016/j.jclinepi.2023.05.016},
	abstract = {{\textless}h2{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater}{\textless}p{\textgreater}Registered reports ({RR}) is a publication format implying a peer-review of the protocol before the start of the study, followed by an in-principle acceptance ({IPA}) by the journal before the study starts. We aimed to describe randomized controlled trials ({RCTs}) in the clinical field published as {RR}.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Study design and setting{\textless}/h3{\textgreater}{\textless}p{\textgreater}This cross-sectional study included {RR} results for {RCTs}, identified on {PubMed}/Medline and on a list compiled by the Center for Open Science. It explored the proportion of reports that received {IPA} (and/or published a protocol before inclusion of the first patient) and changes in the primary outcome.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater}{\textless}p{\textgreater}A total of 93 {RCTs} publications identified as {RR} were included. All but one were published in the same journal group. The date of the {IPA} was never documented. For most of these reports (79/93, 84.9 \%) a protocol was published after the date of inclusion of the first patient. A change in the primary outcome was noted in 40/93 (44\%) of them. Thirteen out of the 40 (33\%) mentioned this change.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions{\textless}/h3{\textgreater}{\textless}p{\textgreater}{RCTs} in the clinical field identified as {RR} were rare, originated from a single journal group and did not comply with the basic features of this format.{\textless}/p{\textgreater}},
	number = {0},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Anthony, Norah and Tisseaux, Antoine and Naudet, Florian},
	urldate = {2023-05-29},
	date = {2023-05-26},
	pmid = {37245701},
	note = {Publisher: Elsevier},
}

@misc{decker_preregistration_2023,
	title = {Preregistration and Credibility of Clinical Trials*},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.medrxiv.org/content/10.1101/2023.05.22.23290326v1},
	doi = {10.1101/2023.05.22.23290326},
	abstract = {Preregistration at public research registries is considered a promising solution to the credibility crisis in science, but empirical evidence of its actual benefit is limited. Guaranteeing research integrity is especially vital in clinical research, where human lives are at stake and investigators might suffer from financial pressure. This paper analyzes the distribution of p-values from pre-approval drug trials reported to {ClinicalTrials}.gov, the largest registry for research studies in human volunteers, conditional on the preregistration status. The z-score density of non-preregistered trials displays a significant upward discontinuity at the salient 5\% threshold for statistical significance, indicative of p-hacking or selective reporting. The density of preregistered trials appears smooth at this threshold. With caliper tests, we establish that these differences between preregistered and non-preregistered trials are robust when conditioning on sponsor fixed effects and other design features commonly indicative of research integrity, such as blinding and data monitoring committees. Our results suggest that preregistration is a credible signal for the integrity of clinical trials, as far as it can be assessed with the currently available methods to detect p-hacking.},
	publisher = {{medRxiv}},
	author = {Decker, Christian and Ottaviani, Marco},
	urldate = {2023-05-30},
	date = {2023-05-23},
	langid = {english},
	note = {Pages: 2023.05.22.23290326},
	file = {Full Text PDF:/Users/tom/Zotero/storage/Z9X2L9XZ/Decker and Ottaviani - 2023 - Preregistration and Credibility of Clinical Trials.pdf:application/pdf},
}

@misc{heirene_preregistration_2021-1,
	title = {Preregistration specificity \& adherence: A review of preregistered gambling studies \& cross-disciplinary comparison},
	url = {https://psyarxiv.com/nj4es/},
	doi = {10.31234/osf.io/nj4es},
	shorttitle = {Preregistration specificity \& adherence},
	abstract = {Study preregistration is one of several “open science” practices (e.g., open data, preprints) that researchers use to improve the transparency and rigour of their research. As more researchers adopt preregistration as a regular research practice, examining the nature and content of preregistrations can help identify strengths and weaknesses of current practices. The value of preregistration, in part, relates to the specificity of the study plan and the extent to which investigators adhere to this plan. We identified 53 preregistrations from the gambling studies field meeting our predefined eligibility criteria and scored their level of specificity using a 23-item protocol developed to measure the extent to which a clear and exhaustive preregistration plan restricts various researcher degrees of freedom ({RDoF}; i.e., the many methodological choices available to researchers when collecting and analysing data, and when reporting their findings). We also scored studies on a 32-item protocol that measured adherence to the preregistered plan in the study manuscript. We found that gambling preregistrations had low specificity levels on most {RDoF}. However, a comparison with a sample of cross-disciplinary preregistrations (N = 52; Bakker et al., 2020) indicated that gambling preregistrations scored higher on 12 (of 29) items. Thirteen (65\%) of the 20 associated published articles or preprints deviated from the protocol without declaring as much (the mean number of undeclared deviations per article was 2.25, {SD} = 2.34). Overall, while we found improvements in specificity and adherence over time (2017-2020), our findings suggest the purported benefits of preregistration—including increasing transparency and reducing {RDoF}—are not fully achieved by current practices. Using our findings, we provide 10 practical recommendations that can be used to support and refine preregistration practices.},
	publisher = {{PsyArXiv}},
	author = {Heirene, Robert and {LaPlante}, Debi and Louderback, Eric R. and Keen, Brittany and Bakker, Marjan and Serafimovska, Anastasia and Gainsbury, Sally Melissa},
	urldate = {2023-06-02},
	date = {2021-07-16},
	langid = {english},
	keywords = {Meta-science, Clinical Psychology, Social and Behavioral Sciences, Open science, Preregistration, Gambling, Addiction},
	file = {Full Text PDF:/Users/tom/Zotero/storage/678EQHL3/Heirene et al. - 2021 - Preregistration specificity & adherence A review .pdf:application/pdf},
}

@article{clark_keep_2022,
	title = {Keep your enemies close: Adversarial collaborations will improve behavioral science},
	volume = {11},
	issn = {2211-369X},
	doi = {10.1037/mac0000004},
	shorttitle = {Keep your enemies close},
	abstract = {Behavioral scientists enjoy vast methodological freedom in how they operationalize theoretical constructs. This freedom may promote creativity in designing laboratory paradigms that shed light on real-world phenomena, but it also enables questionable research practices that undercut our collective credibility. Open Science norms impose some discipline but cannot constrain cherry-picking operational definitions that insulate preferred theories from rejection. All too often scholars conduct performative research to score points instead of engaging each other’s strongest arguments—a pattern that allows contradictory claims to fester unresolved for decades. Adversarial collaborations, which call on disputants to codevelop tests of competing hypotheses, are an efficient method of improving our science’s capacity for self-correction and of promoting intellectual competition that exposes false claims. Although individual researchers are often initially reluctant to participate, the research community would be better served by institutionalizing adversarial collaboration into its peer-review process. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {1--18},
	journaltitle = {Journal of Applied Research in Memory and Cognition},
	author = {Clark, Cory J. and Costello, Thomas and Mitchell, Gregory and Tetlock, Philip E.},
	date = {2022},
	note = {Place: {US}
Publisher: Educational Publishing Foundation},
	keywords = {Scientists, Sciences, Cognition, Motivation, Open Data, Behavioral Sciences, Peer Evaluation, Experimental Methods, Theories, Collaboration, Competition},
	file = {Clark et al. - 2022 - Keep your enemies close Adversarial collaboration.pdf:/Users/tom/Zotero/storage/VDMB9VM5/Clark et al. - 2022 - Keep your enemies close Adversarial collaboration.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/TDAVJFIE/doiLanding.html:text/html},
}

@article{hallsworth_manifesto_2023,
	title = {A manifesto for applying behavioural science},
	volume = {7},
	rights = {2023 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01555-3},
	doi = {10.1038/s41562-023-01555-3},
	abstract = {Recent years have seen a rapid increase in the use of behavioural science to address the priorities of public and private sector actors. There is now a vibrant ecosystem of practitioners, teams and academics building on each other’s findings across the globe. Their focus on robust evaluation means we know that this work has had an impact on important issues such as antimicrobial resistance, educational attainment and climate change. However, several critiques have also emerged; taken together, they suggest that applied behavioural science needs to evolve further over its next decade. This manifesto for the future of applied behavioural science looks at the challenges facing the field and sets out ten proposals to address them. Meeting these challenges will mean that behavioural science is better equipped to help to build policies, products and services on stronger empirical foundations—and thereby address the world’s crucial challenges.},
	pages = {310--322},
	number = {3},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Hallsworth, Michael},
	urldate = {2023-06-06},
	date = {2023-03},
	langid = {english},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Psychology, Economics, Human behaviour},
	file = {Full Text PDF:/Users/tom/Zotero/storage/4HDQU87E/Hallsworth - 2023 - A manifesto for applying behavioural science.pdf:application/pdf},
}

@article{millar_trends_2022,
	title = {Trends in the Use of Promotional Language (Hype) in Abstracts of Successful National Institutes of Health Grant Applications, 1985-2020},
	volume = {5},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2022.28676},
	doi = {10.1001/jamanetworkopen.2022.28676},
	abstract = {The integrity of the grant application process is important to the success of the entire research enterprise. However, little information is available concerning the prevalence and evolution of subjective or promotional language (“hype”) that has the potential to undermine objectivity in the writing and evaluation of grant applications.To assess changes over time in the use of hype in abstracts of National Institutes of Health ({NIH}) grant applications.This cross-sectional study assessed the prevalence of promotional adjectives in abstracts in the {NIH} archive from 1985 to 2020.From all abstracts in the {NIH} {RePORTER} (Research Portfolio Online Reporting Tools: Expenditures and Results) archive, adjectives were automatically extracted, and their frequencies in the most recent year (2020) were assessed relative to the start year (1985). Adjectives that shifted significantly in frequency and that carried a promotional sense (ie, hype) were retained, and patterns of change were assessed by plotting yearly frequencies (1985-2020). By grouping the adjectives based on shared semantic properties, broad meanings commonly expressed by hype were identified. Absolute change was measured as the difference in normalized frequency between 1985 and 2020. Relative change was measured as the percentage change in normalized frequency in 2020 relative to 1985, or the first year of occurrence.In total, 901 717 abstracts were analyzed and 139 adjective forms were identified as hype. Among these 139 adjective forms, 130 hype adjectives increased in frequency by 7690 words per million (wpm) (mean [{SD}] relative increase, 1378\% [3132\%]), while 9 hype adjectives decreased in frequency by 686 wpm (mean [{SD}] relative decrease, 44\% [18\%]). The largest absolute increases were for the terms novel (1054 wpm), critical (555 wpm), and key (461 wpm), while the largest relative increases were for the terms sustainable (25 157\%), actionable (16 114\%), and scalable (13 029\%). Hype most often serves to promote the significance, novelty, scale, and rigor of a project; the utility of the expected outcomes; the qualities of the investigators and research environment; and the gravity of the problem; as well as conveying the personal attitudes of the applicants.Levels of hype in successful {NIH} grant applications have increased over time from 1985 to 2020. The findings in this study should serve to sensitize applicants, reviewers, and funding agencies to the increasing prevalence of subjective, promotional language in funding applications.},
	pages = {e2228676},
	number = {8},
	journaltitle = {{JAMA} Network Open},
	shortjournal = {{JAMA} Network Open},
	author = {Millar, Neil and Batalo, Bojan and Budgell, Brian},
	urldate = {2023-06-08},
	date = {2022-08-25},
	file = {Full Text:/Users/tom/Zotero/storage/2U3BLS4L/Millar et al. - 2022 - Trends in the Use of Promotional Language (Hype) i.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/X5P7XCAP/2795635.html:text/html},
}

@article{fiedler_afterthoughts_2013,
	title = {Afterthoughts on precognition: No cogent evidence for anomalous influences of consequent events on preceding cognition},
	volume = {23},
	issn = {0959-3543},
	url = {https://doi.org/10.1177/0959354313485504},
	doi = {10.1177/0959354313485504},
	shorttitle = {Afterthoughts on precognition},
	abstract = {In a recently published major article in the Journal of Personality and Social Psychology, Daryl Bem (2011) made a strong claim for the existence of a parapsychological phenomenon called retroactive causation. Across nine experiments, aspects of stimuli were shown to correlate with participants’ responses provided before the stimuli were generated by the computer’s random generator. Early critical debates of these provocative findings have been focused on issues of statistical significance testing. Going beyond these issues, we argue that Bem’s research has three crucial shortcomings: (a) a lack of a theoretical explanation, (b) the possibility of selective filtering of empirical results, and (c) the confusion of the explanans and the explanandum. We propose that all three methodological principles should be rigorously applied during the journal review process and the communication of empirical findings in general.},
	pages = {323--333},
	number = {3},
	journaltitle = {Theory \& Psychology},
	author = {Fiedler, Klaus and Krueger, Joachim I.},
	urldate = {2023-06-08},
	date = {2013-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd},
	file = {Afterthoughts on precognition\: No cogent evidence for anomalous influences of consequent events on preceding cognition:/Users/tom/Zotero/storage/TQR3QKCG/fiedler2013.pdf.pdf:application/pdf},
}

@misc{clarke_looking_2023,
	title = {Looking Our Limitations in the Eye: A Tutorial for Writing About Research Limitations in Psychology},
	url = {https://psyarxiv.com/386bh/},
	doi = {10.31234/osf.io/386bh},
	shorttitle = {Looking Our Limitations in the Eye},
	abstract = {Limitations are an inherent part of the research process. Looking these limitations in the eye is no easy task, but it is important if the field of psychology wishes to be considered a credible science. Current practices for reporting limitations in psychology leave much room for improvement. Concrete guidance for discussing specific limitations is lacking. The aim of this tutorial is to enable psychology researchers to “own” their research limitations (inspired by Whitcomb et al., 2017). We provide general recommendations, such as the ‘steel-person principle’ (reflecting on what the best argument is against your conclusions), and specific advice for different types of limitations. We assembled a team with expertise in assessing various aspects of validity, and structured this tutorial around recommendations for discussing common threats to construct, internal, external, and statistical conclusion validity (Shadish et al., 2002). Our goal is to prompt psychologists to write more deeply and clearly about the limitations of their research, and to hold each other to higher standards when reviewing each other’s work.  A major limitation of this tutorial is that our advice risks being applied formulaically, and as a substitute for critical thinking about limitations. Further, this tutorial should not replace efforts to prevent or reduce research limitations in the first place. Instead, readers should use this tutorial as a starting point for reflecting on their limitations which should be thoughtfully incorporated in all relevant conclusions throughout their paper.},
	publisher = {{PsyArXiv}},
	author = {Clarke, Beth and Alley, Lindsay and Ghai, Sakshi and Flake, Jessica Kay and Rohrer, Julia M. and Simmons, Joseph P. and Schiavone, Sarah R. and Vazire, Simine},
	urldate = {2023-06-08},
	date = {2023-06-08},
	langid = {english},
	keywords = {Meta-science, Social and Behavioral Sciences, research methods, Social and Personality Psychology, writing, research practices, limitations, the four validities},
	file = {Full Text PDF:/Users/tom/Zotero/storage/TYH4RBRG/Clarke et al. - 2023 - Looking Our Limitations in the Eye A Tutorial for.pdf:application/pdf},
}

@article{noauthor_daryl_2017,
	title = {Daryl Bem Proved {ESP} Is Real},
	issn = {1091-2339},
	url = {https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html},
	abstract = {Which means science is broken.},
	journaltitle = {Slate},
	urldate = {2023-06-10},
	date = {2017-06-07},
	langid = {american},
	note = {Section: Science},
	keywords = {psychology, science, cover story, redux},
	file = {Snapshot:/Users/tom/Zotero/storage/SHZEC3AT/daryl-bem-proved-esp-is-real-showed-science-is-broken.html:text/html},
}

@article{utts_replication_1991,
	title = {Replication and Meta-Analysis in Parapsychology},
	volume = {6},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-6/issue-4/Replication-and-Meta-Analysis-in-Parapsychology/10.1214/ss/1177011577.full},
	doi = {10.1214/ss/1177011577},
	abstract = {Parapsychology, the laboratory study of psychic phenomena, has had its history interwoven with that of statistics. Many of the controversies in parapsychology have focused on statistical issues, and statistical models have played an integral role in the experimental work. Recently, parapsychologists have been using meta-analysis as a tool for synthesizing large bodies of work. This paper presents an overview of the use of statistics in parapsychology and offers a summary of the meta-analyses that have been conducted. It begins with some anecdotal information about the involvement of statistics and statisticians with the early history of parapsychology. Next, it is argued that most nonstatisticians do not appreciate the connection between power and "successful" replication of experimental effects. Returning to parapsychology, a particular experimental regime is examined by summarizing an extended debate over the interpretation of the results. A new set of experiments designed to resolve the debate is then reviewed. Finally, meta-analyses from several areas of parapsychology are summarized. It is concluded that the overall evidence indicates that there is an anomalous effect in need of an explanation.},
	number = {4},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Utts, Jessica},
	urldate = {2023-06-10},
	date = {1991-11-01},
	langid = {english},
	file = {Replication and Meta-Analysis in Parapsychology:/Users/tom/Zotero/storage/3E37CNDH/utts1991.pdf.pdf:application/pdf;Utts - 1991 - Replication and Meta-Analysis in Parapsychology.pdf:/Users/tom/Zotero/storage/Q2U957SG/Utts - 1991 - Replication and Meta-Analysis in Parapsychology.pdf:application/pdf},
}

@article{shepard_toward_1987,
	title = {Toward a Universal Law of Generalization for Psychological Science},
	volume = {237},
	url = {https://www.science.org/doi/10.1126/science.3629243},
	doi = {10.1126/science.3629243},
	abstract = {A psychological space is established for any set of stimuli by determining metric distances between the stimuli such that the probability that a response learned to any stimulus will generalize to any other is an invariant monotonic function of the distance between them. To a good approximation, this probability of generalization (i) decays exponentially with this distance, and (ii) does so in accordance with one of two metrics, depending on the relation between the dimensions along which the stimuli vary. These empirical regularities are mathematically derivable from universal principles of natural kinds and probabilistic geometry that may, through evolutionary internalization, tend to govern the behaviors of all sentient organisms.},
	pages = {1317--1323},
	number = {4820},
	journaltitle = {Science},
	author = {Shepard, Roger N.},
	urldate = {2023-06-12},
	date = {1987-09-11},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Toward a Universal Law of Generalization for Psychological Science:/Users/tom/Zotero/storage/XMBSB72D/shepard1987.pdf.pdf:application/pdf},
}

@article{rosenthal_replication_1990,
	title = {Replication in behavioral research},
	volume = {5},
	issn = {0886-1641},
	abstract = {Addresses evaluation of the importance of 1 or more replications ({RPCs}) and how to define the success of {RPCs}. Variables affecting the assessment of whether an {RPC} is important include when, how, and by whom the {RPC} was conducted. Conducting {RPCs} in batteries varying in degree of similarity to the original study may tell more about the external validity of the result being replicated. A useful view of {RPC} focuses on effect size as the more important summary statistic of a study and evaluates the success of an {RPC} in a continuous fashion. Concepts of a successful {RPC} are presented, some metrics of {RPC} success are described, and suggestions are offered as to what should be reported in an {RPC} study. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {1--30},
	journaltitle = {Journal of Social Behavior \& Personality},
	author = {Rosenthal, Robert},
	date = {1990},
	note = {Place: {US}
Publisher: Select Press},
	keywords = {Methodology, Experimental Replication, Social Sciences},
	file = {Snapshot:/Users/tom/Zotero/storage/S27H8WM8/1991-00046-001.html:text/html},
}

@article{lund_integration_2008,
	title = {Integration of renewable energy into the transport and electricity sectors through V2G},
	volume = {36},
	issn = {0301-4215},
	url = {https://www.sciencedirect.com/science/article/pii/S0301421508002838},
	doi = {10.1016/j.enpol.2008.06.007},
	abstract = {Large-scale sustainable energy systems will be necessary for substantial reduction of {CO}2. However, large-scale implementation faces two major problems: (1) we must replace oil in the transportation sector, and (2) since today's inexpensive and abundant renewable energy resources have fluctuating output, to increase the fraction of electricity from them, we must learn to maintain a balance between demand and supply. Plug-in electric vehicles ({EVs}) could reduce or eliminate oil for the light vehicle fleet. Adding “vehicle-to-grid” (V2G) technology to {EVs} can provide storage, matching the time of generation to time of load. Two national energy systems are modelled, one for Denmark, including combined heat and power ({CHP}) and the other a similarly sized country without {CHP} (the latter being more typical of other industrialized countries). The model ({EnergyPLAN}) integrates energy for electricity, transport and heat, includes hourly fluctuations in human needs and the environment (wind resource and weather-driven need for heat). Four types of vehicle fleets are modelled, under levels of wind penetration varying from 0\% to 100\%. {EVs} were assumed to have high power (10kW) connections, which provide important flexibility in time and duration of charging. We find that adding {EVs} and V2G to these national energy systems allows integration of much higher levels of wind electricity without excess electric production, and also greatly reduces national {CO}2 emissions.},
	pages = {3578--3587},
	number = {9},
	journaltitle = {Energy Policy},
	shortjournal = {Energy Policy},
	author = {Lund, Henrik and Kempton, Willett},
	urldate = {2023-06-29},
	date = {2008-09-01},
	langid = {english},
	keywords = {Electric vehicle, V2G, Wind power},
	file = {Integration of renewable energy into the transport and electricity sectors through V2G:/Users/tom/Zotero/storage/CGZ7W5ZH/lund2008.pdf.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/39WEX49A/S0301421508002838.html:text/html},
}

@article{geels_enactment_2016,
	title = {The enactment of socio-technical transition pathways: A reformulated typology and a comparative multi-level analysis of the German and {UK} low-carbon electricity transitions (1990–2014)},
	volume = {45},
	issn = {0048-7333},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733316300087},
	doi = {10.1016/j.respol.2016.01.015},
	shorttitle = {The enactment of socio-technical transition pathways},
	abstract = {This paper aims to make two contributions to the sustainability transitions literature, in particular the Geels and Schot (2007. Res. Policy 36(3), 399) transition pathways typology. First, it reformulates and differentiates the typology through the lens of endogenous enactment, identifying the main patterns for actors, formal institutions, and technologies. Second, it suggests that transitions may shift between pathways, depending on struggles over technology deployment and institutions. Both contributions are demonstrated with a comparative analysis of unfolding low-carbon electricity transitions in Germany and the {UK} between 1990–2014. The analysis shows that Germany is on a substitution pathway, enacted by new entrants deploying small-scale renewable electricity technologies ({RETs}), while the {UK} is on a transformation pathway, enacted by incumbent actors deploying large-scale {RETs}. Further analysis shows that the German transition has recently shifted from a ‘stretch-and-transform’ substitution pathway to a ‘fit-and-conform’ pathway, because of a fightback from utilities and altered institutions. It also shows that the {UK} transition moved from moderate to substantial incumbent reorientation, as government policies became stronger. Recent policy changes, however, substantially downscaled {UK} renewables support, which is likely to shift the transition back to weaker reorientation.},
	pages = {896--913},
	number = {4},
	journaltitle = {Research Policy},
	shortjournal = {Research Policy},
	author = {Geels, Frank W. and Kern, Florian and Fuchs, Gerhard and Hinderer, Nele and Kungl, Gregor and Mylan, Josephine and Neukirch, Mario and Wassermann, Sandra},
	urldate = {2023-06-29},
	date = {2016-05-01},
	langid = {english},
	keywords = {Enactment, Low-carbon electricity transition, Multi-level perspective, Transition pathways typology},
	file = {Full Text:/Users/tom/Zotero/storage/RIQLC8NY/Geels et al. - 2016 - The enactment of socio-technical transition pathwa.pdf:application/pdf},
}

@article{de_boef_taking_2008,
	title = {Taking Time Seriously},
	volume = {52},
	issn = {1540-5907},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2007.00307.x},
	doi = {10.1111/j.1540-5907.2007.00307.x},
	abstract = {Dramatic world change has stimulated interest in research questions about the dynamics of politics. We have seen increases in the number of time series data sets and the length of typical time series. But three shortcomings are prevalent in published time series analysis. First, analysts often estimate models without testing restrictions implied by their specification. Second, researchers link the theoretical concept of equilibrium with cointegration and error correction models. Third, analysts often do a poor job of interpreting results. The consequences include weak connections between theory and tests, biased estimates, and incorrect inferences. We outline techniques for estimating linear dynamic regressions with stationary data and weakly exogenous regressors. We recommend analysts (1) start with general dynamic models and test restrictions before adopting a particular specification and (2) use the wide array of information available from dynamic specifications. We illustrate this strategy with data on Congressional approval and tax rates across {OECD} countries.},
	pages = {184--200},
	number = {1},
	journaltitle = {American Journal of Political Science},
	author = {De Boef, Suzanna and Keele, Luke},
	urldate = {2023-06-29},
	date = {2008},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5907.2007.00307.x},
}

@article{dahlander_how_2010,
	title = {How open is innovation?},
	volume = {39},
	issn = {0048-7333},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733310000272},
	doi = {10.1016/j.respol.2010.01.013},
	abstract = {This paper is motivated by a desire to clarify the definition of ‘openness’ as currently used in the literature on open innovation, and to re-conceptualize the idea for future research on the topic. We combine bibliographic analysis of all papers on the topic published in Thomson's {ISI} Web of Knowledge ({ISI}) with a systematic content analysis of the field to develop a deeper understanding of earlier work. Our review indicates two inbound processes: sourcing and acquiring, and two outbound processes, revealing and selling. We analyze the advantages and disadvantages of these different forms of openness. The paper concludes with implications for theory and practice, charting several promising areas for future research.},
	pages = {699--709},
	number = {6},
	journaltitle = {Research Policy},
	shortjournal = {Research Policy},
	author = {Dahlander, Linus and Gann, David M.},
	urldate = {2023-06-29},
	date = {2010-07-01},
	langid = {english},
	keywords = {Review, Appropriability, Complementary assets, Content analysis, Innovation, Open innovation, Openness},
}

@article{rao_anomaly_1987,
	title = {The anomaly called psi: Recent research and criticism},
	volume = {10},
	issn = {0140-525X, 1469-1825},
	url = {http://www.journals.cambridge.org/abstract_S0140525X00054455},
	doi = {10.1017/S0140525X00054455},
	shorttitle = {The anomaly called psi},
	abstract = {Over the past hundred years, a number of scientific investigators claim to have adduced experimental evidence for "psi" phenomena - that is, the apparent ability to receive information shielded from the senses ({ESP}) and to influence systems outside the sphere of motor activity ({PK}). A report of one series of highly significant psi experiments and the objections of critics are discussed in some depth. It is concluded that the possibility of sensory cues, machine bias, cheating by subjects, and experimenter error or incompetence cannot reasonably account for the significant results. In addition, less detailed reviews of the experimental results in several broad areas of psi research indicate that psi results are statistically replicable and that significant patterns exist across a large body of experimental data. For example, a wide range of research seems to converge on the idea that, because {ESP} "information" seems to behave like a weak signal that has to compete for the information-processing resources of the organism, a reduction of ongoing sensorimotor activity may facilitate {ESP} detection. Such a meaningful convergence of results suggests that psi phenomena may represent a unitary, coherent process whose nature and compatibility with current physical theory have yet to be determined. The theoretical implications and potential practical applications of psi could be significant, irrespective of the small magnitude of psi effects in laboratory settings.},
	pages = {539},
	number = {4},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Rao, K. Ramakrishna and Palmer, John},
	urldate = {2023-06-30},
	date = {1987-12},
	langid = {english},
	file = {Rao and Palmer - 1987 - The anomaly called psi Recent research and critic.pdf:/Users/tom/Zotero/storage/ZLUFIXD2/Rao and Palmer - 1987 - The anomaly called psi Recent research and critic.pdf:application/pdf},
}

@article{schlitz_two_2006,
	title = {Of two minds: Sceptic-proponent collaboration within parapsychology},
	volume = {97},
	rights = {2006 The British Psychological Society},
	issn = {2044-8295},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/000712605X80704},
	doi = {10.1348/000712605X80704},
	shorttitle = {Of two minds},
	abstract = {The first author, a proponent of evidence for psychic ability, and the second, a sceptic, have been conducting a systematic programme of collaborative sceptic-proponent research in parapsychology. This has involved carrying out joint experiments in which each investigator individually attempted to mentally influence the electrodermal activity of participants at a distant location. The first two collaborations obtained evidence of ‘experimenter effects’, that is, experiments conducted by the proponent obtained significant results but those conducted by the sceptic did not. This paper describes a new collaborative study that attempted to replicate our previous findings and explore potential explanations for past results. The new study failed to replicate our previous findings. The paper investigates whether the results obtained in our initial studies may have been caused by a genuine psychic effect, and this third experiment failed to replicate this finding because some aspect of the study disrupted the production of that effect, or whether the results from our first two studies represented chance findings or undetected subtle artifacts, and the results obtained in the present study accurately reflect the absence of a remote detection of staring effect. The implications of this work are discussed, along with the benefits of conducting collaborative work for resolving disagreements in other controversial areas of psychology.},
	pages = {313--322},
	number = {3},
	journaltitle = {British Journal of Psychology},
	author = {Schlitz, Marilyn and Wiseman, Richard and Watt, Caroline and Radin, Dean},
	urldate = {2023-07-01},
	date = {2006},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000712605X80704},
	file = {Submitted Version:/Users/tom/Zotero/storage/IRNCAK5P/Schlitz et al. - 2006 - Of two minds Sceptic-proponent collaboration with.pdf:application/pdf},
}

@article{schlitz_two_2006-1,
	title = {Of two minds: Sceptic-proponent collaboration within parapsychology},
	volume = {97},
	rights = {2006 The British Psychological Society},
	issn = {2044-8295},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/000712605X80704},
	doi = {10.1348/000712605X80704},
	shorttitle = {Of two minds},
	abstract = {The first author, a proponent of evidence for psychic ability, and the second, a sceptic, have been conducting a systematic programme of collaborative sceptic-proponent research in parapsychology. This has involved carrying out joint experiments in which each investigator individually attempted to mentally influence the electrodermal activity of participants at a distant location. The first two collaborations obtained evidence of ‘experimenter effects’, that is, experiments conducted by the proponent obtained significant results but those conducted by the sceptic did not. This paper describes a new collaborative study that attempted to replicate our previous findings and explore potential explanations for past results. The new study failed to replicate our previous findings. The paper investigates whether the results obtained in our initial studies may have been caused by a genuine psychic effect, and this third experiment failed to replicate this finding because some aspect of the study disrupted the production of that effect, or whether the results from our first two studies represented chance findings or undetected subtle artifacts, and the results obtained in the present study accurately reflect the absence of a remote detection of staring effect. The implications of this work are discussed, along with the benefits of conducting collaborative work for resolving disagreements in other controversial areas of psychology.},
	pages = {313--322},
	number = {3},
	journaltitle = {British Journal of Psychology},
	author = {Schlitz, Marilyn and Wiseman, Richard and Watt, Caroline and Radin, Dean},
	urldate = {2023-07-01},
	date = {2006},
	langid = {english},
}

@online{noauthor_rhetoric_nodate,
	title = {Rhetoric Over Substance: The Impoverished State of Skepticism - {ProQuest}},
	url = {https://www.proquest.com/openview/43a0eae1e9e2431fd643735b37f5b2e0/1?pq-origsite=gscholar&cbl=1818062},
	shorttitle = {Rhetoric Over Substance},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the {ProQuest} Platform.},
	urldate = {2023-07-01},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/8XDE7L8L/1.html:text/html},
}

@article{milton_does_nodate,
	title = {Does Psi Exist? Lack of Replication of an Anomalous Process of Information Transfer},
	author = {Milton, Julie and Wiseman, Richard},
	langid = {english},
	file = {Milton and Wiseman - Does Psi Exist Lack of Replication of an Anomalou.pdf:/Users/tom/Zotero/storage/HWUE8IRM/Milton and Wiseman - Does Psi Exist Lack of Replication of an Anomalou.pdf:application/pdf},
}

@online{noauthor_comment_nodate,
	title = {Comment: The transparent Psi project},
	url = {https://docs.google.com/document/d/1PF7c8k-mH17K8PAFNlup6kSpms48Xx1kuSqpz8RyCXc/edit?usp=drive_web&ouid=104475743010452729807&usp=embed_facebook},
	shorttitle = {Comment},
	abstract = {Credible research of the incredible: The transparent Psi project raises the bar for scientific rigour  A false belief bumps up against solid reality Tom Hardwicke University of Melbourne  \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  [aim for 1000 words]   In 2011, two articles were published...},
	titleaddon = {Google Docs},
	urldate = {2023-07-02},
	langid = {english},
	file = {Snapshot:/Users/tom/Zotero/storage/4BMVKY4Y/edit.html:text/html},
}

@article{t_mello_credibility_2023,
	title = {Credibility at stake: only two-thirds of randomized trials of nutrition interventions are registered and lack transparency in outcome and treatment effect definitions},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435623001683},
	doi = {10.1016/j.jclinepi.2023.06.021},
	shorttitle = {Credibility at stake},
	abstract = {Objective
This study aimed to investigate the adherence of randomized controlled trials of nutrition interventions to transparency practices informing assessments of selective reporting biases, including the availability of a trial registration entry, protocol and statistical analysis plan.
Study Design and Setting
Retrospective observational study with cross-sectional design. We systematically searched for trials published from 1 July 2019 to 30 June 2020 and included a randomly selected sample of 400 studies. We searched for registry entries, protocols, and statistical analysis plans for all included studies. We extracted data to characterize the disclosure of sufficient information in the available materials to inform assessments of selective reporting biases, considering the definition of outcome domain, measure, metric, method of aggregation, time point, analysis population, methods to handle missing data and method of adjustment.
Results
Most trials (69\%) were registered, but these often lacked sufficient specification of outcomes and intended treatment effects. Protocols and statistical analysis plans provided more details, but were less often available (14\% and 3\%, respectively), and even then, almost all studies presented limited information to inform the assessments of risk of bias due to the selection of the reported result.
Conclusion
Lack of full specification of outcomes and intended treatment effects hinder a full adherence of randomized controlled trials of nutrition interventions to transparency practices and may affect their credibility.},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {T Mello, Arthur and V Kammer, Pedro and M Nascimento, Giovanna and P de Lima, Luana and Pessini, Júlia and Valmorbida, Aline and Page, Matthew J. and B S M Trindade, Erasmo},
	urldate = {2023-07-05},
	date = {2023-07-01},
	langid = {english},
	keywords = {transparency, randomized controlled trials, selective reporting, risk of bias, nutrition},
}

@article{christian_we_2023,
	title = {We must improve conditions and options for Australian {ECRs}},
	rights = {2023 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01621-w},
	doi = {10.1038/s41562-023-01621-w},
	abstract = {Early-career researchers in Australia report dissatisfaction, bullying and questionable research practices. We discuss how this may contribute to the replication crisis and suggest local and international strategies to improve the industry.},
	pages = {1--4},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Christian, Katherine and Larkins, Jo-ann and Doran, Michael R.},
	urldate = {2023-07-05},
	date = {2023-06-05},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Careers, Technology},
	file = {Christian et al. - 2023 - We must improve conditions and options for Austral.pdf:/Users/tom/Zotero/storage/QEZAPEY2/Christian et al. - 2023 - We must improve conditions and options for Austral.pdf:application/pdf},
}

@article{coupe_getting_2023,
	title = {Getting seen: Results from an online experiment to draw more attention to replications},
	volume = {52},
	issn = {0048-7333},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733323001257},
	doi = {10.1016/j.respol.2023.104841},
	shorttitle = {Getting seen},
	abstract = {Are users of bibliographic databases interested in learning about replications? Can they be induced to learn? To answer these questions, we performed an experiment using an online research bibliography, Research Papers in Economics ({RePEc}). {RePEc} is the main research bibliography for preprints and published papers in economics. Using stratified randomization, we allocated 324 replications and their corresponding original papers to clusters. We then drew from these clusters to construct the treatment and control groups. We added brightly coloured tabs to the relevant webpages to alert visitors to the existence of a replication paper. We monitored traffic over three phases lasting several months: a) no treatment, b) treatment in one group, and c) treatment in both groups. Our estimates indicate that this intervention generated an average click-through rate ({CTR}) of 1.6 \%, resulting in a 13 \% increase in the number of visits to the webpages of the replication papers, although only the former estimate was statistically significant.},
	pages = {104841},
	number = {8},
	journaltitle = {Research Policy},
	shortjournal = {Research Policy},
	author = {Coupé, Tom and Reed, W. Robert and Zimmermann, Christian},
	urldate = {2023-07-05},
	date = {2023-10-01},
	langid = {english},
	keywords = {Experiment, Replications, Click-throughs, Online research bibliography, {RePEc}, Webpages},
}

@inbook{watt_give_2017,
	edition = {1},
	title = {Give the null hypothesis a chance: reasons to remain doubtful about the existence of psi},
	isbn = {978-1-315-24736-6},
	url = {https://www.taylorfrancis.com/books/9781351912839/chapters/10.4324/9781315247366-24},
	pages = {441--462},
	booktitle = {Parapsychology},
	publisher = {Routledge},
	author = {Alcock, James E.},
	bookauthor = {Watt, Caroline},
	editor = {Wiseman, Richard and Watt, Caroline},
	urldate = {2023-07-06},
	date = {2017-07-05},
	langid = {english},
	doi = {10.4324/9781315247366-24},
	file = {Alcock - 2017 - Give the Null Hypothesis a Chance.pdf:/Users/tom/Zotero/storage/ASDD3SAU/Alcock - 2017 - Give the Null Hypothesis a Chance.pdf:application/pdf},
}

@misc{peikert_why_2023,
	title = {Why does preregistration increase the persuasiveness of evidence? A Bayesian rationalization},
	url = {https://psyarxiv.com/cs8wb/},
	doi = {10.31234/osf.io/cs8wb},
	shorttitle = {Why does preregistration increase the persuasiveness of evidence?},
	abstract = {The replication crisis has led many researchers to preregister their hypotheses and data analysis plans before collecting data.
A widely held view is that preregistration is supposed to limit the extent to which data may influence the hypotheses to be tested.
Only if data have no influence an analysis is considered confirmatory. 
Consequently, many researchers believe that preregistration is only applicable in confirmatory paradigms.
In practice, researchers may struggle to preregister their hypotheses because of vague theories that necessitate data-dependent decisions (aka exploration).
We argue that preregistration benefits any study on the continuum between confirmatory and exploratory research.
To that end, we formalize a general objective of preregistration and demonstrate that exploratory studies also benefit from preregistration.
Drawing on Bayesian philosophy of science, we argue that preregistration should primarily aim to reduce uncertainty about the inferential procedure used to derive results.
This approach provides a principled justification of preregistration, separating the procedure from the goal of ensuring strictly confirmatory research.
We acknowledge that knowing the extent to which a study is exploratory is central, but certainty about the inferential procedure is a prerequisite for persuasive evidence.
Finally, we discuss the implications of these insights for the practice of preregistration.},
	publisher = {{PsyArXiv}},
	author = {Peikert, Aaron and Ernst, Maximilian Stefan and Brandmaier, Andreas Markus},
	urldate = {2023-07-06},
	date = {2023-02-17},
	langid = {english},
	keywords = {Meta-science, preregistration, Social and Behavioral Sciences, Quantitative Methods, Theory and Philosophy of Science, hypothesis testing, Open Science, confirmation, Bayesian, exploration},
	file = {Full Text PDF:/Users/tom/Zotero/storage/IXXJQ9EL/Peikert et al. - 2023 - Why does preregistration increase the persuasivene.pdf:application/pdf},
}

@article{rhine_new_1974,
	title = {A new case of experimenter unreliability},
	volume = {38},
	pages = {215--225},
	number = {2},
	journaltitle = {The Journal of Parapsychology},
	author = {Rhine, J. B.},
	date = {1974},
	file = {Comments_A_New_Case_of_Exper.pdf:/Users/tom/Zotero/storage/8LR496F7/Comments_A_New_Case_of_Exper.pdf:application/pdf},
}

@article{zingrone_text_nodate,
	title = {From text to self: the interplay of criticism and response in the history of parapsychology},
	author = {Zingrone, Nancy L},
	langid = {english},
	file = {Zingrone - FROM TEXT TO SELF THE INTERPLAY OF CRITICISM AND .pdf:/Users/tom/Zotero/storage/YLMWF83Z/Zingrone - FROM TEXT TO SELF THE INTERPLAY OF CRITICISM AND .pdf:application/pdf},
}

@article{hacking_telepathy_1988,
	title = {Telepathy: Origins of Randomization in Experimental Design},
	volume = {79},
	issn = {0021-1753},
	url = {https://www.jstor.org/stable/234674},
	shorttitle = {Telepathy},
	pages = {427--451},
	number = {3},
	journaltitle = {Isis},
	author = {Hacking, Ian},
	urldate = {2023-07-10},
	date = {1988},
	note = {Publisher: [The University of Chicago Press, The History of Science Society]},
	file = {Hacking - 1988 - Telepathy Origins of Randomization in Experimenta.pdf:/Users/tom/Zotero/storage/2MWV37BX/Hacking - 1988 - Telepathy Origins of Randomization in Experimenta.pdf:application/pdf},
}

@article{godin_experimenters_2002,
	title = {The experimenters' regress: from skepticism to argumentation},
	volume = {33},
	issn = {0039-3681},
	url = {https://www.sciencedirect.com/science/article/pii/S0039368101000322},
	doi = {10.1016/S0039-3681(01)00032-2},
	shorttitle = {The experimenters' regress},
	abstract = {Harry Collins' central argument about experimental practice revolves around the thesis that facts can only be generated by good instruments but good instruments can only be recognized as such if they produce facts. This is what Collins calls the experimenters' regress. For Collins, scientific controversies cannot be closed by the ‘facts’ themselves because there are no formal criteria independent of the outcome of the experiment that scientists can apply to decide whether an experimental apparatus works properly or not. No one seems to have noticed that the debate is in fact a rehearsal of the ancient philosophical debate about skepticism. The present article suggests that the way out of radical skepticism offered by the so-called mitigated skeptics is a solution to the problem of consensus formation in science.},
	pages = {133--148},
	number = {1},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Godin, Benoı̂t and Gingras, Yves},
	urldate = {2023-07-10},
	date = {2002-03-01},
	langid = {english},
	keywords = {Philosophy of science, Skepticism, Sociology of science, Argumentation, Scientific controversies},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/9I3CZAR2/S0039368101000322.html:text/html;The experimenters' regress\: from skepticism to argumentation:/Users/tom/Zotero/storage/EV5PJXJB/godin2002.pdf.pdf:application/pdf},
}

@article{jahn_persistent_1982,
	title = {The persistent paradox of psychic phenomena: An engineering perspective},
	volume = {70},
	issn = {1558-2256},
	doi = {10.1109/PROC.1982.12260},
	shorttitle = {The persistent paradox of psychic phenomena},
	abstract = {Although a variety of so-called psychic phenomena have attracted man's attention throughout recorded history, organized scholarly effort to comprehend such effects is just one century old, and systematic academic research roughly half that age. Over recent years, a sizeable spectrum of evidence has been brought forth from reputable laboratories in several disciplines to suggest that at times human consciousness can acquire information inaccessible by any known physical mechanism ({ESP}), and can influence the behavior of physical systems or processes ({PK}), but even the most rigorous and sophisticated of these studies display a characteristic dilemma: The experimental results are rarely replicable in the strict scientific sense, but the anomalous yields are well beyond chance expectations and a number of common features thread through the broad range of reported effects. Various attempts at theoretical modeling have so far shown little functional value in explicating experimental results, but have served to stimulate fundamental re-examination of the role of consciousness in the determination of physical reality. Further careful study of this formidable field seems justified, but only within the context of very well conceived and technically impeccable experiments of large data-base capability, with disciplined attention to the pertinent aesthetic factors, and with more constructive involvement of the critical community.},
	pages = {136--170},
	number = {2},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Jahn, R.G.},
	date = {1982-02},
	note = {Conference Name: Proceedings of the {IEEE}},
	keywords = {Psychology, Humans, Calibration, Casting, Catalogs, Displays, Electrostatic precipitators, Laboratories, Tail},
	file = {IEEE Xplore Abstract Record:/Users/tom/Zotero/storage/2DGDMIMR/1456528.html:text/html;The persistent paradox of psychic phenomena\: An engineering perspective:/Users/tom/Zotero/storage/A4JEUX52/jahn1982.pdf.pdf:application/pdf},
}

@incollection{hyman_parapsychologys_2010,
	location = {Santa Barbara, {CA}, {US}},
	title = {Parapsychology's Achilles heel: Persistent inconsistency},
	isbn = {978-0-313-39261-0 978-0-313-39262-7},
	shorttitle = {Parapsychology's Achilles heel},
	abstract = {Some contemporary parapsychologists claim that the existence of psi has been conclusively demonstrated. They argue that the evidence for this claim meets strict scientific criteria. Furthermore, they state that the evidence is independently replicable. In contrast, other contemporary parapsychologists insist that the evidence for psi is capricious, elusive, and fails to meet accepted scientific standards. In particular, they bemoan the fact that the data cannot be replicated. These contrasting positions pose a problem for anyone who wants to make a fair assessment of the status of parapsychology. If, indeed, the evidence meets scientific standards and supports the existence of psi, then the scientific community should take parapsychological claims seriously. On the other hand if the data are elusive and incapable of being replicated, the scientific and the general communities can safely dismiss or ignore the claims for psi. Science can deal only with data and evidence that are objective, lawful, and independently replicable. This chapter will examine both the claims that psi has been scientifically demonstrated and the opposing claims that the evidence for psi is capricious and cannot be independently replicated. My conclusions about the status of parapsychology and its potential for becoming a serious field within science will be based on this examination. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {43--52},
	booktitle = {Debating psychic experience: Human potential or human illusion?},
	publisher = {Praeger/{ABC}-{CLIO}},
	author = {Hyman, Ray},
	date = {2010},
	keywords = {Experimentation, Sciences, Parapsychology, Parapsychological Phenomena},
	file = {Snapshot:/Users/tom/Zotero/storage/2NHLSAZB/2010-19817-003.html:text/html},
}

@article{stefan_big_2023,
	title = {Big little lies: a compendium and simulation of p-hacking strategies},
	volume = {10},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.220346},
	doi = {10.1098/rsos.220346},
	shorttitle = {Big little lies},
	abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12 p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
	pages = {220346},
	number = {2},
	journaltitle = {Royal Society Open Science},
	author = {Stefan, Angelika M. and Schönbrodt, Felix D.},
	urldate = {2023-07-13},
	date = {2023-02-08},
	note = {Publisher: Royal Society},
	keywords = {questionable research practices, p-curve, error rates, false-positive rate, Shiny app, significance, simulation},
	file = {Stefan and Schönbrodt - 2023 - Big little lies a compendium and simulation of p-.pdf:/Users/tom/Zotero/storage/AZDUPX7K/Stefan and Schönbrodt - 2023 - Big little lies a compendium and simulation of p-.pdf:application/pdf},
}

@collection{krippner_debating_2010,
	location = {Santa Barbara},
	title = {Debating psychic experience: human potential or human illusion?},
	isbn = {978-0-313-39261-0 978-0-313-39262-7},
	shorttitle = {Debating psychic experience},
	pagetotal = {236},
	publisher = {{ABC}-{CLIO}},
	editor = {Krippner, Stanley and Friedman, Harris L.},
	date = {2010},
	langid = {english},
	keywords = {Parapsychology},
	file = {Krippner and Friedman - 2010 - Debating psychic experience human potential or hu.pdf:/Users/tom/Zotero/storage/LG5UCW9V/Krippner and Friedman - 2010 - Debating psychic experience human potential or hu.pdf:application/pdf},
}

@article{watt_options_2017,
	title = {Options for Prospective Meta-Analysis and Introduction of Registration-Based Prospective Meta-Analysis},
	volume = {7},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.02030},
	journaltitle = {Frontiers in Psychology},
	author = {Watt, Caroline A. and Kennedy, James E.},
	urldate = {2023-07-14},
	date = {2017},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MFFQIQHN/Watt and Kennedy - 2017 - Options for Prospective Meta-Analysis and Introduc.pdf:application/pdf},
}

@article{scott_enabling_2019,
	title = {Enabling Confirmatory Secondary Data Analysis by Logging Data Checkout},
	volume = {2},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918815849},
	doi = {10.1177/2515245918815849},
	abstract = {As more researchers make their data sets openly available, the potential of secondary data analysis to address new questions increases. However, the distinction between primary and secondary data analysis is unnecessarily confounded with the distinction between confirmatory and exploratory research. We propose a framework, akin to library-book checkout records, for logging access to data sets in order to support confirmatory analysis when appropriate. This system would support a standard form of preregistration for secondary data analysis, allowing authors to demonstrate that their plans were registered prior to data access. We discuss the critical elements of such a system, its strengths and limitations, and potential extensions.},
	pages = {45--54},
	number = {1},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Scott, Kimberly M. and Kline, Melissa},
	urldate = {2023-07-14},
	date = {2019-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Enabling Confirmatory Secondary Data Analysis by Logging Data Checkout:/Users/tom/Zotero/storage/CLMAFN82/10.1177@2515245918815849.pdf.pdf:application/pdf;SAGE PDF Full Text:/Users/tom/Zotero/storage/2HFEAW54/Scott and Kline - 2019 - Enabling Confirmatory Secondary Data Analysis by L.pdf:application/pdf},
}

@article{kennedy_can_2013,
	title = {Can parapsychology move beyond the controversies of retrospective meta-analyses},
	volume = {77},
	abstract = {Retrospective meta-analyses are post hoc analyses that have not been effective at resolving scientific controversies, particularly when based on substantially underpowered experiments. Evaluations of moderating factors, including study flaws, small-study effects, and other sources of heterogeneity, do not neutralize confounding as in a well-designed experiment and cannot fully compensate for weaknesses in the original experiments. A group of welldesigned experiments with adequate power and reliable results is needed for convincing evidence for a controversial effect. The widely recommended standard for experimental research is adequate power to obtain significant results on at least 80\% of confirmatory experiments. Meta-analyses in parapsychology typically have found that 20\% to 33\% of studies with good methodology obtained significant results. Power analysis during experimental design is needed to achieve much better replication rates. Meta-analyses of {RNG} studies have consistently found that z value does not increase with sample size—which is contrary to statistical theory and has been and will be interpreted as an indication of methodological problems. This anomalous property and other sources of heterogeneity for parapsychological results must be addressed. Challenging topics such as experimenter effects, goal-oriented psi, and capricious psimissing can no longer be ignored in research syntheses.},
	pages = {21--35},
	journaltitle = {The Journal of Parapsychology},
	author = {Kennedy, J E},
	date = {2013},
	langid = {english},
	file = {Kennedy - Can Parapsychology Move Beyond the Controversies o.pdf:/Users/tom/Zotero/storage/WMGTC6KJ/Kennedy - Can Parapsychology Move Beyond the Controversies o.pdf:application/pdf},
}

@article{watt_lessons_2015,
	title = {Lessons from the first two years of operating a study registry},
	volume = {6},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00173},
	journaltitle = {Frontiers in Psychology},
	author = {Watt, Caroline and Kennedy, James E.},
	urldate = {2023-07-14},
	date = {2015},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7CAHZYCY/Watt and Kennedy - 2015 - Lessons from the first two years of operating a st.pdf:application/pdf},
}

@article{johnson_publication_1976,
	title = {On publication policy regarding non-significant results},
	volume = {1},
	pages = {1--5},
	number = {2},
	journaltitle = {European Journal of Parapsychology},
	author = {Johnson, M.},
	date = {1976},
	file = {EJP v1 copy.pdf:/Users/tom/Zotero/storage/AKCXSNLJ/EJP v1 copy.pdf:application/pdf},
}

@article{rhine_security_1974,
	title = {Security versus deception in parapsychology},
	volume = {38},
	pages = {99--121},
	number = {1},
	journaltitle = {The Journal of Parapsychology},
	author = {Rhine, J. B.},
	date = {1974},
	file = {Rhine - 1974 - Security versus deception in parapsychology.pdf:/Users/tom/Zotero/storage/TF8KDWG3/Rhine - 1974 - Security versus deception in parapsychology.pdf:application/pdf},
}

@article{hyman_how_1995,
	title = {How to critique a published article},
	volume = {118},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.118.2.178},
	abstract = {Few replies get published. Many that do get published could be more effective. A reply can be effective only if it is published, readable, and credible. This article suggests guidelines for writing good replies. The guidelines begin with things to keep in mind—for example, that you might be wrong—and things to do before you begin to write. The latter include considering not responding, considering alternative viewpoints, and distancing yourself from the task. Things to do while writing include using the principle of charity, avoiding ad hominem and emotionally laden arguments, and keeping the rebuttal short. After completing the first draft, you should put it aside for awhile as well as get a second opinion. A good reply complements rather than discredits the argument of the target author. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {178--182},
	number = {2},
	journaltitle = {Psychological Bulletin},
	author = {Hyman, Ray},
	date = {1995},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Scientific Communication},
	file = {How to critique a published article:/Users/tom/Zotero/storage/YAGDRHIK/hyman1995.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/4RYFBG3W/doiLanding.html:text/html},
}

@incollection{rakow_adversarial_2022,
	location = {Cham},
	title = {Adversarial Collaboration},
	isbn = {978-3-031-04968-2},
	url = {https://doi.org/10.1007/978-3-031-04968-2_16},
	abstract = {Adversarial collaboration is an approach to resolving scientific disputes, wherein researchers who have different positions on the issue at hand collaborate with the aim of making progress on their disputed research question. As an approach to research, adversarial collaboration sits squarely within the open science framework because it puts a premium on transparency in hypothesis specification, study design, data analysis, study interpretation and reporting, and supplies a framework that can encourage rigour in these components of the research process. It is, however, far less common than many other open science innovations such as open materials and data sharing or study preregistration. Therefore, this chapter will begin by familiarising readers with adversarial collaboration, outlining some of its key features, and identifying potential benefits of the approach. The chapter ends with a discussion of what the approach can offer.},
	pages = {359--377},
	booktitle = {Avoiding Questionable Research Practices in Applied Psychology},
	publisher = {Springer International Publishing},
	author = {Rakow, Tim},
	editor = {O'Donohue, William and Masuda, Akihiko and Lilienfeld, Scott},
	urldate = {2023-07-14},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-04968-2_16},
	keywords = {Open science, Adversarial collaboration, Remedies for questionable research practices},
	file = {Rakow - 2022 - Adversarial Collaboration.pdf:/Users/tom/Zotero/storage/PC97L4VX/Rakow - 2022 - Adversarial Collaboration.pdf:application/pdf},
}

@book{ross_first_2019,
	location = {Boston},
	edition = {Tenth edition},
	title = {A first course in probability},
	isbn = {978-0-13-475311-9},
	pagetotal = {505},
	publisher = {Pearson},
	author = {Ross, Sheldon M.},
	date = {2019},
	langid = {english},
	keywords = {Textbooks, Probabilities},
	file = {Ross - 2019 - A first course in probability.pdf:/Users/tom/Zotero/storage/U35GYJWH/Ross - 2019 - A first course in probability.pdf:application/pdf},
}

@inproceedings{liesenfeld_opening_2023,
	title = {Opening up {ChatGPT}: Tracking openness, transparency, and accountability in instruction-tuned text generators},
	url = {http://arxiv.org/abs/2307.05532},
	doi = {10.1145/3571884.3604316},
	shorttitle = {Opening up {ChatGPT}},
	abstract = {Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of {OpenAI}'s {ChatGPT}, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback ({LLM}+{RLHF}). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, {RLHF} data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.},
	pages = {1--6},
	booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
	author = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
	urldate = {2023-07-19},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2307.05532 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/tom/Zotero/storage/A87GM5I8/2307.html:text/html;Liesenfeld et al. - 2023 - Opening up ChatGPT Tracking openness, transparenc.pdf:/Users/tom/Zotero/storage/DQ398VUF/Liesenfeld et al. - 2023 - Opening up ChatGPT Tracking openness, transparenc.pdf:application/pdf},
}

@misc{chen_how_2023,
	title = {How is {ChatGPT}'s behavior changing over time?},
	url = {http://arxiv.org/abs/2307.09009},
	abstract = {{GPT}-3.5 and {GPT}-4 are the two most widely used large language model ({LLM}) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of {GPT}-3.5 and {GPT}-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both {GPT}-3.5 and {GPT}-4 can vary greatly over time. For example, {GPT}-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6\%) but {GPT}-4 (June 2023) was very poor on these same questions (accuracy 2.4\%). Interestingly {GPT}-3.5 (June 2023) was much better than {GPT}-3.5 (March 2023) in this task. {GPT}-4 was less willing to answer sensitive questions in June than in March, and both {GPT}-4 and {GPT}-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same {LLM} service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of {LLM} quality.},
	number = {{arXiv}:2307.09009},
	publisher = {{arXiv}},
	author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
	urldate = {2023-07-19},
	date = {2023-07-18},
	eprinttype = {arxiv},
	eprint = {2307.09009 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/tom/Zotero/storage/AFKXAWBA/2307.html:text/html;Full Text PDF:/Users/tom/Zotero/storage/RMHBT5X4/Chen et al. - 2023 - How is ChatGPT's behavior changing over time.pdf:application/pdf},
}

@article{marshall_toward_2019,
	title = {Toward systematic review automation: a practical guide to using machine learning tools in research synthesis},
	volume = {8},
	issn = {2046-4053},
	url = {https://doi.org/10.1186/s13643-019-1074-9},
	doi = {10.1186/s13643-019-1074-9},
	shorttitle = {Toward systematic review automation},
	abstract = {Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. However, how these technologies work in practice and when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an overview of current machine learning methods that have been proposed to expedite evidence synthesis. We also offer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review team might go about using them in practice.},
	pages = {163},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Systematic Reviews},
	author = {Marshall, Iain J. and Wallace, Byron C.},
	urldate = {2023-07-19},
	date = {2019-07-11},
	keywords = {Machine learning, Evidence synthesis, Natural language processing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/EZA2STEX/Marshall and Wallace - 2019 - Toward systematic review automation a practical g.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/CXLMVVX6/s13643-019-1074-9.html:text/html;Toward systematic review automation\: a practical guide to using machine learning tools in research synthesis:/Users/tom/Zotero/storage/SQVEVEKR/10.1186@s13643-019-1074-9.pdf.pdf:application/pdf},
}

@article{ferguson_comment_2014,
	title = {Comment: Why Meta-Analyses Rarely Resolve Ideological Debates},
	volume = {6},
	issn = {1754-0739},
	url = {https://doi.org/10.1177/1754073914523046},
	doi = {10.1177/1754073914523046},
	shorttitle = {Comment},
	abstract = {In their meta-analysis Wood, Kressel, Joshi, and Louie (2014) argue little evidence supports shifts in mating preferences across the menstrual cycle. They imply this may represent a critical weakness of evolutionary psychology theories of mating preferences more generally. This report represents a fairly common use of meta-analysis: to assemble data to support or reject a particular proposition over which there is debate. Yet, rarely do meta-analyses succeed at resolving ideological debates. Multiple decision points related to the selection, coding, effect size extraction, and interpretation of studies leaves considerable room for meta-analytic authors to interject their own beliefs. Meta-analyses are typically hailed by those who agreed a priori with their conclusion, and rejected as fatally flawed by those in disagreement. As such, meta-analyses have failed in replacing narrative reviews as more objective.},
	pages = {251--252},
	number = {3},
	journaltitle = {Emotion Review},
	author = {Ferguson, Christopher J.},
	urldate = {2023-07-20},
	date = {2014-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
	file = {Comment\: Why Meta-Analyses Rarely Resolve Ideological Debates:/Users/tom/Zotero/storage/4UQS4BXU/ferguson2014.pdf.pdf:application/pdf},
}

@misc{corker_strengths_2018,
	title = {Strengths and Weaknesses of Meta-Analyses},
	url = {https://psyarxiv.com/6gcnm/},
	doi = {10.31234/osf.io/6gcnm},
	abstract = {The current chapter briefly reviews the history of meta-analysis before turning to an assessment of its strengths and weaknesses, as well as considerations of alternatives to and modifications of traditional meta-analysis. It closes with recommendations to make meta-analyses more informative.},
	publisher = {{PsyArXiv}},
	author = {Corker, Katherine S.},
	urldate = {2023-07-20},
	date = {2018-08-10},
	langid = {english},
	keywords = {Meta-science, questionable research practices, Social and Behavioral Sciences, Quantitative Methods, publication bias, research methods, Statistical Methods, meta-analysis, systematic review, evidence synthesis, Quantitative Psychology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/QI6FI679/Corker - 2018 - Strengths and Weaknesses of Meta-Analyses.pdf:application/pdf},
}

@article{hedges_early_2015,
	title = {The early history of meta-analysis},
	volume = {6},
	rights = {Copyright © 2015 John Wiley \& Sons, Ltd.},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1149},
	doi = {10.1002/jrsm.1149},
	pages = {284--286},
	number = {3},
	journaltitle = {Research Synthesis Methods},
	author = {Hedges, Larry V.},
	urldate = {2023-07-20},
	date = {2015},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1149},
	file = {Snapshot:/Users/tom/Zotero/storage/SNSI6GBG/jrsm.html:text/html;The early history of meta-analysis:/Users/tom/Zotero/storage/TBH9QHF7/hedges2015.pdf.pdf:application/pdf},
}

@article{rosenthal_meta-analytic_1986,
	title = {Meta-analytic procedures and the nature of replication: The ganzfeld debate},
	volume = {50},
	pages = {315--336},
	number = {4},
	journaltitle = {The Journal of Parapsychology},
	author = {Rosenthal, Robert},
	date = {1986},
	file = {Meta-Analytic_Procedures_and_t.pdf:/Users/tom/Zotero/storage/2558EGMH/Meta-Analytic_Procedures_and_t.pdf:application/pdf},
}

@article{shadish_meta-analytic_2015,
	title = {The meta-analytic big bang},
	volume = {6},
	rights = {Copyright © 2015 John Wiley \& Sons, Ltd.},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1132},
	doi = {10.1002/jrsm.1132},
	abstract = {This article looks at the impact of meta-analysis and then explores why meta-analysis was developed at the time and by the scholars it did in the social sciences in the 1970s. For the first problem, impact, it examines the impact of meta-analysis using citation network analysis. The impact is seen in the sciences, arts and humanities, and on such contemporaneous developments as multilevel modeling, medical statistics, qualitative methods, program evaluation, and single-case design. Using a constrained snowball sample of citations, we highlight key articles that are either most highly cited or most central to the systematic review network. Then, the article examines why meta-analysis came to be in the 1970s in the social sciences through the work of Gene Glass, Robert Rosenthal, and Frank Schmidt, each of whom developed similar theories of meta-analysis at about the same time. The article ends by explaining how Simonton's chance configuration theory and Campbell's evolutionary epistemology can illuminate why meta-analysis occurred with these scholars when it did and not in medical sciences. Copyright © 2015 John Wiley \& Sons, Ltd.},
	pages = {246--264},
	number = {3},
	journaltitle = {Research Synthesis Methods},
	author = {Shadish, William R. and Lecy, Jesse D.},
	urldate = {2023-07-20},
	date = {2015},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1132},
	file = {Snapshot:/Users/tom/Zotero/storage/IEWPHL8U/jrsm.html:text/html;The meta-analytic big bang:/Users/tom/Zotero/storage/6P2BXJJH/shadish2015.pdf.pdf:application/pdf},
}

@article{ioannidis_inverse_2023,
	title = {Inverse publication reporting bias favouring null, negative results},
	rights = {© Author(s) (or their employer(s)) 2023. No commercial re-use. See rights and permissions. Published by {BMJ}.},
	issn = {2515-446X, 2515-4478},
	url = {https://ebm.bmj.com/content/early/2023/06/13/bmjebm-2023-112292},
	doi = {10.1136/bmjebm-2023-112292},
	abstract = {\#\#\# {WHAT} {IS} {ALREADY} {KNOWN} {ON} {THIS} {TOPIC}

\#\#\#\# {WHAT} {THIS} {STUDY} {ADDS}

\#\#\#\# {HOW} {THIS} {STUDY} {MIGHT} {AFFECT} {RESEARCH}, {PRACTICE} {OR} {POLICY}

Classic publication (‘file drawer’) bias and related reporting biases threaten the validity of the scientific literature. Theoretical considerations and empirical data have demonstrated that in many fields and settings, there is preference for publishing studies with statistically significant results and/or larger effect sizes; moreover, among published studies, there is a preference for reporting outcomes and analyses with similarly biased profile.1 2 However, sometimes a preference may exist for reporting studies, outcomes and/or analyses with non-statistically significant, ‘null’ results and/or smaller effects sizes. This could manifest as suppression of the publication of entire studies with significant results (the inverse of the classic file drawer problem); suppression of the reporting of specific unfavourable outcomes and analyses; or manipulation/alteration of the undesirable statistically significant studies or of specific outcomes and analyses. Besides numerical data being suppressed and/or distorted, biased interpretation may add another layer: highlighting preferentially non-significant and/or smaller effects and making and disseminating conclusions disproportionately favouring the null.

For example, the reporting of the Merck-sponsored {VIGOR} trial attenuated the significantly increased cardiovascular harm of rofecoxib (Vioxx).3 The {VIGOR} publication reported an interim analysis with different termination dates for cardiovascular and gastrointestinal events, thus several rofecoxib cardiovascular events were not counted. The harm was further minimised by a post hoc subgroup analysis. The trial also added spin in …},
	journaltitle = {{BMJ} Evidence-Based Medicine},
	author = {Ioannidis, John P. A.},
	urldate = {2023-07-20},
	date = {2023-06-14},
	langid = {english},
	pmid = {37315987},
	note = {Publisher: Royal Society of Medicine
Section: Analysis},
	keywords = {Publishing, Evidence-Based Practice, Information Science},
}

@article{grossmann_ai_2023,
	title = {{AI} and the transformation of social science research},
	volume = {380},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.adi1778},
	doi = {10.1126/science.adi1778},
	abstract = {Careful bias management and data fidelity are key
          , 
            
              Advances in artificial intelligence ({AI}), particularly large language models ({LLMs}), are substantially affecting social science research. These transformer-based machine-learning models pretrained on vast amounts of text data are increasingly capable of simulating human-like responses and behaviors (
              
                1
              
              ,
              
                2
              
              ), offering opportunities to test theories and hypotheses about human behavior at great scale and speed. This presents urgent challenges: How can social science research practices be adapted, even reinvented, to harness the power of foundational {AI}? And how can this be done while ensuring transparent and replicable research?},
	pages = {1108--1109},
	number = {6650},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Grossmann, Igor and Feinberg, Matthew and Parker, Dawn C. and Christakis, Nicholas A. and Tetlock, Philip E. and Cunningham, William A.},
	urldate = {2023-07-20},
	date = {2023-06-16},
	langid = {english},
}

@article{marks-anglin_historical_2020,
	title = {A historical review of publication bias},
	volume = {11},
	rights = {© 2020 John Wiley \& Sons Ltd},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1452},
	doi = {10.1002/jrsm.1452},
	abstract = {Publication bias is a well-known threat to the validity of meta-analyses and, more broadly, the reproducibility of scientific findings. When policies and recommendations are predicated on an incomplete evidence base, it undermines the goals of evidence-based decision-making. Great strides have been made in the last 50 years to understand and address this problem, including calls for mandatory trial registration and the development of statistical methods to detect and correct for publication bias. We offer an historical account of seminal contributions by the evidence synthesis community, with an emphasis on the parallel development of graph-based and selection model approaches. We also draw attention to current innovations and opportunities for future methodological work.},
	pages = {725--742},
	number = {6},
	journaltitle = {Research Synthesis Methods},
	author = {Marks-Anglin, Arielle and Chen, Yong},
	urldate = {2023-07-20},
	date = {2020},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1452},
	keywords = {reproducibility, publication bias, meta-analysis, evidence-based medicine, selection bias},
	file = {A historical review of publication bias:/Users/tom/Zotero/storage/C9782FTY/10.1002@jrsm.1452.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/I5H4PC4I/jrsm.html:text/html;Submitted Version:/Users/tom/Zotero/storage/6TUXWFCG/Marks-Anglin and Chen - 2020 - A historical review of publication bias.pdf:application/pdf},
}

@incollection{campbell_ethnocentrism_1969,
	location = {New York},
	title = {Ethnocentrism of disciplines andthe fish-scale model of omniscience},
	isbn = {978-0-203-78836-3},
	booktitle = {Interdisciplinary Relationships in the Social Sciences},
	publisher = {Routledge},
	author = {Campbell, Donald T},
	editor = {Sherif, Muzafer and Sherif, Carolyn W},
	date = {1969},
	langid = {english},
	file = {Campbell - 1969 - Ethnocentrism of disciplines andthe fish-scale mod.pdf:/Users/tom/Zotero/storage/FEZJLSFU/Campbell - 1969 - Ethnocentrism of disciplines andthe fish-scale mod.pdf:application/pdf},
}

@article{shaver_problems_nodate,
	title = {Problems facing campbell's "experimenting society"},
	author = {Shaver, Phillip and Staines, Graham},
	langid = {english},
	file = {Shaver and Staines - Problems Facing Campbell's Experimenting Society.pdf:/Users/tom/Zotero/storage/YZZVI3D8/Shaver and Staines - Problems Facing Campbell's Experimenting Society.pdf:application/pdf},
}

@article{neimeyer_optimizing_nodate,
	title = {Optimizing scientific validity},
	author = {Neimeyer, Robert A and Shadish, William R},
	langid = {english},
	file = {Neimeyer and Shadish - Optimizing Scientific Validity.pdf:/Users/tom/Zotero/storage/KLC3K7VQ/Neimeyer and Shadish - Optimizing Scientific Validity.pdf:application/pdf},
}

@article{van_den_akker_how_2023,
	title = {How do psychology researchers interpret the results of multiple replication studies?},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-022-02235-5},
	doi = {10.3758/s13423-022-02235-5},
	abstract = {Employing two vignette studies, we examined how psychology researchers interpret the results of a set of four experiments that all test a given theory. In both studies, we found that participants’ belief in the theory increased with the number of statistically significant results, and that the result of a direct replication had a stronger effect on belief in the theory than the result of a conceptual replication. In Study 2, we additionally found that participants’ belief in the theory was lower when they assumed the presence of p-hacking, but that belief in the theory did not differ between preregistered and non-preregistered replication studies. In analyses of individual participant data from both studies, we examined the heuristics academics use to interpret the results of four experiments. Only a small proportion (Study 1: 1.6\%; Study 2: 2.2\%) of participants used the normative method of Bayesian inference, whereas many of the participants’ responses were in line with generally dismissed and problematic vote-counting approaches. Our studies demonstrate that many psychology researchers overestimate the evidence in favor of a theory if one or more results from a set of replication studies are statistically significant, highlighting the need for better statistical education.},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {van den Akker, Olmo R. and Wicherts, Jelte M. and Alvarez, Linda Dominguez and Bakker, Marjan and van Assen, Marcel A. L. M.},
	urldate = {2023-07-24},
	date = {2023-01-12},
	langid = {english},
	keywords = {Replication, Bayesian inference, Vote counting, Heuristics, Multi-study paper, Statistical misinterpretation},
	file = {Full Text PDF:/Users/tom/Zotero/storage/7352GGYW/van den Akker et al. - 2023 - How do psychology researchers interpret the result.pdf:application/pdf},
}

@article{armstrong_advocacy_1979,
	title = {Advocacy and objectivity in science},
	volume = {25},
	issn = {0025-1909},
	url = {https://www.jstor.org/stable/2630273},
	abstract = {Three strategies for scientific research in management are examined: advocacy, induction, and multiple hypotheses. Advocacy of a single dominant hypothesis is efficient, but biased. Induction is not biased, but it is inefficient. The multiple hypotheses strategy seems to be both efficient and unbiased. Despite its apparent lack of objectivity, most management scientists use advocacy. For example, 2/3 of the papers published in a sampling of issues of Management Science (1955-1976) used advocacy. A review of the published empirical evidence indicates that advocacy reduces the objectivity of the scientists. No evidence was found to suggest that this lack of objectivity could be overcome by a "marketplace for ideas" (i.e., publication for peer review). It is recommended that the method of multiple hypotheses be used.},
	pages = {423--428},
	number = {5},
	journaltitle = {Management Science},
	author = {Armstrong, J. Scott},
	urldate = {2023-07-24},
	date = {1979},
	note = {Publisher: {INFORMS}},
	file = {JSTOR Full Text PDF:/Users/tom/Zotero/storage/PAFG6EZH/Armstrong - 1979 - Advocacy and Objectivity in Science.pdf:application/pdf},
}

@article{mitroff_myth_1972,
	title = {The Myth of Objectivity or Why Science Needs a New Psychology of Science},
	volume = {18},
	issn = {0025-1909},
	url = {https://www.jstor.org/stable/2629184},
	pages = {B613--B618},
	number = {10},
	journaltitle = {Management Science},
	author = {Mitroff, Ian I.},
	urldate = {2023-07-24},
	date = {1972},
	note = {Publisher: {INFORMS}},
	file = {JSTOR Full Text PDF:/Users/tom/Zotero/storage/R32KJ9FY/Mitroff - 1972 - The Myth of Objectivity or Why Science Needs a New.pdf:application/pdf},
}

@article{boffey_experiment_1976,
	title = {Experiment Planned to Test Feasibility of a "Science Court"},
	volume = {193},
	url = {https://www.science.org/doi/10.1126/science.193.4248.129},
	doi = {10.1126/science.193.4248.129},
	pages = {129--129},
	number = {4248},
	journaltitle = {Science},
	author = {Boffey, Philip M.},
	urldate = {2023-07-24},
	date = {1976-07-09},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Experiment Planned to Test Feasibility of a "Science Court":/Users/tom/Zotero/storage/4C6L22DX/boffey1976.pdf.pdf:application/pdf},
}

@article{barber_resistance_1961,
	title = {Resistance by scientists to scientific discovery},
	volume = {134},
	url = {https://www.science.org/doi/10.1126/science.134.3479.596},
	doi = {10.1126/science.134.3479.596},
	pages = {596--602},
	number = {3479},
	journaltitle = {Science},
	author = {Barber, Bernard},
	urldate = {2023-07-24},
	date = {1961-09},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Resistance by Scientists to Scientific Discovery:/Users/tom/Zotero/storage/S24S96PM/barber1961.pdf.pdf:application/pdf},
}

@misc{linden_publication_2023,
	title = {Publication Bias in Psychology: A Closer Look at the Correlation Between Sample Size and Effect Size},
	url = {https://psyarxiv.com/s4znd/},
	doi = {10.31234/osf.io/s4znd},
	shorttitle = {Publication Bias in Psychology},
	abstract = {Previously observed negative correlations between sample size and effect size (n-{ES} correlation) in psychological research have been interpreted as evidence for publication bias and related undesirable biases. Here, we present two studies aimed at better understanding to what extent negative n-{ES} correlations reflect such biases or might be explained by unproblematic adjustments of sample size to expected effect sizes. In Study 1, we analysed n-{ES} correlations in 150 meta-analyses from cognitive, organizational, and social psychology and in 57 multiple replications, which are free from relevant biases. In Study 2, we used a random sample of 160 psychology papers to compare the n-{ES} correlation for effects that are central to these papers and effects selected at random from these papers. n-{ES} correlations proved inconspicuous in meta-analyses. In line with previous research, they do not suggest that publication bias and related biases have a strong impact on meta-analyses in psychology. A much higher n-{ES} correlation emerged for publications’ focal effects. To what extent this should be attributed to publication bias and related biases remains unclear.},
	publisher = {{PsyArXiv}},
	author = {Linden, Audrey and Pollet, Thomas V. and Hönekopp, Johannes},
	urldate = {2023-07-24},
	date = {2023-07-21},
	langid = {english},
	keywords = {Social and Behavioral Sciences, Quantitative Methods, sample size, meta-analysis, effect size, Quantitative Psychology, publication selections bias},
	file = {Full Text PDF:/Users/tom/Zotero/storage/XVNDRPFA/Linden et al. - 2023 - Publication Bias in Psychology A Closer Look at t.pdf:application/pdf},
}

@article{hernan_selection_2023,
	title = {Selection bias due to conditioning on a collider},
	volume = {381},
	rights = {© Author(s) (or their employer(s)) 2019. Re-use permitted under {CC} {BY}-{NC}. No commercial re-use. See rights and permissions. Published by {BMJ}.. http://creativecommons.org/licenses/by-nc/4.0/This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/381/bmj.p1135},
	doi = {10.1136/bmj.p1135},
	abstract = {{\textless}p{\textgreater}Effect estimates may be biased when the study design or the data analysis is conditional on a collider—a variable that is caused by two other variables. Causal directed acyclic graphs are a helpful tool to identify colliders that may introduce selection bias in observational research.{\textless}/p{\textgreater}},
	pages = {p1135},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Hernán, Miguel A. and Monge, Susana},
	urldate = {2023-07-24},
	date = {2023-06-07},
	langid = {english},
	pmid = {37286200},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NNPR46QG/Hernán and Monge - 2023 - Selection bias due to conditioning on a collider.pdf:application/pdf},
}

@article{james_methodology_2016-1,
	title = {A methodology for systematic mapping in environmental sciences},
	volume = {5},
	issn = {2047-2382},
	url = {https://doi.org/10.1186/s13750-016-0059-6},
	doi = {10.1186/s13750-016-0059-6},
	abstract = {Systematic mapping was developed in social sciences in response to a lack of empirical data when answering questions using systematic review methods, and a need for a method to describe the literature across a broad subject of interest. Systematic mapping does not attempt to answer a specific question as do systematic reviews, but instead collates, describes and catalogues available evidence (e.g. primary, secondary, theoretical, economic) relating to a topic or question of interest. The included studies can be used to identify evidence for policy-relevant questions, knowledge gaps (to help direct future primary research) and knowledge clusters (sub-sets of evidence that may be suitable for secondary research, for example systematic review). Evidence synthesis in environmental sciences faces similar challenges to those found in social sciences. Here we describe the translation of systematic mapping methodology from social sciences for use in environmental sciences. We provide the first process-based methodology for systematic maps, describing the stages involved: establishing the review team and engaging stakeholders; setting the scope and question; setting inclusion criteria for studies; scoping stage; protocol development and publication; searching for evidence; screening evidence; coding; production of a systematic map database; critical appraisal (optional); describing and visualising the findings; report production and supporting information. We discuss the similarities and differences in methodology between systematic review and systematic mapping and provide guidance for those choosing which type of synthesis is most suitable for their requirements. Furthermore, we discuss the merits and uses of systematic mapping and make recommendations for improving this evolving methodology in environmental sciences.},
	pages = {7},
	number = {1},
	journaltitle = {Environmental Evidence},
	shortjournal = {Environmental Evidence},
	author = {James, Katy L. and Randall, Nicola P. and Haddaway, Neal R.},
	urldate = {2023-07-24},
	date = {2016-04-26},
	keywords = {Evidence review, Evidence-based environmental management, Knowledge clusters, Knowledge gaps, Systematic evidence synthesis, Systematic mapping},
	file = {A methodology for systematic mapping in environmental sciences:/Users/tom/Zotero/storage/XVL8U2N9/james2016.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/KX562PZ8/James et al. - 2016 - A methodology for systematic mapping in environmen.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/F78KZVUL/s13750-016-0059-6.html:text/html},
}

@article{haddaway_systematic_2016,
	title = {Systematic reviews: Separating fact from fiction},
	volume = {92-93},
	issn = {0160-4120},
	url = {https://www.sciencedirect.com/science/article/pii/S0160412015300179},
	doi = {10.1016/j.envint.2015.07.011},
	shorttitle = {Systematic reviews},
	abstract = {The volume of scientific literature continues to expand and decision-makers are faced with increasingly unmanageable volumes of evidence to assess. Systematic reviews ({SRs}) are powerful tools that aim to provide comprehensive, transparent, reproducible and updateable summaries of evidence. {SR} methods were developed, and have been employed, in healthcare for more than two decades, and they are now widely used across a broad range of topics, including environmental management and social interventions in crime and justice, education, international development, and social welfare. Despite these successes and the increasing acceptance of {SR} methods as a ‘gold standard’ in evidence-informed policy and practice, misconceptions still remain regarding their applicability. The aim of this article is to separate fact from fiction, addressing twelve common misconceptions that can influence the decision as to whether a {SR} is the most appropriate method for evidence synthesis for a given topic. Through examples, we illustrate the flexibility of {SR} methods and demonstrate their suitability for addressing issues on environmental health and chemical risk assessment.},
	pages = {578--584},
	journaltitle = {Environment International},
	shortjournal = {Environment International},
	author = {Haddaway, Neal R. and Bilotta, Gary S.},
	urldate = {2023-07-24},
	date = {2016-07-01},
	langid = {english},
	keywords = {Systematic review, Evidence-based medicine, Evidence synthesis, Evidence-based practice, Evidence reviews},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/U96IM5ZT/S0160412015300179.html:text/html;Systematic reviews\: Separating fact from fiction:/Users/tom/Zotero/storage/WGZTDY63/haddaway2015.pdf.pdf:application/pdf},
}

@incollection{meehl_whats_1991,
	location = {Minneapolis},
	title = {What's Wrong with Psychology Anyway},
	isbn = {978-0-8166-1918-4 978-0-8166-1891-0},
	booktitle = {Thinking clearly about psychology: essays in honor of Paul E. Meehl},
	publisher = {University of Minnesota Press},
	author = {Lykken, David T.},
	editor = {Meehl, Paul E. and Cicchetti, Dante and Grove, William M.},
	date = {1991},
	langid = {english},
	keywords = {Psychology, Psychology, Clinical},
	file = {Meehl et al. - 1991 - Thinking clearly about psychology essays in honor.pdf:/Users/tom/Zotero/storage/6N7JYCZD/Meehl et al. - 1991 - Thinking clearly about psychology essays in honor.pdf:application/pdf},
}

@article{oakes_alleged_1975,
	title = {On The Alleged Falsity of the Null Hypothesis},
	volume = {25},
	issn = {2163-3452},
	url = {https://doi.org/10.1007/BF03394312},
	doi = {10.1007/BF03394312},
	abstract = {Consideration is given to the contention by Bakan, Meehl, Nunnally, and others that the null hypothesis in behavioral research is generally false in nature and that if the N is large enough, it will always be rejected. A distinction is made between self-selected-groups research designs and true experiments, and it is suggested that the null hypothesis probably is generally false in the case of research involving the former design, but is not in the case of research involving the latter. Reasons for the falsity of the null hypothesis in the one case but not in the other are suggested.},
	pages = {265--272},
	number = {2},
	journaltitle = {The Psychological Record},
	shortjournal = {Psychol Rec},
	author = {Oakes, William F.},
	urldate = {2023-07-24},
	date = {1975-04-01},
	langid = {english},
	file = {On The Alleged Falsity of the Null Hypothesis:/Users/tom/Zotero/storage/CMXRFFXM/oakes1975.pdf.pdf:application/pdf},
}

@article{collins_construction_1979,
	title = {The Construction of the Paranormal: Nothing Unscientific is Happening},
	volume = {27},
	issn = {0038-0261},
	url = {https://doi.org/10.1111/j.1467-954X.1979.tb00064.x},
	doi = {10.1111/j.1467-954X.1979.tb00064.x},
	shorttitle = {The Construction of the Paranormal},
	pages = {237--270},
	number = {1},
	journaltitle = {The Sociological Review},
	author = {Collins, H. M. and Pinch, T. J.},
	urldate = {2023-07-24},
	date = {1979-05-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd},
	file = {The Construction of the Paranormal\: Nothing Unscientific is Happening:/Users/tom/Zotero/storage/S48MUJ8G/collins1979.pdf.pdf:application/pdf},
}

@article{boring_present_1955,
	title = {The present status of parapsychology},
	volume = {43},
	issn = {0003-0996},
	url = {https://www.jstor.org/stable/27826593},
	pages = {108--117},
	number = {1},
	journaltitle = {American Scientist},
	author = {Boring, Edwin G.},
	urldate = {2023-07-24},
	date = {1955},
	note = {Publisher: Sigma Xi, The Scientific Research Society},
	file = {JSTOR Full Text PDF:/Users/tom/Zotero/storage/L54NJKKF/Boring - 1955 - The Present Status of Parapsychology.pdf:application/pdf},
}

@article{reber_searching_2020,
	title = {Searching for the impossible: Parapsychology’s elusive quest.},
	volume = {75},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000486},
	doi = {10.1037/amp0000486},
	shorttitle = {Searching for the impossible},
	abstract = {Recently, American Psychologist published a review of the evidence for parapsychology that supported the general claims of psi (the umbrella term often used for anomalous or paranormal phenomena). We present an opposing perspective and a broad-based critique of the entire parapsychology enterprise. Our position is straightforward. Claims made by parapsychologists cannot be true. The effects reported can have no ontological status; the data have no existential value. We examine a variety of reasons for this conclusion based on wellunderstood scientific principles. In the classic English adynaton, “pigs cannot fly.” Hence, data that suggest that they can are necessarily flawed and result from weak methodology or improper data analyses or are Type I errors. So it must be with psi effects. What we find particularly intriguing is that, despite the existential impossibility of psi phenomena and the nearly 150 years of efforts during which there has been, literally, no progress, there are still scientists who continue to embrace the pursuit.},
	pages = {391--399},
	number = {3},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Reber, Arthur S. and Alcock, James E.},
	urldate = {2023-07-26},
	date = {2020-04},
	langid = {english},
	file = {Reber and Alcock - 2020 - Searching for the impossible Parapsychology’s elu.pdf:/Users/tom/Zotero/storage/IF3W3H5H/Reber and Alcock - 2020 - Searching for the impossible Parapsychology’s elu.pdf:application/pdf},
}

@misc{frank_large_2023,
	title = {Large language models as models of human cognition},
	url = {https://psyarxiv.com/wxt69/},
	doi = {10.31234/osf.io/wxt69},
	abstract = {Can a large language model be used as a ‘cognitive model’, a scientific artifact that helps us understand the human mind? If {LLMs} can be made openly accessible to scientific investigation then they may provide a valuable model system for studying the emergence of language, reasoning, and other uniquely human behaviors.},
	publisher = {{PsyArXiv}},
	author = {Frank, Michael C.},
	urldate = {2023-07-26},
	date = {2023-07-26},
	langid = {english},
	keywords = {Social and Behavioral Sciences, Cognitive Psychology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/MSX9SV2P/Frank - 2023 - Large language models as models of human cognition.pdf:application/pdf},
}

@article{hyman_meta-analysis_2010,
	title = {Meta-analysis that conceals more than it reveals: comment on Storm et al. (2010)},
	volume = {136},
	issn = {1939-1455},
	doi = {10.1037/a0019676},
	shorttitle = {Meta-analysis that conceals more than it reveals},
	abstract = {Storm, Tressoldi, and Di Risio (2010) rely on meta-analyses to justify their claim that the evidence for psi is consistent and reliable. They manufacture apparent homogeneity and consistency by eliminating many outliers and combining databases whose combined effect sizes are not significantly different-even though these combined effect sizes consist of arbitrary and meaningless composites. At best, their study provides a recipe for conducting a replicable extrasensory perception experiment. This recipe includes following a design that employs the standard ganzfeld psi methodology and uses "selected" subjects. An experiment, having adequate power and that meets these criteria, has already been conducted and failed to produce evidence for psi. Parapsychology will achieve scientific acceptability only when it provides a positive theory with evidence based on independently replicable evidence. This is something it has yet to achieve after more than a century of trying.},
	pages = {486--490},
	number = {4},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychol Bull},
	author = {Hyman, Ray},
	date = {2010-07},
	pmid = {20565165},
	keywords = {Humans, Reproducibility of Results, Meta-Analysis as Topic, Research Design, Parapsychology},
	file = {Meta-analysis that conceals more than it reveals\: comment on Storm et al. (2010):/Users/tom/Zotero/storage/Q8C36SGK/hyman2010.pdf.pdf:application/pdf},
}

@article{kozlowski_disagreement_1992,
	title = {A disagreement about within-group agreement: Disentangling issues of consistency versus consensus.},
	volume = {77},
	issn = {1939-1854, 0021-9010},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0021-9010.77.2.161},
	doi = {10.1037/0021-9010.77.2.161},
	shorttitle = {A disagreement about within-group agreement},
	pages = {161--167},
	number = {2},
	journaltitle = {Journal of Applied Psychology},
	shortjournal = {Journal of Applied Psychology},
	author = {Kozlowski, Steve W. and Hattrup, Keith},
	urldate = {2023-07-28},
	date = {1992-04},
	langid = {english},
	file = {A disagreement about within-group agreement\: Disentangling issues of consistency versus consensus.:/Users/tom/Zotero/storage/GECNRCTL/kozlowski1992.pdf.pdf:application/pdf},
}

@book{lakatos_methodology_1978,
	location = {Cambridge},
	title = {The methodology of scientific research Programmes: Philosophical Papers Volume 1},
	publisher = {Cambridge University Press},
	author = {Lakatos, Imre},
	editor = {Worrall, John and Francis, Gregory},
	date = {1978},
	langid = {english},
	file = {Lakatos - T h e methodology of scientific research Programme.pdf:/Users/tom/Zotero/storage/MKHWECVP/Lakatos - T h e methodology of scientific research Programme.pdf:application/pdf},
}

@article{lakatos_criticism_1969,
	title = {Criticism and the Methodology of Scientific Research Programmes},
	volume = {6},
	url = {http://www.jstor.org/stable/4544774},
	pages = {149--186},
	journaltitle = {Proceedings of the Aristotelian Society, New Series},
	author = {Lakatos, Imre},
	date = {1969},
	langid = {english},
	file = {Lakatos - 1969 - Criticism and the Methodology of Scientific Resear.pdf:/Users/tom/Zotero/storage/L4NV5J4R/Lakatos - 1969 - Criticism and the Methodology of Scientific Resear.pdf:application/pdf},
}

@book{shapin_leviathan_2011,
	location = {Princeton, N.J},
	title = {Leviathan and the air-pump: Hobbes, Boyle, and the experimental life},
	isbn = {978-0-691-15020-8},
	shorttitle = {Leviathan and the air-pump},
	pagetotal = {391},
	publisher = {Princeton University Press},
	author = {Shapin, Steven and Schaffer, Simon},
	date = {2011},
	langid = {english},
	note = {{OCLC}: ocn694831592},
	keywords = {Science, History, Physics, 17th century, Air-pumps, Boyle, Robert, England, Experiments Philosophy, Hobbes, Thomas, Leviathan},
	file = {Shapin and Schaffer - 2011 - Leviathan and the air-pump Hobbes, Boyle, and the.pdf:/Users/tom/Zotero/storage/IIFYF5NI/Shapin and Schaffer - 2011 - Leviathan and the air-pump Hobbes, Boyle, and the.pdf:application/pdf},
}

@article{hardwicke_statistical_2023,
	title = {Statistical guidance to authors at top-ranked journals across scientific disciplines},
	volume = {77},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2022.2143897},
	doi = {10.1080/00031305.2022.2143897},
	abstract = {Scientific journals may counter the misuse, misreporting, and misinterpretation of statistics by providing guidance to authors. We described the nature and prevalence of statistical guidance at 15 journals (top-ranked by Impact Factor) in each of 22 scientific disciplines across five high-level domains (N = 330 journals). The frequency of statistical guidance varied across domains (Health \& Life Sciences: 122/165 journals, 74\%; Multidisciplinary: 9/15 journals, 60\%; Social Sciences: 8/30 journals, 27\%; Physical Sciences: 21/90 journals, 23\%; Formal Sciences: 0/30 journals, 0\%). In one discipline (Clinical Medicine), statistical guidance was provided by all examined journals and in two disciplines (Mathematics and Computer Science) no examined journals provided statistical guidance. Of the 160 journals providing statistical guidance, 93 had a dedicated statistics section in their author instructions. The most frequently mentioned topics were confidence intervals (90 journals) and p-values (88 journals). For six “hotly debated” topics (statistical significance, p-values, Bayesian statistics, effect sizes, confidence intervals, and sample size planning/justification) journals typically offered implicit or explicit endorsement and rarely provided opposition. The heterogeneity of statistical guidance provided by top-ranked journals within and between disciplines highlights a need for further research and debate about the role journals can play in improving statistical practice.},
	pages = {239--247},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Hardwicke, Tom E. and Salholz-Hillel, Maia and Malički, Mario and Szűcs, Dénes and Bendixen, Theiss and Ioannidis, John P. A.},
	urldate = {2023-07-31},
	date = {2023-07-03},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2022.2143897},
	keywords = {Statistics, Meta-research, Statistical guidelines, Author instructions, Journal policy},
	file = {Full Text PDF:/Users/tom/Zotero/storage/PD9RXCF8/Hardwicke et al. - 2023 - Statistical Guidance to Authors at Top-Ranked Jour.pdf:application/pdf},
}

@article{meehl_theoretical_1978,
	title = {Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology},
	volume = {46},
	issn = {1939-2117},
	doi = {10.1037/0022-006X.46.4.806},
	shorttitle = {Theoretical risks and tabular asterisks},
	abstract = {Theories in "soft" areas of psychology (e.g., clinical, counseling, social, personality, school, and community) lack the cumulative character of scientific knowledge because they tend neither to be refuted nor corroborated, but instead merely fade away as people lose interest. Even though intrinsic subject matter difficulties (20 are listed) contribute to this, the excessive reliance on significance testing is partly responsible (Ronald A. Fisher). Karl Popper's approach, with modifications, would be prophylactic. Since the null hypothesis is quasi-always false, tables summarizing research in terms of patterns of "significant differences" are little more than complex, causally uninterpretable outcomes of statistical power functions. Multiple paths to estimating numerical point values ("consistency tests") are better, even if approximate with rough tolerances; and lacking this, ranges, orderings, 2nd-order differences, curve peaks and valleys, and function forms should be used. Such methods are usual in developed sciences that seldom report statistical significance. Consistency tests of a conjectural taxometric model yielded 94\% success with no false negatives. (3 p ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {806--834},
	number = {4},
	journaltitle = {Journal of Consulting and Clinical Psychology},
	author = {Meehl, Paul E.},
	date = {1978},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Clinical Psychology, Personality, Statistical Analysis, Methodology, Social Psychology, Community Psychology, Counseling Psychology, Experimental Design, School Psychology},
	file = {Meehl - 1978 - Theoretical risks and tabular asterisks Sir Karl,.pdf:/Users/tom/Zotero/storage/J2SXED5A/Meehl - 1978 - Theoretical risks and tabular asterisks Sir Karl,.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/ENIXFUJB/doiLanding.html:text/html},
}

@article{meehl_appraising_1990,
	title = {Appraising and Amending Theories: The Strategy of Lakatosian Defense and Two Principles that Warrant It},
	volume = {1},
	issn = {1047-840X, 1532-7965},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s15327965pli0102_1},
	doi = {10.1207/s15327965pli0102_1},
	shorttitle = {Appraising and Amending Theories},
	pages = {108--141},
	number = {2},
	journaltitle = {Psychological Inquiry},
	shortjournal = {Psychological Inquiry},
	author = {Meehl, Paul E.},
	urldate = {2023-07-31},
	date = {1990-04},
	langid = {english},
	file = {Meehl - 1990 - Appraising and Amending Theories The Strategy of .pdf:/Users/tom/Zotero/storage/LZZPJKFB/Meehl - 1990 - Appraising and Amending Theories The Strategy of .pdf:application/pdf},
}

@article{quine_two_1951,
	title = {Two dogmas of empiricism},
	volume = {60},
	issn = {0031-8108},
	url = {https://www.jstor.org/stable/2181906},
	doi = {10.2307/2181906},
	shorttitle = {Main Trends in Recent Philosophy},
	pages = {20--43},
	number = {1},
	journaltitle = {The Philosophical Review},
	author = {Quine, W. V.},
	urldate = {2023-08-01},
	date = {1951},
	note = {Publisher: [Duke University Press, Philosophical Review]},
	file = {Main Trends in Recent Philosophy\: Two Dogmas of Empiricism:/Users/tom/Zotero/storage/G6ASTQKL/quine1951.pdf.pdf:application/pdf},
}

@book{laudan_progress_1978,
	location = {Berkeley, Calif.},
	edition = {1st paperback print},
	title = {Progress and its problems: towards a theory of scientific growth},
	isbn = {978-0-520-03721-2},
	shorttitle = {Progress and its problems},
	pagetotal = {257},
	publisher = {Univ. of Calif. Press},
	author = {Laudan, Larry},
	date = {1978},
	file = {Larry Laudan - Progress and Its Problems_ Towards a Theory of Scientific Growth-University of California Press (1978).pdf:/Users/tom/Zotero/storage/BTQ6EREK/Larry Laudan - Progress and Its Problems_ Towards a Theory of Scientific Growth-University of California Press (1978).pdf:application/pdf},
}

@article{hacking_reasons_1993,
	title = {Some reasons for not taking parapsychology very seriously},
	volume = {32},
	issn = {1759-0949, 0012-2173},
	url = {https://www.cambridge.org/core/journals/dialogue-canadian-philosophical-review-revue-canadienne-de-philosophie/article/some-reasons-for-not-taking-parapsychology-very-seriously/D7922CEC32C2873D8A5294155087ED74},
	doi = {10.1017/S0012217300012361},
	abstract = {Stephen Braude, a philosopher, believes that scientists, scholars and intellectuals ignore the wide range of evidence for psychic phenomena. They dismiss what is known and refuse to inquire further. He uses strong words such as “intellectual dishonesty and cowardice.” He means me and probably you. He made these allegations in his second book on parapsychology, The Limits of Influence, which is subtitled Psychokinesis and the Philosophy of Science. It was published in 1986. The editor of Dialogue thought that the charges had not been adequately discussed, and that the appearance of a paperbound edition would be a good occasion to treat them seriously.},
	pages = {587--594},
	number = {3},
	journaltitle = {Dialogue: Canadian Philosophical Review / Revue canadienne de philosophie},
	author = {Hacking, Ian},
	urldate = {2023-08-02},
	date = {1993-07},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Some Reasons for Not Taking Parapsychology Very Seriously:/Users/tom/Zotero/storage/MMQRQ557/hacking1993.pdf.pdf:application/pdf},
}

@book{pigliucci_nonsense_2018,
	title = {Nonsense on Stilts},
	isbn = {978-0-226-49599-6 978-0-226-49604-7},
	url = {http://www.bibliovault.org/BV.landing.epl?ISBN=9780226496047},
	publisher = {University of Chicago Press},
	author = {Pigliucci, Massimo},
	urldate = {2023-08-03},
	date = {2018},
	langid = {english},
	doi = {10.7208/chicago/9780226496047.001.0001},
	file = {Pigliucci - 2018 - Nonsense on Stilts.pdf:/Users/tom/Zotero/storage/2F6APXH9/Pigliucci - 2018 - Nonsense on Stilts.pdf:application/pdf},
}

@article{bond_meta-analysis_2003,
	title = {Meta-analysis of raw mean differences},
	volume = {8},
	issn = {1082-989X},
	doi = {10.1037/1082-989X.8.4.406},
	abstract = {This article discusses the meta-analysis of raw mean differences. It presents a rationale for cumulating psychological effects in a raw metric and compares raw mean differences to standardized mean differences. Some limitations of standardization are noted, and statistical techniques for raw meta-analysis are described. These include a graphical device for decomposing effect sizes. Several illustrative data sets are analyzed.},
	pages = {406--418},
	number = {4},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychol Methods},
	author = {Bond, Charles F. and Wiitala, Wyndy L. and Richard, F. Dan},
	date = {2003-12},
	pmid = {14664679},
	keywords = {Data Interpretation, Statistical, Humans, Reproducibility of Results, Randomized Controlled Trials as Topic, Psychometrics, Psychotherapy, Meta-Analysis as Topic},
	file = {Meta-analysis of raw mean differences:/Users/tom/Zotero/storage/KMVFXUA3/bond2003.pdf.pdf:application/pdf},
}

@article{glicksohn_anomaly_1998,
	title = {The anomaly of the anomalous},
	volume = {21},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X98211186/type/journal_article},
	doi = {10.1017/S0140525X98211186},
	abstract = {Pseudoscience is not an appropriate label for parapsychology. Although the noise reduction model of extrasensory perception ({ESP}) is explanatory only in a limited sense, research does exist addressing the correlation between {ESP} and altered states of consciousness ({ASCs}). The term anomaly is not appropriately applied to experiences such as out of body experiences ({OBEs}) per se, but only to the question of their source. Research on both topics should be encouraged.},
	pages = {301--302},
	number = {2},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Glicksohn, Joseph},
	urldate = {2023-08-05},
	date = {1998-04},
	langid = {english},
	file = {Glicksohn - 1998 - The anomaly of the anomalous.pdf:/Users/tom/Zotero/storage/T5ZGT9TT/Glicksohn - 1998 - The anomaly of the anomalous.pdf:application/pdf;The anomaly of the anomalous:/Users/tom/Zotero/storage/TTEYJNA7/glicksohn1998.pdf.pdf:application/pdf},
}

@article{judd_editorial_2011,
	title = {Editorial comment.},
	volume = {100},
	issn = {1939-1315, 0022-3514},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022789},
	doi = {10.1037/0022789},
	pages = {406--406},
	number = {3},
	journaltitle = {Journal of Personality and Social Psychology},
	shortjournal = {Journal of Personality and Social Psychology},
	author = {Judd, Charles M. and Gawronski, Bertram},
	urldate = {2023-08-05},
	date = {2011},
	langid = {english},
	file = {Editorial comment.:/Users/tom/Zotero/storage/TNPE6R8H/judd2011.pdf.pdf:application/pdf},
}

@article{meehl_compatibility_1956,
	title = {Compatibility of Science and {ESP}},
	volume = {123},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.123.3184.14},
	doi = {10.1126/science.123.3184.14},
	pages = {14--15},
	number = {3184},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Meehl, Paul E. and Scriven, Michael},
	urldate = {2023-08-05},
	date = {1956-01-06},
	langid = {english},
	file = {Compatibility of Science and ESP:/Users/tom/Zotero/storage/R32YBC79/meehl1956.pdf.pdf:application/pdf},
}

@article{nguyen_transparency_2022,
	title = {Transparency is Surveillance},
	volume = {105},
	rights = {© 2021 Philosophy and Phenomenological Research, Inc},
	issn = {1933-1592},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/phpr.12823},
	doi = {10.1111/phpr.12823},
	abstract = {In her {BBC} Reith Lectures on Trust, Onora O’Neill offers a short, but biting, criticism of transparency. People think that trust and transparency go together but in reality, says O'Neill, they are deeply opposed. Transparency forces people to conceal their actual reasons for action and invent different ones for public consumption. Transparency forces deception. I work out the details of her argument and worsen her conclusion. I focus on public transparency – that is, transparency to the public over expert domains. I offer two versions of the criticism. First, the epistemic intrusion argument: The drive to transparency forces experts to explain their reasoning to non-experts. But expert reasons are, by their nature, often inaccessible to non-experts. So the demand for transparency can pressure experts to act only in those ways for which they can offer public justification. Second, the intimate reasons argument: In many cases of practical deliberation, the relevant reasons are intimate to a community and not easily explicable to those who lack a particular shared background. The demand for transparency, then, pressures community members to abandon the special understanding and sensitivity that arises from their particular experiences. Transparency, it turns out, is a form of surveillance. By forcing reasoning into the explicit and public sphere, transparency roots out corruption — but it also inhibits the full application of expert skill, sensitivity, and subtle shared understandings. The difficulty here arises from the basic fact that human knowledge vastly outstrips any individual’s capacities. We all depend on experts, which makes us vulnerable to their biases and corruption. But if we try to wholly secure our trust — if we leash groups of experts to pursuing only the goals and taking only the actions that can be justified to the non-expert public — then we will undermine their expertise. We need both trust and transparency, but they are in essential tension. This is a deep practical dilemma; it admits of no neat resolution, but only painful compromise.},
	pages = {331--361},
	number = {2},
	journaltitle = {Philosophy and Phenomenological Research},
	author = {Nguyen, C. Thi},
	urldate = {2023-08-08},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/phpr.12823},
	file = {Snapshot:/Users/tom/Zotero/storage/45QACKUA/phpr.html:text/html;Submitted Version:/Users/tom/Zotero/storage/6HJJ4MQB/Nguyen - 2022 - Transparency is Surveillance.pdf:application/pdf;Transparency is Surveillance:/Users/tom/Zotero/storage/9M7YWTE7/nguyen2021.pdf.pdf:application/pdf},
}

@article{krahmer_care_2023,
	title = {Care to share? Experimental evidence on code sharing behavior in the social sciences},
	volume = {18},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0289380},
	doi = {10.1371/journal.pone.0289380},
	shorttitle = {Care to share?},
	abstract = {Transparency and peer control are cornerstones of good scientific practice and entail the replication and reproduction of findings. The feasibility of replications, however, hinges on the premise that original researchers make their data and research code publicly available. This applies in particular to large-N observational studies, where analysis code is complex and may involve several ambiguous analytical decisions. To investigate which specific factors influence researchers’ code sharing behavior upon request, we emailed code requests to 1,206 authors who published research articles based on data from the European Social Survey between 2015 and 2020. In this preregistered multifactorial field experiment, we randomly varied three aspects of our code request’s wording in a 2x4x2 factorial design: the overall framing of our request (enhancement of social science research, response to replication crisis), the appeal why researchers should share their code ({FAIR} principles, academic altruism, prospect of citation, no information), and the perceived effort associated with code sharing (no code cleaning required, no information). Overall, 37.5\% of successfully contacted authors supplied their analysis code. Of our experimental treatments, only framing affected researchers’ code sharing behavior, though in the opposite direction we expected: Scientists who received the negative wording alluding to the replication crisis were more likely to share their research code. Taken together, our results highlight that the availability of research code will hardly be enhanced by small-scale individual interventions but instead requires large-scale institutional norms.},
	pages = {e0289380},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Krähmer, Daniel and Schächtele, Laura and Schneck, Andreas},
	urldate = {2023-08-11},
	date = {2023-08-07},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open data, Research design, Social sciences, Experimental design, Computer software, Decision making, Social research, Altruistic behavior},
	file = {Full Text PDF:/Users/tom/Zotero/storage/AWJQ6JRQ/Krähmer et al. - 2023 - Care to share Experimental evidence on code shari.pdf:application/pdf},
}

@misc{carneiro_mapping_2022-1,
	title = {Mapping the content of comments on {bioRxiv} and {medRxiv} preprints},
	url = {http://biorxiv.org/lookup/doi/10.1101/2022.11.23.517621},
	doi = {10.1101/2022.11.23.517621},
	abstract = {Abstract
          
            Introduction
            Preprints have been increasingly used in biomedical sciences, providing the opportunity for research to be publicly assessed before journal publication. With the increase in attention over preprints during the {COVID}-19 pandemic, we decided to assess the content of comments left on preprint platforms.
          
          
            Methods
            Preprints posted on {bioRxiv} and {medRxiv} in 2020 were accessed through each platform’s {API}, and a random sample of preprints that had received between 1 and 20 comments was analyzed. Comments were evaluated in triplicate by independent evaluators using an instrument that assessed their features and general content.
          
          
            Results
            7.3\% of preprints received at least 1 comment during a mean follow-up of 7.5 months. Analyzed comments had a median size of 43 words. Criticisms, corrections or suggestions were the most prevalent type of content, followed by compliments or positive appraisals and questions. Most critical comments regarded interpretation, data collection and methodological design, while compliments were usually about relevance and implications.
          
          
            Conclusions
            Only a small percentage of preprints posted in 2020 in {bioRxiv} and {medRxiv} received comments in these platforms. When present, however, these comments address content that is similar to that analyzed by traditional peer review. A more precise taxonomy of peer review functions would be desirable to describe whether post-publication peer review fulfills these roles.},
	number = {{bioRxiv}},
	publisher = {Scientific Communication and Education},
	author = {Carneiro, Clarissa F. D. and Costa, Gabriel and Neves, Kleber and Abreu, Mariana B. and Tan, Pedro B. and Rayêe, Danielle and Boos, Flávia and Andrejew, Roberta and Lubiana, Tiago and Malički, Mario and Amaral, Olavo B.},
	urldate = {2023-08-14},
	date = {2022-11-24},
	langid = {english},
	file = {Mapping the content of comments on bioRxiv and med.pdf:/Users/tom/Zotero/storage/6GUUYCY8/Mapping the content of comments on bioRxiv and med.pdf:application/pdf},
}

@article{errington_investigating_2021,
	title = {Investigating the replicability of preclinical cancer biology},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.71601},
	doi = {10.7554/eLife.71601},
	abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary – the replication was either a success or a failure – and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
	pages = {e71601},
	journaltitle = {{eLife}},
	author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	editor = {Pasqualini, Renata and Franco, Eduardo},
	urldate = {2023-08-15},
	date = {2021-12-07},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {transparency, reproducibility, replication, meta-analysis, Reproducibility Project: Cancer Biology, credibility, reproducibility in cancer biology},
	file = {Full Text:/Users/tom/Zotero/storage/VBR3HG72/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf:application/pdf},
}

@article{mathur_challenges_2019-1,
	title = {Challenges and suggestions for defining replication “success” when effects may be heterogeneous: Comment on Hedges and Schauer (2019).},
	volume = {24},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000223},
	doi = {10.1037/met0000223},
	shorttitle = {Challenges and suggestions for defining replication “success” when effects may be heterogeneous},
	pages = {571--575},
	number = {5},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Mathur, Maya B. and {VanderWeele}, Tyler J.},
	urldate = {2023-08-15},
	date = {2019-10},
	langid = {english},
	file = {Accepted Version:/Users/tom/Zotero/storage/KLI9BJZP/Mathur and VanderWeele - 2019 - Challenges and suggestions for defining replicatio.pdf:application/pdf},
}

@article{vazire_credibility_2022,
	title = {Credibility beyond replicability: improving the four validities in psychological science},
	volume = {31},
	issn = {0963-7214},
	url = {https://doi.org/10.1177/09637214211067779},
	doi = {10.1177/09637214211067779},
	shorttitle = {Credibility beyond replicability},
	abstract = {Psychological science’s “credibility revolution” has produced an explosion of metascientific work on improving research practices. Although much attention has been paid to replicability (reducing false positives), improving credibility depends on addressing a wide range of problems afflicting psychological science, beyond simply making psychology research more replicable. Here we focus on the “four validities” and highlight recent developments—many of which have been led by early-career researchers—aimed at improving these four validities in psychology research. We propose that the credibility revolution in psychology, which has its roots in replicability, can be harnessed to improve psychology’s validity more broadly.},
	pages = {162--168},
	number = {2},
	journaltitle = {Current Directions in Psychological Science},
	shortjournal = {Curr Dir Psychol Sci},
	author = {Vazire, Simine and Schiavone, Sarah R. and Bottesini, Julia G.},
	urldate = {2023-08-16},
	date = {2022-04-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Submitted Version:/Users/tom/Zotero/storage/2TH86EQL/Vazire et al. - 2022 - Credibility Beyond Replicability Improving the Fo.pdf:application/pdf},
}

@article{juni_hazards_1999-1,
	title = {The hazards of scoring the quality of clinical trials for meta-analysis},
	volume = {282},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.282.11.1054},
	doi = {10.1001/jama.282.11.1054},
	abstract = {{ContextAlthough} it is widely recommended that clinical trials undergo some type of quality review, the number and variety of quality assessment scales
that exist make it unclear how to achieve the best assessment.{ObjectiveTo} determine whether the type of quality assessment scale used affects
the conclusions of meta-analytic studies.Design and {SettingMeta}-analysis of 17 trials comparing low-molecular-weight heparin ({LMWH})
with standard heparin for prevention of postoperative thrombosis using 25
different scales to identify high-quality trials. The association between
treatment effect and summary scores and the association with 3 key domains
(concealment of treatment allocation, blinding of outcome assessment, and
handling of withdrawals) were examined in regression models.Main Outcome {MeasurePooled} relative risks of deep vein thrombosis with {LMWH} vs standard
heparin in high-quality vs low-quality trials as determined by 25 quality
scales.{ResultsPooled} relative risks from high-quality trials ranged from 0.63 (95\%
confidence interval [{CI}], 0.44-0.90) to 0.90 (95\% {CI}, 0.67-1.21) vs 0.52 (95\%
{CI}, 0.24-1.09) to 1.13 (95\% {CI}, 0.70-1.82) for low-quality trials. For 6 scales,
relative risks of high-quality trials were close to unity, indicating that
{LMWH} was not significantly superior to standard heparin, whereas low-quality
trials showed better protection with {LMWH} (P\&lt;.05).
Seven scales showed the opposite: high quality trials showed an effect whereas
low quality trials did not. For the remaining 12 scales, effect estimates
were similar in the 2 quality strata. In regression analysis, summary quality
scores were not significantly associated with treatment effects. There was
no significant association of treatment effects with allocation concealment
and handling of withdrawals. Open outcome assessment, however, influenced
effect size with the effect of {LMWH}, on average, being exaggerated by 35\%
(95\% {CI}, 1\%-57\%; P=.046).{ConclusionsOur} data indicate that the use of summary scores to identify trials
of high quality is problematic. Relevant methodological aspects should be
assessed individually and their influence on effect sizes explored.},
	pages = {1054--1060},
	number = {11},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Jüni, Peter and Witschi, Anne and Bloch, Ralph and Egger, Matthias},
	urldate = {2023-08-16},
	date = {1999-09-15},
	file = {Snapshot:/Users/tom/Zotero/storage/XSR9T6N7/191652.html:text/html;The Hazards of Scoring the Quality of Clinical Trials for Meta-analysis:/Users/tom/Zotero/storage/QNJ973MM/10.1001@jama.282.11.1054.pdf.pdf:application/pdf},
}

@article{schulz_empirical_1995-1,
	title = {Empirical evidence of bias: dimensions of methodological quality associated with estimates of treatment effects in controlled trials},
	volume = {273},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.1995.03520290060030},
	doi = {10.1001/jama.1995.03520290060030},
	shorttitle = {Empirical evidence of bias},
	abstract = {Objective.—To determine if inadequate approaches to randomized controlled trial design and execution are associated with evidence of bias in estimating treatment effects.Design.—An observational study in which we assessed the methodological quality of 250 controlled trials from 33 meta-analyses and then analyzed, using multiple logistic regression models, the associations between those assessments and estimated treatment effects.Data Sources.—Meta-analyses from the Cochrane Pregnancy and Childbirth Database.Main Outcome Measures.—The associations between estimates of treatment effects and inadequate allocation concealment, exclusions after randomization, and lack of double-blinding.Results.—Compared with trials in which authors reported adequately concealed treatment allocation, trials in which concealment was either inadequate or unclear (did not report or incompletely reported a concealment approach) yielded larger estimates of treatment effects (P\&lt;.001). Odds ratios were exaggerated by 41\% for inadequately concealed trials and by 30\% for unclearly concealed trials (adjusted for other aspects of quality). Trials in which participants had been excluded after randomization did not yield larger estimates of effects, but that lack of association may be due to incomplete reporting. Trials that were not double-blind also yielded larger estimates of effects (P=.01), with odds ratios being exaggerated by 17\%.Conclusions.—This study provides empirical evidence that inadequate methodological approaches in controlled trials, particularly those representing poor allocation concealment, are associated with bias. Readers of trial reports should be wary of these pitfalls, and investigators must improve their design, execution, and reporting of trials.({JAMA}. 1995;273:408-412)},
	pages = {408--412},
	number = {5},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Schulz, Kenneth F. and Chalmers, Iain and Hayes, Richard J. and Altman, Douglas G.},
	urldate = {2023-08-16},
	date = {1995-02-01},
	file = {Empirical Evidence of Bias\: Dimensions of Methodological Quality Associated With Estimates of Treatment Effects in Controlled Trials:/Users/tom/Zotero/storage/LR39VG85/schulz1995.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/9FN73KLQ/386770.html:text/html},
}

@article{pigliucci_prove_2014,
	title = {Prove it! The burden of proof game in science vs. pseudoscience disputes},
	volume = {42},
	issn = {1574-9274},
	url = {https://doi.org/10.1007/s11406-013-9500-z},
	doi = {10.1007/s11406-013-9500-z},
	abstract = {The concept of burden of proof is used in a wide range of discourses, from philosophy to law, science, skepticism, and even in everyday reasoning. This paper provides an analysis of the proper deployment of burden of proof, focusing in particular on skeptical discussions of pseudoscience and the paranormal, where burden of proof assignments are most poignant and relatively clear-cut. We argue that burden of proof is often misapplied or used as a mere rhetorical gambit, with little appreciation of the underlying principles. The paper elaborates on an important distinction between evidential and prudential varieties of burdens of proof, which is cashed out in terms of Bayesian probabilities and error management theory. Finally, we explore the relationship between burden of proof and several (alleged) informal logical fallacies. This allows us to get a firmer grip on the concept and its applications in different domains, and also to clear up some confusions with regard to when exactly some fallacies (ad hominem, ad ignorantiam, and petitio principii) may or may not occur.},
	pages = {487--502},
	number = {2},
	journaltitle = {Philosophia},
	shortjournal = {Philosophia},
	author = {Pigliucci, Massimo and Boudry, Maarten},
	urldate = {2023-08-18},
	date = {2014-06-01},
	langid = {english},
	keywords = {Pseudoscience, Bayesian theory, Burden of proof, Logical fallacies},
	file = {Prove it! The Burden of Proof Game in Science vs. Pseudoscience Disputes:/Users/tom/Zotero/storage/58KH6TMA/pigliucci2013.pdf.pdf:application/pdf;Submitted Version:/Users/tom/Zotero/storage/WA82L6HH/Pigliucci and Boudry - 2014 - Prove it! The Burden of Proof Game in Science vs. .pdf:application/pdf},
}

@article{walton_burden_1988,
	title = {Burden of proof},
	volume = {2},
	issn = {1572-8374},
	url = {https://doi.org/10.1007/BF00178024},
	doi = {10.1007/BF00178024},
	abstract = {This paper presents an analysis of the concept of burden of proof in argument. Relationship of burden of proof to three traditional informal fallacies is considered: (i) argumentum ad hominem, (ii) petitio principii, and (iii) argumentum ad ignorantiam. Other topics discussed include persuasive dialoque, pragmatic reasoning, legal burden of proof, plausible reasoning in regulated disputes, rules of dialogue, and the value of reasoned dialogue.},
	pages = {233--254},
	number = {2},
	journaltitle = {Argumentation},
	shortjournal = {Argumentation},
	author = {Walton, Douglas N.},
	urldate = {2023-08-18},
	date = {1988-05-01},
	langid = {english},
	keywords = {Argumentation, dialogue, fallacies, informal logic, persuasion, pragmatics, presumption, rhetoric},
	file = {Burden of proof:/Users/tom/Zotero/storage/FTLWQVVX/walton1988.pdf.pdf:application/pdf},
}

@article{nguyen_transparency_2022-1,
	title = {Transparency is Surveillance},
	volume = {105},
	rights = {© 2021 Philosophy and Phenomenological Research, Inc},
	issn = {1933-1592},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/phpr.12823},
	doi = {10.1111/phpr.12823},
	abstract = {In her {BBC} Reith Lectures on Trust, Onora O’Neill offers a short, but biting, criticism of transparency. People think that trust and transparency go together but in reality, says O'Neill, they are deeply opposed. Transparency forces people to conceal their actual reasons for action and invent different ones for public consumption. Transparency forces deception. I work out the details of her argument and worsen her conclusion. I focus on public transparency – that is, transparency to the public over expert domains. I offer two versions of the criticism. First, the epistemic intrusion argument: The drive to transparency forces experts to explain their reasoning to non-experts. But expert reasons are, by their nature, often inaccessible to non-experts. So the demand for transparency can pressure experts to act only in those ways for which they can offer public justification. Second, the intimate reasons argument: In many cases of practical deliberation, the relevant reasons are intimate to a community and not easily explicable to those who lack a particular shared background. The demand for transparency, then, pressures community members to abandon the special understanding and sensitivity that arises from their particular experiences. Transparency, it turns out, is a form of surveillance. By forcing reasoning into the explicit and public sphere, transparency roots out corruption — but it also inhibits the full application of expert skill, sensitivity, and subtle shared understandings. The difficulty here arises from the basic fact that human knowledge vastly outstrips any individual’s capacities. We all depend on experts, which makes us vulnerable to their biases and corruption. But if we try to wholly secure our trust — if we leash groups of experts to pursuing only the goals and taking only the actions that can be justified to the non-expert public — then we will undermine their expertise. We need both trust and transparency, but they are in essential tension. This is a deep practical dilemma; it admits of no neat resolution, but only painful compromise.},
	pages = {331--361},
	number = {2},
	journaltitle = {Philosophy and Phenomenological Research},
	author = {Nguyen, C. Thi},
	urldate = {2023-08-20},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/phpr.12823},
	file = {Snapshot:/Users/tom/Zotero/storage/8WMEW67F/phpr.html:text/html;Submitted Version:/Users/tom/Zotero/storage/ZL8IY2KU/Nguyen - 2022 - Transparency is Surveillance.pdf:application/pdf;Transparency is Surveillance:/Users/tom/Zotero/storage/JRRD3N7D/nguyen2021.pdf.pdf:application/pdf},
}

@article{clarke_what_2023,
	title = {What limitations are reported in short articles in social and personality psychology?},
	issn = {1939-1315},
	doi = {10.1037/pspp0000458},
	abstract = {Every research project has limitations. The limitations that authors acknowledge in their articles offer a glimpse into some of the concerns that occupy a field’s attention. We examine the types of limitations authors discuss in their published articles by categorizing them according to the four validities framework and investigate whether the field’s attention to each of the four validities has shifted from 2010 to 2020. We selected one journal in social and personality psychology (Social Psychological and Personality Science; {SPPS}), the subfield most in the crosshairs of psychology’s replication crisis. We sampled 440 articles (with half of those articles containing a subsection explicitly addressing limitations), and we identified and categorized 831 limitations across the 440 articles. Articles with limitations sections reported more limitations than those without (avg. 2.6 vs. 1.2 limitations per article). Threats to external validity were the most common type of reported limitation (est. 52\% of articles), and threats to statistical conclusion validity were the least common (est. 17\% of articles). Authors reported slightly more limitations over time. Despite the extensive attention paid to statistical conclusion validity in the scientific discourse throughout psychology’s credibility revolution, our results suggest that concerns about statistics-related issues were not reflected in social and personality psychologists’ reported limitations. The high prevalence of limitations concerning external validity might suggest it is time that we improve our practices in this area, rather than apologizing for these limitations after the fact. ({PsycInfo} Database Record (c) 2023 {APA}, all rights reserved)},
	pages = {No Pagination Specified--No Pagination Specified},
	journaltitle = {Journal of Personality and Social Psychology},
	author = {Clarke, Beth and Schiavone, Sarah and Vazire, Simine},
	date = {2023},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Psychology, Personality, Experimentation, Methodology, Social Psychology, External Validity, Statistical Validity},
	file = {Clarke et al. - 2023 - What limitations are reported in short articles in.pdf:/Users/tom/Zotero/storage/HP3R6FX4/Clarke et al. - 2023 - What limitations are reported in short articles in.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/6KTMBV77/doiLanding.html:text/html},
}

@article{manago_preregistration_2023,
	title = {Preregistration and registered reports in sociology: strengths, weaknesses, and other considerations},
	volume = {54},
	issn = {1936-4784},
	url = {https://doi.org/10.1007/s12108-023-09563-6},
	doi = {10.1007/s12108-023-09563-6},
	shorttitle = {Preregistration and registered reports in sociology},
	abstract = {Both within and outside of sociology, there are conversations about methods to reduce error and improve research quality—one such method is preregistration and its counterpart, registered reports. Preregistration is the process of detailing research questions, variables, analysis plans, etc. before conducting research. Registered reports take this one step further, with a paper being reviewed on the merit of these plans, not its findings. In this manuscript, I detail preregistration’s and registered reports’ strengths and weaknesses for improving the quality of sociological research. I conclude by considering the implications of a structural-level adoption of preregistration and registered reports. Importantly, I do not recommend that all sociologists use preregistration and registered reports for all studies. Rather, I discuss the potential benefits and genuine limitations of preregistration and registered reports for the individual sociologist and the discipline.},
	pages = {193--210},
	number = {1},
	journaltitle = {The American Sociologist},
	shortjournal = {Am Soc},
	author = {Manago, Bianca},
	urldate = {2023-08-23},
	date = {2023-03-01},
	langid = {english},
	keywords = {Transparency, Open science, Reproducibility, Preregistration, Registered reports},
	file = {Full Text PDF:/Users/tom/Zotero/storage/RVUA9X75/Manago - 2023 - Preregistration and Registered Reports in Sociolog.pdf:application/pdf},
}

@article{millar_trends_2022-1,
	title = {Trends in the use of promotional language (hype) in abstracts of successful national institutes of health grant applications, 1985-2020},
	volume = {5},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2022.28676},
	doi = {10.1001/jamanetworkopen.2022.28676},
	abstract = {The integrity of the grant application process is important to the success of the entire research enterprise. However, little information is available concerning the prevalence and evolution of subjective or promotional language (“hype”) that has the potential to undermine objectivity in the writing and evaluation of grant applications.To assess changes over time in the use of hype in abstracts of National Institutes of Health ({NIH}) grant applications.This cross-sectional study assessed the prevalence of promotional adjectives in abstracts in the {NIH} archive from 1985 to 2020.From all abstracts in the {NIH} {RePORTER} (Research Portfolio Online Reporting Tools: Expenditures and Results) archive, adjectives were automatically extracted, and their frequencies in the most recent year (2020) were assessed relative to the start year (1985). Adjectives that shifted significantly in frequency and that carried a promotional sense (ie, hype) were retained, and patterns of change were assessed by plotting yearly frequencies (1985-2020). By grouping the adjectives based on shared semantic properties, broad meanings commonly expressed by hype were identified. Absolute change was measured as the difference in normalized frequency between 1985 and 2020. Relative change was measured as the percentage change in normalized frequency in 2020 relative to 1985, or the first year of occurrence.In total, 901 717 abstracts were analyzed and 139 adjective forms were identified as hype. Among these 139 adjective forms, 130 hype adjectives increased in frequency by 7690 words per million (wpm) (mean [{SD}] relative increase, 1378\% [3132\%]), while 9 hype adjectives decreased in frequency by 686 wpm (mean [{SD}] relative decrease, 44\% [18\%]). The largest absolute increases were for the terms novel (1054 wpm), critical (555 wpm), and key (461 wpm), while the largest relative increases were for the terms sustainable (25 157\%), actionable (16 114\%), and scalable (13 029\%). Hype most often serves to promote the significance, novelty, scale, and rigor of a project; the utility of the expected outcomes; the qualities of the investigators and research environment; and the gravity of the problem; as well as conveying the personal attitudes of the applicants.Levels of hype in successful {NIH} grant applications have increased over time from 1985 to 2020. The findings in this study should serve to sensitize applicants, reviewers, and funding agencies to the increasing prevalence of subjective, promotional language in funding applications.},
	pages = {e2228676},
	number = {8},
	journaltitle = {{JAMA} Network Open},
	shortjournal = {{JAMA} Network Open},
	author = {Millar, Neil and Batalo, Bojan and Budgell, Brian},
	urldate = {2023-08-26},
	date = {2022-08-25},
	file = {Full Text:/Users/tom/Zotero/storage/E5AXIZZZ/Millar et al. - 2022 - Trends in the Use of Promotional Language (Hype) i.pdf:application/pdf},
}

@article{khan_level_2019,
	title = {Level and prevalence of spin in published cardiovascular randomized clinical trial reports with statistically nonsignificant primary outcomes: a systematic review},
	volume = {2},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2019.2622},
	doi = {10.1001/jamanetworkopen.2019.2622},
	shorttitle = {Level and prevalence of spin in published cardiovascular randomized clinical trial reports with statistically nonsignificant primary outcomes},
	abstract = {Clinical researchers are obligated to present results objectively and accurately to ensure readers are not misled. In studies in which primary end points are not statistically significant, placing a spin, defined as the manipulation of language to potentially mislead readers from the likely truth of the results, can distract the reader and lead to misinterpretation and misapplication of the findings.To determine the level and prevalence of spin in published reports of cardiovascular randomized clinical trial ({RCT}) reports.{MEDLINE} was searched from January 1, 2015, to December 31, 2017, using the Cochrane highly sensitive search strategy.Inclusion criteria were parallel-group {RCTs} published from January 1, 2015, to December 31, 2017 in 1 of 6 high-impact journals (New England Journal of Medicine, The Lancet, {JAMA}, European Heart Journal, Circulation, and Journal of the American College of Cardiology) with primary outcomes that were not statistically significant were included in the analysis.Analysis began in August 2018. Data were extracted and verified by 2 independent investigators using a standard collection form. In cases of disagreement between the 2 investigators, a third investigators served as arbitrator.The classifications of spin type, severity, and extent were determined according to predefined criteria. Primary clinical outcomes were divided into safety of treatment, efficacy of treatment, and both.Of 587 studies identified, 93 {RCT} reports (15.8\%) met inclusion criteria. Spin was identified in 53 abstracts (57\%; 95\% {CI}, 47\%-67\%) and 62 main texts of published articles (67\%; 95\% {CI}, 57\%-75\%). Ten reports (11\%; 95\% {CI}, 6\%-19\%) had spin in the title, 35 reports (38\%; 95\% {CI}, 28\%-48\%) had spin in the results section, and 50 reports (54\%; 95\% {CI}, 44\%-64\%) had spin in the conclusions. Among the abstracts, spin was observed in 38 results sections (41\%; 95\% {CI}, 31\%-51\%) and 45 conclusions sections (48\%; 95\% {CI}, 38\%-58\%).This study suggests that in reports of cardiovascular {RCTs} with statistically nonsignificant primary outcomes, investigators often manipulate the language of the report to detract from the neutral primary outcomes. To best apply evidence to patient care, consumers of cardiovascular research should be aware that peer review does not always preclude the use of misleading language in scientific articles.},
	pages = {e192622},
	number = {5},
	journaltitle = {{JAMA} Network Open},
	shortjournal = {{JAMA} Network Open},
	author = {Khan, Muhammad Shahzeb and Lateef, Noman and Siddiqi, Tariq Jamal and Rehman, Karim Abdur and Alnaimat, Saed and Khan, Safi U. and Riaz, Haris and Murad, M. Hassan and Mandrola, John and Doukky, Rami and Krasuski, Richard A.},
	urldate = {2023-08-26},
	date = {2019-05-03},
	file = {Full Text:/Users/tom/Zotero/storage/IRD9974I/Khan et al. - 2019 - Level and Prevalence of Spin in Published Cardiova.pdf:application/pdf;Level and Prevalence of Spin in Published Cardiovascular Randomized Clinical Trial Reports With Statistically Nonsignificant Primary Outcomes\: A Systematic Review:/Users/tom/Zotero/storage/UYHE8UFI/10.1001@jamanetworkopen.2019.2622.pdf.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/8QSSCN75/2732330.html:text/html},
}

@article{gilardi_chatgpt_2023,
	title = {{ChatGPT} outperforms crowd workers for text-annotation tasks},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2305016120},
	doi = {10.1073/pnas.2305016120},
	abstract = {Many {NLP} applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as {MTurk} as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (
              n
              = 6,183), we show that {ChatGPT} outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of {ChatGPT} exceeds that of crowd workers by about 25 percentage points on average, while {ChatGPT}’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of {ChatGPT} is less than \$0.003—about thirty times cheaper than {MTurk}. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.},
	pages = {e2305016120},
	number = {30},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
	urldate = {2023-08-26},
	date = {2023-07-25},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/66K5Y6N9/Gilardi et al. - 2023 - ChatGPT outperforms crowd workers for text-annotat.pdf:application/pdf},
}

@article{feest_experimenters_2016,
	title = {The experimenters' regress reconsidered: Replication, tacit knowledge, and the dynamics of knowledge generation},
	volume = {58},
	issn = {0039-3681},
	url = {https://www.sciencedirect.com/science/article/pii/S0039368116300036},
	doi = {10.1016/j.shpsa.2016.04.003},
	shorttitle = {The experimenters' regress reconsidered},
	abstract = {This paper revisits the debate between Harry Collins and Allan Franklin, concerning the experimenters' regress. Focusing my attention on a case study from recent psychology (regarding experimental evidence for the existence of a Mozart Effect), I argue that Franklin is right to highlight the role of epistemological strategies in scientific practice, but that his account does not sufficiently appreciate Collins's point about the importance of tacit knowledge in experimental practice. In turn, Collins rightly highlights the epistemic uncertainty (and skepticism) surrounding much experimental research. However, I will argue that his analysis of tacit knowledge fails to elucidate the reasons why scientists often are (and should be) skeptical of other researchers' experimental results. I will present an analysis of tacit knowledge in experimental research that not only answers to this desideratum, but also shows how such skepticism can in fact be a vital enabling factor for the dynamic processes of experimental knowledge generation.},
	pages = {34--45},
	journaltitle = {Studies in History and Philosophy of Science Part A},
	shortjournal = {Studies in History and Philosophy of Science Part A},
	author = {Feest, Uljana},
	urldate = {2023-08-27},
	date = {2016-08-01},
	keywords = {Replication, Conceptual openness, Epistemic uncertainty, Experimenters' regress, Operationalism, Tacit knowledge},
	file = {ScienceDirect Snapshot:/Users/tom/Zotero/storage/7PINUN6S/S0039368116300036.html:text/html;The experimenters' regress reconsidered\: Replication, tacit knowledge, and the dynamics of knowledge generation:/Users/tom/Zotero/storage/2BHVXD3Z/feest2016.pdf.pdf:application/pdf},
}

@article{rothman_show_1978,
	title = {A Show of Confidence},
	volume = {299},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJM197812142992410},
	doi = {10.1056/NEJM197812142992410},
	abstract = {Many medical researchers believe that it would be fruitless to submit for publication any paper that lacks statistical tests of significance. Their belief is not ill founded: editors and referees commonly rely on tests of significance as indicators of a sophisticated and meaningful statistical analysis, as well as the primary means to assess sampling variability in a study. The preoccupation with significance tests is embodied in the focus on whether the P value is less than 0.05; results are considered "significant" or "not significant" according to whether or not the P value is less than or greater than 0.05. Dr. . . .},
	pages = {1362--1363},
	number = {24},
	journaltitle = {New England Journal of Medicine},
	author = {Rothman, Kenneth J.},
	urldate = {2023-08-28},
	date = {1978-12-14},
	pmid = {362205},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJM}197812142992410},
}

@article{rennie_vive_1978,
	title = {Vive la Différence (P{\textless}0.05)},
	volume = {299},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJM197810122991509},
	doi = {10.1056/NEJM197810122991509},
	abstract = {Francis Galton was sheltering from a brief summer shower in the grounds of Naworth Castle when suddenly the true nature of correlation flashed into his mind, and for a moment he forgot everything else in his "great delight."1 Thus was the correlation coefficient, r, conceived, its public birth being before the Royal Society in December of that year, 1888.2 Few researchers have shared in the mathematical ecstasy of this inventive statistician, but most of us have, with differing degrees of reluctance, buckled down to learning about α, β, P, F, z, t, and the like, recognizing . . .},
	pages = {828--829},
	number = {15},
	journaltitle = {New England Journal of Medicine},
	author = {Rennie, Drummond},
	urldate = {2023-08-28},
	date = {1978-10-12},
	pmid = {692566},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJM}197810122991509},
}

@article{langman_towards_1986,
	title = {Towards estimation and confidence intervals},
	volume = {292},
	issn = {0267-0623},
	doi = {10.1136/bmj.292.6522.716},
	pages = {716},
	number = {6522},
	journaltitle = {British Medical Journal (Clinical Research Ed.)},
	shortjournal = {Br Med J (Clin Res Ed)},
	author = {Langman, M. J.},
	date = {1986-03-15},
	pmid = {3082408},
	pmcid = {PMC1339775},
	keywords = {Clinical Trials as Topic, Humans, Probability, Statistics as Topic},
	file = {Full Text:/Users/tom/Zotero/storage/L8L8K2DW/Langman - 1986 - Towards estimation and confidence intervals.pdf:application/pdf},
}

@article{gardner_confidence_1986,
	title = {Confidence intervals rather than P values: estimation rather than hypothesis testing},
	volume = {292},
	issn = {0267-0623},
	doi = {10.1136/bmj.292.6522.746},
	shorttitle = {Confidence intervals rather than P values},
	abstract = {Overemphasis on hypothesis testing--and the use of P values to dichotomise significant or non-significant results--has detracted from more useful approaches to interpreting study results, such as estimation and confidence intervals. In medical studies investigators are usually interested in determining the size of difference of a measured outcome between groups, rather than a simple indication of whether or not it is statistically significant. Confidence intervals present a range of values, on the basis of the sample data, in which the population value for such a difference may lie. Some methods of calculating confidence intervals for means and differences between means are given, with similar information for proportions. The paper also gives suggestions for graphical display. Confidence intervals, if appropriate to the type of study, should be used for major findings in both the main text of a paper and its abstract.},
	pages = {746--750},
	number = {6522},
	journaltitle = {British Medical Journal (Clinical Research Ed.)},
	shortjournal = {Br Med J (Clin Res Ed)},
	author = {Gardner, M. J. and Altman, D. G.},
	date = {1986-03-15},
	pmid = {3082422},
	pmcid = {PMC1339793},
	keywords = {Clinical Trials as Topic, Humans, Adult, Probability, Statistics as Topic, Middle Aged, Blood Pressure, Diabetes Mellitus},
	file = {Full Text:/Users/tom/Zotero/storage/84C3F7Y7/Gardner and Altman - 1986 - Confidence intervals rather than P values estimat.pdf:application/pdf},
}

@article{schoenegger_social_2023,
	title = {Social sciences in crisis: on the proposed elimination of the discussion section},
	volume = {202},
	issn = {1573-0964},
	url = {https://link.springer.com/10.1007/s11229-023-04267-3},
	doi = {10.1007/s11229-023-04267-3},
	shorttitle = {Social sciences in crisis},
	abstract = {The social sciences are facing numerous crises including those related to replication, theory, and applicability. We highlight that these crises imply epistemic malfunctions and affect science communication negatively. Several potential solutions have already been proposed, ranging from statistical improvements to changes in norms of scientiﬁc conduct. In this paper, we propose a structural solution: the elimination of the discussion section from social science research papers. We point out that discussion sections allow for an inappropriate narrativization of research that disguises actual results and enables the misstatement of true limitations. We go on to claim that removing this section and outsourcing it to other publications provides several epistemic advantages such as a division of academic labour, adversarial modes of progress, and a better alignment of the personal aims of scientists with the aims of science. After responding to several objections, we conclude that the potential beneﬁts of moving away from the traditional model of academic papers outweigh the costs and have the potential to play a part in addressing the crises in the social sciences alongside other reforms. As such, we take our paper as proffering a further potential solution that should be applied complimentarily with other reform movements such as Open Science and hope that our paper can start a debate on this or similar proposals.},
	pages = {54},
	number = {2},
	journaltitle = {Synthese},
	shortjournal = {Synthese},
	author = {Schoenegger, Philipp and Pils, Raimund},
	urldate = {2023-08-30},
	date = {2023-08-09},
	langid = {english},
	file = {Schoenegger and Pils - 2023 - Social sciences in crisis on the proposed elimina.pdf:/Users/tom/Zotero/storage/KCHFR3IA/Schoenegger and Pils - 2023 - Social sciences in crisis on the proposed elimina.pdf:application/pdf},
}

@misc{baker_reproduceme_2023,
	title = {{ReproduceMe}: lessons from a pilot project on computational reproducibility},
	url = {https://psyarxiv.com/k8d4u/},
	doi = {10.31234/osf.io/k8d4u},
	shorttitle = {{ReproduceMe}},
	abstract = {If a scientific paper is computationally reproducible, the analyses it reports can be repeated independently by others. At the present time most papers are not reproducible. However, the tools to enable computational reproducibility are now widely available, using free and open source software. We conducted a pilot study in which we offered ‘reproducibility as a service’ within a {UK} psychology department for a period of 6 months. Our rationale was that most researchers lack either the time or expertise to make their own work reproducible, but might be willing to allow this to be done by an independent team. Ten papers were converted into reproducible format using R markdown, such that all analyses were conducted by a single script that could download raw data from online platforms as required, generate figures, and produce a pdf of the final manuscript. For some studies this involved reproducing analyses originally conducted using commercial software. The project was an overall success, with strong support from the contributing authors who saw clear benefit from this work, including greater transparency and openness, and ease of use for the reader. Here we describe our framework for reproducibility, summarise the specific lessons learned during the project, and discuss the future of computational reproducibility. Our view is that computationally reproducible manuscripts embody many of the core principles of open science, and should become the default format for scientific communication.},
	publisher = {{PsyArXiv}},
	author = {Baker, Daniel and Berg, Mareike and Hansford, Kirralise and Quinn, Bartholomew and Segala, Federico Gabriele and English, Erin},
	urldate = {2023-08-30},
	date = {2023-08-30},
	langid = {english},
	keywords = {Meta-science, computational reproducibility, markdown, meta-psychology},
	file = {Full Text PDF:/Users/tom/Zotero/storage/C5EHJ75R/Baker et al. - 2023 - ReproduceMe lessons from a pilot project on compu.pdf:application/pdf},
}

@article{clark_constructing_1995,
	title = {Constructing validity: Basic issues in objective scale development},
	volume = {7},
	issn = {1939-134X},
	doi = {10.1037/1040-3590.7.3.309},
	shorttitle = {Constructing validity},
	abstract = {A primary goal of scale development is to create a valid measure of an underlying construct. We discuss theoretical principles, practical issues, and pragmatic decisions to help developers maximize the construct validity of scales and subscales. First, it is essential to begin with a clear conceptualization of the target construct. Moreover, the content of the initial item pool should be overinclusive and item wording needs careful attention. Next, the item pool should be tested, along with variables that assess closely related constructs, on a heterogeneous sample representing the entire range of the target population. Finally, in selecting scale items, the goal is unidimensionality rather than internal consistency; this means that virtually all interitem correlations should be moderate in magnitude. Factor analysis can play a crucial role in ensuring the unidimensionality and discriminant validity of scales. ({PsycINFO} Database Record (c) 2019 {APA}, all rights reserved)},
	pages = {309--319},
	number = {3},
	journaltitle = {Psychological Assessment},
	author = {Clark, Lee Anna and Watson, David},
	date = {1995},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Statistical Validity, Construct Validity, Item Analysis (Test), Test Construction},
	file = {Constructing validity\: Basic issues in objective scale development:/Users/tom/Zotero/storage/LPTZ6KAT/clark1995.pdf.pdf:application/pdf},
}

@article{crosetto_comparing_2023,
	title = {Comparing input interfaces to elicit belief distributions},
	volume = {18},
	issn = {1930-2975},
	url = {https://www.cambridge.org/core/journals/judgment-and-decision-making/article/comparing-input-interfaces-to-elicit-belief-distributions/8B1A4A3302011671BCAF5BBF2FE61AC7},
	doi = {10.1017/jdm.2023.21},
	abstract = {This paper introduces a new software interface to elicit belief distributions of any shape: Click-and-Drag. The interface was tested against the state of the art in the experimental literature—a text-based interface and multiple sliders—and in the online forecasting industry—a distribution-manipulation interface similar to the one used by the most popular crowd-forecasting website. By means of a pre-registered experiment on Amazon Mechanical Turk, quantitative data on the accuracy of reported beliefs in a series of induced-value scenarios varying by granularity, shape, and time constraints, as well as subjective data on user experience were collected. Click-and-Drag outperformed all other interfaces by accuracy and speed, and was self-reported as being more intuitive and less frustrating, confirming the pre-registered hypothesis. Aside of the pre-registered results, Click-and-Drag generated the least drop-out rate from the task, and scored best in a sentiment analysis of an open-ended general question. Further, the interface was used to collect homegrown predictions on temperature in New York City in 2022 and 2042. Click-and-Drag elicited distributions were smoother with less idiosyncratic spikes. Free and open source, ready to use {oTree}, Qualtrics and Limesurvey plugins for Click-and-Drag, and all other tested interfaces are available at https://beliefelicitation.github.io/.},
	pages = {e27},
	journaltitle = {Judgment and Decision Making},
	author = {Crosetto, Paolo and Haan, Thomas de},
	urldate = {2023-09-01},
	date = {2023-01},
	langid = {english},
	keywords = {belief elicitation, forecasting, interfaces, scoring rules},
	file = {Full Text PDF:/Users/tom/Zotero/storage/57VA7IH3/Crosetto and Haan - 2023 - Comparing input interfaces to elicit belief distri.pdf:application/pdf},
}

@article{hartley_current_2014,
	title = {Current findings from research on structured abstracts: an update},
	volume = {102},
	issn = {1536-5050},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4076121/},
	doi = {10.3163/1536-5050.102.3.002},
	shorttitle = {Current findings from research on structured abstracts},
	pages = {146--148},
	number = {3},
	journaltitle = {Journal of the Medical Library Association : {JMLA}},
	shortjournal = {J Med Libr Assoc},
	author = {Hartley, James},
	urldate = {2023-09-01},
	date = {2014-07},
	pmid = {25031553},
	pmcid = {PMC4076121},
	file = {Current findings from research on structured abstracts\: an update:/Users/tom/Zotero/storage/PGD5G4LL/hartley2014.pdf.pdf:application/pdf;PubMed Central Full Text PDF:/Users/tom/Zotero/storage/PBLVSBUU/Hartley - 2014 - Current findings from research on structured abstr.pdf:application/pdf},
}

@article{li_scoping_2017,
	title = {A scoping review of comparisons between abstracts and full reports in primary biomedical research},
	volume = {17},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-017-0459-5},
	doi = {10.1186/s12874-017-0459-5},
	abstract = {Evidence shows that research abstracts are commonly inconsistent with their corresponding full reports, and may mislead readers. In this scoping review, which is part of our series on the state of reporting of primary biomedical research, we summarized the evidence from systematic reviews and surveys, to investigate the current state of inconsistent abstract reporting, and to evaluate factors associated with improved reporting by comparing abstracts and their full reports.},
	pages = {181},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Li, Guowei and Abbade, Luciana P. F. and Nwosu, Ikunna and Jin, Yanling and Leenus, Alvin and Maaz, Muhammad and Wang, Mei and Bhatt, Meha and Zielinski, Laura and Sanger, Nitika and Bantoto, Bianca and Luo, Candice and Shams, Ieta and Shahid, Hamnah and Chang, Yaping and Sun, Guangwen and Mbuagbaw, Lawrence and Samaan, Zainab and Levine, Mitchell A. H. and Adachi, Jonathan D. and Thabane, Lehana},
	urldate = {2023-09-01},
	date = {2017-12-29},
	keywords = {Scoping review, Spin, Abstract, Accuracy, Deficiency, Discrepancy, Inconsistent reporting},
	file = {A scoping review of comparisons between abstracts and full reports in primary biomedical research:/Users/tom/Zotero/storage/N2DX5Z5B/li2017.pdf.pdf:application/pdf},
}

@article{boutron_impact_2014-1,
	title = {Impact of spin in the abstracts of articles reporting results of randomized controlled trials in the field of cancer: the spiin randomized controlled trial},
	volume = {32},
	issn = {0732-183X},
	url = {https://ascopubs.org/doi/full/10.1200/JCO.2014.56.7503},
	doi = {10.1200/JCO.2014.56.7503},
	shorttitle = {Impact of spin in the abstracts of articles reporting results of randomized controlled trials in the field of cancer},
	abstract = {Purpose
We aimed to assess the impact of spin (ie, reporting to convince readers that the beneficial effect of the experimental treatment is greater than shown by the results) on the interpretation of results of abstracts of randomized controlled trials ({RCTs}) in the field of cancer.

Methods
We performed a two-arm, parallel-group {RCT}. We selected a sample of published {RCTs} with statistically nonsignificant primary outcome and with spin in the abstract conclusion. Two versions of these abstracts were used—the original with spin and a rewritten version without spin. Participants were clinician corresponding authors of articles reporting {RCTs}, investigators of trials, and reviewers of French national grants. The primary outcome was clinicians' interpretation of the beneficial effect of the experimental treatment (0 to 10 scale). Participants were blinded to study hypothesis.

Results
Three hundred clinicians were randomly assigned using a Web-based system; 150 clinicians assessed an abstract with spin and 150 assessed an abstract without spin. For abstracts with spin, the experimental treatment was rated as being more beneficial (mean difference, 0.71; 95\% {CI}, 0.07 to 1.35; P = .030), the trial was rated as being less rigorous (mean difference, −0.59; 95\% {CI}, −1.13 to 0.05; P = .034), and clinicians were more interested in reading the full-text article (mean difference, 0.77; 95\% {CI}, 0.08 to 1.47; P = .029). There was no statistically significant difference in the clinicians' rating of the importance of the study or the need to run another trial.

Conclusion
Spin in abstracts can have an impact on clinicians' interpretation of the trial results.},
	pages = {4120--4126},
	number = {36},
	journaltitle = {Journal of Clinical Oncology},
	shortjournal = {{JCO}},
	author = {Boutron, Isabelle and Altman, Douglas G. and Hopewell, Sally and Vera-Badillo, Francisco and Tannock, Ian and Ravaud, Philippe},
	urldate = {2023-09-01},
	date = {2014-12-20},
	note = {Publisher: Wolters Kluwer},
	file = {A-History-of-Scientific-Journals.pdf:/Users/tom/Zotero/storage/I2RUMA3V/A-History-of-Scientific-Journals.pdf:application/pdf;Full Text:/Users/tom/Zotero/storage/QALES2IW/Boutron et al. - 2014 - Impact of Spin in the Abstracts of Articles Report.pdf:application/pdf;Impact of Spin in the Abstracts of Articles Reporting Results of Randomized Controlled Trials in the Field of Cancer\: The SPIIN Randomized Controlled Trial:/Users/tom/Zotero/storage/IKLWLZY9/boutron2014.pdf.pdf:application/pdf},
}

@article{wintle_verbal_2019,
	title = {Verbal probabilities: Very likely to be somewhat more confusing than numbers},
	volume = {14},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6469752/},
	doi = {10.1371/journal.pone.0213522},
	shorttitle = {Verbal probabilities},
	abstract = {People interpret verbal expressions of probabilities (e.g. ‘very likely’) in different ways, yet words are commonly preferred to numbers when communicating uncertainty. Simply providing numerical translations alongside reports or text containing verbal probabilities should encourage consistency, but these guidelines are often ignored. In an online experiment with 924 participants, we compared four different formats for presenting verbal probabilities with the numerical guidelines used in the {US} Intelligence Community Directive ({ICD}) 203 to see whether any could improve the correspondence between the intended meaning and participants’ interpretation (‘in-context’). This extends previous work in the domain of climate science. The four experimental conditions we tested were: 1. numerical guidelines bracketed in text, e.g. X is very unlikely (05–20\%), 2. click to see the full guidelines table in a new window, 3. numerical guidelines appear in a mouse over tool tip, and 4. no guidelines provided (control). Results indicate that correspondence with the {ICD} 203 standard is substantially improved only when numerical guidelines are bracketed in text. For this condition, average correspondence was 66\%, compared with 32\% in the control. We also elicited ‘context-free’ numerical judgements from participants for each of the seven verbal probability expressions contained in {ICD} 203 (i.e., we asked participants what range of numbers they, personally, would assign to those expressions), and constructed ‘evidence-based lexicons’ based on two methods from similar research, ‘membership functions’ and ‘peak values’, that reflect our large sample’s intuitive translations of the terms. Better aligning the intended and assumed meaning of fuzzy words like ‘unlikely’ can reduce communication problems between the reporter and receiver of probabilistic information. In turn, this can improve decision making under uncertainty.},
	pages = {e0213522},
	number = {4},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} One},
	author = {Wintle, Bonnie C. and Fraser, Hannah and Wills, Ben C. and Nicholson, Ann E. and Fidler, Fiona},
	urldate = {2023-09-04},
	date = {2019-04-17},
	pmid = {30995242},
	pmcid = {PMC6469752},
	file = {Wintle et al. - 2019 - Verbal probabilities Very likely to be somewhat m.pdf:/Users/tom/Zotero/storage/DPUG4PV2/Wintle et al. - 2019 - Verbal probabilities Very likely to be somewhat m.pdf:application/pdf},
}

@article{uygun_tunc_is_2023,
	title = {Is open science neoliberal?},
	volume = {18},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/17456916221114835},
	doi = {10.1177/17456916221114835},
	abstract = {The scientific-reform movement, frequently referred to as open science, has the potential to substantially reshape the nature of the scientific activity. For this reason, its sociopolitical antecedents and consequences deserve serious scholarly attention. In a recently formed literature that professes to meet this need, it has been widely argued that the movement is neoliberal. However, for two reasons it is hard to justify this widescale attribution: First, the critics mistakenly represent the movement as a monolithic structure, and second, the critics’ arguments associating the movement with neoliberalism because of the movement’s (a) preferential focus on methodological issues, (b) underlying philosophy of science, and (c) allegedly promarket ideological proclivities reflected in the methodology and science-policy proposals do not hold under closer scrutiny. These shortcomings show a lack of sufficient engagement with the reform literature. What is needed is more nuanced accounts of the sociopolitical underpinnings of scientific reform. To address this need, we propose a model for the analysis of reform proposals, which represents scientific methodology, axiology, science policy, and ideology as interconnected but relatively distinct domains, and thus allows for recognizing the divergent tendencies in the movement and the uniqueness of particular proposals.},
	pages = {1047--1061},
	number = {5},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Uygun Tunç, Duygu and Tunç, Mehmet Necip and Eper, Ziya Batuhan},
	urldate = {2023-09-04},
	date = {2023-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Uygun Tunç et al. - 2023 - Is open science neoliberal.pdf:/Users/tom/Zotero/storage/W7FPHDB5/Uygun Tunç et al. - 2023 - Is open science neoliberal.pdf:application/pdf},
}

@article{trafimow_new_2023,
	title = {A new way to think about internal and external validity},
	volume = {18},
	issn = {1745-6924},
	doi = {10.1177/17456916221136117},
	abstract = {Researchers have been concerned with internal and external validity for decades, and the discussion continues. The present proposal is that there are less important and more important senses in which one can interpret internal and external validity, and these can be integrated with a taxonomy that includes theoretical, auxiliary, statistical, and inferential assumptions. The integration sheds light on recent exchanges in the literature on validity and suggests that the vaunted internal-external validity trade-off is false for more important senses of internal and external validity; internal and external validity increase or decrease together when there is an emphasis on underlying theories. Finally, the integration implies the desirability of some changes in typical research advice and practice.},
	pages = {1028--1046},
	number = {5},
	journaltitle = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Trafimow, David},
	date = {2023-09},
	pmid = {36469834},
	keywords = {empirical hypotheses, external validity, inferential hypotheses, internal validity, statistical hypotheses, theories},
	file = {Trafimow - 2023 - A new way to think about internal and external val.pdf:/Users/tom/Zotero/storage/AMAWNQ8H/Trafimow - 2023 - A new way to think about internal and external val.pdf:application/pdf},
}

@article{ross_many_2022-1,
	title = {Many analysts and few incentives},
	volume = {0},
	issn = {2153-599X},
	url = {https://doi.org/10.1080/2153599X.2022.2070248},
	doi = {10.1080/2153599X.2022.2070248},
	pages = {1--3},
	number = {0},
	journaltitle = {Religion, Brain \& Behavior},
	author = {Ross, Robert M. and Sulik, Justin and Buczny, Jacek and Schivinski, Bruno},
	urldate = {2023-09-04},
	date = {2022-07-06},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/2153599X.2022.2070248},
	file = {Ross et al. - 2022 - Many analysts and few incentives.pdf:/Users/tom/Zotero/storage/R3F695JC/Ross et al. - 2022 - Many analysts and few incentives.pdf:application/pdf},
}

@article{mcneish_thinking_2020,
	title = {Thinking twice about sum scores},
	volume = {52},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-020-01398-0},
	doi = {10.3758/s13428-020-01398-0},
	abstract = {A common way to form scores from multiple-item scales is to sum responses of all items. Though sum scoring is often contrasted with factor analysis as a competing method, we review how factor analysis and sum scoring both fall under the larger umbrella of latent variable models, with sum scoring being a constrained version of a factor analysis. Despite similarities, reporting of psychometric properties for sum scored or factor analyzed scales are quite different. Further, if researchers use factor analysis to validate a scale but subsequently sum score the scale, this employs a model that differs from validation model. By framing sum scoring within a latent variable framework, our goal is to raise awareness that (a) sum scoring requires rather strict constraints, (b) imposing these constraints requires the same type of justification as any other latent variable model, and (c) sum scoring corresponds to a statistical model and is not a model-free arithmetic calculation. We discuss how unjustified sum scoring can have adverse effects on validity, reliability, and qualitative classification from sum score cut-offs. We also discuss considerations for how to use scale scores in subsequent analyses and how these choices can alter conclusions. The general goal is to encourage researchers to more critically evaluate how they obtain, justify, and use multiple-item scale scores.},
	pages = {2287--2305},
	number = {6},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {{McNeish}, Daniel and Wolf, Melissa Gordon},
	urldate = {2023-09-04},
	date = {2020-12-01},
	langid = {english},
	keywords = {Psychometrics, Factor analysis, Scale scores, Scales},
	file = {A-History-of-Scientific-Journals.pdf:/Users/tom/Zotero/storage/EMSLUEPM/A-History-of-Scientific-Journals.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/XR2CGTR5/McNeish and Wolf - 2020 - Thinking twice about sum scores.pdf:application/pdf;Thinking twice about sum scores:/Users/tom/Zotero/storage/XZGPNMIB/10.3758@s13428-020-01398-0.pdf.pdf:application/pdf},
}

@article{birkle_web_2020,
	title = {Web of Science as a data source for research on scientific and scholarly activity},
	volume = {1},
	issn = {2641-3337},
	url = {https://doi.org/10.1162/qss_a_00018},
	doi = {10.1162/qss_a_00018},
	abstract = {Web of Science ({WoS}) is the world’s oldest, most widely used and authoritative database of research publications and citations. Based on the Science Citation Index, founded by Eugene Garfield in 1964, it has expanded its selective, balanced, and complete coverage of the world’s leading research to cover around 34,000 journals today. A wide range of use cases are supported by {WoS} from daily search and discovery by researchers worldwide through to the supply of analytical data sets and the provision of specialized access to raw data for bibliometric partners. A long- and well-established network of such partners enables the Institute for Scientific Information ({ISI}) to continue to work closely with bibliometric groups around the world to the benefit of both the community and the services that the company provides to researchers and analysts.},
	pages = {363--376},
	number = {1},
	journaltitle = {Quantitative Science Studies},
	shortjournal = {Quantitative Science Studies},
	author = {Birkle, Caroline and Pendlebury, David A. and Schnell, Joshua and Adams, Jonathan},
	urldate = {2023-09-05},
	date = {2020-02-01},
	file = {Full Text PDF:/Users/tom/Zotero/storage/4KLSDFWH/Birkle et al. - 2020 - Web of Science as a data source for research on sc.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/88HD8QN5/Web-of-Science-as-a-data-source-for-research-on.html:text/html;Web of Science as a data source for research on scientific and scholarly activity:/Users/tom/Zotero/storage/LLHJGQCM/birkle2020.pdf.pdf:application/pdf},
}

@book{fyfe_history_2022,
	title = {A History of Scientific Journals},
	isbn = {978-1-80008-232-8},
	url = {https://discovery.ucl.ac.uk/id/eprint/10156072/},
	publisher = {{UCL} Press},
	author = {Fyfe, Aileen and Moxham, Noah and {McDougall}-Waters, Julie and Mørk Røstvik, Camilla},
	urldate = {2023-09-05},
	date = {2022-10-03},
	langid = {english},
	doi = {10.14324/111.9781800082328},
	file = {Fyfe et al. - 2022 - A History of Scientific Journals.pdf:/Users/tom/Zotero/storage/3YIHA9HW/Fyfe et al. - 2022 - A History of Scientific Journals.pdf:application/pdf},
}

@article{korbmacher_replication_2023,
	title = {The replication crisis has led to positive structural, procedural, and community changes},
	volume = {1},
	rights = {2023 The Author(s)},
	issn = {2731-9121},
	url = {https://www.nature.com/articles/s44271-023-00003-2},
	doi = {10.1038/s44271-023-00003-2},
	abstract = {The emergence of large-scale replication projects yielding successful rates substantially lower than expected caused the behavioural, cognitive, and social sciences to experience a so-called ‘replication crisis’. In this Perspective, we reframe this ‘crisis’ through the lens of a credibility revolution, focusing on positive structural, procedural and community-driven changes. Second, we outline a path to expand ongoing advances and improvements. The credibility revolution has been an impetus to several substantive changes which will have a positive, long-term impact on our research environment.},
	pages = {1--13},
	number = {1},
	journaltitle = {Communications Psychology},
	shortjournal = {Commun Psychol},
	author = {Korbmacher, Max and Azevedo, Flavio and Pennington, Charlotte R. and Hartmann, Helena and Pownall, Madeleine and Schmidt, Kathleen and Elsherif, Mahmoud and Breznau, Nate and Robertson, Olly and Kalandadze, Tamara and Yu, Shijun and Baker, Bradley J. and O’Mahony, Aoife and Olsnes, Jørgen Ø-S. and Shaw, John J. and Gjoneska, Biljana and Yamada, Yuki and Röer, Jan P. and Murphy, Jennifer and Alzahawi, Shilaan and Grinschgl, Sandra and Oliveira, Catia M. and Wingen, Tobias and Yeung, Siu Kit and Liu, Meng and König, Laura M. and Albayrak-Aydemir, Nihan and Lecuona, Oscar and Micheli, Leticia and Evans, Thomas},
	urldate = {2023-09-06},
	date = {2023-07-25},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Psychology, Scientific community, Publication characteristics},
	file = {Full Text PDF:/Users/tom/Zotero/storage/H2JTP39Z/Korbmacher et al. - 2023 - The replication crisis has led to positive structu.pdf:application/pdf},
}

@article{logg_pre-registration_2021-1,
	title = {Pre-registration: Weighing costs and benefits for researchers},
	volume = {167},
	issn = {0749-5978},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597821000649},
	doi = {10.1016/j.obhdp.2021.05.006},
	shorttitle = {Pre-registration},
	abstract = {In the past decade, the social and behavioral sciences underwent a methodological revolution, offering practical prescriptions for improving the replicability and reproducibility of research results. One key to reforming science is a simple and scalable practice: pre-registration. Pre-registration constitutes pre-specifying an analysis plan prior to data collection. A growing chorus of articles discusses the prescriptive, field-wide benefits of pre-registration. To increase adoption, however, scientists need to know who currently pre-registers and understand perceived barriers to doing so. Thus, we weigh costs and benefits of pre-registration. Our survey of researchers reveals generational differences in who pre-registers and uncertainty regarding how pre-registration benefits individual researchers. We leverage these data to directly address researchers’ uncertainty by clarifying why pre-registration improves the research process itself. Finally, we discuss how to pre-register and compare available resources. The present work examines the who, why, and how of pre-registration in order to weigh the costs and benefits of pre-registration to researchers and motivate continued adoption.},
	pages = {18--27},
	journaltitle = {Organizational Behavior and Human Decision Processes},
	shortjournal = {Organizational Behavior and Human Decision Processes},
	author = {Logg, Jennifer M. and Dorison, Charles A.},
	urldate = {2023-09-08},
	date = {2021-11-01},
	keywords = {Replication, Open science, Pre-registration, Methodology},
	file = {Pre-registration\: Weighing costs and benefits for researchers:/Users/tom/Zotero/storage/7JVNV7ZM/logg2021.pdf.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/VSMQTGHW/S0749597821000649.html:text/html},
}

@article{haslam_scholarly_2017,
	title = {Scholarly productivity and citation impact of academic psychologists in Group of Eight universities},
	volume = {69},
	issn = {0004-9530},
	url = {https://doi.org/10.1111/ajpy.12142},
	doi = {10.1111/ajpy.12142},
	abstract = {{ObjectiveThis} study sought to update norms for scholarly publication and citation impact for Australian Group of Eight (Go8) university psychology academics published by {McNally} (2010).{MethodPublication} and citation data for 279 Go8 psychology academics were extracted using the Scopus and Google Scholar databases. Norms for career‐wise publications, citations, and the h‐index were developed for each academic level (from Lecturer to Professor), and eight‐year publication counts for 2009–2016 were compared with the 2001–2008 figures reported by {McNally}.{ResultsEvidence} of a steep increase in scholarly productivity was found relative to {McNally} (2010), and new norms were generated. There was notable variation between psychology subdisciplines, with neuroscience and clinical science academics typically having higher publication and citation counts than their cognitive psychology peers.{ConclusionsNorms} for scholarly productivity and citation impact among Australian psychology academics have undergone substantial change in recent years. Caveats concerning the application of research metrics are discussed.},
	pages = {162--166},
	number = {3},
	journaltitle = {Australian Journal of Psychology},
	author = {Haslam, Nick and Stratemeyer, Michelle and Vargas‐sáenz, Adriana},
	urldate = {2023-09-11},
	date = {2017-09-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1111/ajpy.12142},
	keywords = {citations, impact, h‐index, productivity},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NA5WDPTH/Haslam et al. - 2017 - Scholarly productivity and citation impact of acad.pdf:application/pdf;Scholarly productivity and citation impact of academic psychologists in Group of Eight universities:/Users/tom/Zotero/storage/2RIQFKW6/haslam2016.pdf.pdf:application/pdf},
}

@article{mazzucchelli_scholarly_2019,
	title = {Scholarly productivity and citation impact of Australian academic psychologists},
	volume = {71},
	issn = {0004-9530},
	url = {https://doi.org/10.1111/ajpy.12248},
	doi = {10.1111/ajpy.12248},
	abstract = {{ObjectiveThis} study sought to provide up‐to‐date normative data on the productivity and citation impact of publications by Australian academic psychologists at each academic level (lecturer to professor) and for each university grouping (e.g., Group of Eight [Go8], Australian Technology Network, etc.).{MethodPublication} and citation data for a representative sample of 732 psychology academics were extracted using the Scopus database. Norms for lifetime publications, citations, and h‐index were developed for each academic level and were compared with those reported in previous studies.{ResultsJudgements} of academic level based on number of publications, citations and h‐indexes are highly reliable with publication means for the ranks roughly doubling, and citation means more than doubling, for each successive level. Lifetime publication means have increased by a factor of 2 to 3 since the norms published in 2010, consistent with the suggestion that rates of scholarly publication have increased over the last decade. Academics at the research‐intensive Go8 universities had, as a group, significantly higher publication averages at every level than academics at other universities; however, these differences varied considerably in size across the university groupings.{ConclusionsIndices} of research productivity and impact are important when evaluating academic psychologists' performance, and the present article provides up‐to‐date, comprehensive, and representative norms of Australian academic psychologists.},
	pages = {305--311},
	number = {3},
	journaltitle = {Australian Journal of Psychology},
	author = {Mazzucchelli, Trevor G. and Burton, Emma and Roberts, Lynne},
	urldate = {2023-09-11},
	date = {2019-09-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1111/ajpy.12248},
	keywords = {citations, impact, h‐index, productivity, academic psychologists, publication rates},
	file = {Full Text PDF:/Users/tom/Zotero/storage/W9345FQL/Mazzucchelli et al. - 2019 - Scholarly productivity and citation impact of Aust.pdf:application/pdf;Scholarly productivity and citation impact of Australian academic psychologists:/Users/tom/Zotero/storage/6N436WRY/10.1111@ajpy.12248.pdf.pdf:application/pdf},
}

@misc{iarkaeva_workflow_2023,
	title = {Workflow for detecting biomedical articles with openly available underlying datasets},
	url = {https://osf.io/preprints/metaarxiv/z4bkf/},
	doi = {10.31222/osf.io/z4bkf},
	abstract = {To monitor the sharing of research data through repositories is increasingly of interest to institutions and funders, as well as from a meta-research perspective. Automated screening tools exist, but they are based on narrow and/or vague definitions of open data. Where manual validation has been performed, it was based on a small article sample. At our biomedical research institution, we developed detailed criteria for such a screening, as well as a workflow which combines an automated and a manual step, and considers both fully open and restricted-access data. We use the results for an internal incentivization scheme, as well as for a monitoring in a dashboard. Here, we describe in detail our screening procedure and its validation, based on automated screening of 10960 biomedical research articles, of which 1381 articles with potential data sharing were subsequently screened manually. The screening results were highly reliable, as witnessed by inter-rater reliability values of {\textgreater}0.8 in two different validation samples. We also report the results of the screening, both for our institution and an independent sample from a meta-research study. In the largest of the three samples, the 2021 institutional sample, underlying data had been openly shared for 7.9\% of research articles. For an additional 1.1\% of articles, restricted-access data had been shared, resulting in 8.4\% of articles overall having open and/or restricted-access data. The extraction workflow is then discussed with regard to its applicability in different contexts, limitations, possible variations, and future developments. In summary, we present a comprehensive, validated, semi-automated workflow for the detection of shared research data underlying biomedical article publications.},
	publisher = {{MetaArXiv}},
	author = {Iarkaeva, Anastasiia and Nachev, Vladislav and Bobrov, Evgeny},
	urldate = {2023-09-17},
	date = {2023-07-25},
	langid = {english},
	keywords = {open science, Medicine and Health Sciences, open data, data sharing, screening, assessment, Other Medicine and Health Sciences, restricted-access data},
	file = {Full Text PDF:/Users/tom/Zotero/storage/EEQL8XKG/Iarkaeva et al. - 2023 - Workflow for detecting biomedical articles with op.pdf:application/pdf},
}

@article{corneille_beware_2023,
	title = {Beware ‘persuasive communication devices’ when writing and reading scientific articles},
	volume = {12},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.88654},
	doi = {10.7554/eLife.88654},
	abstract = {Authors rely on a range of devices and techniques to attract and maintain the interest of readers, and to convince them of the merits of the author’s point of view. However, when writing a scientific article, authors must use these ‘persuasive communication devices’ carefully. In particular, they must be explicit about the limitations of their work, avoid obfuscation, and resist the temptation to oversell their results. Here we discuss a list of persuasive communication devices and we encourage authors, as well as reviewers and editors, to think carefully about their use.},
	pages = {e88654},
	journaltitle = {{eLife}},
	author = {Corneille, Olivier and Havemann, Jo and Henderson, Emma L and {IJzerman}, Hans and Hussey, Ian and Orban de Xivry, Jean-Jacques and Jussim, Lee and Holmes, Nicholas P and Pilacinski, Artur and Beffara, Brice and Carroll, Harriet and Outa, Nicholas Otieno and Lush, Peter and Lotter, Leon D},
	urldate = {2023-09-18},
	date = {2023-05-25},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {reporting, scientific publishing, scientific writing, citation, language, point of view},
	file = {Full Text:/Users/tom/Zotero/storage/N3HLUNZG/Corneille et al. - 2023 - Beware ‘persuasive communication devices’ when wri.pdf:application/pdf},
}

@article{cobey_epidemiological_2023,
	title = {Epidemiological characteristics and prevalence rates of research reproducibility across disciplines: A scoping review of articles published in 2018-2019},
	volume = {12},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.78518},
	doi = {10.7554/eLife.78518},
	shorttitle = {Epidemiological characteristics and prevalence rates of research reproducibility across disciplines},
	abstract = {Background:. Reproducibility is a central tenant of research. We aimed to synthesize the literature on reproducibility and describe its epidemiological characteristics, including how reproducibility is defined and assessed. We also aimed to determine and compare estimates for reproducibility across different fields. Methods:. We conducted a scoping review to identify English language replication studies published between 2018 and 2019 in economics, education, psychology, health sciences, and biomedicine. We searched Medline, Embase, {PsycINFO}, Cumulative Index of Nursing and Allied Health Literature – {CINAHL}, Education Source via {EBSCOHost}, {ERIC}, {EconPapers}, International Bibliography of the Social Sciences ({IBSS}), and {EconLit}. Documents retrieved were screened in duplicate against our inclusion criteria. We extracted year of publication, number of authors, country of affiliation of the corresponding author, and whether the study was funded. For the individual replication studies, we recorded whether a registered protocol for the replication study was used, whether there was contact between the reproducing team and the original authors, what study design was used, and what the primary outcome was. Finally, we recorded how reproducibilty was defined by the authors, and whether the assessed study(ies) successfully reproduced based on this definition. Extraction was done by a single reviewer and quality controlled by a second reviewer. Results:. Our search identified 11,224 unique documents, of which 47 were included in this review. Most studies were related to either psychology (48.6\%) or health sciences (23.7\%). Among these 47 documents, 36 described a single reproducibility study while the remaining 11 reported at least two reproducibility studies in the same paper. Less than the half of the studies referred to a registered protocol. There was variability in the definitions of reproduciblity success. In total, across the 47 documents 177 studies were reported. Based on the definition used by the author of each study, 95 of 177 (53.7\%) studies reproduced. Conclusions:. This study gives an overview of research across five disciplines that explicitly set out to reproduce previous research. Such reproducibility studies are extremely scarce, the definition of a successfully reproduced study is ambiguous, and the reproducibility rate is overall modest. Funding:. No external funding was received for this work},
	pages = {e78518},
	journaltitle = {{eLife}},
	author = {Cobey, Kelly D and Fehlmann, Christophe A and Christ Franco, Marina and Ayala, Ana Patricia and Sikora, Lindsey and Rice, Danielle B and Xu, Chenchen and Ioannidis, John {PA} and Lalu, Manoj M and Ménard, Alixe and Neitzel, Andrew and Nguyen, Bea and Tsertsvadze, Nino and Moher, David},
	editor = {Allison, David B and Zaidi, Mone and Vorland, Colby J and Lupia, Arthur and Agley, Jon},
	urldate = {2023-09-18},
	date = {2023-06-21},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {open science, reproducibility, replication, meta-research},
	file = {Full Text:/Users/tom/Zotero/storage/FTGEJHW5/Cobey et al. - 2023 - Epidemiological characteristics and prevalence rat.pdf:application/pdf},
}

@article{cobey_community_2023,
	title = {Community consensus on core open science practices to monitor in biomedicine},
	volume = {21},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001949},
	doi = {10.1371/journal.pbio.3001949},
	abstract = {The state of open science needs to be monitored to track changes over time and identify areas to create interventions to drive improvements. In order to monitor open science practices, they first need to be well defined and operationalized. To reach consensus on what open science practices to monitor at biomedical research institutions, we conducted a modified 3-round Delphi study. Participants were research administrators, researchers, specialists in dedicated open science roles, and librarians. In rounds 1 and 2, participants completed an online survey evaluating a set of potential open science practices, and for round 3, we hosted two half-day virtual meetings to discuss and vote on items that had not reached consensus. Ultimately, participants reached consensus on 19 open science practices. This core set of open science practices will form the foundation for institutional dashboards and may also be of value for the development of policy, education, and interventions.},
	pages = {e3001949},
	number = {1},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Cobey, Kelly D. and Haustein, Stefanie and Brehaut, Jamie and Dirnagl, Ulrich and Franzen, Delwen L. and Hemkens, Lars G. and Presseau, Justin and Riedel, Nico and Strech, Daniel and Alperin, Juan Pablo and Costas, Rodrigo and Sena, Emily S. and Leeuwen, Thed van and Ardern, Clare L. and Bacellar, Isabel O. L. and Camack, Nancy and Correa, Marcos Britto and Buccione, Roberto and Cenci, Maximiliano Sergio and Fergusson, Dean A. and Praag, Cassandra Gould van and Hoffman, Michael M. and Bielemann, Renata Moraes and Moschini, Ugo and Paschetta, Mauro and Pasquale, Valentina and Rac, Valeria E. and Roskams-Edris, Dylan and Schatzl, Hermann M. and Stratton, Jo Anne and Moher, David},
	urldate = {2023-09-18},
	date = {2023-01-24},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Open data, Open science, Clinical trial reporting, Peer review, Systematic reviews, Government funding of science, Research assessment, Open access publishing},
	file = {Full Text PDF:/Users/tom/Zotero/storage/NAFG6SJ4/Cobey et al. - 2023 - Community consensus on core open science practices.pdf:application/pdf},
}

@article{elm_strengthening_2007,
	title = {Strengthening the reporting of observational studies in epidemiology ({STROBE}) statement: guidelines for reporting observational studies},
	volume = {335},
	rights = {© {BMJ} Publishing Group Ltd 2007},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/content/335/7624/806},
	doi = {10.1136/bmj.39335.541782.AD},
	shorttitle = {Strengthening the reporting of observational studies in epidemiology ({STROBE}) statement},
	abstract = {{\textless}p{\textgreater}Poor reporting of research hampers assessment and makes it less useful. An international group of methodologists, researchers, and journal editors sets out guidelines to improve reports of observational studies {\textless}/p{\textgreater}},
	pages = {806--808},
	number = {7624},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Elm, Erik von and Altman, Douglas G. and Egger, Matthias and Pocock, Stuart J. and Gøtzsche, Peter C. and Vandenbroucke, Jan P.},
	urldate = {2023-09-18},
	date = {2007-10-18},
	langid = {english},
	pmid = {17947786},
	note = {Publisher: British Medical Journal Publishing Group
Section: Analysis},
	file = {Full Text PDF:/Users/tom/Zotero/storage/26K4B4DQ/Elm et al. - 2007 - Strengthening the reporting of observational studi.pdf:application/pdf;Strengthening the reporting of observational studies in epidemiology (STROBE) statement\: guidelines for reporting observational studies:/Users/tom/Zotero/storage/RKCNAM7X/elm2007.pdf.pdf:application/pdf},
}

@article{visser_large-scale_2021,
	title = {Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic},
	volume = {2},
	issn = {2641-3337},
	url = {https://doi.org/10.1162/qss_a_00112},
	doi = {10.1162/qss_a_00112},
	shorttitle = {Large-scale comparison of bibliographic data sources},
	abstract = {We present a large-scale comparison of five multidisciplinary bibliographic data
sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic.
The comparison considers scientific documents from the period 2008–2017
covered by these data sources. Scopus is compared in a pairwise manner with each
of the other data sources. We first analyze differences between the data sources
in the coverage of documents, focusing for instance on differences over time,
differences per document type, and differences per discipline. We then study
differences in the completeness and accuracy of citation links. Based on our
analysis, we discuss the strengths and weaknesses of the different data sources.
We emphasize the importance of combining a comprehensive coverage of the
scientific literature with a flexible set of filters for making selections of
the literature.},
	pages = {20--41},
	number = {1},
	journaltitle = {Quantitative Science Studies},
	shortjournal = {Quantitative Science Studies},
	author = {Visser, Martijn and van Eck, Nees Jan and Waltman, Ludo},
	urldate = {2023-09-18},
	date = {2021-04-08},
	file = {Full Text PDF:/Users/tom/Zotero/storage/CDBERAYK/Visser et al. - 2021 - Large-scale comparison of bibliographic data sourc.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/KYX6VCYN/Large-scale-comparison-of-bibliographic-data.html:text/html},
}

@article{wang_large-scale_2016,
	title = {Large-scale analysis of the accuracy of the journal classification systems of Web of Science and Scopus},
	volume = {10},
	issn = {1751-1577},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157715301930},
	doi = {10.1016/j.joi.2016.02.003},
	abstract = {Journal classification systems play an important role in bibliometric analyses. The two most important bibliographic databases, Web of Science and Scopus, each provide a journal classification system. However, no study has systematically investigated the accuracy of these classification systems. To examine and compare the accuracy of journal classification systems, we define two criteria on the basis of direct citation relations between journals and categories. We use Criterion I to select journals that have weak connections with their assigned categories, and we use Criterion {II} to identify journals that are not assigned to categories with which they have strong connections. If a journal satisfies either of the two criteria, we conclude that its assignment to categories may be questionable. Accordingly, we identify all journals with questionable classifications in Web of Science and Scopus. Furthermore, we perform a more in-depth analysis for the field of Library and Information Science to assess whether our proposed criteria are appropriate and whether they yield meaningful results. It turns out that according to our citation-based criteria Web of Science performs significantly better than Scopus in terms of the accuracy of its journal classification system.},
	pages = {347--364},
	number = {2},
	journaltitle = {Journal of Informetrics},
	shortjournal = {Journal of Informetrics},
	author = {Wang, Qi and Waltman, Ludo},
	urldate = {2023-09-18},
	date = {2016-05-01},
	keywords = {Citation analysis, Bibliographic database, Journal classification system, Scopus, Web of Science},
	file = {Large-scale analysis of the accuracy of the journal classification systems of Web of Science and Scopus:/Users/tom/Zotero/storage/XIE82ER3/wang2016.pdf.pdf:application/pdf;ScienceDirect Snapshot:/Users/tom/Zotero/storage/MA57GMQL/S1751157715301930.html:text/html;Submitted Version:/Users/tom/Zotero/storage/FBLLQF8S/Wang and Waltman - 2016 - Large-scale analysis of the accuracy of the journa.pdf:application/pdf},
}

@article{waltman_special_2020,
	title = {Special issue on bibliographic data sources},
	volume = {1},
	issn = {2641-3337},
	url = {https://doi.org/10.1162/qss_e_00026},
	doi = {10.1162/qss_e_00026},
	pages = {360--362},
	number = {1},
	journaltitle = {Quantitative Science Studies},
	shortjournal = {Quantitative Science Studies},
	author = {Waltman, Ludo and Larivière, Vincent},
	urldate = {2023-09-18},
	date = {2020-02-01},
	file = {Full Text PDF:/Users/tom/Zotero/storage/SLD529YS/Waltman and Larivière - 2020 - Special issue on bibliographic data sources.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/848AH8ZT/Special-issue-on-bibliographic-data-sources.html:text/html;Special issue on bibliographic data sources:/Users/tom/Zotero/storage/8B2DLIIE/waltman2020.pdf.pdf:application/pdf},
}

@article{hamilton_prevalence_2023,
	title = {Prevalence and predictors of data and code sharing in the medical and health sciences: systematic review with meta-analysis of individual participant data},
	volume = {382},
	rights = {© Author(s) (or their employer(s)) 2019. Re-use permitted under {CC} {BY}-{NC}. No commercial re-use. See rights and permissions. Published by {BMJ}.. http://creativecommons.org/licenses/by-nc/4.0/This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/382/bmj-2023-075767},
	doi = {10.1136/bmj-2023-075767},
	shorttitle = {Prevalence and predictors of data and code sharing in the medical and health sciences},
	abstract = {Objectives To synthesise research investigating data and code sharing in medicine and health to establish an accurate representation of the prevalence of sharing, how this frequency has changed over time, and what factors influence availability.
Design Systematic review with meta-analysis of individual participant data.
Data sources Ovid Medline, Ovid Embase, and the preprint servers {medRxiv}, {bioRxiv}, and {MetaArXiv} were searched from inception to 1 July 2021. Forward citation searches were also performed on 30 August 2022.
Review methods Meta-research studies that investigated data or code sharing across a sample of scientific articles presenting original medical and health research were identified. Two authors screened records, assessed the risk of bias, and extracted summary data from study reports when individual participant data could not be retrieved. Key outcomes of interest were the prevalence of statements that declared that data or code were publicly or privately available (declared availability) and the success rates of retrieving these products (actual availability). The associations between data and code availability and several factors (eg, journal policy, type of data, trial design, and human participants) were also examined. A two stage approach to meta-analysis of individual participant data was performed, with proportions and risk ratios pooled with the Hartung-Knapp-Sidik-Jonkman method for random effects meta-analysis.
Results The review included 105 meta-research studies examining 2 121 580 articles across 31 specialties. Eligible studies examined a median of 195 primary articles (interquartile range 113-475), with a median publication year of 2015 (interquartile range 2012-2018). Only eight studies (8\%) were classified as having a low risk of bias. Meta-analyses showed a prevalence of declared and actual public data availability of 8\% (95\% confidence interval 5\% to 11\%) and 2\% (1\% to 3\%), respectively, between 2016 and 2021. For public code sharing, both the prevalence of declared and actual availability were estimated to be {\textless}0.5\% since 2016. Meta-regressions indicated that only declared public data sharing prevalence estimates have increased over time. Compliance with mandatory data sharing policies ranged from 0\% to 100\% across journals and varied by type of data. In contrast, success in privately obtaining data and code from authors historically ranged between 0\% and 37\% and 0\% and 23\%, respectively.
Conclusions The review found that public code sharing was persistently low across medical research. Declarations of data sharing were also low, increasing over time, but did not always correspond to actual sharing of data. The effectiveness of mandatory data sharing policies varied substantially by journal and type of data, a finding that might be informative for policy makers when designing policies and allocating resources to audit compliance.
Systematic review registration Open Science Framework doi:10.17605/{OSF}.{IO}/7SX8U.},
	pages = {e075767},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Hamilton, Daniel G. and Hong, Kyungwan and Fraser, Hannah and Rowhani-Farid, Anisa and Fidler, Fiona and Page, Matthew J.},
	urldate = {2023-09-19},
	date = {2023-07-11},
	langid = {english},
	pmid = {37433624},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/VRC8T7RG/Hamilton et al. - 2023 - Prevalence and predictors of data and code sharing.pdf:application/pdf},
}

@article{morey_peer_2016,
	title = {The Peer Reviewers' Openness Initiative: incentivizing open research practices through peer review},
	volume = {3},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.150547},
	doi = {10.1098/rsos.150547},
	shorttitle = {The Peer Reviewers' Openness Initiative},
	abstract = {Openness is one of the central values of science. Open scientific practices such as sharing data, materials and analysis scripts alongside published articles have many benefits, including easier replication and extension studies, increased availability of data for theory-building and meta-analysis, and increased possibility of review and collaboration even after a paper has been published. Although modern information technology makes sharing easier than ever before, uptake of open practices had been slow. We suggest this might be in part due to a social dilemma arising from misaligned incentives and propose a specific, concrete mechanism—reviewers withholding comprehensive review—to achieve the goal of creating the expectation of open practices as a matter of scientific principle.},
	pages = {150547},
	number = {1},
	journaltitle = {Royal Society Open Science},
	author = {Morey, Richard D. and Chambers, Christopher D. and Etchells, Peter J. and Harris, Christine R. and Hoekstra, Rink and Lakens, Daniël and Lewandowsky, Stephan and Morey, Candice Coker and Newman, Daniel P. and Schönbrodt, Felix D. and Vanpaemel, Wolf and Wagenmakers, Eric-Jan and Zwaan, Rolf A.},
	urldate = {2023-09-19},
	date = {2016-01},
	note = {Publisher: Royal Society},
	keywords = {transparency, science, peer review, open research},
	file = {Full Text PDF:/Users/tom/Zotero/storage/FWS9X8Y8/Morey et al. - 2016 - The Peer Reviewers' Openness Initiative incentivi.pdf:application/pdf;The Peer Reviewers' Openness Initiative\: incentivizing open research practices through peer review:/Users/tom/Zotero/storage/J9YUL3RT/morey2016.pdf.pdf:application/pdf},
}

@article{cruwell_whats_2023,
	title = {What’s in a badge? A computational reproducibility investigation of the open data badge policy in one issue of \textit{Psychological Science}},
	volume = {34},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/09567976221140828},
	doi = {10.1177/09567976221140828},
	shorttitle = {What’s in a badge?},
	abstract = {In April 2019, Psychological Science published its first issue in which all Research Articles received the Open Data badge. We used that issue to investigate the effectiveness of this badge, focusing on the adherence to its aim at Psychological Science: sharing both data and code to ensure reproducibility of results. Twelve researchers of varying experience levels attempted to reproduce the results of the empirical articles in the target issue (at least three researchers per article). We found that all 14 articles provided at least some data and six provided analysis code, but only one article was rated to be exactly reproducible, and three were rated as essentially reproducible with minor deviations. We suggest that researchers should be encouraged to adhere to the higher standard in force at Psychological Science. Moreover, a check of reproducibility during peer review may be preferable to the disclosure method of awarding badges.},
	pages = {512--522},
	number = {4},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Crüwell, Sophia and Apthorp, Deborah and Baker, Bradley J. and Colling, Lincoln and Elson, Malte and Geiger, Sandra J. and Lobentanzer, Sebastian and Monéger, Jean and Patterson, Alex and Schwarzkopf, D. Samuel and Zaneva, Mirela and Brown, Nicholas J. L.},
	urldate = {2023-09-19},
	date = {2023-04},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/EI948TAQ/Crüwell et al. - 2023 - What’s in a Badge A Computational Reproducibility.pdf:application/pdf},
}

@article{mayo-wilson_evaluating_2021-2,
	title = {Evaluating implementation of the Transparency and Openness Promotion ({TOP}) guidelines: the {TRUST} process for rating journal policies, procedures, and practices},
	volume = {6},
	issn = {2058-8615},
	url = {https://doi.org/10.1186/s41073-021-00112-8},
	doi = {10.1186/s41073-021-00112-8},
	shorttitle = {Evaluating implementation of the Transparency and Openness Promotion ({TOP}) guidelines},
	abstract = {The Transparency and Openness Promotion ({TOP}) Guidelines describe modular standards that journals can adopt to promote open science. The {TOP} Factor is a metric to describe the extent to which journals have adopted the {TOP} Guidelines in their policies. Systematic methods and rating instruments are needed to calculate the {TOP} Factor. Moreover, implementation of these open science policies depends on journal procedures and practices, for which {TOP} provides no standards or rating instruments.},
	pages = {9},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Research Integrity and Peer Review},
	author = {Mayo-Wilson, Evan and Grant, Sean and Supplee, Lauren and Kianersi, Sina and Amin, Afsah and {DeHaven}, Alex and Mellor, David},
	urldate = {2023-09-20},
	date = {2021-06-02},
	keywords = {Open science, Reproducibility, Research transparency, {TOP} factor, {TOP} guidelines},
	file = {Evaluating implementation of the Transparency and Openness Promotion (TOP) guidelines\: the TRUST process for rating journal policies, procedures, and practices:/Users/tom/Zotero/storage/EH9W3R7Y/mayo-wilson2021.pdf.pdf:application/pdf;Full Text PDF:/Users/tom/Zotero/storage/JPYIK99U/Mayo-Wilson et al. - 2021 - Evaluating implementation of the Transparency and .pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/IBENQPQR/s41073-021-00112-8.html:text/html},
}

@article{van_den_akker_selective_2023,
	title = {Selective hypothesis reporting in psychology: comparing preregistrations and corresponding publications},
	volume = {6},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/25152459231187988},
	doi = {10.1177/25152459231187988},
	shorttitle = {Selective hypothesis reporting in psychology},
	abstract = {In this study, we assessed the extent of selective hypothesis reporting in psychological research by comparing the hypotheses found in a set of 459 preregistrations with the hypotheses found in the corresponding articles. We found that more than half of the preregistered studies we assessed contained omitted hypotheses (N = 224; 52\%) or added hypotheses (N = 227; 57\%), and about one-fifth of studies contained hypotheses with a direction change (N = 79; 18\%). We found only a small number of studies with hypotheses that were demoted from primary to secondary importance (N = 2; 1\%) and no studies with hypotheses that were promoted from secondary to primary importance. In all, 60\% of studies included at least one hypothesis in one or more of these categories, indicating a substantial bias in presenting and selecting hypotheses by researchers and/or reviewers/editors. Contrary to our expectations, we did not find sufficient evidence that added hypotheses and changed hypotheses were more likely to be statistically significant than nonselectively reported hypotheses. For the other types of selective hypothesis reporting, we likely did not have sufficient statistical power to test for a relationship with statistical significance. Finally, we found that replication studies were less likely to include selectively reported hypotheses than original studies. In all, selective hypothesis reporting is problematically common in psychological research. We urge researchers, reviewers, and editors to ensure that hypotheses outlined in preregistrations are clearly formulated and accurately presented in the corresponding articles.},
	pages = {25152459231187988},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {van den Akker, Olmo R. and van Assen, Marcel A. L. M. and Enting, Manon and de Jonge, Myrthe and Ong, How Hwee and Rüffer, Franziska and Schoenmakers, Martijn and Stoevenbelt, Andrea H. and Wicherts, Jelte M. and Bakker, Marjan},
	urldate = {2023-09-20},
	date = {2023-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/tom/Zotero/storage/QCBJCPWC/van den Akker et al. - 2023 - Selective Hypothesis Reporting in Psychology Comp.pdf:application/pdf},
}

@article{rethlefsen_prisma-s_2021,
	title = {{PRISMA}-S: an extension to the {PRISMA} Statement for Reporting Literature Searches in Systematic Reviews},
	volume = {10},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-020-01542-z},
	doi = {10.1186/s13643-020-01542-z},
	shorttitle = {{PRISMA}-S},
	abstract = {Abstract
            
              Background
              Literature searches underlie the foundations of systematic reviews and related review types. Yet, the literature searching component of systematic reviews and related review types is often poorly reported. Guidance for literature search reporting has been diverse, and, in many cases, does not offer enough detail to authors who need more specific information about reporting search methods and information sources in a clear, reproducible way. This document presents the {PRISMA}-S (Preferred Reporting Items for Systematic reviews and Meta-Analyses literature search extension) checklist, and explanation and elaboration.
            
            
              Methods
              The checklist was developed using a 3-stage Delphi survey process, followed by a consensus conference and public review process.
            
            
              Results
              The final checklist includes 16 reporting items, each of which is detailed with exemplar reporting and rationale.
            
            
              Conclusions
              The intent of {PRISMA}-S is to complement the {PRISMA} Statement and its extensions by providing a checklist that could be used by interdisciplinary authors, editors, and peer reviewers to verify that each component of a search is completely reported and therefore reproducible.},
	pages = {39},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {Rethlefsen, Melissa L. and Kirtley, Shona and Waffenschmidt, Siw and Ayala, Ana Patricia and Moher, David and Page, Matthew J. and Koffel, Jonathan B. and {PRISMA-S Group} and Blunt, Heather and Brigham, Tara and Chang, Steven and Clark, Justin and Conway, Aislinn and Couban, Rachel and De Kock, Shelley and Farrah, Kelly and Fehrmann, Paul and Foster, Margaret and Fowler, Susan A. and Glanville, Julie and Harris, Elizabeth and Hoffecker, Lilian and Isojarvi, Jaana and Kaunelis, David and Ket, Hans and Levay, Paul and Lyon, Jennifer and McGowan, Jessie and Murad, M. Hassan and Nicholson, Joey and Pannabecker, Virginia and Paynter, Robin and Pinotti, Rachel and Ross-White, Amanda and Sampson, Margaret and Shields, Tracy and Stevens, Adrienne and Sutton, Anthea and Weinfurter, Elizabeth and Wright, Kath and Young, Sarah},
	urldate = {2023-10-01},
	date = {2021-01-26},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/3NPEEJ9V/Rethlefsen et al. - 2021 - PRISMA-S an extension to the PRISMA Statement for.pdf:application/pdf;PRISMA-S\: an extension to the PRISMA Statement for Reporting Literature Searches in Systematic Reviews:/Users/tom/Zotero/storage/F55933AV/rethlefsen2021.pdf.pdf:application/pdf},
}

@article{jonnalagadda_automating_2015,
	title = {Automating data extraction in systematic reviews: a systematic review},
	volume = {4},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-015-0066-7},
	doi = {10.1186/s13643-015-0066-7},
	shorttitle = {Automating data extraction in systematic reviews},
	pages = {78},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {Jonnalagadda, Siddhartha R. and Goyal, Pawan and Huffman, Mark D.},
	urldate = {2023-10-10},
	date = {2015-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/76TCKN39/Jonnalagadda et al. - 2015 - Automating data extraction in systematic reviews a systematic review.pdf:application/pdf},
}

@article{scott_systematic_2021,
	title = {Systematic review automation tools improve efficiency but lack of knowledge impedes their adoption: a survey},
	volume = {138},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435621002080},
	doi = {10.1016/j.jclinepi.2021.06.030},
	shorttitle = {Systematic review automation tools improve efficiency but lack of knowledge impedes their adoption},
	pages = {80--94},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Scott, Anna Mae and Forbes, Connor and Clark, Justin and Carter, Matt and Glasziou, Paul and Munn, Zachary},
	urldate = {2023-10-10},
	date = {2021-10},
	langid = {english},
	file = {Scott et al. - 2021 - Systematic review automation tools improve efficiency but lack of knowledge impedes their adoption .pdf:/Users/tom/Zotero/storage/IV9Z57VM/Scott et al. - 2021 - Systematic review automation tools improve efficiency but lack of knowledge impedes their adoption .pdf:application/pdf},
}

@article{wang_instruments_2022,
	title = {Instruments assessing risk of bias of randomized trials frequently included items that are not addressing risk of bias issues},
	volume = {152},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435622002682},
	doi = {10.1016/j.jclinepi.2022.10.018},
	abstract = {Objectives: To establish whether items included in instruments published in the last decade assessing risk of bias of randomized controlled trials ({RCTs}) are indeed addressing risk of bias. Study Design and Setting: We searched Medline, Embase, Web of Science, and Scopus from 2010 to October 2021 for instruments assessing risk of bias of {RCTs}. By extracting items and summarizing their essential content, we generated an item list. Items that two reviewers agreed clearly did not address risk of bias were excluded. We included the remaining items in a survey in which 13 experts judged the issue each item is addressing: risk of bias, applicability, random error, reporting quality, or none of the above.},
	pages = {218--225},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Wang, Ying and Ghadimi, Maryam and Wang, Qi and Hou, Liangying and Zeraatkar, Dena and Iqbal, Atiya and Ho, Cameron and Yao, Liang and Hu, Malini and Ye, Zhikang and Couban, Rachel and Armijo-Olivo, Susan and Bassler, Dirk and Briel, Matthias and Gluud, Lise Lotte and Glasziou, Paul and Jackson, Rod and Keitz, Sheri A. and Letelier, Luz M. and Ravaud, Philippe and Schulz, Kenneth F. and Siemieniuk, Reed A.C. and Brignardello-Petersen, Romina and Guyatt, Gordon H.},
	urldate = {2023-10-10},
	date = {2022-12},
	langid = {english},
	file = {Wang et al. - 2022 - Instruments assessing risk of bias of randomized trials frequently included items that are not addre.pdf:/Users/tom/Zotero/storage/25ZM2DIT/Wang et al. - 2022 - Instruments assessing risk of bias of randomized trials frequently included items that are not addre.pdf:application/pdf},
}

@article{hair_automated_2023,
	title = {The automated systematic search deduplicator ({ASySD}): a rapid, open-source, interoperable tool to remove duplicate citations in biomedical systematic reviews},
	volume = {21},
	issn = {1741-7007},
	url = {https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-023-01686-z},
	doi = {10.1186/s12915-023-01686-z},
	shorttitle = {The automated systematic search deduplicator ({ASySD})},
	abstract = {Background Researchers performing high-quality systematic reviews search across multiple databases to identify relevant evidence. However, the same publication is often retrieved from several databases. Identifying and removing such duplicates (“deduplication”) can be extremely time-consuming, but failure to remove these citations can lead to the wrongful inclusion of duplicate data. Many existing tools are not sensitive enough, lack interoperability with other tools, are not freely accessible, or are difficult to use without programming knowledge. Here, we report the performance of our Automated Systematic Search Deduplicator ({ASySD}), a novel tool to perform automated deduplication of systematic searches for biomedical reviews.
Methods We evaluated {ASySD}’s performance on 5 unseen biomedical systematic search datasets of various sizes (1845–79,880 citations). We compared the performance of {ASySD} with {EndNote}’s automated deduplication option and with the Systematic Review Assistant Deduplication Module ({SRA}-{DM}).
Results {ASySD} identified more duplicates than either {SRA}-{DM} or {EndNote}, with a sensitivity in different datasets of 0.95 to 0.99. The false-positive rate was comparable to human performance, with a specificity of {\textgreater} 0.99. The tool took less than 1 h to identify and remove duplicates within each dataset.
Conclusions For duplicate removal in biomedical systematic reviews, {ASySD} is a highly sensitive, reliable, and timesaving tool. It is open source and freely available online as both an R package and a user-friendly web application.},
	pages = {189},
	number = {1},
	journaltitle = {{BMC} Biology},
	shortjournal = {{BMC} Biol},
	author = {Hair, Kaitlyn and Bahor, Zsanett and Macleod, Malcolm and Liao, Jing and Sena, Emily S.},
	urldate = {2023-10-10},
	date = {2023-09-07},
	langid = {english},
	file = {Hair et al. - 2023 - The Automated Systematic Search Deduplicator (ASySD) a rapid, open-source, interoperable tool to re.pdf:/Users/tom/Zotero/storage/DUYRRNBW/Hair et al. - 2023 - The Automated Systematic Search Deduplicator (ASySD) a rapid, open-source, interoperable tool to re.pdf:application/pdf},
}

@article{kilicoglu_automatic_2018-1,
	title = {Automatic recognition of self-acknowledged limitations in clinical research literature},
	volume = {25},
	issn = {1067-5027, 1527-974X},
	url = {https://academic.oup.com/jamia/article/25/7/855/4990607},
	doi = {10.1093/jamia/ocy038},
	abstract = {Abstract
            
              Objective
              To automatically recognize self-acknowledged limitations in clinical research publications to support efforts in improving research transparency.
            
            
              Methods
              To develop our recognition methods, we used a set of 8431 sentences from 1197 {PubMed} Central articles. A subset of these sentences was manually annotated for training/testing, and inter-annotator agreement was calculated. We cast the recognition problem as a binary classification task, in which we determine whether a given sentence from a publication discusses self-acknowledged limitations or not. We experimented with three methods: a rule-based approach based on document structure, supervised machine learning, and a semi-supervised method that uses self-training to expand the training set in order to improve classification performance. The machine learning algorithms used were logistic regression ({LR}) and support vector machines ({SVM}).
            
            
              Results
              Annotators had good agreement in labeling limitation sentences (Krippendorff’s α = 0.781). Of the three methods used, the rule-based method yielded the best performance with 91.5\% accuracy (95\% {CI} [90.1-92.9]), while self-training with {SVM} led to a small improvement over fully supervised learning (89.9\%, 95\% {CI} [88.4-91.4] vs 89.6\%, 95\% {CI} [88.1-91.1]).
            
            
              Conclusions
              The approach presented can be incorporated into the workflows of stakeholders focusing on research transparency to improve reporting of limitations in clinical studies.},
	pages = {855--861},
	number = {7},
	journaltitle = {Journal of the American Medical Informatics Association},
	author = {Kilicoglu, Halil and Rosemblat, Graciela and Malički, Mario and Ter Riet, Gerben},
	urldate = {2023-10-10},
	date = {2018-07-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/SZNWEAZH/Kilicoglu et al. - 2018 - Automatic recognition of self-acknowledged limitations in clinical research literature.pdf:application/pdf},
}

@article{moher_guidance_2010,
	title = {Guidance for Developers of Health Research Reporting Guidelines},
	volume = {7},
	issn = {1549-1676},
	url = {https://dx.plos.org/10.1371/journal.pmed.1000217},
	doi = {10.1371/journal.pmed.1000217},
	pages = {e1000217},
	number = {2},
	journaltitle = {{PLoS} Medicine},
	shortjournal = {{PLoS} Med},
	author = {Moher, David and Schulz, Kenneth F. and Simera, Iveta and Altman, Douglas G.},
	urldate = {2023-10-10},
	date = {2010-02-16},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/EFURXFHP/Moher et al. - 2010 - Guidance for Developers of Health Research Reporting Guidelines.pdf:application/pdf},
}

@article{clark_full_2020,
	title = {A full systematic review was completed in 2 weeks using automation tools: a case study},
	volume = {121},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089543561930719X},
	doi = {10.1016/j.jclinepi.2020.01.008},
	shorttitle = {A full systematic review was completed in 2 weeks using automation tools},
	pages = {81--90},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Clark, Justin and Glasziou, Paul and Del Mar, Chris and Bannach-Brown, Alexandra and Stehlik, Paulina and Scott, Anna Mae},
	urldate = {2023-10-10},
	date = {2020-05},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/ZE3I7I2F/Clark et al. - 2020 - A full systematic review was completed in 2 weeks using automation tools a case study.pdf:application/pdf},
}

@article{oconnor_question_2019,
	title = {A question of trust: can we build an evidence base to gain trust in systematic review automation technologies?},
	volume = {8},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1062-0},
	doi = {10.1186/s13643-019-1062-0},
	shorttitle = {A question of trust},
	pages = {143},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {O’Connor, Annette M. and Tsafnat, Guy and Thomas, James and Glasziou, Paul and Gilbert, Stephen B. and Hutton, Brian},
	urldate = {2023-10-10},
	date = {2019-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/WJFZHJNZ/O’Connor et al. - 2019 - A question of trust can we build an evidence base to gain trust in systematic review automation tec.pdf:application/pdf},
}

@article{omara-eves_using_2015,
	title = {Using text mining for study identification in systematic reviews: a systematic review of current approaches},
	volume = {4},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/2046-4053-4-5},
	doi = {10.1186/2046-4053-4-5},
	shorttitle = {Using text mining for study identification in systematic reviews},
	pages = {5},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {O’Mara-Eves, Alison and Thomas, James and {McNaught}, John and Miwa, Makoto and Ananiadou, Sophia},
	urldate = {2023-10-11},
	date = {2015-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/4CRVSJR6/O’Mara-Eves et al. - 2015 - Using text mining for study identification in systematic reviews a systematic review of current app.pdf:application/pdf},
}

@article{vagia_literature_2016,
	title = {A literature review on the levels of automation during the years. What are the different taxonomies that have been proposed?},
	volume = {53},
	issn = {00036870},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003687015300855},
	doi = {10.1016/j.apergo.2015.09.013},
	pages = {190--202},
	journaltitle = {Applied Ergonomics},
	shortjournal = {Applied Ergonomics},
	author = {Vagia, Marialena and Transeth, Aksel A. and Fjerdingen, Sigurd A.},
	urldate = {2023-10-11},
	date = {2016-03},
	langid = {english},
	file = {Vagia et al. - 2016 - A literature review on the levels of automation during the years. What are the different taxonomies .pdf:/Users/tom/Zotero/storage/VZ6YX9US/Vagia et al. - 2016 - A literature review on the levels of automation during the years. What are the different taxonomies .pdf:application/pdf},
}

@article{damen_indicators_2023,
	title = {Indicators of questionable research practices were identified in 163,129 randomized controlled trials},
	volume = {154},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435622003079},
	doi = {10.1016/j.jclinepi.2022.11.020},
	abstract = {Objectives: To explore indicators of the following questionable research practices ({QRPs}) in randomized controlled trials ({RCTs}): (1) risk of bias in four domains (random sequence generation, allocation concealment, blinding of participants and personnel, and blinding of outcome assessment); (2) modiﬁcations in primary outcomes that were registered in trial registration records (proxy for selective reporting bias); (3) ratio of the achieved to planned sample sizes; and (4) statistical discrepancy.},
	pages = {23--32},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Damen, Johanna A. and Heus, Pauline and Lamberink, Herm J. and Tijdink, Joeri K. and Bouter, Lex and Glasziou, Paul and Moher, David and Otte, Willem M. and Vinkers, Christiaan H. and Hooft, Lotty},
	urldate = {2023-10-11},
	date = {2023-02},
	langid = {english},
	file = {Damen et al. - 2023 - Indicators of questionable research practices were identified in 163,129 randomized controlled trial.pdf:/Users/tom/Zotero/storage/27BL978T/Damen et al. - 2023 - Indicators of questionable research practices were identified in 163,129 randomized controlled trial.pdf:application/pdf},
}

@article{vinkers_methodological_2021-1,
	title = {The methodological quality of 176,620 randomized controlled trials published between 1966 and 2018 reveals a positive trend but also an urgent need for improvement},
	volume = {19},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.3001162},
	doi = {10.1371/journal.pbio.3001162},
	abstract = {Many randomized controlled trials ({RCTs}) are biased and difficult to reproduce due to methodological flaws and poor reporting. There is increasing attention for responsible research practices and implementation of reporting guidelines, but whether these efforts have improved the methodological quality of {RCTs} (e.g., lower risk of bias) is unknown. We, therefore, mapped risk-of-bias trends over time in {RCT} publications in relation to journal and author characteristics. Meta-information of 176,620 {RCTs} published between 1966 and 2018 was extracted. The risk-of-bias probability (random sequence generation, allocation concealment, blinding of patients/personnel, and blinding of outcome assessment) was assessed using a risk-of-bias machine learning tool. This tool was simultaneously validated using 63,327 human risk-of-bias assessments obtained from 17,394 {RCTs} evaluated in the Cochrane Database of Systematic Reviews ({CDSR}). Moreover, {RCT} registration and {CONSORT} Statement reporting were assessed using automated searches. Publication characteristics included the number of authors, journal impact factor ({JIF}), and medical discipline. The annual number of published {RCTs} substantially increased over 4 decades, accompanied by increases in authors (5.2 to 7.8) and institutions (2.9 to 4.8). The risk of bias remained present in most {RCTs} but decreased over time for allocation concealment (63\% to 51\%), random sequence generation (57\% to 36\%), and blinding of outcome assessment (58\% to 52\%). Trial registration (37\% to 47\%) and the use of the {CONSORT} Statement (1\% to 20\%) also rapidly increased. In journals with a higher impact factor ({\textgreater}10), the risk of bias was consistently lower with higher levels of {RCT} registration and the use of the {CONSORT} Statement. Automated risk-of-bias predictions had accuracies above 70\% for allocation concealment (70.7\%), random sequence generation (72.1\%), and blinding of patients/personnel (79.8\%), but not for blinding of outcome assessment (62.7\%). In conclusion, the likelihood of bias in {RCTs} has generally decreased over the last decades. This optimistic trend may be driven by increased knowledge augmented by mandatory trial registration and more stringent reporting guidelines and journal requirements. Nevertheless, relatively high probabilities of bias remain, particularly in journals with lower impact factors. This emphasizes that further improvement of {RCT} registration, conduct, and reporting is still urgently needed.},
	pages = {e3001162},
	number = {4},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLoS} Biol},
	author = {Vinkers, Christiaan H. and Lamberink, Herm J. and Tijdink, Joeri K. and Heus, Pauline and Bouter, Lex and Glasziou, Paul and Moher, David and Damen, Johanna A. and Hooft, Lotty and Otte, Willem M.},
	editor = {Siegerink, Bob},
	urldate = {2023-10-11},
	date = {2021-04-19},
	langid = {english},
	file = {Vinkers et al. - 2021 - The methodological quality of 176,620 randomized controlled trials published between 1966 and 2018 r.pdf:/Users/tom/Zotero/storage/IFUF2PDX/Vinkers et al. - 2021 - The methodological quality of 176,620 randomized controlled trials published between 1966 and 2018 r.pdf:application/pdf},
}

@article{hair_systematic_2023,
	title = {Systematic online living evidence summaries: emerging tools to accelerate evidence synthesis},
	volume = {137},
	issn = {0143-5221, 1470-8736},
	url = {https://portlandpress.com/clinsci/article/137/10/773/233083/Systematic-online-living-evidence-summaries},
	doi = {10.1042/CS20220494},
	shorttitle = {Systematic online living evidence summaries},
	abstract = {Abstract
            Systematic reviews and meta-analysis are the cornerstones of evidence-based decision making and priority setting. However, traditional systematic reviews are time and labour intensive, limiting their feasibility to comprehensively evaluate the latest evidence in research-intensive areas. Recent developments in automation, machine learning and systematic review technologies have enabled efficiency gains. Building upon these advances, we developed Systematic Online Living Evidence Summaries ({SOLES}) to accelerate evidence synthesis. In this approach, we integrate automated processes to continuously gather, synthesise and summarise all existing evidence from a research domain, and report the resulting current curated content as interrogatable databases via interactive web applications. {SOLES} can benefit various stakeholders by (i) providing a systematic overview of current evidence to identify knowledge gaps, (ii) providing an accelerated starting point for a more detailed systematic review, and (iii) facilitating collaboration and coordination in evidence synthesis.},
	pages = {773--784},
	number = {10},
	journaltitle = {Clinical Science},
	author = {Hair, Kaitlyn and Wilson, Emma and Wong, Charis and Tsang, Anthony and Macleod, Malcolm and Bannach-Brown, Alexandra},
	urldate = {2023-10-11},
	date = {2023-05-31},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/F7Y3F53E/Hair et al. - 2023 - Systematic online living evidence summaries emerging tools to accelerate evidence synthesis.pdf:application/pdf},
}

@article{kiritchenko_exact_2010,
	title = {{ExaCT}: automatic extraction of clinical trial characteristics from journal publications},
	volume = {10},
	issn = {1472-6947},
	url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/1472-6947-10-56},
	doi = {10.1186/1472-6947-10-56},
	shorttitle = {{ExaCT}},
	pages = {56},
	number = {1},
	journaltitle = {{BMC} Medical Informatics and Decision Making},
	shortjournal = {{BMC} Med Inform Decis Mak},
	author = {Kiritchenko, Svetlana and De Bruijn, Berry and Carini, Simona and Martin, Joel and Sim, Ida},
	urldate = {2023-10-11},
	date = {2010-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/K3PR8KMH/Kiritchenko et al. - 2010 - ExaCT automatic extraction of clinical trial characteristics from journal publications.pdf:application/pdf},
}

@article{targ_meta-research_group__collaborators_estimating_2023,
	title = {Estimating the prevalence of discrepancies between study registrations and publications: a systematic review and meta-analyses},
	volume = {13},
	issn = {2044-6055, 2044-6055},
	url = {https://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2023-076264},
	doi = {10.1136/bmjopen-2023-076264},
	shorttitle = {Estimating the prevalence of discrepancies between study registrations and publications},
	abstract = {Objectives Prospectively registering study plans in a permanent time-stamped and publicly accessible document is becoming more common across disciplines and aims to reduce risk of bias and make risk of bias transparent. Selective reporting persists, however, when researchers deviate from their registered plans without disclosure. This systematic review aimed to estimate the prevalence of undisclosed discrepancies between prospectively registered study plans and their associated publication. We further aimed to identify the research disciplines where these discrepancies have been observed, whether interventions to reduce discrepancies have been conducted, and gaps in the literature. Design Systematic review and meta-analyses. Data sources Scopus and Web of Knowledge, published up to 15 December 2019. Eligibility criteria Articles that included quantitative data about discrepancies between registrations or study protocols and their associated publications. Data extraction and synthesis Each included article was independently coded by two reviewers using a coding form designed for this review (osf.io/728ys). We used randomeffects meta-analyses to synthesise the results.
Results We reviewed k=89 articles, which included k=70 that reported on primary outcome discrepancies from n=6314 studies and, k=22 that reported on secondary outcome discrepancies from n=1436 studies. Meta-analyses indicated that between 29\% and 37\% (95\% {CI}) of studies contained at least one primary outcome discrepancy and between 50\% and 75\% (95\% {CI}) contained at least one secondary outcome discrepancy. Almost all articles assessed clinical literature, and there was considerable heterogeneity. We identified only one article that attempted to correct discrepancies.
Conclusions Many articles did not include information on whether discrepancies were disclosed, which version of a registration they compared publications to and whether the registration was prospective. Thus, our estimates represent discrepancies broadly, rather than our target of undisclosed discrepancies between prospectively registered study plans and their associated publications. Discrepancies are common and reduce the trustworthiness of medical research. Interventions to reduce discrepancies could prove valuable. Registration osf.io/ktmdg. Protocol amendments are listed in online supplemental material A.},
	pages = {e076264},
	number = {10},
	journaltitle = {{BMJ} Open},
	shortjournal = {{BMJ} Open},
	author = {{TARG Meta-Research Group \& Collaborators}},
	urldate = {2023-10-11},
	date = {2023-10},
	langid = {english},
	file = {TARG Meta-Research Group & Collaborators - 2023 - Estimating the prevalence of discrepancies between study registrations and publications a systemati.pdf:/Users/tom/Zotero/storage/QR8P2VFY/TARG Meta-Research Group & Collaborators - 2023 - Estimating the prevalence of discrepancies between study registrations and publications a systemati.pdf:application/pdf},
}

@article{ivimeycook_implementing_2023,
	title = {Implementing code review in the scientific workflow: Insights from ecology and evolutionary biology},
	volume = {36},
	issn = {1010-061X, 1420-9101},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jeb.14230},
	doi = {10.1111/jeb.14230},
	shorttitle = {Implementing code review in the scientific workflow},
	abstract = {Abstract
            Code review increases reliability and improves reproducibility of research. As such, code review is an inevitable step in software development and is common in fields such as computer science. However, despite its importance, code review is noticeably lacking in ecology and evolutionary biology. This is problematic as it facilitates the propagation of coding errors and a reduction in reproducibility and reliability of published results. To address this, we provide a detailed commentary on how to effectively review code, how to set up your project to enable this form of review and detail its possible implementation at several stages throughout the research process. This guide serves as a primer for code review, and adoption of the principles and advice here will go a long way in promoting more open, reliable, and transparent ecology and evolutionary biology.},
	pages = {1347--1356},
	number = {10},
	journaltitle = {Journal of Evolutionary Biology},
	shortjournal = {J of Evolutionary Biology},
	author = {Ivimey‐Cook, Edward R. and Pick, Joel L. and Bairos‐Novak, Kevin R. and Culina, Antica and Gould, Elliot and Grainger, Matthew and Marshall, Benjamin M. and Moreau, David and Paquet, Matthieu and Royauté, Raphaël and Sánchez‐Tójar, Alfredo and Silva, Inês and Windecker, Saras M.},
	urldate = {2023-10-12},
	date = {2023-10},
	langid = {english},
	file = {Ivimey‐Cook et al. - 2023 - Implementing code review in the scientific workflow Insights from ecology and evolutionary biology.pdf:/Users/tom/Zotero/storage/57U33E2U/Ivimey‐Cook et al. - 2023 - Implementing code review in the scientific workflow Insights from ecology and evolutionary biology.pdf:application/pdf},
}

@article{aczel_consensus-based_2019,
	title = {A consensus-based transparency checklist},
	volume = {4},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-019-0772-6},
	doi = {10.1038/s41562-019-0772-6},
	pages = {4--6},
	number = {1},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharský, Šimon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munafò, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ronán M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and Giner-Sorolla, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and De La Guardia, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
	urldate = {2023-10-13},
	date = {2019-12-02},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/XGBHCBJ4/Aczel et al. - 2019 - A consensus-based transparency checklist.pdf:application/pdf},
}

@book{llaudet_data_2023,
	location = {Princeton Oxford},
	title = {Data analysis for social science: a friendly and practical introduction},
	isbn = {978-0-691-19942-9 978-0-691-19943-6},
	shorttitle = {Data analysis for social science},
	pagetotal = {238},
	publisher = {Princeton University Press},
	author = {Llaudet, Elena and Imai, Kosuke},
	date = {2023},
	file = {Llaudet and Imai - 2023 - Data analysis for social science a friendly and practical introduction.pdf:/Users/tom/Zotero/storage/BL6H4695/Llaudet and Imai - 2023 - Data analysis for social science a friendly and practical introduction.pdf:application/pdf},
}

@article{oconnor_focus_2020,
	title = {A focus on cross-purpose tools, automated recognition of study design in multiple disciplines, and evaluation of automation tools: a summary of significant discussions at the fourth meeting of the International Collaboration for Automation of Systematic Reviews ({ICASR})},
	volume = {9},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-020-01351-4},
	doi = {10.1186/s13643-020-01351-4},
	shorttitle = {A focus on cross-purpose tools, automated recognition of study design in multiple disciplines, and evaluation of automation tools},
	abstract = {Abstract
            The fourth meeting of the International Collaboration for Automation of Systematic Reviews ({ICASR}) was held 5–6 November 2019 in The Hague, the Netherlands. {ICASR} is an interdisciplinary group whose goal is to maximize the use of technology for conducting rapid, accurate, and efficient systematic reviews of scientific evidence. The group seeks to facilitate the development and acceptance of automated techniques for systematic reviews. In 2018, the major themes discussed were the transferability of automation tools (i.e., tools developed for other purposes that might be used by systematic reviewers), the automated recognition of study design in multiple disciplines and applications, and approaches for the evaluation of automation tools.},
	pages = {100},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {O’Connor, Annette M. and Glasziou, Paul and Taylor, Michele and Thomas, James and Spijker, René and Wolfe, Mary S.},
	urldate = {2023-10-13},
	date = {2020-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/UY6HL4YL/O’Connor et al. - 2020 - A focus on cross-purpose tools, automated recognition of study design in multiple disciplines, and e.pdf:application/pdf},
}

@article{sever_biomedical_2023,
	title = {Biomedical publishing: Past historic, present continuous, future conditional},
	volume = {21},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.3002234},
	doi = {10.1371/journal.pbio.3002234},
	shorttitle = {Biomedical publishing},
	abstract = {Academic journals have been publishing the results of biomedical research for more than 350 years. Reviewing their history reveals that the ways in which journals vet submissions have changed over time, culminating in the relatively recent appearance of the current peer-review process. Journal brand and Impact Factor have meanwhile become quality proxies that are widely used to filter articles and evaluate scientists in a hypercompetitive prestige economy. The Web created the potential for a more decoupled publishing system in which articles are initially disseminated by preprint servers and then undergo evaluation elsewhere. To build this future, we must first understand the roles journals currently play and consider what types of content screening and review are necessary and for which papers. A new, open ecosystem involving preprint servers, journals, independent content-vetting initiatives, and curation services could provide more multidimensional signals for papers and avoid the current conflation of trust, quality, and impact. Academia should strive to avoid the alternative scenario, however, in which stratified publisher silos lock in submissions and simply perpetuate this conflation.},
	pages = {e3002234},
	number = {10},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLoS} Biol},
	author = {Sever, Richard},
	urldate = {2023-10-13},
	date = {2023-10-03},
	langid = {english},
	keywords = {toread},
	file = {Full Text:/Users/tom/Zotero/storage/GJD8P7SV/Sever - 2023 - Biomedical publishing Past historic, present continuous, future conditional.pdf:application/pdf},
}

@article{turner_does_2012,
	title = {Does use of the {CONSORT} Statement impact the completeness of reporting of randomised controlled trials published in medical journals? A Cochrane reviewa},
	volume = {1},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/2046-4053-1-60},
	doi = {10.1186/2046-4053-1-60},
	shorttitle = {Does use of the {CONSORT} Statement impact the completeness of reporting of randomised controlled trials published in medical journals?},
	abstract = {Abstract
            
              Background
              The Consolidated Standards of Reporting Trials ({CONSORT}) Statement is intended to facilitate better reporting of randomised clinical trials ({RCTs}). A systematic review recently published in the Cochrane Library assesses whether journal endorsement of {CONSORT} impacts the completeness of reporting of {RCTs}; those findings are summarised here.
            
            
              Methods
              Evaluations assessing the completeness of reporting of {RCTs} based on any of 27 outcomes formulated based on the 1996 or 2001 {CONSORT} checklists were included; two primary comparisons were evaluated. The 27 outcomes were: the 22 items of the 2001 {CONSORT} checklist, four sub-items describing blinding and a ‘total summary score’ of aggregate items, as reported. Relative risks ({RR}) and 99\% confidence intervals were calculated to determine effect estimates for each outcome across evaluations.
            
            
              Results
              Fifty-three reports describing 50 evaluations of 16,604 {RCTs} were assessed for adherence to at least one of 27 outcomes. Sixty-nine of 81 meta-analyses show relative benefit from {CONSORT} endorsement on completeness of reporting. Between endorsing and non-endorsing journals, 25 outcomes are improved with {CONSORT} endorsement, five of these significantly (α = 0.01). The number of evaluations per meta-analysis was often low with substantial heterogeneity; validity was assessed as low or unclear for many evaluations.
            
            
              Conclusions
              The results of this review suggest that journal endorsement of {CONSORT} may benefit the completeness of reporting of {RCTs} they publish. No evidence suggests that endorsement hinders the completeness of {RCT} reporting. However, despite relative improvements when {CONSORT} is endorsed by journals, the completeness of reporting of trials remains sub-optimal. Journals are not sending a clear message about endorsement to authors submitting manuscripts for publication. As such, fidelity of endorsement as an ‘intervention’ has been weak to date. Journals need to take further action regarding their endorsement and implementation of {CONSORT} to facilitate accurate, transparent and complete reporting of trials.},
	pages = {60},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {Turner, Lucy and Shamseer, Larissa and Altman, Douglas G and Schulz, Kenneth F and Moher, David},
	urldate = {2023-10-13},
	date = {2012-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/JSQB4QDE/Turner et al. - 2012 - Does use of the CONSORT Statement impact the completeness of reporting of randomised controlled tria.pdf:application/pdf},
}

@article{van_zwet_how_2022,
	title = {How large should the next study be? Predictive power and sample size requirements for replication studies},
	volume = {41},
	issn = {0277-6715, 1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.9406},
	doi = {10.1002/sim.9406},
	shorttitle = {How large should the next study be?},
	abstract = {Abstract
            We use information derived from over 40K trials in the Cochrane Collaboration database of systematic reviews ({CDSR}) to compute the replication probability, or predictive power of an experiment given its observed (two‐sided) ‐value. We find that an exact replication of a marginally significant result with  has less than 30\% chance of again reaching significance. Moreover, the replication of a result with  still has only 50\% chance of significance. We also compute the probability that the direction (sign) of the estimated effect is correct, which is closely related to the type S error of Gelman and Tuerlinckx. We find that if an estimated effect has , there is a 93\% probability that its sign is correct. If , then that probability is 99\%. Finally, we compute the required sample size for a replication study to achieve some specified power conditional on the ‐value of the original study. We find that the replication of a result with  requires a sample size more than 16 times larger than the original study to achieve 80\% power, while  requires at least 3.5 times larger sample size. These findings confirm that failure to replicate the statistical significance of a trial does not necessarily indicate that the original result was a fluke.},
	pages = {3090--3101},
	number = {16},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Statistics in Medicine},
	author = {Van Zwet, Erik W. and Goodman, Steven N.},
	urldate = {2023-10-13},
	date = {2022-07-20},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/Q2TK3WRJ/Van Zwet and Goodman - 2022 - How large should the next study be Predictive power and sample size requirements for replication st.pdf:application/pdf},
}

@article{hoenig_abuse_2001,
	title = {The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis},
	volume = {55},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313001300339897},
	doi = {10.1198/000313001300339897},
	shorttitle = {The Abuse of Power},
	pages = {19--24},
	number = {1},
	journaltitle = {The American Statistician},
	shortjournal = {The American Statistician},
	author = {Hoenig, John M and Heisey, Dennis M},
	urldate = {2023-10-13},
	date = {2001-02},
	langid = {english},
	file = {Hoenig and Heisey - 2001 - The Abuse of Power The Pervasive Fallacy of Power Calculations for Data Analysis.pdf:/Users/tom/Zotero/storage/VQEXNZLH/Hoenig and Heisey - 2001 - The Abuse of Power The Pervasive Fallacy of Power Calculations for Data Analysis.pdf:application/pdf},
}

@article{moustgaard_impact_2020,
	title = {Impact of blinding on estimated treatment effects in randomised clinical trials: meta-epidemiological study},
	issn = {1756-1833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.l6802},
	doi = {10.1136/bmj.l6802},
	shorttitle = {Impact of blinding on estimated treatment effects in randomised clinical trials},
	abstract = {Abstract
            
              Objectives
              To study the impact of blinding on estimated treatment effects, and their variation between trials; differentiating between blinding of patients, healthcare providers, and observers; detection bias and performance bias; and types of outcome (the {MetaBLIND} study).
            
            
              Design
              Meta-epidemiological study.
            
            
              Data source
              Cochrane Database of Systematic Reviews (2013-14).
            
            
              Eligibility criteria for selecting studies
              Meta-analyses with both blinded and non-blinded trials on any topic.
            
            
              Review methods
              Blinding status was retrieved from trial publications and authors, and results retrieved automatically from the Cochrane Database of Systematic Reviews. Bayesian hierarchical models estimated the average ratio of odds ratios ({ROR}), and estimated the increases in heterogeneity between trials, for non-blinded trials (or of unclear status) versus blinded trials. Secondary analyses adjusted for adequacy of concealment of allocation, attrition, and trial size, and explored the association between outcome subjectivity (high, moderate, low) and average bias. An {ROR} lower than 1 indicated exaggerated effect estimates in trials without blinding.
            
            
              Results
              The study included 142 meta-analyses (1153 trials). The {ROR} for lack of blinding of patients was 0.91 (95\% credible interval 0.61 to 1.34) in 18 meta-analyses with patient reported outcomes, and 0.98 (0.69 to 1.39) in 14 meta-analyses with outcomes reported by blinded observers. The {ROR} for lack of blinding of healthcare providers was 1.01 (0.84 to 1.19) in 29 meta-analyses with healthcare provider decision outcomes (eg, readmissions), and 0.97 (0.64 to 1.45) in 13 meta-analyses with outcomes reported by blinded patients or observers. The {ROR} for lack of blinding of observers was 1.01 (0.86 to 1.18) in 46 meta-analyses with subjective observer reported outcomes, with no clear impact of degree of subjectivity. Information was insufficient to determine whether lack of blinding was associated with increased heterogeneity between trials. The {ROR} for trials not reported as double blind versus those that were double blind was 1.02 (0.90 to 1.13) in 74 meta-analyses.
            
            
              Conclusion
              No evidence was found for an average difference in estimated treatment effect between trials with and without blinded patients, healthcare providers, or outcome assessors. These results could reflect that blinding is less important than often believed or meta-epidemiological study limitations, such as residual confounding or imprecision. At this stage, replication of this study is suggested and blinding should remain a methodological safeguard in trials.},
	pages = {l6802},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Moustgaard, Helene and Clayton, Gemma L and Jones, Hayley E and Boutron, Isabelle and Jørgensen, Lars and Laursen, David R T and Olsen, Mette F and Paludan-Müller, Asger and Ravaud, Philippe and Savović, Jelena and Sterne, Jonathan A C and Higgins, Julian P T and Hróbjartsson, Asbjørn},
	urldate = {2023-10-13},
	date = {2020-01-21},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/G4CCTISM/Moustgaard et al. - 2020 - Impact of blinding on estimated treatment effects in randomised clinical trials meta-epidemiologica.pdf:application/pdf},
}

@article{kilicoglu_toward_2021,
	title = {Toward assessing clinical trial publications for reporting transparency},
	volume = {116},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046421000460},
	doi = {10.1016/j.jbi.2021.103717},
	pages = {103717},
	journaltitle = {Journal of Biomedical Informatics},
	shortjournal = {Journal of Biomedical Informatics},
	author = {Kilicoglu, Halil and Rosemblat, Graciela and Hoang, Linh and Wadhwa, Sahil and Peng, Zeshan and Malički, Mario and Schneider, Jodi and Ter Riet, Gerben},
	urldate = {2023-10-13},
	date = {2021-04},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/GFZ8FJZM/Kilicoglu et al. - 2021 - Toward assessing clinical trial publications for reporting transparency.pdf:application/pdf},
}

@article{chopra_null_2023,
	title = {The Null Result Penalty},
	issn = {0013-0133, 1468-0297},
	url = {https://academic.oup.com/ej/advance-article/doi/10.1093/ej/uead060/7238466},
	doi = {10.1093/ej/uead060},
	abstract = {Abstract
            We examine how the evaluation of research studies in economics depends on whether a study yielded a null result. Studies with null results are perceived to be less publishable, of lower quality, less important and less precisely estimated than studies with large and statistically significant results, even when holding constant all other study features, including the sample size and the precision of the estimates. The null result penalty is of similar magnitude among {PhD} students and journal editors. The penalty is larger when experts predict a large effect and when statistical uncertainty is communicated with p-values rather than standard errors. Our findings highlight the value of a pre-result review.},
	pages = {uead060},
	journaltitle = {The Economic Journal},
	author = {Chopra, Felix and Haaland, Ingar and Roth, Christopher and Stegmann, Andreas},
	urldate = {2023-10-13},
	date = {2023-08-07},
	langid = {english},
	keywords = {toread},
}

@article{wang_scientific_2023,
	title = {Scientific discovery in the age of artificial intelligence},
	volume = {620},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06221-2},
	doi = {10.1038/s41586-023-06221-2},
	pages = {47--60},
	number = {7972},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and Anandkumar, Anima and Bergen, Karianne and Gomes, Carla P. and Ho, Shirley and Kohli, Pushmeet and Lasenby, Joan and Leskovec, Jure and Liu, Tie-Yan and Manrai, Arjun and Marks, Debora and Ramsundar, Bharath and Song, Le and Sun, Jimeng and Tang, Jian and Veličković, Petar and Welling, Max and Zhang, Linfeng and Coley, Connor W. and Bengio, Yoshua and Zitnik, Marinka},
	urldate = {2023-10-13},
	date = {2023-08-03},
	langid = {english},
	file = {Wang et al. - 2023 - Scientific discovery in the age of artificial intelligence.pdf:/Users/tom/Zotero/storage/Q7NWHZ2C/Wang et al. - 2023 - Scientific discovery in the age of artificial intelligence.pdf:application/pdf},
}

@report{kennedy_lessons_2023,
	title = {Lessons and recommendations from a research audit for the transparent psi project ({TPP})},
	url = {https://osf.io/3wz6f},
	abstract = {The lessons and recommendations described here were developed from conducting a research audit of the Transparent Psi Project ({TPP}). The {TPP} was intended as a potential model and learning experience for optimal research practices. The topics discussed here are among the potential lessons from the {TPP}.  The recommendations are:  (1) Validation of data collection software is essential.  (2) Evaluation of power or operating characteristics (inferential errors) is essential, including for sequential Bayesian analyses.  (3) An online repository copy of data is very valuable, but easy-to-use, reliable, secure processes remain to be developed.  (4) A good research audit is much more extensive than current peer review for publication, including Registered Reports.  (5) Quality control ({QC}) comes before auditing.  (6) Good research practice guidelines and/or standard operating procedures ({SOPs}) for general use would be very valuable for confirmatory psychological research.  (7) It is time to implement routine measures to prevent experimenter fraud.  (8) Git appears to be not optimal for software tracking to prevent programming fraud.},
	institution = {{PsyArXiv}},
	type = {preprint},
	author = {Kennedy, James E.},
	urldate = {2023-10-13},
	date = {2023-04-19},
	doi = {10.31234/osf.io/3wz6f},
	file = {Submitted Version:/Users/tom/Zotero/storage/XPWAFUZI/Kennedy - 2023 - Lessons and Recommendations from a Research Audit for the Transparent Psi Project (TPP).pdf:application/pdf},
}

@article{bierman_testing_2016,
	title = {Testing for questionable research practices in a meta-analysis: an example from experimental parapsychology},
	volume = {11},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0153049},
	doi = {10.1371/journal.pone.0153049},
	shorttitle = {Testing for questionable research practices in a meta-analysis},
	pages = {e0153049},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Bierman, Dick J. and Spottiswoode, James P. and Bijl, Aron},
	editor = {Laws, Keith},
	urldate = {2023-10-13},
	date = {2016-05-04},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/K8SXVBXB/Bierman et al. - 2016 - Testing for Questionable Research Practices in a Meta-Analysis An Example from Experimental Parapsy.pdf:application/pdf},
}

@article{demszky_using_2023,
	title = {Using large language models in psychology},
	issn = {2731-0574},
	url = {https://www.nature.com/articles/s44159-023-00241-5},
	doi = {10.1038/s44159-023-00241-5},
	journaltitle = {Nature Reviews Psychology},
	shortjournal = {Nat Rev Psychol},
	author = {Demszky, Dorottya and Yang, Diyi and Yeager, David S. and Bryan, Christopher J. and Clapper, Margarett and Chandhok, Susannah and Eichstaedt, Johannes C. and Hecht, Cameron and Jamieson, Jeremy and Johnson, Meghann and Jones, Michaela and Krettek-Cobb, Danielle and Lai, Leslie and {JonesMitchell}, Nirel and Ong, Desmond C. and Dweck, Carol S. and Gross, James J. and Pennebaker, James W.},
	urldate = {2023-10-15},
	date = {2023-10-13},
	langid = {english},
	file = {Demszky et al. - 2023 - Using large language models in psychology.pdf:/Users/tom/Zotero/storage/HISTSP8J/Demszky et al. - 2023 - Using large language models in psychology.pdf:application/pdf},
}

@article{fergusson_post-randomisation_2002-1,
	title = {Post-randomisation exclusions: the intention to treat principle and excluding patients from analysis},
	volume = {325},
	issn = {09598138, 14685833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.325.7365.652},
	doi = {10.1136/bmj.325.7365.652},
	shorttitle = {Post-randomisation exclusions},
	pages = {652--654},
	number = {7365},
	journaltitle = {{BMJ}},
	author = {Fergusson, D.},
	urldate = {2023-10-15},
	date = {2002-09-21},
	file = {Full Text:/Users/tom/Zotero/storage/ZSUWZDHL/Fergusson - 2002 - Post-randomisation exclusions the intention to treat principle and excluding patients from analysis.pdf:application/pdf},
}

@article{hernan_causal_nodate,
	title = {Causal Inference: What If},
	author = {Hernan, Miguel A and Robins, James M},
	langid = {english},
	file = {Hernan and Robins - Causal Inference What If.pdf:/Users/tom/Zotero/storage/IFAS3SP9/Hernan and Robins - Causal Inference What If.pdf:application/pdf},
}

@article{scoggins_measuring_nodate,
	title = {Measuring transparency in the social sciences: political science and international relations},
	abstract = {The scientiﬁc method is predicated on transparency – yet the pace at which transparent research practices are being adopted by the scientiﬁc community is slow. The replication crisis in psychology showed that published ﬁndings employing statistical inference are threatened by undetected errors, data manipulation, and data falsiﬁcation. To mitigate these problems and bolster research credibility, open data and preregistration practices have gained traction in the natural and social sciences. However, the extent of their adoption in diﬀerent disciplines are unknown. We introduce procedures to identify the transparency of a research ﬁeld using large-scale text analysis and machine learning classiﬁers. Using political science and international relations as an illustrative case, we examine 93,931 articles across the top 160 political science and international relations journals between 2010 and 2021. We ﬁnd that approximately 21\% of all statistical inference papers have open data and 5\% of all experiments are preregistered. Despite this shortfall, the example of leading journals in the ﬁeld shows that change is feasible and can be eﬀected quickly.},
	number = {14},
	author = {Scoggins, Bermond and Robertson, Matthew P},
	langid = {english},
	file = {Scoggins and Robertson - Measuring Transparency in the Social Sciences Political Science and International Relations.pdf:/Users/tom/Zotero/storage/YWK57XQX/Scoggins and Robertson - Measuring Transparency in the Social Sciences Political Science and International Relations.pdf:application/pdf},
}

@article{schultz_assessing_2019,
	title = {Assessing the effectiveness of open access finding tools},
	volume = {38},
	issn = {2163-5226, 0730-9295},
	url = {https://ital.corejournals.org/index.php/ital/article/view/11009},
	doi = {10.6017/ital.v38i3.11009},
	abstract = {The open access ({OA}) movement seeks to ensure that scholarly knowledge is available to anyone with internet access, but being available for free online is of little use if people cannot find open versions. A handful of tools have become available in recent years to help address this problem by searching for an open version of a document whenever a user hits a paywall. This project set out to study how effective four of these tools are when compared to each other and to Google Scholar, which has long been a source of finding {OA} versions. To do this, the project used Open Access Button, Unpaywall, Lazy Scholar, and Kopernio to search for open versions of 1,000 articles. Results show none of the tools found as many successful hits as Google Scholar, but two of the tools did register unique successful hits, indicating a benefit to incorporating them in searches for {OA} versions. Some of the tools also include additional features that can further benefit users in their search for accessible scholarly knowledge.},
	pages = {82--90},
	number = {3},
	journaltitle = {Information Technology and Libraries},
	shortjournal = {{ITAL}},
	author = {Schultz, Teresa Auch and Azadbakht, Elena and Bull, Jonathan and Bucy, Rosalind and Floyd, Jeremy},
	urldate = {2023-10-15},
	date = {2019-09-15},
	file = {Full Text:/Users/tom/Zotero/storage/H4CIPSED/Schultz et al. - 2019 - Assessing the Effectiveness of Open Access Finding Tools.pdf:application/pdf},
}

@report{morehouse_responsible_2023,
	title = {Responsible data sharing: Identifying and remedying possible re-identification of human participants},
	url = {https://osf.io/5m3cx},
	shorttitle = {Responsible data sharing},
	abstract = {Open data collected from humans creates a tension between scholarly values of transparency and sharing on the one hand, and privacy and security on the other. A common solution is to make datasets anonymous by removing personally identifying information before sharing. However, ostensibly anonymized datasets may be at risk of re-identification if they include demographic information. In the present article, we (a) review current privacy standards; (b) describe computer science data protection frameworks and their adaptability to the social sciences; (c) provide practical guidance for assessing and addressing re-identification risk; (d) introduce two open-source algorithms – {MinBlur} and {MinBlurLite} – to increase privacy while maintaining the integrity of open data; and (e) highlight aspects of ethical data sharing that require further attention. Technical innovations can support competing values so that science can be as open as possible to promote transparency and sharing, and as closed as necessary to maintain privacy and security.},
	institution = {{MetaArXiv}},
	type = {preprint},
	author = {Morehouse, Kirsten and Kurdi, Benedek and Nosek, Brian A.},
	urldate = {2023-10-15},
	date = {2023-09-25},
	doi = {10.31222/osf.io/5m3cx},
	file = {Submitted Version:/Users/tom/Zotero/storage/L4M2XWZN/Morehouse et al. - 2023 - Responsible data sharing Identifying and remedying possible re-identification of human participants.pdf:application/pdf},
}

@article{siebert_assessing_2023,
	title = {Assessing the magnitude of changes from protocol to publication—a survey on Cochrane and non-Cochrane Systematic Reviews},
	volume = {11},
	issn = {2167-8359},
	url = {https://peerj.com/articles/16016},
	doi = {10.7717/peerj.16016},
	abstract = {Objective. To explore differences between published reviews and their respective protocols in a sample of 97 non-Cochrane Systematic Reviews (non-{CSRs}) and 97 Cochrane Systematic Reviews ({CSRs}) in terms of {PICOS} (Patients/Population, Intervention, Comparison/Control, Outcome, Study type) elements and the extent to which they were reported. Study Design and Setting. We searched {PubMed} and Cochrane databases to identify non-{CSRs} and {CSRs} that were published in 2018. We then searched for their corresponding Cochrane or {PROSPERO} protocols. The published reviews were compared to their protocols. The primary outcome was changes from protocol to review in terms of {PICOS} elements.
Results. We identified a total of 227 changes from protocol to review in {PICOS} elements, 1.11 (Standard Deviation ({SD}), 1.22) changes per review for {CSRs} and 1.23 ({SD}, 1.12) for non-{CSRs} per review. More than half of each sub-sample (54.6\% of {CSRs} and 67.0\% of non-{CSRs}) (Absolute Risk Reduction ({ARR}) 12.4\% [−1.3\%; 26.0\%]) had changes in {PICOS} elements. For both subsamples, approximately a third of all changes corresponded to changes related to primary outcomes. Marked differences were found between the sub-samples for the reporting of changes. 95.8\% of the changes in {PICOS} items were not reported in the non-{CSRs} compared to 42.6\% in the {CSRs} ({ARR} 53.2\% [43.2\%; 63.2\%]).
Conclusion. {CSRs} showed better results than non-{CSRs} in terms of the reporting of changes. Reporting of changes from protocol needs to be promoted and requires general improvement. The limitations of this study lie in its observational design. Registration: https://osf.io/6j8gd/.},
	pages = {e16016},
	journaltitle = {{PeerJ}},
	author = {Siebert, Maximilian and Caquelin, Laura and Madera, Meisser and Acosta-Dighero, Roberto and Naudet, Florian and Roqué, Marta},
	urldate = {2023-10-15},
	date = {2023-10-02},
	langid = {english},
	file = {Siebert et al. - 2023 - Assessing the magnitude of changes from protocol to publication—a survey on Cochrane and non-Cochran.pdf:/Users/tom/Zotero/storage/ZVIDPKX7/Siebert et al. - 2023 - Assessing the magnitude of changes from protocol to publication—a survey on Cochrane and non-Cochran.pdf:application/pdf},
}

@article{manago_preregistration_2023-1,
	title = {Preregistration and Registered Reports in Sociology: Strengths, Weaknesses, and Other Considerations},
	volume = {54},
	issn = {0003-1232, 1936-4784},
	url = {https://link.springer.com/10.1007/s12108-023-09563-6},
	doi = {10.1007/s12108-023-09563-6},
	shorttitle = {Preregistration and Registered Reports in Sociology},
	abstract = {Abstract
            Both within and outside of sociology, there are conversations about methods to reduce error and improve research quality—one such method is preregistration and its counterpart, registered reports. Preregistration is the process of detailing research questions, variables, analysis plans, etc. before conducting research. Registered reports take this one step further, with a paper being reviewed on the merit of these plans, not its findings. In this manuscript, I detail preregistration’s and registered reports’ strengths and weaknesses for improving the quality of sociological research. I conclude by considering the implications of a structural-level adoption of preregistration and registered reports. Importantly, I do not recommend that all sociologists use preregistration and registered reports for all studies. Rather, I discuss the potential benefits and genuine limitations of preregistration and registered reports for the individual sociologist and the discipline.},
	pages = {193--210},
	number = {1},
	journaltitle = {The American Sociologist},
	shortjournal = {Am Soc},
	author = {Manago, Bianca},
	urldate = {2023-10-15},
	date = {2023-03},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/QJWM8IJT/Manago - 2023 - Preregistration and Registered Reports in Sociology Strengths, Weaknesses, and Other Considerations.pdf:application/pdf},
}

@article{kilicoglu_methodology_2023,
	title = {Methodology reporting improved over time in 176,469 randomized controlled trials},
	volume = {162},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435623002020},
	doi = {10.1016/j.jclinepi.2023.08.004},
	pages = {19--28},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {Journal of Clinical Epidemiology},
	author = {Kilicoglu, Halil and Jiang, Lan and Hoang, Linh and Mayo-Wilson, Evan and Vinkers, Christiaan H. and Otte, Willem M.},
	urldate = {2023-10-15},
	date = {2023-10},
	langid = {english},
	file = {Kilicoglu et al. - 2023 - Methodology reporting improved over time in 176,469 randomized controlled trials.pdf:/Users/tom/Zotero/storage/9XKKG2LE/Kilicoglu et al. - 2023 - Methodology reporting improved over time in 176,469 randomized controlled trials.pdf:application/pdf},
}

@article{boschen_changes_2023,
	title = {Changes in methodological study characteristics in psychology between 2010-2021},
	volume = {18},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0283353},
	doi = {10.1371/journal.pone.0283353},
	abstract = {In 2015, the Open Science Collaboration repeated a series of 100 psychological experiments. Since a considerable part of these replications could not confirm the original effects and some of them pointed in the opposite direction, psychological research is said to lack reproducibility. Several general criticisms can explain this finding, such as the standardized use of undirected nil-null hypothesis tests, samples being too small and selective, lack of corrections for multiple testing, but also some widespread questionable research practices and incentives to publish positive results only. A selection of 57,909 articles from 12 renowned journals is processed with the
              {JATSdecoder}
              software to analyze the extent to which several empirical research practices in psychology have changed over the past 12 years. To identify journal- and time-specific changes, the relative use of statistics based on p-values, the number of reported p-values per paper, the relative use of confidence intervals, directed tests, power analysis, Bayesian procedures, non-standard
              α
              levels, correction procedures for multiple testing, and median sample sizes are analyzed for articles published between 2010 and 2015 and after 2015, and in more detail for every included journal and year of publication. In addition, the origin of authorships is analyzed over time. Compared to articles that were published in and before 2015, the median number of reported p-values per article has decreased from 14 to 12, whereas the median proportion of significant p-values per article remained constant at 69\%. While reports of effect sizes and confidence intervals have increased, the
              α
              level is usually set to the default value of .05. The use of corrections for multiple testing has decreased. Although uncommon in each case (4\% in total), directed testing is used less frequently, while Bayesian inference has become more common after 2015. The overall median estimated sample size has increased from 105 to 190.},
	pages = {e0283353},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Böschen, Ingmar},
	editor = {Toffalini, Enrico},
	urldate = {2023-10-15},
	date = {2023-05-10},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/XVLW2D2Z/Böschen - 2023 - Changes in methodological study characteristics in psychology between 2010-2021.pdf:application/pdf},
}

@article{findley_external_nodate,
	title = {External Validity},
	abstract = {External validity captures the extent to which inferences drawn from a given study’s sample apply to a broader population or other target populations. Social scientists frequently invoke external validity as an ideal, but they rarely attempt to make rigorous, credible external validity inferences. In recent years, methodologically oriented scholars have advanced a flurry of work on various components of external validity, and this article reviews and systematizes many of those insights. We first clarify the core conceptual dimensions of external validity and introduce a simple formalization that demonstrates why external validity matters so critically. We then organize disparate arguments about how to address external validity by advancing three evaluative criteria: model utility, scope plausibility, and specification credibility. We conclude with a practical aspiration that scholars supplement existing reporting standards to include routine discussion of external validity. It is our hope that these evaluation and reporting standards help rebalance scientific inquiry, such that the current obsession with causal inference is complemented with an equal interest in generalized knowledge.},
	author = {Findley, Michael G and Kikuta, Kyosuke and Denly, Michael},
	langid = {english},
	keywords = {toread},
	file = {Findley et al. - External Validity.pdf:/Users/tom/Zotero/storage/FFY8VLWE/Findley et al. - External Validity.pdf:application/pdf},
}

@article{fidler_epistemic_2018-1,
	title = {The epistemic importance of establishing the absence of an effect},
	volume = {1},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/2515245918770407},
	doi = {10.1177/2515245918770407},
	pages = {237--244},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Fidler, Fiona and Singleton Thorn, Felix and Barnett, Ashley and Kambouris, Steven and Kruger, Ariel},
	urldate = {2023-10-15},
	date = {2018-06},
	langid = {english},
	keywords = {toread},
	file = {Full Text:/Users/tom/Zotero/storage/H835S4L5/Fidler et al. - 2018 - The Epistemic Importance of Establishing the Absence of an Effect.pdf:application/pdf},
}

@article{crockett_limitations_2023,
	title = {The limitations of machine learning models for predicting scientific replicability},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2307596120},
	doi = {10.1073/pnas.2307596120},
	pages = {e2307596120},
	number = {33},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Crockett, M. J. and Bai, Xuechunzi and Kapoor, Sayash and Messeri, Lisa and Narayanan, Arvind},
	urldate = {2023-10-15},
	date = {2023-08-15},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/7KM3FVF3/Crockett et al. - 2023 - The limitations of machine learning models for predicting scientific replicability.pdf:application/pdf},
}

@article{pigliucci_prove_2014-1,
	title = {Prove it! The Burden of Proof Game in Science vs. Pseudoscience Disputes},
	volume = {42},
	issn = {0048-3893, 1574-9274},
	url = {http://link.springer.com/10.1007/s11406-013-9500-z},
	doi = {10.1007/s11406-013-9500-z},
	abstract = {The concept of burden of proof is used in a wide range of discourses, from philosophy to law, science, skepticism, and even in everyday reasoning. This paper provides an analysis of the proper deployment of burden of proof, focusing in particular on skeptical discussions of pseudoscience and the paranormal, where burden of proof assignments are most poignant and relatively clear-cut. We argue that burden of proof is often misapplied or used as a mere rhetorical gambit, with little appreciation of the underlying principles. The paper elaborates on an important distinction between evidential and prudential varieties of burdens of proof, which is cashed out in terms of Bayesian probabilities and error management theory. Finally, we explore the relationship between burden of proof and several (alleged) informal logical fallacies. This allows us to get a firmer grip on the concept and its applications in different domains, and also to clear up some confusions with regard to when exactly some fallacies (ad hominem, ad ignorantiam, and petitio principii) may or may not occur.},
	pages = {487--502},
	number = {2},
	journaltitle = {Philosophia},
	shortjournal = {Philosophia},
	author = {Pigliucci, Massimo and Boudry, Maarten},
	urldate = {2023-10-15},
	date = {2014-06},
	langid = {english},
	file = {Pigliucci and Boudry - 2014 - Prove it! The Burden of Proof Game in Science vs. Pseudoscience Disputes.pdf:/Users/tom/Zotero/storage/KE3L76MW/Pigliucci and Boudry - 2014 - Prove it! The Burden of Proof Game in Science vs. Pseudoscience Disputes.pdf:application/pdf},
}

@article{sofi-mahmudi_transparency_2023,
	title = {Transparency of {COVID}-19-related research: A meta-research study},
	volume = {18},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0288406},
	doi = {10.1371/journal.pone.0288406},
	shorttitle = {Transparency of {COVID}-19-related research},
	abstract = {Background
              We aimed to assess the adherence to five transparency practices (data availability, code availability, protocol registration and conflicts of interest ({COI}), and funding disclosures) from open access Coronavirus disease 2019 ({COVID}-19) related articles.
            
            
              Methods
              We searched and exported all open access {COVID}-19-related articles from {PubMed}-indexed journals in the Europe {PubMed} Central database published from January 2020 to June 9, 2022. With a validated and automated tool, we detected transparent practices of three paper types: research articles, randomized controlled trials ({RCTs}), and reviews. Basic journal- and article-related information were retrieved from the database. We used R for the descriptive analyses.
            
            
              Results
              The total number of articles was 258,678, of which we were able to retrieve full texts of 186,157 (72\%) articles from the database Over half of the papers (55.7\%, n = 103,732) were research articles, 10.9\% (n = 20,229) were review articles, and less than one percent (n = 1,202) were {RCTs}. Approximately nine-tenths of articles (in all three paper types) had a statement to disclose {COI}. Funding disclosure (83.9\%, confidence interval ({CI}): 81.7–85.8 95\%) and protocol registration (53.5\%, 95\% {CI}: 50.7–56.3) were more frequent in {RCTs} than in reviews or research articles. Reviews shared data (2.5\%, 95\% {CI}: 2.3–2.8) and code (0.4\%, 95\% {CI}: 0.4–0.5) less frequently than {RCTs} or research articles. Articles published in 2022 had the highest adherence to all five transparency practices. Most of the reviews (62\%) and research articles (58\%) adhered to two transparency practices, whereas almost half of the {RCTs} (47\%) adhered to three practices. There were journal- and publisher-related differences in all five practices, and articles that did not adhere to transparency practices were more likely published in lowest impact journals and were less likely cited.
            
            
              Conclusion
              While most articles were freely available and had a {COI} disclosure, adherence to other transparent practices was far from acceptable. A much stronger commitment to open science practices, particularly to protocol registration, data and code sharing, is needed from all stakeholders.},
	pages = {e0288406},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Sofi-Mahmudi, Ahmad and Raittio, Eero and Uribe, Sergio E.},
	editor = {Lucas-Dominguez, Rut},
	urldate = {2023-10-15},
	date = {2023-07-26},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/XABUED6H/Sofi-Mahmudi et al. - 2023 - Transparency of COVID-19-related research A meta-research study.pdf:application/pdf},
}

@article{dennett_higher-order_2006,
	title = {Higher-order truths about chmess},
	volume = {25},
	issn = {0167-7411, 1572-8749},
	url = {http://link.springer.com/10.1007/s11245-006-0005-2},
	doi = {10.1007/s11245-006-0005-2},
	pages = {39--41},
	number = {1},
	journaltitle = {Topoi},
	shortjournal = {Topoi},
	author = {Dennett, Daniel C.},
	urldate = {2023-10-15},
	date = {2006-09},
	langid = {english},
	file = {Dennett - 2006 - Higher-order truths about chmess.pdf:/Users/tom/Zotero/storage/8DIRE3M9/Dennett - 2006 - Higher-order truths about chmess.pdf:application/pdf},
}

@article{semmelrock_reproducibility_2023,
	title = {Reproducibility in Machine Learning-Driven Research},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2307.10320},
	doi = {10.48550/ARXIV.2307.10320},
	abstract = {Research is facing a reproducibility crisis, in which the results and findings of many studies are difficult or even impossible to reproduce. This is also the case in machine learning ({ML}) and artificial intelligence ({AI}) research. Often, this is the case due to unpublished data and/or source-code, and due to sensitivity to {ML} training conditions. Although different solutions to address this issue are discussed in the research community such as using {ML} platforms, the level of reproducibility in {ML}-driven research is not increasing substantially. Therefore, in this mini survey, we review the literature on reproducibility in {ML}-driven research with three main aims: (i) reflect on the current situation of {ML} reproducibility in various research fields, (ii) identify reproducibility issues and barriers that exist in these research fields applying {ML}, and (iii) identify potential drivers such as tools, practices, and interventions that support {ML} reproducibility. With this, we hope to contribute to decisions on the viability of different solutions for supporting {ML} reproducibility.},
	author = {Semmelrock, Harald and Kopeinik, Simone and Theiler, Dieter and Ross-Hellauer, Tony and Kowald, Dominik},
	urldate = {2023-10-15},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Computers and Society (cs.{CY}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Methodology (stat.{ME})},
}

@article{gelman_how_2022,
	title = {How should scientific journals handle ‘big if true’ submissions?},
	volume = {35},
	issn = {0933-2480, 1867-2280},
	url = {https://www.tandfonline.com/doi/full/10.1080/09332480.2022.2066415},
	doi = {10.1080/09332480.2022.2066415},
	pages = {41--43},
	number = {2},
	journaltitle = {{CHANCE}},
	shortjournal = {{CHANCE}},
	author = {Gelman, Andrew},
	urldate = {2023-10-15},
	date = {2022-04-03},
	langid = {english},
	file = {Gelman - 2022 - How Should Scientific Journals Handle ‘Big If True’ Submissions.pdf:/Users/tom/Zotero/storage/JC4TEHAH/Gelman - 2022 - How Should Scientific Journals Handle ‘Big If True’ Submissions.pdf:application/pdf},
}

@article{liu_data_2023,
	title = {Data, measurement and empirical methods in the science of science},
	volume = {7},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01562-4},
	doi = {10.1038/s41562-023-01562-4},
	pages = {1046--1058},
	number = {7},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Liu, Lu and Jones, Benjamin F. and Uzzi, Brian and Wang, Dashun},
	urldate = {2023-10-15},
	date = {2023-06-01},
	langid = {english},
	keywords = {toread},
	file = {Liu et al. - 2023 - Data, measurement and empirical methods in the science of science.pdf:/Users/tom/Zotero/storage/A877Q35B/Liu et al. - 2023 - Data, measurement and empirical methods in the science of science.pdf:application/pdf},
}

@misc{liang_can_2023,
	title = {Can large language models provide useful feedback on research papers? A large-scale empirical analysis},
	url = {http://arxiv.org/abs/2310.01783},
	shorttitle = {Can large language models provide useful feedback on research papers?},
	abstract = {Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models ({LLM}) such as {GPT}-4, there is growing interest in using {LLMs} to generate scientific feedback on research manuscripts. However, the utility of {LLM}-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using {GPT}-4 to provide comments on the full {PDFs} of scientific papers. We evaluated the quality of {GPT}-4’s feedback through two large-scale studies. We first quantitatively compared {GPT}-4’s generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the {ICLR} machine learning conference (1,709 papers). The overlap in the points raised by {GPT}-4 and by human reviewers (average overlap 30.85\% for Nature journals, 39.23\% for {ICLR}) is comparable to the overlap between two human reviewers (average overlap 28.58\% for Nature journals, 35.25\% for {ICLR}). The overlap between {GPT}-4 and human reviewers is larger for the weaker papers (i.e., rejected {ICLR} papers; average overlap 43.80\%). We then conducted a prospective user study with 308 researchers from 110 {US} institutions in the field of {AI} and computational biology to understand how researchers perceive feedback generated by our {GPT}-4 system on their own papers. Overall, more than half (57.4\%) of the users found {GPT}-4 generated feedback helpful/very helpful and 82.4\% found it more beneficial than feedback from at least some human reviewers. While our findings show that {LLM}-generated feedback can help researchers, we also identify several limitations. For example, {GPT}-4 tends to focus on certain aspects of scientific feedback (e.g., ‘add experiments on more datasets’), and often struggles to provide in-depth critique of method design. Together our results suggest that {LLM} and human feedback can complement each other. While human expert review is and should continue to be the foundation of rigorous scientific process, {LLM} feedback could benefit researchers, especially when timely expert feedback is not available and in earlier stages of manuscript preparation before peer-review.},
	number = {{arXiv}:2310.01783},
	publisher = {{arXiv}},
	author = {Liang, Weixin and Zhang, Yuhui and Cao, Hancheng and Wang, Binglu and Ding, Daisy and Yang, Xinyu and Vodrahalli, Kailas and He, Siyu and Smith, Daniel and Yin, Yian and {McFarland}, Daniel and Zou, James},
	urldate = {2023-10-15},
	date = {2023-10-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.01783 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Liang et al. - 2023 - Can large language models provide useful feedback on research papers A large-scale empirical analys.pdf:/Users/tom/Zotero/storage/6EURMAIR/Liang et al. - 2023 - Can large language models provide useful feedback on research papers A large-scale empirical analys.pdf:application/pdf},
}

@article{klau_comparing_2023,
	title = {Comparing the vibration of effects due to model, data pre-processing and sampling uncertainty on a large data set in personality psychology},
	volume = {7},
	issn = {2003-2714},
	url = {https://open.lnu.se/index.php/metapsychology/article/view/2556},
	doi = {10.15626/MP.2020.2556},
	abstract = {Researchers have great flexibility in the analysis of observational data. If combined with selective reporting and pressure to publish, this flexibility can have devastating consequences on the validity of research findings. We extend the recently proposed vibration of effects approach to provide a framework comparing three main sources of uncertainty which lead to instability in empirical findings, namely data pre-processing, model, and sampling uncertainty. We analyze the behavior of these sources for varying sample sizes for two associations in personality psychology. Through the joint investigation of model and data pre-processing vibration, we can compare the relative impact of these two types of uncertainty and identify the most influential analytical choices. While all types of vibration show a decrease for increasing sample sizes, data pre-processing and model vibration remain non-negligible, even for a sample of over 80000 participants. The increasing availability of large data sets that are not initially recorded for research purposes can make data pre-processing and model choices very influential. We therefore recommend the framework as a tool for transparent reporting of the stability of research findings.},
	journaltitle = {Meta-Psychology},
	shortjournal = {{MP}},
	author = {Klau, Simon and {Felix} and Patel, Chirag J. and Ioannidis, John P. A. and Boulesteix, Anne-Laure and Hoffmann, Sabine},
	urldate = {2023-10-20},
	date = {2023-05-10},
	file = {Full Text:/Users/tom/Zotero/storage/QI27GB7T/Klau et al. - 2023 - Comparing the vibration of effects due to model, data pre-processing and sampling uncertainty on a l.pdf:application/pdf},
}

@article{stewart_reforms_2022,
	title = {Reforms to improve reproducibility and quality must be coordinated across the research ecosystem: the view from the {UKRN} Local Network Leads},
	volume = {15},
	issn = {1756-0500},
	url = {https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-022-05949-w},
	doi = {10.1186/s13104-022-05949-w},
	shorttitle = {Reforms to improve reproducibility and quality must be coordinated across the research ecosystem},
	abstract = {Many disciplines are facing a “reproducibility crisis”, which has precipitated much discussion about how to improve research integrity, reproducibility, and transparency. A unified effort across all sectors, levels, and stages of the research ecosystem is needed to coordinate goals and reforms that focus on open and transparent research practices. Promoting a more positive incentive culture for all ecosystem members is also paramount. In this commentary, we—the Local Network Leads of the {UK} Reproducibility Network—outline our response to the {UK} House of Commons Science and Technology Committee’s inquiry on research integrity and reproducibility. We argue that coordinated change is needed to create (1) a positive research culture, (2) a unified stance on improving research quality, (3) common foundations for open and transparent research practice, and (4) the routinisation of this practice. For each of these areas, we outline the roles that individuals, institutions, funders, publishers, and Government can play in shaping the research ecosystem. Working together, these constituent members must also partner with sectoral and coordinating organisations to produce effective and long-lasting reforms that are fit-for-purpose and future-proof. These efforts will strengthen research quality and create research capable of generating far-reaching applications with a sustained impact on society.},
	pages = {58},
	number = {1},
	journaltitle = {{BMC} Research Notes},
	shortjournal = {{BMC} Res Notes},
	author = {Stewart, Suzanne L. K. and Pennington, Charlotte R. and Da Silva, Gonçalo R. and Ballou, Nick and Butler, Jessica and Dienes, Zoltan and Jay, Caroline and Rossit, Stephanie and Samara, Anna and {U. K. Reproducibility Network (UKRN) Local Network Leads}},
	urldate = {2023-10-20},
	date = {2022-12},
	langid = {english},
	keywords = {toread},
	file = {Stewart et al. - 2022 - Reforms to improve reproducibility and quality must be coordinated across the research ecosystem th.pdf:/Users/tom/Zotero/storage/4EFVTE7H/Stewart et al. - 2022 - Reforms to improve reproducibility and quality must be coordinated across the research ecosystem th.pdf:application/pdf},
}

@article{hardwicke_bayesian_2018,
	title = {A Bayesian decision-making framework for replication},
	volume = {41},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X18000675/type/journal_article},
	doi = {10.1017/S0140525X18000675},
	abstract = {Abstract
            Replication is the cornerstone of science – but when and why? Not all studies need replication, especially when resources are limited. We propose that a decision-making framework based on Bayesian philosophy of science provides a basis for choosing which studies to replicate.},
	pages = {e132},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Hardwicke, Tom E. and Tessler, Michael Henry and Peloquin, Benjamin N. and Frank, Michael C.},
	urldate = {2023-10-21},
	date = {2018},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/4EKYRUYA/Hardwicke et al. - 2018 - A Bayesian decision-making framework for replication.pdf:application/pdf},
}

@article{aczel_billion-dollar_2021,
	title = {A billion-dollar donation: estimating the cost of researchers’ time spent on peer review},
	volume = {6},
	issn = {2058-8615},
	url = {https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-021-00118-2},
	doi = {10.1186/s41073-021-00118-2},
	shorttitle = {A billion-dollar donation},
	abstract = {Abstract
            
              Background
              The amount and value of researchers’ peer review work is critical for academia and journal publishing. However, this labor is under-recognized, its magnitude is unknown, and alternative ways of organizing peer review labor are rarely considered.
            
            
              Methods
              Using publicly available data, we provide an estimate of researchers’ time and the salary-based contribution to the journal peer review system.
            
            
              Results
              We found that the total time reviewers globally worked on peer reviews was over 100 million hours in 2020, equivalent to over 15 thousand years. The estimated monetary value of the time {US}-based reviewers spent on reviews was over 1.5 billion {USD} in 2020. For China-based reviewers, the estimate is over 600 million {USD}, and for {UK}-based, close to 400 million {USD}.
            
            
              Conclusions
              By design, our results are very likely to be under-estimates as they reflect only a portion of the total number of journals worldwide. The numbers highlight the enormous amount of work and time that researchers provide to the publication system, and the importance of considering alternative ways of structuring, and paying for, peer review. We foster this process by discussing some alternative models that aim to boost the benefits of peer review, thus improving its cost-benefit ratio.},
	pages = {14},
	number = {1},
	journaltitle = {Research Integrity and Peer Review},
	shortjournal = {Res Integr Peer Rev},
	author = {Aczel, Balazs and Szaszi, Barnabas and Holcombe, Alex O.},
	urldate = {2023-10-21},
	date = {2021-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/9DCZ37PW/Aczel et al. - 2021 - A billion-dollar donation estimating the cost of researchers’ time spent on peer review.pdf:application/pdf},
}

@article{ioannidis_meta-research_2015,
	title = {Meta-research: Evaluation and Improvement of Research Methods and Practices},
	volume = {13},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.1002264},
	doi = {10.1371/journal.pbio.1002264},
	shorttitle = {Meta-research},
	pages = {e1002264},
	number = {10},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLoS} Biol},
	author = {Ioannidis, John P. A. and Fanelli, Daniele and Dunne, Debbie Drake and Goodman, Steven N.},
	urldate = {2023-10-21},
	date = {2015-10-02},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/YFJICZDQ/Ioannidis et al. - 2015 - Meta-research Evaluation and Improvement of Research Methods and Practices.pdf:application/pdf},
}

@misc{chen_how_2023-1,
	title = {How is {ChatGPT}'s behavior changing over time?},
	url = {http://arxiv.org/abs/2307.09009},
	abstract = {{GPT}-3.5 and {GPT}-4 are the two most widely used large language model ({LLM}) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of {GPT}-3.5 and {GPT}-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) {US} Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both {GPT}-3.5 and {GPT}-4 can vary greatly over time. For example, {GPT}-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84\% accuracy) but {GPT}-4 (June 2023) was poor on these same questions (51\% accuracy). This is partly explained by a drop in {GPT}-4's amenity to follow chain-of-thought prompting. Interestingly, {GPT}-3.5 was much better in June than in March in this task. {GPT}-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. {GPT}-4 performed better at multi-hop questions in June than in March, while {GPT}-3.5's performance dropped on this task. Both {GPT}-4 and {GPT}-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings show that the behavior of the "same" {LLM} service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of {LLMs}.},
	number = {{arXiv}:2307.09009},
	publisher = {{arXiv}},
	author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
	urldate = {2023-10-22},
	date = {2023-08-01},
	eprinttype = {arxiv},
	eprint = {2307.09009 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/tom/Zotero/storage/H6WV7JYP/Chen et al. - 2023 - How is ChatGPT's behavior changing over time.pdf:application/pdf;arXiv.org Snapshot:/Users/tom/Zotero/storage/2K5TGJYD/2307.html:text/html},
}

@article{wagner_accuracy_2023,
	title = {Accuracy of information and references using {ChatGPT}-3 for retrieval of clinical radiological information},
	issn = {0846-5371, 1488-2361},
	url = {http://journals.sagepub.com/doi/10.1177/08465371231171125},
	doi = {10.1177/08465371231171125},
	abstract = {Purpose: To assess the accuracy of answers provided by {ChatGPT}-3 when prompted with questions from the daily routine of radiologists and to evaluate the text response when {ChatGPT}-3 was prompted to provide references for a given answer. Methods: {ChatGPT}-3 (San Francisco, {OpenAI}) is an artiﬁcial intelligence chatbot based on a large language model ({LLM}) that has been designed to generate human-like text. A total of 88 questions were submitted to {ChatGPT}-3 using textual prompt. These 88 questions were equally dispersed across 8 subspecialty areas of radiology. The responses provided by {ChatGPT}-3 were assessed for correctness by crosschecking them with peer-reviewed, {PubMed}-listed references. In addition, the references provided by {ChatGPT}-3 were evaluated for authenticity. Results: A total of 59 of 88 responses (67\%) to radiological questions were correct, while 29 responses (33\%) had errors. Out of 343 references provided, only 124 references (36.2\%) were available through internet search, while 219 references (63.8\%) appeared to be generated by {ChatGPT}-3. When examining the 124 identiﬁed references, only 47 references (37.9\%) were considered to provide enough background to correctly answer 24 questions (37.5\%). Conclusion: In this pilot study, {ChatGPT}-3 provided correct responses to questions from the daily clinical routine of radiologists in only about two thirds, while the remainder of responses contained errors. The majority of provided references were not found and only a minority of the provided references contained the correct information to answer the question. Caution is advised when using {ChatGPT}-3 to retrieve radiological information.},
	pages = {084653712311711},
	journaltitle = {Canadian Association of Radiologists Journal},
	shortjournal = {Can Assoc Radiol J},
	author = {Wagner, Matthias W. and Ertl-Wagner, Birgit B.},
	urldate = {2023-10-22},
	date = {2023-04-20},
	langid = {english},
	file = {Wagner and Ertl-Wagner - 2023 - Accuracy of Information and References Using ChatGPT-3 for Retrieval of Clinical Radiological Inform.pdf:/Users/tom/Zotero/storage/YQ8QN2MB/Wagner and Ertl-Wagner - 2023 - Accuracy of Information and References Using ChatGPT-3 for Retrieval of Clinical Radiological Inform.pdf:application/pdf},
}

@article{percie_du_sert_arrive_2020,
	title = {The {ARRIVE} guidelines 2.0: Updated guidelines for reporting animal research},
	volume = {177},
	issn = {0007-1188, 1476-5381},
	url = {https://bpspubs.onlinelibrary.wiley.com/doi/10.1111/bph.15193},
	doi = {10.1111/bph.15193},
	shorttitle = {The {ARRIVE} guidelines 2.0},
	abstract = {Reproducible science requires transparent reporting. The {ARRIVE} guidelines (Animal Research: Reporting of In Vivo Experiments) were originally developed in 2010 to improve the reporting of animal research. They consist of a checklist of information to include in publications describing in vivo experiments to enable others to scrutinise the work adequately, evaluate its methodological rigour, and reproduce the methods and results. Despite considerable levels of endorsement by funders and journals over the years, adherence to the guidelines has been inconsistent, and the anticipated improvements in the quality of reporting in animal research publications have not been achieved. Here, we introduce {ARRIVE} 2.0. The guidelines have been updated and information reorganised to facilitate their use in practice. We used a Delphi exercise to prioritise and divide the items of the guidelines into 2 sets, the “{ARRIVE} Essential 10,” which constitutes the minimum requirement, and the “Recommended Set,” which describes the research context. This division facilitates improved reporting of animal research by supporting a stepwise approach to implementation. This helps journal editors and reviewers verify that the most important items are being reported in manuscripts. We have also developed the accompanying Explanation and Elaboration (E\&E) document, which serves (1) to explain the rationale behind each item in the guidelines, (2) to clarify key concepts, and (3) to provide illustrative examples. We aim, through these changes, to help ensure that researchers, reviewers, and journal editors are better equipped to improve the rigour and transparency of the scientific process and thus reproducibility.},
	pages = {3617--3624},
	number = {16},
	journaltitle = {British Journal of Pharmacology},
	shortjournal = {British J Pharmacology},
	author = {Percie Du Sert, Nathalie and Hurst, Viki and Ahluwalia, Amrita and Alam, Sabina and Avey, Marc T. and Baker, Monya and Browne, William J. and Clark, Alejandra and Cuthill, Innes C. and Dirnagl, Ulrich and Emerson, Michael and Garner, Paul and Holgate, Stephen T. and Howells, David W. and Karp, Natasha A. and Lazic, Stanley E. and Lidster, Katie and {MacCallum}, Catriona J. and Macleod, Malcolm and Pearl, Esther J. and Petersen, Ole H. and Rawle, Frances and Reynolds, Penny and Rooney, Kieron and Sena, Emily S. and Silberberg, Shai D. and Steckler, Thomas and Würbel, Hanno},
	urldate = {2023-10-22},
	date = {2020-08},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/WDRHC3TZ/Percie Du Sert et al. - 2020 - The ARRIVE guidelines 2.0 Updated guidelines for reporting animal research.pdf:application/pdf},
}

@article{riedel_oddpub_2020,
	title = {{ODDPub} – a Text-Mining Algorithm to Detect Data Sharing in Biomedical Publications},
	volume = {19},
	issn = {1683-1470},
	url = {https://datascience.codata.org/article/10.5334/dsj-2020-042/},
	doi = {10.5334/dsj-2020-042},
	pages = {42},
	number = {1},
	journaltitle = {Data Science Journal},
	shortjournal = {{CODATA}},
	author = {Riedel, Nico and Kip, Miriam and Bobrov, Evgeny},
	urldate = {2023-10-22},
	date = {2020-10-29},
	langid = {english},
	file = {Riedel et al. - 2020 - ODDPub – a Text-Mining Algorithm to Detect Data Sharing in Biomedical Publications.pdf:/Users/tom/Zotero/storage/NUBTP82K/Riedel et al. - 2020 - ODDPub – a Text-Mining Algorithm to Detect Data Sharing in Biomedical Publications.pdf:application/pdf},
}

@article{weissgerber_automated_2021-1,
	title = {Automated screening of {COVID}-19 preprints: can we help authors to improve transparency and reproducibility?},
	volume = {27},
	issn = {1078-8956, 1546-170X},
	url = {https://www.nature.com/articles/s41591-020-01203-7},
	doi = {10.1038/s41591-020-01203-7},
	shorttitle = {Automated screening of {COVID}-19 preprints},
	pages = {6--7},
	number = {1},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Weissgerber, Tracey and Riedel, Nico and Kilicoglu, Halil and Labbé, Cyril and Eckmann, Peter and Ter Riet, Gerben and Byrne, Jennifer and Cabanac, Guillaume and Capes-Davis, Amanda and Favier, Bertrand and Saladi, Shyam and Grabitz, Peter and Bannach-Brown, Alexandra and Schulz, Robert and {McCann}, Sarah and Bernard, Rene and Bandrowski, Anita},
	urldate = {2023-10-22},
	date = {2021-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/K2Y7Z2GD/Weissgerber et al. - 2021 - Automated screening of COVID-19 preprints can we help authors to improve transparency and reproduci.pdf:application/pdf},
}

@report{unesco_unesco_2021,
	title = {{UNESCO} Recommendation on Open Science},
	url = {https://unesdoc.unesco.org/ark:/48223/pf0000379949},
	institution = {{UNESCO}},
	author = {{UNESCO}},
	urldate = {2023-10-22},
	date = {2021},
	doi = {10.54677/MNMH8546},
	file = {UNESCO - 2021 - UNESCO Recommendation on Open Science.pdf:/Users/tom/Zotero/storage/75T2WSAC/UNESCO - 2021 - UNESCO Recommendation on Open Science.pdf:application/pdf},
}

@report{jacobs_open_2023,
	title = {Open Research Indicators: sector priorities},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://doi.org/10.17605/OSF.IO/P8F25},
	shorttitle = {Open Research Indicators},
	author = {Jacobs, Neil and UK Reproducibility Network},
	editora = {{Open Science Framework}},
	editoratype = {collaborator},
	urldate = {2023-10-22},
	date = {2023},
	note = {Publisher: Open Science Framework},
	keywords = {Open Research Indicators, sector priorities},
	file = {Open Research Indicators UKRN working paper 230619.docx:/Users/tom/Zotero/storage/BFSNBMBR/Open Research Indicators UKRN working paper 230619.docx:application/vnd.openxmlformats-officedocument.wordprocessingml.document},
}

@article{cobb_problem_2023,
	title = {The problem of miscitation in psychological science: Righting the ship.},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0001138},
	doi = {10.1037/amp0001138},
	shorttitle = {The problem of miscitation in psychological science},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Cobb, Cory L. and Crumly, Brianna and Montero-Zamora, Pablo and Schwartz, Seth J. and Martínez, Charles R.},
	urldate = {2023-10-23},
	date = {2023-02-23},
	langid = {english},
	keywords = {toread},
	file = {Cobb et al. - 2023 - The problem of miscitation in psychological science Righting the ship..pdf:/Users/tom/Zotero/storage/NAPCA7R9/Cobb et al. - 2023 - The problem of miscitation in psychological science Righting the ship..pdf:application/pdf},
}

@article{fernandez-juricic_why_2021,
	title = {Why sharing data and code during peer review can enhance behavioral ecology research},
	volume = {75},
	issn = {0340-5443, 1432-0762},
	url = {https://link.springer.com/10.1007/s00265-021-03036-x},
	doi = {10.1007/s00265-021-03036-x},
	pages = {103, s00265--021--03036--x},
	number = {7},
	journaltitle = {Behavioral Ecology and Sociobiology},
	shortjournal = {Behav Ecol Sociobiol},
	author = {Fernández-Juricic, Esteban},
	urldate = {2023-10-23},
	date = {2021-07},
	langid = {english},
	keywords = {toread},
	file = {Fernández-Juricic - 2021 - Why sharing data and code during peer review can enhance behavioral ecology research.pdf:/Users/tom/Zotero/storage/NMNGU2W6/Fernández-Juricic - 2021 - Why sharing data and code during peer review can enhance behavioral ecology research.pdf:application/pdf},
}

@article{chen_recognizing_2023,
	title = {Recognizing and marshalling the pre-publication error correction potential of open data for more reproducible science},
	volume = {7},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-023-02152-3},
	doi = {10.1038/s41559-023-02152-3},
	pages = {1597--1599},
	number = {10},
	journaltitle = {Nature Ecology \& Evolution},
	shortjournal = {Nat Ecol Evol},
	author = {Chen, Rebecca Shuhua and Berthelsen, Ane Liv and Lamartinière, Etienne Brejon and Spangenberg, Matthias Christian and Schmoll, Tim},
	urldate = {2023-10-23},
	date = {2023-07-31},
	langid = {english},
	keywords = {toread},
	file = {Chen et al. - 2023 - Recognizing and marshalling the pre-publication error correction potential of open data for more rep.pdf:/Users/tom/Zotero/storage/4JLV9JUQ/Chen et al. - 2023 - Recognizing and marshalling the pre-publication error correction potential of open data for more rep.pdf:application/pdf},
}

@report{alzaabi_chatgpt_2023,
	title = {{ChatGPT} applications in Academic Research: A Review of Benefits, Concerns, and Recommendations},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.08.17.553688},
	shorttitle = {{ChatGPT} applications in Academic Research},
	abstract = {Background {ChatGPT} has emerged as a valuable tool for enhancing scientific writing. It is the first openly available Large Language Model ({LLM}) with unrestricted access to its capabilities. {ChatGPT} has the potential to alleviate researchers' workload and enhance various aspects of research, from planning to execution and presentation. However, due to the rapid growth of publications and diverse opinions surrounding {ChatGPT}, a comprehensive review is necessary to understand its benefits, risks, and safe utilization in scientific research. This review aims to provide a comprehensive overview of the topic by extensively examining existing literature on the utilization of {ChatGPT} in academic research. The goal is to gain insights into the potential benefits and risks of using {ChatGPT} in scientific research, exploring secure and efficient methods for its application while identifying potential pitfalls to minimize negative consequences.},
	institution = {Scientific Communication and Education},
	type = {preprint},
	author = {{AlZaabi}, Adhari and {ALAmri}, Amira and Albalushi, Halima and Aljabri, Ruqaya and {AalAbdulsalam}, {AbdulRahman}},
	urldate = {2023-10-23},
	date = {2023-08-18},
	langid = {english},
	doi = {10.1101/2023.08.17.553688},
	file = {AlZaabi et al. - 2023 - ChatGPT applications in Academic Research A Review of Benefits, Concerns, and Recommendations.pdf:/Users/tom/Zotero/storage/FPCLFBF6/AlZaabi et al. - 2023 - ChatGPT applications in Academic Research A Review of Benefits, Concerns, and Recommendations.pdf:application/pdf},
}

@article{teresi_guidelines_2022,
	title = {Guidelines for Designing and Evaluating Feasibility Pilot Studies},
	volume = {60},
	issn = {0025-7079},
	url = {https://journals.lww.com/10.1097/MLR.0000000000001664},
	doi = {10.1097/MLR.0000000000001664},
	abstract = {Background:
              Pilot studies test the feasibility of methods and procedures to be used in larger-scale studies. Although numerous articles describe guidelines for the conduct of pilot studies, few have included specific feasibility indicators or strategies for evaluating multiple aspects of feasibility. In addition, using pilot studies to estimate effect sizes to plan sample sizes for subsequent randomized controlled trials has been challenged; however, there has been little consensus on alternative strategies.
            
            
              Methods:
              In Section 1, specific indicators (recruitment, retention, intervention fidelity, acceptability, adherence, and engagement) are presented for feasibility assessment of data collection methods and intervention implementation. Section 1 also highlights the importance of examining feasibility when adapting an intervention tested in mainstream populations to a new more diverse group. In Section 2, statistical and design issues are presented, including sample sizes for pilot studies, estimates of minimally important differences, design effects, confidence intervals ({CI}) and nonparametric statistics. An in-depth treatment of the limits of effect size estimation as well as process variables is presented. Tables showing {CI} around parameters are provided. With small samples, effect size, completion and adherence rate estimates will have large {CI}.
            
            
              Conclusion:
              This commentary offers examples of indicators for evaluating feasibility, and of the limits of effect size estimation in pilot studies. As demonstrated, most pilot studies should not be used to estimate effect sizes, provide power calculations for statistical tests or perform exploratory analyses of efficacy. It is hoped that these guidelines will be useful to those planning pilot/feasibility studies before a larger-scale study.},
	pages = {95--103},
	number = {1},
	journaltitle = {Medical Care},
	author = {Teresi, Jeanne A. and Yu, Xiaoying and Stewart, Anita L. and Hays, Ron D.},
	urldate = {2023-10-23},
	date = {2022-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/NRNJMHH4/Teresi et al. - 2022 - Guidelines for Designing and Evaluating Feasibility Pilot Studies.pdf:application/pdf},
}

@article{riley_interpretation_2011,
	title = {Interpretation of random effects meta-analyses},
	volume = {342},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.d549},
	doi = {10.1136/bmj.d549},
	pages = {d549--d549},
	issue = {feb10 2},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Riley, R. D. and Higgins, J. P. T. and Deeks, J. J.},
	urldate = {2023-10-23},
	date = {2011-02-10},
	langid = {english},
}

@article{boot_pervasive_2013-1,
	title = {The pervasive problem with placebos in psychology: why active control groups are not sufficient to rule out placebo effects},
	volume = {8},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691613491271},
	doi = {10.1177/1745691613491271},
	shorttitle = {The pervasive problem with placebos in psychology},
	abstract = {To draw causal conclusions about the efficacy of a psychological intervention, researchers must compare the treatment condition with a control group that accounts for improvements caused by factors other than the treatment. Using an active control helps to control for the possibility that improvement by the experimental group resulted from a placebo effect. Although active control groups are superior to “no-contact” controls, only when the active control group has the same expectation of improvement as the experimental group can we attribute differential improvements to the potency of the treatment. Despite the need to match expectations between treatment and control groups, almost no psychological interventions do so. This failure to control for expectations is not a minor omission—it is a fundamental design flaw that potentially undermines any causal inference. We illustrate these principles with a detailed example from the video-game-training literature showing how the use of an active control group does not eliminate expectation differences. The problem permeates other interventions as well, including those targeting mental health, cognition, and educational achievement. Fortunately, measuring expectations and adopting alternative experimental designs makes it possible to control for placebo effects, thereby increasing confidence in the causal efficacy of psychological interventions.},
	pages = {445--454},
	number = {4},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Boot, Walter R. and Simons, Daniel J. and Stothart, Cary and Stutts, Cassie},
	urldate = {2023-10-23},
	date = {2013-07},
	langid = {english},
	keywords = {toread},
	file = {Boot et al. - 2013 - The pervasive problem with placebos in psychology why active control groups are not sufficient to r.pdf:/Users/tom/Zotero/storage/WGNWALYF/Boot et al. - 2013 - The pervasive problem with placebos in psychology why active control groups are not sufficient to r.pdf:application/pdf},
}

@article{liu_engagement_2023,
	title = {The engagement of academic libraries in open science: A systematic review},
	volume = {49},
	issn = {00991333},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0099133323000502},
	doi = {10.1016/j.acalib.2023.102711},
	shorttitle = {The engagement of academic libraries in open science},
	abstract = {Open science is reshaping the environment of scholarly communication. In this context, academic libraries are beginning to redefine or expand their functions and role. Using a systematic review and meta-synthesis approach, this study analyses 65 literature related to the engagement of academic libraries in open science. The results show that the existing research mainly focuses on four aspects of open science: open access, research data management, open educational resources, and citizen science. Based on these themes, academic libraries have implemented a range of practical activities, playing the roles of open science service providers, advocates or educators, policy makers, publishers or knowledge producers, etc. At the same time, however, academic libraries are facing a number of challenges caused by inadequate security of internal resources and insufficient support from the external environment.},
	pages = {102711},
	number = {3},
	journaltitle = {The Journal of Academic Librarianship},
	shortjournal = {The Journal of Academic Librarianship},
	author = {Liu, Li and Liu, Wenyun},
	urldate = {2023-10-23},
	date = {2023-05},
	langid = {english},
	file = {Liu and Liu - 2023 - The engagement of academic libraries in open science A systematic review.pdf:/Users/tom/Zotero/storage/H2JJQ35S/Liu and Liu - 2023 - The engagement of academic libraries in open science A systematic review.pdf:application/pdf},
}

@article{wulff_common_2023,
	title = {Common methodological mistakes},
	volume = {34},
	issn = {10489843},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1048984323000036},
	doi = {10.1016/j.leaqua.2023.101677},
	abstract = {For scientiﬁc discoveries to be valid—whether in theory or empirically—a phenomenon must be accurately described: The scientist must use appropriate counterfactuals and eliminate competing explanations. Empirical work must also use an appropriate design and method, and empirical claims made about the phenomenon must be correctly characterized. Moreover, valid empirical discoveries must be reliable in the sense that scientists who reexamine the data must be able to reproduce the ﬁnding or to replicate the effect from data gathered in a similar context. Only discoveries adhering to the above criteria can be scientiﬁcally informative, serve as building blocks for theory, or have policy implications. Unfortunately, as several recent surveys of the literature show, much of the published works in the management and applied psychology ﬁelds are uninformative; contributing reasons include several intractable problems in the study design and analysis as well as the failure of the ﬁeld to adopt open science practices. Against this backdrop, we identify common methodological mistakes made in applied work. We group these mistakes into three major categories: (a) study design and data collection (e.g., ﬁt between hypotheses and methods, design, measurement, open science, literature reviews), (b) data analysis (e.g., data preprocessing, choice of estimators, analysis of data, issues concerning endogeneity, and use of instrumental variables), and (c) diagnostics, inferences, and reporting. We also explain how to avoid these issues, so that published work makes for a useful contribution to the scientiﬁc record.},
	pages = {101677},
	number = {1},
	journaltitle = {The Leadership Quarterly},
	shortjournal = {The Leadership Quarterly},
	author = {Wulff, Jesper N. and Sajons, Gwendolin B. and Pogrebna, Ganna and Lonati, Sirio and Bastardoz, Nicolas and Banks, George C. and Antonakis, John},
	urldate = {2023-10-23},
	date = {2023-02},
	langid = {english},
	file = {Wulff et al. - 2023 - Common methodological mistakes.pdf:/Users/tom/Zotero/storage/J97XHLFG/Wulff et al. - 2023 - Common methodological mistakes.pdf:application/pdf},
}

@article{shah_policy_nodate,
	title = {From policy to practice: tracking an open science funding initiative},
	abstract = {This is a critical moment in the open science landscape. Over the past few years there has been growing momentum to improve open research policies and require grantees to share all research outputs, from datasets to code to protocols, in {FAIR} (findable, accessible, interoperable and reusable [{FAIR}]) repositories with persistent identifiers attached. The Aligning Science Across Parkinson’s ({ASAP}) initiative has made substantial investments in improving open science compliance monitoring for its grantees, requiring grantees to update their manuscripts if not all research outputs have been linked in the initial manuscript version. Here, we evaluate {ASAP}’s effectiveness in improving research output sharing for all articles processed through the {ASAP} compliance workflow between March 1, 2022, and October 1, 2022. Our ultimate goal in sharing our findings is to assist other funders and institutions as they consider open science implementation. By normalizing the open science and compliance process across funding bodies, we hope to simplify and streamline researcher, institutional, and funder workflows, allowing researchers to focus on science by easily leveraging resources and building upon the work of others.},
	author = {Shah, Hetal},
	langid = {english},
	file = {Shah - From Policy to Practice Tracking an Open Science Funding Initiative.pdf:/Users/tom/Zotero/storage/77YERUNZ/Shah - From Policy to Practice Tracking an Open Science Funding Initiative.pdf:application/pdf},
}

@article{steiner_causal_2019-1,
	title = {A causal replication framework for designing and assessing replication efforts},
	volume = {227},
	issn = {2190-8370, 2151-2604},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000385},
	doi = {10.1027/2151-2604/a000385},
	abstract = {Replication has long been a cornerstone for establishing trustworthy scientific results, but there remains considerable disagreement about what constitutes a replication, how results from these studies should be interpreted, and whether direct replication of results is even possible. This article addresses these concerns by presenting the methodological foundations for a replication science. It provides an introduction to the causal replication framework, which defines “replication” as a research design that tests whether two (or more) studies produce the same causal effect within the limits of sampling error. The framework formalizes the conditions under which replication success can be expected, and allows for the causal interpretation of replication failures. Through two applied examples, the article demonstrates how the causal replication framework may be utilized to plan prospective replication designs, as well as to interpret results from existing replication efforts.},
	pages = {280--292},
	number = {4},
	journaltitle = {Zeitschrift für Psychologie},
	shortjournal = {Zeitschrift für Psychologie},
	author = {Steiner, Peter M. and Wong, Vivian C. and Anglin, Kylie},
	urldate = {2023-10-23},
	date = {2019-10},
	langid = {english},
	keywords = {toread},
	file = {Steiner et al. - 2019 - A Causal Replication Framework for Designing and Assessing Replication Efforts.pdf:/Users/tom/Zotero/storage/AUG2S3CK/Steiner et al. - 2019 - A Causal Replication Framework for Designing and Assessing Replication Efforts.pdf:application/pdf},
}

@article{maccorquodale_distinction_1948,
	title = {On a distinction between hypothetical constructs and intervening variables.},
	volume = {55},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0056029},
	doi = {10.1037/h0056029},
	pages = {95--107},
	number = {2},
	journaltitle = {Psychological Review},
	shortjournal = {Psychological Review},
	author = {{MacCorquodale}, Kenneth and Meehl, Paul E.},
	urldate = {2023-10-23},
	date = {1948},
	langid = {english},
	keywords = {toread},
	file = {MacCorquodale and Meehl - 1948 - On a distinction between hypothetical constructs and intervening variables..pdf:/Users/tom/Zotero/storage/L8KCDC56/MacCorquodale and Meehl - 1948 - On a distinction between hypothetical constructs and intervening variables..pdf:application/pdf},
}

@article{huff_low_2023,
	title = {Low research-data availability in educational-psychology journals: no indication of effective research-data policies},
	abstract = {Research-data availability contributes to the transparency of the research process and the credibility of educationalpsychology research and science in general. Recently, there have been many initiatives to increase the availability and quality of research data. Many research institutions have adopted research-data policies. This increased awareness might have raised the sharing of research data in empirical articles. To test this idea, we coded 1,242 publications from six educational-psychology journals and the psychological journal Cognition (as a baseline) published in 2018 and 2020. Research-data availability was low (3.85\% compared with 62.74\% in Cognition) but has increased from 0.32\% (2018) to 7.16\% (2020). However, neither the data-transparency level of the journal nor the existence of an official research-data policy on the level of the corresponding author’s institution was related to research-data availability. We discuss the consequences of these findings for institutional research-data-management processes.},
	author = {Huff, Markus and Bongartz, Elke C},
	date = {2023},
	langid = {english},
	keywords = {toread},
	file = {Huff and Bongartz - Low Research-Data Availability in Educational-Psychology Journals No Indication of Effective Resear.pdf:/Users/tom/Zotero/storage/4YKFLPT9/Huff and Bongartz - Low Research-Data Availability in Educational-Psychology Journals No Indication of Effective Resear.pdf:application/pdf},
}

@article{ludwig_algorithmic_2022,
	title = {Algorithmic behavioral science: machine learning as a tool for scientific discovery},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4164272},
	doi = {10.2139/ssrn.4164272},
	shorttitle = {Algorithmic behavioral science},
	abstract = {While hypothesis testing is a highly formalized activity, hypothesis generation remains largely informal. We propose a procedure that uses machine learning algorithms—and their capacity to notice patterns that people might not—to generate novel hypotheses about human behavior. We illustrate the procedure with a concrete empirical application: pre-trial decisions by judges. We begin with a striking fact. An algorithmic model reveals that a single factor explains nearly half of the predictable variation in who judges choose to jail: the pixels in the defendant’s mugshot. The mugshot remains highly predictive even after controlling for race, skin color, demographics, and facial features previously emphasized by psychologists. Moreover, human judgments about who will be jailed—based on the mugshots—do signiﬁcantly worse than the algorithm’s. What, then, has the algorithm discovered about who judges choose to jail? To answer this question, we build a communication procedure that allows people to see what the algorithm “sees.” We ﬁnd that subjects using the procedure appear to understand the algorithm: they are able to articulate facial features that turn out to explain the algorithm’s predictions. These novel features also explain the actual choices judges make: defendants with these features are jailed at signiﬁcantly higher rates. Though our results are speciﬁc, our approach is general. The modern world produces troves of high-dimensional data, e.g., from cell phones, satellites, online behavior, news headlines, corporate ﬁlings, and high-frequency time series. Our framework provides a way to produce novel interpretable hypotheses from high-dimensional data such as these, a way to marry the predictive power of machine learning with human intuition. A central tenet of our paper is that hypothesis generation is in and of itself a valuable activity, and hope this encourages future work in this “pre-scientiﬁc” stage of science.},
	journaltitle = {{SSRN} Electronic Journal},
	shortjournal = {{SSRN} Journal},
	author = {Ludwig, Jens and Mullainathan, Sendhil},
	urldate = {2023-10-23},
	date = {2022},
	langid = {english},
	keywords = {toread},
	file = {Ludwig and Mullainathan - 2022 - Algorithmic Behavioral Science Machine Learning as a Tool for Scientific Discovery.pdf:/Users/tom/Zotero/storage/5ASF9X44/Ludwig and Mullainathan - 2022 - Algorithmic Behavioral Science Machine Learning as a Tool for Scientific Discovery.pdf:application/pdf},
}

@report{moris_fernandez_retracted_2019,
	title = {Retracted papers clinging on to life: An observational study of post-retraction citations in psychology},
	url = {https://osf.io/cszpy},
	shorttitle = {Retracted papers clinging on to life},
	abstract = {Self-correction is assumed to be a defining feature of science. However, science’s ability to correct itself is far from optimal as shown, for instance, by the persistent influence of papers that have been retracted. In this study, we investigated citation patterns for 140 papers retracted in psychology due to data fabrication, scientific misconduct, or error, according to the Retraction Watch Database. After retraction, 88 (63\%) of these papers received at least one positive citation (median = 2, interquartile range = 4, min = 0, max = 89 positive citations). These results demonstrate the enduring influence of erroneous or flawed data, even when they have been formally declared as invalid. To ameliorate this problem, we propose several procedures and tools that may enhance the discoverability of retracted papers, including standardization of publisher retraction notices and meta-data, issuance of retraction notices within researchers’ reference management software, and automated pre-publication screening for citations to retracted papers.},
	institution = {{PsyArXiv}},
	type = {preprint},
	author = {Morís Fernández, Luis and Hardwicke, Tom Elis and Vadillo, Miguel A.},
	urldate = {2023-10-29},
	date = {2019-06-19},
	doi = {10.31234/osf.io/cszpy},
	file = {Submitted Version:/Users/tom/Zotero/storage/HYKY4GWC/Morís Fernández et al. - 2019 - Retracted papers clinging on to life An observational study of post-retraction citations in psychol.pdf:application/pdf},
}

@article{choi_defense_2023,
	title = {In defense of the resampling account of replication.},
	volume = {43},
	issn = {2151-3341, 1068-8471},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/teo0000224},
	doi = {10.1037/teo0000224},
	pages = {249--251},
	number = {4},
	journaltitle = {Journal of Theoretical and Philosophical Psychology},
	shortjournal = {Journal of Theoretical and Philosophical Psychology},
	author = {Choi, Hong Hui},
	urldate = {2023-11-03},
	date = {2023-11},
	langid = {english},
	file = {Choi - 2023 - In defense of the resampling account of replication..pdf:/Users/tom/Zotero/storage/EZEIXH5Q/Choi - 2023 - In defense of the resampling account of replication..pdf:application/pdf},
}

@article{bauer_responding_2023,
	title = {Responding to the Association for Psychological Science Strategic Plan, 2022–2027},
	volume = {34},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/09567976221133816},
	doi = {10.1177/09567976221133816},
	pages = {3--7},
	number = {1},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Bauer, Patricia J.},
	urldate = {2023-11-28},
	date = {2023-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/VC6Y6ADC/Bauer - 2023 - Responding to the Association for Psychological Science Strategic Plan, 2022–2027.pdf:application/pdf},
}

@article{bauer_psychological_2022,
	title = {Psychological Science Stepping Up a Level},
	volume = {33},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/09567976221078527},
	doi = {10.1177/09567976221078527},
	pages = {179--183},
	number = {2},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Bauer, Patricia J.},
	urldate = {2023-11-28},
	date = {2022-02},
	langid = {english},
}

@article{bauer_new_2021,
	title = {A New Option for Scientific Exchange and an Alternative to the Commentary Format},
	volume = {32},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/09567976211042300},
	doi = {10.1177/09567976211042300},
	pages = {1343--1345},
	number = {9},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Bauer, Patricia J.},
	urldate = {2023-11-28},
	date = {2021-09},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/UQ3G7BQZ/Bauer - 2021 - A New Option for Scientific Exchange and an Alternative to the Commentary Format.pdf:application/pdf},
}

@article{bauer_why_2021,
	title = {Why It Is Important to Know How the Sausage Is Made: Benefits, Risks, and Responsibilities of Using Third-Party Data},
	volume = {32},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/09567976211021594},
	doi = {10.1177/09567976211021594},
	shorttitle = {Why It Is Important to Know How the Sausage Is Made},
	pages = {861--862},
	number = {6},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Bauer, Patricia J.},
	urldate = {2023-11-28},
	date = {2021-06},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/SNEZGPDF/Bauer - 2021 - Why It Is Important to Know How the Sausage Is Made Benefits, Risks, and Responsibilities of Using .pdf:application/pdf},
}

@article{bauer_call_2020,
	title = {A Call for Greater Sensitivity in the Wake of a Publication Controversy},
	volume = {31},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797620941482},
	doi = {10.1177/0956797620941482},
	pages = {767--769},
	number = {7},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Bauer, Patricia J.},
	urldate = {2023-11-28},
	date = {2020-07},
	langid = {english},
}

@article{kravitz_practicing_2020,
	title = {Practicing Good Laboratory Hygiene, Even in a Pandemic},
	volume = {31},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797620920547},
	doi = {10.1177/0956797620920547},
	pages = {483--487},
	number = {5},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Kravitz, Dwight J. and Mitroff, Stephen R. and Bauer, Patricia J.},
	urldate = {2023-11-28},
	date = {2020-05},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/U9EDIZVX/Kravitz et al. - 2020 - Practicing Good Laboratory Hygiene, Even in a Pandemic.pdf:application/pdf},
}

@article{bauer_expanding_2020,
	title = {Expanding the Reach of \textit{Psychological Science}},
	volume = {31},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797619898664},
	doi = {10.1177/0956797619898664},
	pages = {3--5},
	number = {1},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Bauer, Patricia J.},
	urldate = {2023-11-28},
	date = {2020-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/2GLSVQAJ/Bauer - 2020 - Expanding the Reach of Psychological Science.pdf:application/pdf},
}

@article{lindsay_swan_2019,
	title = {Swan Song Editorial},
	volume = {30},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797619893653},
	doi = {10.1177/0956797619893653},
	pages = {1669--1673},
	number = {12},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Lindsay, D. Stephen},
	urldate = {2023-11-28},
	date = {2019-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/3HTBB228/Lindsay - 2019 - Swan Song Editorial.pdf:application/pdf},
}

@article{lindsay_preregistered_2017,
	title = {Preregistered Direct Replications in \textit{Psychological Science}},
	volume = {28},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797617718802},
	doi = {10.1177/0956797617718802},
	pages = {1191--1192},
	number = {9},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Lindsay, D. Stephen},
	urldate = {2023-11-28},
	date = {2017-09},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/Z7NKQG6F/Lindsay - 2017 - Preregistered Direct Replications in Psychological Science.pdf:application/pdf},
}

@article{lindsay_sharing_2017,
	title = {Sharing Data and Materials in \textit{Psychological Science}},
	volume = {28},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797617704015},
	doi = {10.1177/0956797617704015},
	pages = {699--702},
	number = {6},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Lindsay, D. Stephen},
	urldate = {2023-11-28},
	date = {2017-06},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/2QF8U5KD/Lindsay - 2017 - Sharing Data and Materials in Psychological Science.pdf:application/pdf},
}

@article{lindsay_replication_2015,
	title = {Replication in Psychological Science},
	volume = {26},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797615616374},
	doi = {10.1177/0956797615616374},
	pages = {1827--1832},
	number = {12},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Lindsay, D. Stephen},
	urldate = {2023-11-28},
	date = {2015-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/DKS4NJJP/Lindsay - 2015 - Replication in Psychological Science.pdf:application/pdf},
}

@article{thibault_reflections_2023,
	title = {Reflections on Preregistration: Core Criteria, Badges, Complementary Workflows},
	issn = {2667-1204},
	url = {https://journal.trialanderror.org/pub/reflections-on-preregistration},
	doi = {10.36850/mr6},
	shorttitle = {Reflections on Preregistration},
	abstract = {Clinical trials are routinely preregistered. In psychology and the social sciences, however, only a small percentage of studies are preregistered, and those preregistrations often contain ambiguities. As advocates strive for broader uptake and effective use of preregistration, they can benefit from drawing on the experience of preregistration in clinical trials and adapting some of those successes to the psychology and social sciences context. We recommend that individuals and organizations who promote preregistration: (1) Establish core preregistration criteria required to consider a preregistration complete; (2) Award preregistered badges only to articles that meet the badge criteria; and (3) Leverage complementary workflows that provide a similar function as preregistration.},
	journaltitle = {Journal of Trial and Error},
	shortjournal = {{JOTE}},
	author = {Thibault, Robert T. and Pennington, Charlotte R. and Munafò, Marcus R.},
	urldate = {2023-11-28},
	date = {2023-05-15},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/4SUNJQ3V/Thibault et al. - 2023 - Reflections on Preregistration Core Criteria, Badges, Complementary Workflows.pdf:application/pdf},
}

@report{homewood_investigating_2023,
	title = {Investigating Issues with Reproducibility in Journal Articles Published in Psychological Science},
	url = {https://osf.io/6d5nq},
	abstract = {Amidst growing concern over the reproducibility of psychological research (Open Science Collaboration, 2015; Baker, 2016), efforts have been made by academic journals to incentivise open science practises ({OSPs}) that help to defend the integrity of psychological research from the fallout of the “replication crisis” (Shrout \&amp; Rodgers, 2018). Psychological Science implimented their “Open practises badge” policy in 2014 (Eich, 2014) to award authors who adhere to 3 {OSPs}: preregistration, open data and open materials. However, such a policy has been criticised for substantially failing to achieve its goals (Nuijten et al., 2017; Hardwicke et al., 2021; Crüwell et al., 2022). The present investigation aims to highlight issues with reproducibility in articles published in Psychological Science by attempting the reproduction of a total of 80 analyses, tables and figures from 3 articles published in the journal in 2022/23. This investigation found only 40 (50\%) of these were reproducible “entirely” and results highlighted a number of important issues that hinder reproducibility. Potential solutions to enhance reproducibility in psychological research are discussed. Supplementary materials can be found in the following {OSF} repository (https://osf.io/yw3cs/view\_only=1c7e5cc286484eb7978784ffc345dd54)},
	institution = {{PsyArXiv}},
	type = {preprint},
	author = {Homewood, Charlie},
	urldate = {2023-11-28},
	date = {2023-08-02},
	doi = {10.31234/osf.io/6d5nq},
	file = {Submitted Version:/Users/tom/Zotero/storage/MAIMU2N2/Homewood - 2023 - Investigating Issues with Reproducibility in Journal Articles Published in Psychological Science.pdf:application/pdf},
}

@report{willroth_best_2023,
	title = {Best Laid Plans: A Guide to Reporting Preregistration Deviations},
	url = {https://osf.io/dwx69},
	shorttitle = {Best Laid Plans},
	abstract = {Psychological scientists are increasingly using preregistration as a tool to increase the credibility of research findings. Many of the benefits of preregistration rest on the assumption that preregistered plans are followed perfectly. However, research suggests that this is the exception rather than the norm, and there are many reasons why researchers may deviate from their preregistered plans. Preregistration can still be a valuable tool, even in the presence of deviations, as long as those deviations are well-documented and transparently reported. Unfortunately, most preregistration deviations in psychology go unreported or are reported in unsystematic ways. The current paper offers a solution to this problem by providing a framework for transparent and standardized reporting of preregistration deviations, which was developed by drawing upon our own experiences with preregistration, existing unpublished templates, feedback from colleagues and reviewers, and the results of a survey of 34 psychology journal editors. This framework provides a clear template for what to do when things do not go as planned. We conclude by encouraging researchers to adopt this framework in their own preregistered research, and for journals to implement structural policies around the transparent reporting of preregistration deviations.},
	institution = {{PsyArXiv}},
	type = {preprint},
	author = {Willroth, Emily C and Atherton, Olivia E.},
	urldate = {2023-11-28},
	date = {2023-10-26},
	langid = {english},
	doi = {10.31234/osf.io/dwx69},
	file = {Willroth and Atherton - 2023 - Best Laid Plans A Guide to Reporting Preregistration Deviations.pdf:/Users/tom/Zotero/storage/52ST9SSA/Willroth and Atherton - 2023 - Best Laid Plans A Guide to Reporting Preregistration Deviations.pdf:application/pdf},
}

@article{hostler_invisible_2023,
	title = {The invisible workload of open research},
	issn = {2667-1204},
	url = {https://journal.trialanderror.org/pub/the-invisible-workload},
	doi = {10.36850/mr5},
	abstract = {It is acknowledged that conducting open research requires additional time and effort compared to conducting ‘closed’ research. However, this additional work is often discussed only in abstract terms, a discourse which ignores the practicalities of how researchers are expected to find the time to engage with these practices in the context of their broader role as multifaceted academics. In the context of a sector that is blighted by stress, burnout, untenable workloads, and hyper-competitive pressures to produce, there is a clear danger that additional expectations to engage in open practices add to the workload burden and increase pressure on academics even further. In this article, the theories of academic capitalism and workload creep are used to explore how workload models currently exploit researchers by mismeasuring academic labour. The specific increase in workload resulting from open practices and associated administration is then outlined, including via the cumulative effects of administrative burden. It is argued that there is a high chance that without intervention, increased expectations to engage in open research practices may lead to unacceptable increases in demands on academics. Finally, the individual and systematic responsibilities to mitigate this are discussed.},
	journaltitle = {Journal of Trial and Error},
	shortjournal = {{JOTE}},
	author = {Hostler, Thomas J.},
	urldate = {2023-11-28},
	date = {2023-05-04},
	langid = {english},
	file = {Hostler - 2023 - The Invisible Workload of Open Research.pdf:/Users/tom/Zotero/storage/BJ3FZWPC/Hostler - 2023 - The Invisible Workload of Open Research.pdf:application/pdf},
}

@article{alter_responsible_2018,
	title = {Responsible practices for data sharing.},
	volume = {73},
	issn = {1935-990X, 0003-066X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/amp0000258},
	doi = {10.1037/amp0000258},
	pages = {146--156},
	number = {2},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Alter, George and Gonzalez, Richard},
	urldate = {2023-11-28},
	date = {2018-02},
	langid = {english},
	file = {Accepted Version:/Users/tom/Zotero/storage/PHGBBWYE/Alter and Gonzalez - 2018 - Responsible practices for data sharing..pdf:application/pdf},
}

@article{page_rob-me_2023,
	title = {{ROB}-{ME}: a tool for assessing risk of bias due to missing evidence in systematic reviews with meta-analysis},
	issn = {1756-1833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj-2023-076754},
	doi = {10.1136/bmj-2023-076754},
	shorttitle = {{ROB}-{ME}},
	pages = {e076754},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Page, Matthew J and Sterne, Jonathan A C and Boutron, Isabelle and Hróbjartsson, Asbjørn and Kirkham, Jamie J and Li, Tianjing and Lundh, Andreas and Mayo-Wilson, Evan and {McKenzie}, Joanne E and Stewart, Lesley A and Sutton, Alex J and Bero, Lisa and Dunn, Adam G and Dwan, Kerry and Elbers, Roy G and Kanukula, Raju and Meerpohl, Joerg J and Turner, Erick H and Higgins, Julian P T},
	urldate = {2023-11-28},
	date = {2023-11-20},
	langid = {english},
	keywords = {toread},
}

@article{borghi_data_2021,
	title = {Data management and sharing: Practices and perceptions of psychology researchers},
	volume = {16},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0252047},
	doi = {10.1371/journal.pone.0252047},
	shorttitle = {Data management and sharing},
	abstract = {Research data is increasingly viewed as an important scholarly output. While a growing body of studies have investigated researcher practices and perceptions related to data sharing, information about data-related practices throughout the research process (including data collection and analysis) remains largely anecdotal. Building on our previous study of data practices in neuroimaging research, we conducted a survey of data management practices in the field of psychology. Our survey included questions about the type(s) of data collected, the tools used for data analysis, practices related to data organization, maintaining documentation, backup procedures, and long-term archiving of research materials. Our results demonstrate the complexity of managing and sharing data in psychology. Data is collected in multifarious forms from human participants, analyzed using a range of software tools, and archived in formats that may become obsolete. As individuals, our participants demonstrated relatively good data management practices, however they also indicated that there was little standardization within their research group. Participants generally indicated that they were willing to change their current practices in light of new technologies, opportunities, or requirements.},
	pages = {e0252047},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Borghi, John A. and Van Gulick, Ana E.},
	editor = {Suleman, Hussein},
	urldate = {2023-11-28},
	date = {2021-05-21},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/44Y9GER9/Borghi and Van Gulick - 2021 - Data management and sharing Practices and perceptions of psychology researchers.pdf:application/pdf},
}

@article{vazire_we_2019,
	title = {Do we want to be credible or incredible?},
	volume = {33},
	url = {https://www.psychologicalscience.org/observer/do-we-want-to-be-credible-or-incredible},
	abstract = {Transparency plus scrutiny guarantee that research gets the credibility it deserves, according to {APS} Fellow Simine Vazire, a professor of psychology at the University of California, Davis.},
	journaltitle = {{APS} Observer},
	author = {Vazire, Simine},
	urldate = {2023-11-29},
	date = {2019-12-23},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/XWDLGV84/do-we-want-to-be-credible-or-incredible.html:text/html},
}

@article{vilhuber_reproducibility_2020,
	title = {Reproducibility and replicability in economics},
	volume = {2},
	url = {https://hdsr.mitpress.mit.edu/pub/fgpmpj1l/release/5},
	doi = {10.1162/99608f92.4f6b9e67},
	abstract = {I provide a summary description of the history and state of reproducibility and replicability in the academic field of economics. I include a discussion of more general replicability and transparency, including the tradition of sharing of research findings and code outside of peer-reviewed publications. I describe the historical context for journals and grey literature in economics, the role of precollected public and nonpublic data, and touch on the role of proprietary software in economics. The increasing importance of restricted-access data environments in economics and the interaction with reproducibility is highlighted. The article concludes with an outlook on current developments, including the role of big data and increased verification of reproducibility in economics.},
	number = {4},
	journaltitle = {Harvard Data Science Review},
	author = {Vilhuber, Lars},
	urldate = {2023-11-29},
	date = {2020-12-21},
	langid = {english},
	file = {Full Text PDF:/Users/tom/Zotero/storage/ERNXHV2S/Vilhuber - 2020 - Reproducibility and Replicability in Economics.pdf:application/pdf},
}

@article{hardwicke_finding_nodate,
	title = {Finding the right words to evaluate research: An empirical appraisal of {eLife}’s assessment vocabulary},
	journaltitle = {{PLOS} Biology},
	author = {Hardwicke, T. E. and Schiavone, Sarah R. and Clarke, Beth and Vazire, S.},
}

@article{ferrero_effectiveness_2020,
	title = {The effectiveness of refutation texts to correct misconceptions among educators.},
	volume = {26},
	issn = {1939-2192, 1076-898X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xap0000258},
	doi = {10.1037/xap0000258},
	pages = {411--421},
	number = {3},
	journaltitle = {Journal of Experimental Psychology: Applied},
	shortjournal = {Journal of Experimental Psychology: Applied},
	author = {Ferrero, Marta and Hardwicke, Tom E. and Konstantinidis, Emmanouil and Vadillo, Miguel A.},
	urldate = {2023-12-04},
	date = {2020-09},
	langid = {english},
	file = {Accepted Version:/Users/tom/Zotero/storage/BG7CLAZ7/Ferrero et al. - 2020 - The effectiveness of refutation texts to correct misconceptions among educators..pdf:application/pdf},
}

@article{tsakiris_re-thinking_2020,
	title = {Re-thinking Cognition’s open data policy: responding to hardwicke and colleagues’ evaluation of its impact},
	volume = {200},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S001002771830266X},
	doi = {10.1016/j.cognition.2018.10.008},
	shorttitle = {Re-thinking cognition’s open data policy},
	pages = {103821},
	journaltitle = {Cognition},
	shortjournal = {Cognition},
	author = {Tsakiris, Manos and Martin, Randi and Wagemans, Johan},
	urldate = {2023-12-05},
	date = {2020-07},
	langid = {english},
}

@article{ludwig_context-gated_2012,
	title = {Context-gated statistical learning and its role in visual-saccadic decisions.},
	volume = {141},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0024916},
	doi = {10.1037/a0024916},
	pages = {150--169},
	number = {1},
	journaltitle = {Journal of Experimental Psychology: General},
	shortjournal = {Journal of Experimental Psychology: General},
	author = {Ludwig, Casimir J. H. and Farrell, Simon and Ellis, Lucy A. and Hardwicke, Tom E. and Gilchrist, Iain D.},
	urldate = {2023-12-05},
	date = {2012-02},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/WGS2CBSL/Ludwig et al. - 2012 - Context-gated statistical learning and its role in visual-saccadic decisions..pdf:application/pdf},
}

@article{thibault_commentary_2020,
	title = {Commentary: Improving our statistical inferences requires meta-research},
	volume = {49},
	issn = {0300-5771, 1464-3685},
	url = {https://academic.oup.com/ije/article/49/3/894/5835353},
	doi = {10.1093/ije/dyaa051},
	shorttitle = {Commentary},
	pages = {894--895},
	number = {3},
	journaltitle = {International Journal of Epidemiology},
	author = {Thibault, Robert T and Munafò, Marcus R},
	urldate = {2023-12-06},
	date = {2020-06-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/8PNQJV96/Thibault and Munafò - 2020 - Commentary Improving our statistical inferences requires meta-research.pdf:application/pdf},
}

@article{navarro_hypothesis_2011,
	title = {Hypothesis generation, sparse categories, and the positive test strategy.},
	volume = {118},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0021110},
	doi = {10.1037/a0021110},
	pages = {120--134},
	number = {1},
	journaltitle = {Psychological Review},
	shortjournal = {Psychological Review},
	author = {Navarro, Daniel J. and Perfors, Amy F.},
	urldate = {2023-12-06},
	date = {2011-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/LTQWJ49Q/Navarro and Perfors - 2011 - Hypothesis generation, sparse categories, and the positive test strategy..pdf:application/pdf},
}

@inproceedings{perfors_confirmation_2009,
	location = {Austin, {TX}},
	title = {Confirmation bias is rational when hypotheses are sparse},
	eventtitle = {Annual conference of the cognitive science society},
	pages = {2741--2746},
	booktitle = {Proceedings of the 31st annual conference of the cognitive science society},
	author = {Perfors, Andrew F. and Navarro, Daniel J.},
	date = {2009},
}

@article{fiedler_voodoo_2011,
	title = {Voodoo correlations are everywhere—not only in neuroscience},
	volume = {6},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691611400237},
	doi = {10.1177/1745691611400237},
	abstract = {A recent set of articles in Perspectives on Psychological Science discussed inflated correlations between brain measures and behavioral criteria when measurement points (voxels) are deliberately selected to maximize criterion correlations (the target article was Vul, Harris, Winkielman, \& Pashler, 2009). However, closer inspection reveals that this problem is only a special symptom of a broader methodological problem that characterizes all paradigmatic research, not just neuroscience. Researchers not only select voxels to inflate effect size, they also select stimuli, task settings, favorable boundary conditions, dependent variables and independent variables, treatment levels, moderators, mediators, and multiple parameter settings in such a way that empirical phenomena become maximally visible and stable. In general, paradigms can be understood as conventional setups for producing idealized, inflated effects. Although the feasibility of representative designs is restricted, a viable remedy lies in a reorientation of paradigmatic research from the visibility of strong effect sizes to genuine validity and scientific scrutiny.},
	pages = {163--171},
	number = {2},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Fiedler, Klaus},
	urldate = {2023-12-08},
	date = {2011-03},
	langid = {english},
}

@article{christian_survey_2021,
	title = {A survey of early-career researchers in Australia},
	volume = {10},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/60613},
	doi = {10.7554/eLife.60613},
	abstract = {Early-career researchers ({ECRs}) make up a large portion of the academic workforce and their experiences often reflect the wider culture of the research system. Here we surveyed 658 {ECRs} working in Australia to better understand the needs and challenges faced by this community. Although most respondents indicated a ‘love of science’, many also expressed an intention to leave their research position. The responses highlight how job insecurity, workplace culture, mentorship and ‘questionable research practices’ are impacting the job satisfaction of {ECRs} and potentially compromising science in Australia. We also make recommendations for addressing some of these concerns.},
	pages = {e60613},
	journaltitle = {{eLife}},
	author = {Christian, Katherine and Johnstone, Carolyn and Larkins, Jo-ann and Wright, Wendy and Doran, Michael R},
	urldate = {2023-12-08},
	date = {2021-01-11},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/Z83WJPRN/Christian et al. - 2021 - A survey of early-career researchers in Australia.pdf:application/pdf},
}

@article{hardwicke_transparency_2023,
	title = {Transparency is now the default at \textit{Psychological Science}},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/09567976231221573},
	doi = {10.1177/09567976231221573},
	pages = {09567976231221573},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Hardwicke, Tom E. and Vazire, Simine},
	urldate = {2023-12-30},
	date = {2023-12-27},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/UQ48QU36/Hardwicke and Vazire - 2023 - Transparency Is Now the Default at Psychological Science.pdf:application/pdf},
}

@book{williams_style_1990,
	location = {Chicago},
	title = {Style: toward clarity and grace},
	isbn = {978-0-226-89914-5},
	series = {Chicago guides to writing, editing, and publishing},
	shorttitle = {Style},
	pagetotal = {208},
	publisher = {University of Chicago Press},
	author = {Williams, Joseph M.},
	date = {1990},
	langid = {english},
	keywords = {English language, Style},
	file = {Williams - 1990 - Style toward clarity and grace.pdf:/Users/tom/Zotero/storage/UYR26GNN/Williams - 1990 - Style toward clarity and grace.pdf:application/pdf},
}

@article{murphy_c_2021,
	title = {C. S. Peirce’s forgotten but enduring relevance to psychological science},
	volume = {134},
	issn = {0002-9556, 1939-8298},
	url = {https://scholarlypublishingcollective.org/ajp/article/134/3/347/287342/C-S-Peirce-s-Forgotten-but-Enduring-Relevance-to},
	doi = {10.5406/amerjpsyc.134.3.0347},
	abstract = {Abstract
            Charles Sanders Peirce (1839-1914) was one of the most polymathically brilliant scientific thinkers in American history. He was also arguably the first American experimental psychologist, strongly influencing some of the nation’s earliest modern psychology pioneers. Yet partly because of the lasting effects of personal scandals and powerful enemies, he has been almost entirely forgotten by the broader field of psychology. This article aims to briefly reintroduce Peirce as a historically important figure in psychology to a general audience and highlight a few ways in which his trailblazing perspectives point to pervasive deficiencies and opportunities in psychological science. First, his pioneering writings on the economy of research call on us to consider how diminishing returns may plague our research programs, potentially leading to waste of time, money, and intellectual labor in our communities of inquiry. Second, his pragmatic maxim of clarity for intellectual constructs provides a compelling framework in which to understand how jingle and jangle fallacies often undercut the cumulativeness of our field. We contend that Peirce’s work, much of it still unpublished, is a rich resource for psychological scientists across many domains.},
	pages = {347--361},
	number = {3},
	journaltitle = {The American Journal of Psychology},
	author = {Murphy, Brett A. and Lilienfeld, Scott O.},
	urldate = {2024-01-06},
	date = {2021-10-01},
	langid = {english},
}

@article{lin_registered_2024,
	title = {Registered report adoption in academic journals: assessing rates in different research domains},
	issn = {0138-9130, 1588-2861},
	url = {https://link.springer.com/10.1007/s11192-023-04896-y},
	doi = {10.1007/s11192-023-04896-y},
	shorttitle = {Registered report adoption in academic journals},
	abstract = {Although the number of journals that have adopted the registered report format has increased rapidly in recent years, they still account for only a tiny portion of academic journals. This article provides a summary and overview of the number and proportion of journals that accept the registered report format in the various scientific domains. The Center for Open Science was searched for journals that have adopted the registered report as a regular submission option. The numbers of such journals in each scientific domain were then counted based on their group and category classification in the Journal Citation Reports ({JCR}). In July 2023, 278 journals had adopted the registered report format, with 186 of these journals included in the {JCR}. The percentage of journals that had adopted the registered report format ranged from 0 to 7\% across the different major research fields (groups in {JCR}) and from 0 to 34\% across the research subfields (categories in {JCR}). The group “Psychiatry/Psychology” and category “Psychology, Experimental” had the highest percentage of journals that had adopted registered reports. Four large-scale replication projects have been published, focusing on psychology, social science, medicine, and economics, respectively. Although all four studies showed unsatisfactory replication success rates, ≤ 1\% of the journals in the corresponding scientific domains had adopted registered reports, with the exception of psychology (7\%). To improve research reliability and transparency, it is critical to increase the use of the registered report publishing format.},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Lin, Ting-Yu and Cheng, Hao-Chien and Cheng, Li-Fu and Hung, Tsung-Min},
	urldate = {2024-01-14},
	date = {2024-01-10},
	langid = {english},
	file = {Lin et al. - 2024 - Registered report adoption in academic journals assessing rates in different research domains.pdf:/Users/tom/Zotero/storage/NHUAKI92/Lin et al. - 2024 - Registered report adoption in academic journals assessing rates in different research domains.pdf:application/pdf},
}

@article{moshontz_guide_2021,
	title = {A guide to posting and managing preprints},
	volume = {4},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/25152459211019948},
	doi = {10.1177/25152459211019948},
	abstract = {Posting preprints online allows psychological scientists to get feedback, speed dissemination, and ensure public access to their work. This guide is designed to help psychological scientists post preprints and manage them across the publication pipeline. We review terminology, provide a historical and legal overview of preprints, and give guidance on posting and managing preprints before, during, or after the peer-review process to achieve different aims (e.g., get feedback, speed dissemination, achieve open access). We offer concrete recommendations to authors, such as post preprints that are complete and carefully proofread; post preprints in a dedicated preprint server that assigns {DOIs}, provides editable metadata, is indexed by {GoogleScholar}, supports review and endorsements, and supports version control; include a draft date and information about the paper’s status on the cover page; license preprints with {CC} {BY} licenses that permit public use with attribution; and keep preprints up to date after major revisions. Although our focus is on preprints (unpublished versions of a work), we also offer information relevant to postprints (author-formatted, post-peer-review versions of a work) and work that will not otherwise be published (e.g., theses and dissertations).},
	pages = {251524592110199},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Moshontz, Hannah and Binion, Grace and Walton, Haley and Brown, Benjamin T. and Syed, Moin},
	urldate = {2024-01-15},
	date = {2021-04},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/WIXWE42Y/Moshontz et al. - 2021 - A Guide to Posting and Managing Preprints.pdf:application/pdf},
}

@article{yarkoni_cognitive_2010,
	title = {Cognitive neuroscience 2.0: building a cumulative science of human brain function},
	volume = {14},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661310002019},
	doi = {10.1016/j.tics.2010.08.004},
	shorttitle = {Cognitive neuroscience 2.0},
	pages = {489--496},
	number = {11},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Yarkoni, Tal and Poldrack, Russell A. and Van Essen, David C. and Wager, Tor D.},
	urldate = {2024-01-15},
	date = {2010-11},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/IQ3RZ4JH/Yarkoni et al. - 2010 - Cognitive neuroscience 2.0 building a cumulative science of human brain function.pdf:application/pdf},
}

@book{wootton_invention_2016,
	location = {New York, {NY}},
	title = {The invention of science: a new history of the scientific revolution},
	isbn = {978-0-06-175953-6},
	shorttitle = {The invention of science},
	abstract = {"A companion to such acclaimed works as The Age of Wonder, A Clockwork Universe, and Darwin's Ghosts--a groundbreaking examination of the greatest event in history, the Scientific Revolution, and how it came to change the way we understand ourselves and our world. We live in a world transformed by scientific discovery. Yet today, science and its practitioners have come under political attack. In this fascinating history spanning continents and centuries, historian David Wootton offers a lively defense of science, revealing why the Scientific Revolution was truly the greatest event in our history. The Invention of Science goes back five hundred years in time to chronicle this crucial transformation, exploring the factors that led to its birth and the people who made it happen. Wootton argues that the Scientific Revolution was actually five separate yet concurrent events that developed independently, but came to intersect and create a new worldview. Here are the brilliant iconoclasts--Galileo, Copernicus, Brahe, Newton, and many more curious minds from across Europe--whose studies of the natural world challenged centuries of religious orthodoxy and ingrained superstition. From gunpowder technology, the discovery of the new world, movable type printing, perspective painting, and the telescope to the practice of conducting experiments, the laws of nature, and the concept of the fact, Wotton shows how these discoveries codified into a social construct and a system of knowledge. Ultimately, he makes clear the link between scientific discovery and the rise of industrialization--and the birth of the modern world we know."--Publisher's website},
	publisher = {Harper Perennial},
	author = {Wootton, David},
	date = {2016},
	note = {{OCLC}: 943697974},
}

@article{campbell_open-science_2023,
	title = {Open-science guidance for qualitative research: an empirically validated approach for de-identifying sensitive narrative data},
	volume = {6},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/25152459231205832},
	doi = {10.1177/25152459231205832},
	shorttitle = {Open-science guidance for qualitative research},
	abstract = {The open-science movement seeks to make research more transparent and accessible. To that end, researchers are increasingly expected to share de-identified data with other scholars for review, reanalysis, and reuse. In psychology, open-science practices have been explored primarily within the context of quantitative data, but demands to share qualitative data are becoming more prevalent. Narrative data are far more challenging to de-identify fully, and because qualitative methods are often used in studies with marginalized, minoritized, and/or traumatized populations, data sharing may pose substantial risks for participants if their information can be later reidentified. To date, there has been little guidance in the literature on how to de-identify qualitative data. To address this gap, we developed a methodological framework for remediating sensitive narrative data. This multiphase process is modeled on common qualitative-coding strategies. The first phase includes consultations with diverse stakeholders and sources to understand reidentifiability risks and data-sharing concerns. The second phase outlines an iterative process for recognizing potentially identifiable information and constructing individualized remediation strategies through group review and consensus. The third phase includes multiple strategies for assessing the validity of the de-identification analyses (i.e., whether the remediated transcripts adequately protect participants’ privacy). We applied this framework to a set of 32 qualitative interviews with sexual-assault survivors. We provide case examples of how blurring and redaction techniques can be used to protect names, dates, locations, trauma histories, help-seeking experiences, and other information about dyadic interactions.},
	pages = {25152459231205832},
	number = {4},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Campbell, Rebecca and Javorka, {McKenzie} and Engleton, Jasmine and Fishwick, Kathryn and Gregory, Katie and Goodman-Williams, Rachael},
	urldate = {2024-01-21},
	date = {2023-10},
	langid = {english},
	keywords = {toread},
	file = {Full Text:/Users/tom/Zotero/storage/6AMB7MLX/Campbell et al. - 2023 - Open-Science Guidance for Qualitative Research An Empirically Validated Approach for De-Identifying.pdf:application/pdf},
}

@article{pownall_opportunities_2023,
	title = {Opportunities, challenges and tensions: Open science through a lens of qualitative social psychology},
	volume = {62},
	issn = {0144-6665, 2044-8309},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/10.1111/bjso.12628},
	doi = {10.1111/bjso.12628},
	shorttitle = {Opportunities, challenges and tensions},
	abstract = {Abstract
            
              In recent years, there has been a focus in social psychology on efforts to improve the robustness, rigour, transparency and openness of psychological research. This has led to a plethora of new tools, practices and initiatives that each aim to combat questionable research practices and improve the credibility of social psychological scholarship. However, the majority of these efforts derive from quantitative, deductive, hypothesis‐testing methodologies, and there has been a notable lack of in‐depth exploration about what the tools, practices and values may mean for research that uses
              qualitative
              methodologies. Here, we introduce a Special Section of {BJSP}:
              Open Science, Qualitative Methods and Social Psychology: Possibilities and Tensions.
              The authors critically discuss a range of issues, including authorship, data sharing and broader research practices. Taken together, these papers urge the discipline to carefully consider the ontological, epistemological and methodological underpinnings of efforts to improve psychological science, and advocate for a critical appreciation of how mainstream open science discourse may (or may not) be compatible with the goals of qualitative research.},
	pages = {1581--1589},
	number = {4},
	journaltitle = {British Journal of Social Psychology},
	shortjournal = {British J Social Psychol},
	author = {Pownall, Madeleine and Talbot, Catherine V. and Kilby, Laura and Branney, Peter},
	urldate = {2024-01-21},
	date = {2023-10},
	langid = {english},
	file = {Accepted Version:/Users/tom/Zotero/storage/88CUVALQ/Pownall et al. - 2023 - Opportunities, challenges and tensions Open science through a lens of qualitative social psychology.pdf:application/pdf},
}

@article{bahlai_open_2019,
	title = {Open science isn't always open to all scientists},
	volume = {107},
	issn = {0003-0996, 1545-2786},
	url = {https://www.americanscientist.org/article/open-science-isnt-always-open-to-all-scientists},
	doi = {10.1511/2019.107.2.78},
	pages = {78},
	number = {2},
	journaltitle = {American Scientist},
	shortjournal = {Am. Sci.},
	author = {Bahlai, Christie and Bartlett, Lewis and Burgio, Kevin and Fournier, Auriel and Keiser, Carl and Poisot, Timothée and Whitney, Kaitlin},
	urldate = {2024-01-21},
	date = {2019},
}

@book{mahoney_scientist_1976,
	location = {Cambridge, Mass},
	title = {Scientist as subject: the psychological imperative},
	isbn = {978-0-88410-505-3},
	shorttitle = {Scientist as subject},
	pagetotal = {249},
	publisher = {Ballinger Pub. Co},
	author = {Mahoney, Michael J.},
	date = {1976},
	keywords = {Scientists},
}

@article{hardwicke_only_2014,
	title = {Only human: Scientists, systems, and suspect statistics},
	volume = {16},
	pages = {1--12},
	number = {25},
	journaltitle = {Opticon1826},
	author = {Hardwicke, T. E. and Jameel, L. and Jones, M. and Walczak, E. J. and Magis-Weinberg},
	date = {2014},
	file = {Hardwicke et al. - 2014 - Only human Scientists, systems, and suspect statistics.pdf:/Users/tom/Zotero/storage/G5WDNIPT/Hardwicke et al. - 2014 - Only human Scientists, systems, and suspect statistics.pdf:application/pdf},
}

@article{dougherty_citation_2022,
	title = {Citation counts and journal impact factors do not capture some indicators of research quality in the behavioural and brain sciences},
	volume = {9},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220334},
	doi = {10.1098/rsos.220334},
	abstract = {Citation data and journal impact factors are important components of faculty dossiers and figure prominently in both promotion decisions and assessments of a researcher’s broader societal impact. Although these metrics play a large role in high-stakes decisions, the evidence is mixed about whether they are strongly correlated with indicators of research quality. We use data from a large-scale dataset comprising 45 144 journal articles with 667 208 statistical tests and data from 190 replication attempts to assess whether citation counts and impact factors predict three indicators of research quality: (i) the accuracy of statistical reporting, (ii) the evidential value of the reported data and (iii) the replicability of a given experimental result. Both citation counts and impact factors were weak and inconsistent predictors of research quality, so defined, and sometimes negatively related to quality. Our findings raise the possibility that citation data and impact factors may be of limited utility in evaluating scientists and their research. We discuss the implications of these findings in light of current incentive structures and discuss alternative approaches to evaluating research.},
	pages = {220334},
	number = {8},
	journaltitle = {Royal Society Open Science},
	shortjournal = {R. Soc. open sci.},
	author = {Dougherty, Michael R. and Horne, Zachary},
	urldate = {2024-01-29},
	date = {2022-08},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/RTVHWCM4/Dougherty and Horne - 2022 - Citation counts and journal impact factors do not capture some indicators of research quality in the.pdf:application/pdf},
}

@report{hardwicke_persistence_2016,
	title = {Persistence and plasticity in the human memory system: An empirical investigation of the overwriting hypothesis},
	url = {https://osf.io/r4c32},
	shorttitle = {Persistence and plasticity in the human memory system},
	abstract = {The human memory system must resolve a critical tension, ensuring that knowledge endures over time (persistence), whilst simultaneously retaining a capacity for updating when knowledge is outdated or erroneous (plasticity). In this thesis, I examine the provocative idea that memory traces can be overwritten with new information, especially during transient periods of retrieval-induced plasticity that occur when a trace undergoes reconsolidation. A systematic review of human reconsolidation studies finds that the evidentiary support for this claim is remarkably tenuous. Furthermore, the theory fails to survive several strong empirical tests. In Experiments 1-7, I do not replicate a previous finding that is widely cited as a convincing demonstration of human reconsolidation. In Experiments 8-10, I revisit the ‘destructive updating’ account of the classic ‘misinformation effect’ in the context of reconsolidation theory. These experiments show that the effect can be eliminated when an appropriate recognition test is used, demonstrating that event traces are not irrecoverably lost, and therefore cannot have been overwritten during reconsolidation. In Experiment 11, I examine whether prior retrieval will help or hinder the correction of naturally occurring semantic misconceptions. Contrary to reconsolidation theory, I find that knowledge updating is not contingent on memory retrieval, nor does it result in the overwriting of prior knowledge.Finally, in the context of media ‘breaking news’ reports (Experiment 12), I find that the provision of an explicit retraction message, coupled with an alternative account with high causal coverage, is insufficient to eliminate reliance on false information. Finally, I contend that the widespread proliferation of ad hoc hypotheses, and the absence of systematic direct replication, has caused the field of reconsolidation to descend into a theoretical quagmire.I make several recommendations based on the principles of open science that may help to restore mechanisms of self-correction and foster genuine theoretical progress.},
	institution = {Thesis Commons},
	type = {preprint},
	author = {Hardwicke, Tom Elis},
	urldate = {2024-01-29},
	date = {2016},
	doi = {10.31237/osf.io/r4c32},
	file = {Submitted Version:/Users/tom/Zotero/storage/M26NH3RG/Hardwicke - 2017 - Persistence and plasticity in the human memory system An empirical investigation of the overwriting.pdf:application/pdf},
}

@article{lavelle_growth_2023,
	title = {Growth from uncertainty: understanding the replication ‘crisis’ in infant cognition},
	issn = {0031-8248, 1539-767X},
	url = {https://www.cambridge.org/core/product/identifier/S0031824823001575/type/journal_article},
	doi = {10.1017/psa.2023.157},
	shorttitle = {Growth from uncertainty},
	abstract = {Psychology is a discipline that has a high number of failed replications, which has been characterised as a ‘crisis’ on the assumption that failed replications are indicative of untrustworthy research. This paper uses Chang’s concept of epistemic iteration to show how a research programme can advance epistemic goals despite many failed replications. It illustrates this through analysing an on-going large-scale replication attempt of Southgate’s 2007 work exploring infants’ understanding of false beliefs. It concludes that epistemic iteration offers a way of understanding the value of replications — both failed and successful — that contradicts the narrative centred around distrust.},
	pages = {1--38},
	journaltitle = {Philosophy of Science},
	shortjournal = {Philos. sci.},
	author = {Lavelle, Jane Suilin},
	urldate = {2024-01-29},
	date = {2023-11-15},
	langid = {english},
	file = {Lavelle - 2023 - Growth From Uncertainty Understanding the Replication ‘Crisis’ in Infant Cognition.pdf:/Users/tom/Zotero/storage/XK5ADFUU/Lavelle - 2023 - Growth From Uncertainty Understanding the Replication ‘Crisis’ in Infant Cognition.pdf:application/pdf},
}

@report{rainey_data_2024,
	title = {Data and code availability in political science publications from 1995 to 2022},
	url = {https://osf.io/a5yxe},
	abstract = {In this paper, we assess the availability of reproduction archives in political science. By “reproduction archive,” we mean the data and code supporting quantitative research articles that allows others to reproduce the computations described in the published paper. We collect a random sample of quantitative research articles published in political science from 1995 to 2022. We find that—even in 2022—most quantitative research articles do not point a reproduction archive. However, practices are improving. In 2014, when the {DA}-{RT} symposium was published in {PS}, about 12\% of quantitative research articles point to the data and code. Eight years later, in 2022, that has increased to 31\%. This underscores a massive shift in norms, requirements, and infrastructure. Still, only a minority of articles share the supporting data and code. In 2014, Lupia and Alter wrote: “Today, information on the data production and analytic decisions that underlie many published works in political science is unavailable.” They could write the same today; much work remains to be done.},
	institution = {{SocArXiv}},
	type = {preprint},
	author = {Rainey, Carlisle and Roe, Harley and Wang, Qing and Zhou, Hao},
	urldate = {2024-02-02},
	date = {2024-01-21},
	langid = {english},
	doi = {10.31235/osf.io/a5yxe},
	keywords = {toread},
	file = {Rainey et al. - 2024 - Data and Code Availability in Political Science Publications from 1995 to 2022.pdf:/Users/tom/Zotero/storage/DNH3LV3W/Rainey et al. - 2024 - Data and Code Availability in Political Science Publications from 1995 to 2022.pdf:application/pdf},
}

@article{scoggins_measuring_nodate-1,
	title = {Measuring transparency in the social sciences: political science and international relations},
	volume = {2024},
	abstract = {The scientiﬁc method is predicated on transparency – yet the pace at which transparent research practices are being adopted by the scientiﬁc community is slow. The replication crisis in psychology showed that published ﬁndings employing statistical inference are threatened by undetected errors, data manipulation, and data falsiﬁcation. To mitigate these problems and bolster research credibility, open data and preregistration practices have gained traction in the natural and social sciences. However, the extent of their adoption in diﬀerent disciplines are unknown. We introduce procedures to identify the transparency of a research ﬁeld using large-scale text analysis and machine learning classiﬁers. Using political science and international relations as an illustrative case, we examine 93,931 articles across the top 160 political science and international relations journals between 2010 and 2021. We ﬁnd that approximately 21\% of all statistical inference papers have open data and 5\% of all experiments are preregistered. Despite this shortfall, the example of leading journals in the ﬁeld shows that change is feasible and can be eﬀected quickly.},
	number = {14},
	author = {Scoggins, Bermond and Robertson, Matthew P},
	langid = {english},
	keywords = {toread},
	file = {Scoggins and Robertson - Measuring Transparency in the Social Sciences Political Science and International Relations.pdf:/Users/tom/Zotero/storage/TBHGLTRN/Scoggins and Robertson - Measuring Transparency in the Social Sciences Political Science and International Relations.pdf:application/pdf},
}

@article{rosman_open_2022,
	title = {Open science and public trust in science: Results from two studies},
	volume = {31},
	issn = {0963-6625, 1361-6609},
	url = {http://journals.sagepub.com/doi/10.1177/09636625221100686},
	doi = {10.1177/09636625221100686},
	shorttitle = {Open science and public trust in science},
	abstract = {In two studies, we examined whether open science practices, such as making materials, data, and code of a study openly accessible, positively affect public trust in science. Furthermore, we investigated whether the potential trust-damaging effects of research being funded privately (e.g. by a commercial enterprise) may be buffered by such practices. After preregistering six hypotheses, we conducted a survey study (Study 1; N = 504) and an experimental study (Study 2; N = 588) in two German general population samples. In both studies, we found evidence for the positive effects of open science practices on trust, though it should be noted that in Study 2, results were more inconsistent. We did not however find evidence for the aforementioned buffering effect. We conclude that while open science practices may contribute to increasing trust in science, the importance of making use of open science practices visible should not be underestimated.},
	pages = {1046--1062},
	number = {8},
	journaltitle = {Public Understanding of Science},
	shortjournal = {Public Underst Sci},
	author = {Rosman, Tom and Bosnjak, Michael and Silber, Henning and Koßmann, Joanna and Heycke, Tobias},
	urldate = {2024-03-21},
	date = {2022-11},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/4B5X9TYU/Rosman et al. - 2022 - Open science and public trust in science Results from two studies.pdf:application/pdf},
}

@article{bosnjak_template_2022,
	title = {A template for preregistration of quantitative research in psychology: Report of the joint psychological societies preregistration task force.},
	volume = {77},
	issn = {1935-990X, 0003-066X},
	url = {https://doi.apa.org/doi/10.1037/amp0000879},
	doi = {10.1037/amp0000879},
	shorttitle = {A template for preregistration of quantitative research in psychology},
	pages = {602--615},
	number = {4},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Bosnjak, Michael and Fiebach, Christian J. and Mellor, David and Mueller, Stefanie and O'Connor, Daryl B. and Oswald, Frederick L. and Sokol, Rosemarie I.},
	urldate = {2024-03-21},
	date = {2022-05},
	langid = {english},
	file = {Accepted Version:/Users/tom/Zotero/storage/QNMHYVZU/Bosnjak et al. - 2022 - A template for preregistration of quantitative research in psychology Report of the joint psycholog.pdf:application/pdf},
}

@article{roettger_preregistration_2021,
	title = {Preregistration in experimental linguistics: applications, challenges, and limitations},
	volume = {59},
	issn = {0024-3949, 1613-396X},
	url = {https://www.degruyter.com/document/doi/10.1515/ling-2019-0048/html},
	doi = {10.1515/ling-2019-0048},
	shorttitle = {Preregistration in experimental linguistics},
	abstract = {Abstract
            The current publication system neither incentivizes publishing null results nor direct replication attempts, which biases the scientific record toward novel findings that appear to support presented hypotheses (referred to as “publication bias”). Moreover, flexibility in data collection, measurement, and analysis (referred to as “researcher degrees of freedom”) can lead to overconfident beliefs in the robustness of a statistical relationship. One way to systematically decrease publication bias and researcher degrees of freedom is preregistration. A preregistration is a time-stamped document that specifies how data is to be collected, measured, and analyzed prior to data collection. While preregistration is a powerful tool to reduce bias, it comes with certain challenges and limitations which have to be evaluated for each scientific discipline individually. This paper discusses the application, challenges and limitations of preregistration for experimental linguistic research.},
	pages = {1227--1249},
	number = {5},
	journaltitle = {Linguistics},
	author = {Roettger, Timo B.},
	urldate = {2024-03-21},
	date = {2021-09-27},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/SCB3XP5B/Roettger - 2021 - Preregistration in experimental linguistics applications, challenges, and limitations.pdf:application/pdf},
}

@article{donner_document_2017,
	title = {Document type assignment accuracy in the journal citation index data of Web of Science},
	volume = {113},
	issn = {0138-9130, 1588-2861},
	url = {http://link.springer.com/10.1007/s11192-017-2483-y},
	doi = {10.1007/s11192-017-2483-y},
	abstract = {This article reports the results of a study of the correctness of document type assignments in the commercial citation index database Web of Science ({SCIE}, {SSCI}, {AHCI} collections). The document type assignments for publication records are compared to those given on the ofﬁcial journal websites or in the publication full-texts for a random sample of 791 Web of Science records across the four document type categories articles, letters, reviews and others, according to the deﬁnitions of {WoS}. The proportion of incorrect assignments across document types and its inﬂuence on document speciﬁc normalized citations scores are analysed. It is found that document type data is correct in 94\% of records. Further analyses show that within records of one document type as assigned in the data source, the records assigned to the type correctly and incorrectly have different average page counts and reference counts.},
	pages = {219--236},
	number = {1},
	journaltitle = {Scientometrics},
	shortjournal = {Scientometrics},
	author = {Donner, Paul},
	urldate = {2024-03-24},
	date = {2017-10},
	langid = {english},
	file = {Donner - 2017 - Document type assignment accuracy in the journal citation index data of Web of Science.pdf:/Users/tom/Zotero/storage/6H9NYV5F/Donner - 2017 - Document type assignment accuracy in the journal citation index data of Web of Science.pdf:application/pdf},
}

@article{hardwicke_transparency_2023-1,
	title = {Transparency is now the default at \textit{Psychological Science}},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/09567976231221573},
	doi = {10.1177/09567976231221573},
	pages = {09567976231221573},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Hardwicke, Tom E. and Vazire, Simine},
	urldate = {2024-04-07},
	date = {2023-12-27},
	langid = {english},
	file = {Hardwicke and Vazire - 2023 - Transparency Is Now the Default at Psychological Science.pdf:/Users/tom/Zotero/storage/FP5QYB3H/Hardwicke and Vazire - 2023 - Transparency Is Now the Default at Psychological Science.pdf:application/pdf},
}

@article{hostler_research_2024,
	title = {Research assessment using a narrow definition of “research quality” is an act of gatekeeping: A comment on Gärtner et al. (2022)},
	volume = {8},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2003-2714},
	url = {https://open.lnu.se/index.php/metapsychology/article/view/3764},
	doi = {10.15626/MP.2023.3764},
	shorttitle = {Research assessment using a narrow definition of “research quality” is an act of gatekeeping},
	abstract = {Gärtner et al. (2022) propose a system for quantitatively scoring the methodological rigour of papers during the hiring and promotion of psychology researchers, with the aim of advantaging researchers who conduct open, reproducible work. However, the quality criteria proposed for assessing methodological rigour are drawn from a narrow post-positivist paradigm of quantitative, confirmatory research conducted from an epistemology of scientific realism. This means that research conducted from a variety of other approaches, including constructivist, qualitative research, becomes structurally disadvantaged under the new system. The implications of this for particular fields, demographics of researcher, and the future of the discipline of psychology are discussed.},
	journaltitle = {Meta-Psychology},
	shortjournal = {{MP}},
	author = {Hostler, Tom},
	urldate = {2024-04-07},
	date = {2024-03-17},
}

@misc{schonbrodt_responsible_2022,
	title = {Responsible Research Assessment I: Implementing {DORA} for hiring and promotion in psychology},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	url = {https://osf.io/rgh5b},
	doi = {10.31234/osf.io/rgh5b},
	shorttitle = {Responsible Research Assessment I},
	abstract = {The use of journal impact factors and other metric indicators of research productivity, such as the h-index, has been heavily criticized for being invalid for the assessment of individual researchers and for fueling a detrimental “publish or perish” culture. Multiple initiatives call for developing alternatives to existing metrics that better reflect quality (instead of quantity) in research assessment. This report, written by a task force established by the German Psychological Society, proposes how responsible research assessment could be done in the field of psychology. We present four principles of responsible research assessment in hiring and promotion and suggest a two-phase assessment procedure that combines the objectivity and efficiency of indicators with a qualitative, discursive assessment of shortlisted candidates. The main aspects of our proposal are (a) to broaden the range of relevant research contributions to include published data sets and research software, along with research papers, and (b) to place greater emphasis on quality and rigor in research evaluation.},
	author = {Schönbrodt, Felix D. and Gärtner, Anne and Frank, Maximilian and Gollwitzer, Mario and Ihle, Malika and Mischkowski, Dorothee and Phan, Le Vy and Schmitt, Manfred and Scheel, Anne M. and Schubert, Anna-Lena and Steinberg, Ulf and Leising, Daniel},
	urldate = {2024-04-08},
	date = {2022-11-25},
	file = {Submitted Version:/Users/tom/Zotero/storage/M4GR6GVD/Schönbrodt et al. - 2022 - Responsible Research Assessment I Implementing DORA for hiring and promotion in psychology.pdf:application/pdf},
}

@misc{gartner_responsible_2022,
	title = {Responsible Research Assessment {II}: A specific proposal for hiring and promotion in psychology},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	url = {https://osf.io/5yexm},
	doi = {10.31234/osf.io/5yexm},
	shorttitle = {Responsible Research Assessment {II}},
	abstract = {Traditional metric indicators of scientific productivity (e.g., journal impact factor; h-index) have been heavily criticized for being invalid and fueling a culture that focuses on the quantity, rather than the quality, of a person’s scientific output. There is now a wide-spread demand for specified alternatives to current academic evaluation practices. In a previous report, we laid out four basic principles of a more responsible research assessment in academic hiring and promotion processes (Schönbrodt et al., 2022). The present paper offers a specific proposal for how these principles may be implemented in practice: We argue in favor of broadening the range of relevant research contributions and thus propose concrete quality criteria (including ready-to-use online templates) for published research articles, data sets and research software. These criteria are supposed to be used primarily in the first phase of the assessment process. Their function is to help establish a minimum threshold of methodological rigor that candidates need to pass in order to be further considered for hiring and promotion. In contrast, the second phase of the assessment process will focus more on the actual content of candidates’ research output and necessarily use more narrative means of assessment. We hope that this proposal will help get our colleagues in the field engaged in a discussion over ways of replacing current invalid evaluation criteria with ones that relate more closely to scientific quality.},
	author = {Gärtner, Anne and Leising, Daniel and Schönbrodt, Felix D.},
	urldate = {2024-04-08},
	date = {2022-11-25},
	file = {Submitted Version:/Users/tom/Zotero/storage/IGII3R2F/Gärtner et al. - 2022 - Responsible Research Assessment II A specific proposal for hiring and promotion in psychology.pdf:application/pdf},
}

@article{bellomo_assessment_2024,
	title = {Assessment of transparency indicators in space medicine},
	volume = {19},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0300701},
	doi = {10.1371/journal.pone.0300701},
	abstract = {Space medicine is a vital discipline with often time-intensive and costly projects and constrained opportunities for studying various elements such as space missions, astronauts, and simulated environments. Moreover, private interests gain increasing influence in this discipline. In scientific disciplines with these features, transparent and rigorous methods are essential. Here, we undertook an evaluation of transparency indicators in publications within the field of space medicine. A meta-epidemiological assessment of {PubMed} Central Open Access ({PMC} {OA}) eligible articles within the field of space medicine was performed for prevalence of code sharing, data sharing, pre-registration, conflicts of interest, and funding. Text mining was performed with the rtransparent text mining algorithms with manual validation of 200 random articles to obtain corrected estimates. Across 1215 included articles, 39 (3\%) shared code, 258 (21\%) shared data, 10 (1\%) were registered, 110 (90\%) contained a conflict-of-interest statement, and 1141 (93\%) included a funding statement. After manual validation, the corrected estimates for code sharing, data sharing, and registration were 5\%, 27\%, and 1\%, respectively. Data sharing was 32\% when limited to original articles and highest in space/parabolic flights (46\%). Overall, across space medicine we observed modest rates of data sharing, rare sharing of code and almost non-existent protocol registration. Enhancing transparency in space medicine research is imperative for safeguarding its scientific rigor and reproducibility.},
	pages = {e0300701},
	number = {4},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Bellomo, Rosa Katia and Zavalis, Emmanuel A. and Ioannidis, John P. A.},
	editor = {Debevec, Tadej},
	urldate = {2024-04-14},
	date = {2024-04-02},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/4NSDKIEY/Bellomo et al. - 2024 - Assessment of transparency indicators in space medicine.pdf:application/pdf},
}

@article{milojevic_practical_2020,
	title = {Practical method to reclassify Web of Science articles into unique subject categories and broad disciplines},
	volume = {1},
	issn = {2641-3337},
	url = {https://direct.mit.edu/qss/article/1/1/183-206/15573},
	doi = {10.1162/qss_a_00014},
	abstract = {Classification of bibliographic items into subjects and disciplines in large databases is essential for many quantitative science studies. The Web of Science classification of journals into approximately 250 subject categories, which has served as a basis for many studies, is known to have some fundamental problems and several practical limitations that may affect the results from such studies. Here we present an easily reproducible method to perform reclassification of the Web of Science into existing subject categories and into 14 broad areas. Our reclassification is at the level of articles, so it preserves disciplinary differences that may exist among individual articles published in the same journal. Reclassification also eliminates ambiguous (multiple) categories that are found for 50\% of items and assigns a discipline/field category to all articles that come from broad-coverage journals such as Nature and Science. The correctness of the assigned subject categories is evaluated manually and is found to be ∼95\%.},
	pages = {183--206},
	number = {1},
	journaltitle = {Quantitative Science Studies},
	shortjournal = {Quantitative Science Studies},
	author = {Milojević, Staša},
	urldate = {2024-04-14},
	date = {2020-02},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/5QBHIEYS/Milojević - 2020 - Practical method to reclassify Web of Science articles into unique subject categories and broad disc.pdf:application/pdf},
}

@article{spodick_editors_1983,
	title = {The editor's correspondence: Analysis of patterns appearing in selected specialty and general journals},
	volume = {52},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00029149},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0002914983905908},
	doi = {10.1016/0002-9149(83)90590-8},
	shorttitle = {The editor's correspondence},
	pages = {1290--1292},
	number = {10},
	journaltitle = {The American Journal of Cardiology},
	shortjournal = {The American Journal of Cardiology},
	author = {Spodick, David H. and Goldberg, Robert J.},
	urldate = {2024-04-14},
	date = {1983-12},
	langid = {english},
	file = {Spodick and Goldberg - 1983 - The editor's correspondence Analysis of patterns appearing in selected specialty and general journa.pdf:/Users/tom/Zotero/storage/9HR88KPJ/Spodick and Goldberg - 1983 - The editor's correspondence Analysis of patterns appearing in selected specialty and general journa.pdf:application/pdf},
}

@book{blair_research_2023,
	location = {Princeton},
	title = {Research design in the social sciences: declaration, diagnosis, and redesign},
	isbn = {978-0-691-19958-0},
	shorttitle = {Research design in the social sciences},
	pagetotal = {1},
	publisher = {Princeton University Press},
	author = {Blair, Graeme and Coppock, Alexander and Humphreys, Macartan},
	date = {2023},
	langid = {english},
	keywords = {Social sciences, Research, {SOCIAL} {SCIENCE} / Research, {SOCIAL} {SCIENCE} / Sociology / General},
	file = {Blair et al. - 2023 - Research design in the social sciences declaration, diagnosis, and redesign.pdf:/Users/tom/Zotero/storage/TGSYA6BW/Blair et al. - 2023 - Research design in the social sciences declaration, diagnosis, and redesign.pdf:application/pdf},
}

@article{grimmer_new_nodate,
	title = {A New Framework for Machine Learning and the Social Sciences},
	author = {Grimmer, Justin and Roberts, Margaret E and Stewart, Brandon M},
	langid = {english},
	file = {Grimmer et al. - A New Framework for Machine Learning and the Social Sciences.pdf:/Users/tom/Zotero/storage/HB85DWV8/Grimmer et al. - A New Framework for Machine Learning and the Social Sciences.pdf:application/pdf},
}

@article{ankelpeters_is_2024,
	title = {Is economics self‐correcting? Replications in the \textit{American Economic Review}},
	issn = {0095-2583, 1465-7295},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ecin.13222},
	doi = {10.1111/ecin.13222},
	shorttitle = {Is economics self‐correcting?},
	abstract = {Abstract
            
              This paper reviews the impact of replications published as comments in the
              American Economic Review
              between 2010 and 2020. We examine their citations and influence on the original papers' ({OPs}) subsequent citations. Our results show that comments are barely cited, and they do not affect the {OP}'s citations—even if the comment diagnoses substantive problems. Furthermore, we conduct an opinion survey among replicators and authors and find that there often is no consensus on whether the {OP}'s contribution sustains. We conclude that the economics literature does not self‐correct, and that robustness and replicability are hard to define in economics.},
	pages = {ecin.13222},
	journaltitle = {Economic Inquiry},
	shortjournal = {Economic Inquiry},
	author = {Ankel‐Peters, Jörg and Fiala, Nathan and Neubauer, Florian},
	urldate = {2024-04-22},
	date = {2024-04-13},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/SM7KBIUQ/Ankel‐Peters et al. - 2024 - Is economics self‐correcting Replications in the American Economic Review.pdf:application/pdf},
}

@article{orben_journal_2019,
	title = {A journal club to fix science},
	volume = {573},
	rights = {http://www.springer.com/tdm},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/d41586-019-02842-8},
	doi = {10.1038/d41586-019-02842-8},
	pages = {465--465},
	number = {7775},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Orben, Amy},
	urldate = {2024-04-30},
	date = {2019-09-26},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/IT6LG8WY/Orben - 2019 - A journal club to fix science.pdf:application/pdf},
}

@article{cruwell_seven_2019,
	title = {Seven Easy Steps to Open Science: An Annotated Reading List},
	volume = {227},
	rights = {https://creativecommons.org/licenses/by-nc-nd/4.0},
	issn = {2190-8370, 2151-2604},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000387},
	doi = {10.1027/2151-2604/a000387},
	shorttitle = {Seven Easy Steps to Open Science},
	abstract = {The open science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz, Gronau, Dablander, Edelsbrunner, and Baribault (2018). Written for researchers and students – particularly in psychological science – it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
	pages = {237--248},
	number = {4},
	journaltitle = {Zeitschrift für Psychologie},
	shortjournal = {Zeitschrift für Psychologie},
	author = {Crüwell, Sophia and Van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse C. and Orben, Amy and Parsons, Sam and Schulte-Mecklenbeck, Michael},
	urldate = {2024-04-30},
	date = {2019-10},
	langid = {english},
	file = {Crüwell et al. - 2019 - Seven Easy Steps to Open Science An Annotated Reading List.pdf:/Users/tom/Zotero/storage/DUY97UJ7/Crüwell et al. - 2019 - Seven Easy Steps to Open Science An Annotated Reading List.pdf:application/pdf},
}

@article{vazire_quality_2017,
	title = {Quality uncertainty erodes trust in science},
	volume = {3},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2474-7394},
	url = {https://online.ucpress.edu/collabra/article/3/1/1/112375/Quality-Uncertainty-Erodes-Trust-in-Science},
	doi = {10.1525/collabra.74},
	abstract = {When consumers of science (readers and reviewers) lack relevant details about the study design, data, and analyses, they cannot adequately evaluate the strength of a scientific study. Lack of transparency is common in science, and is encouraged by journals that place more emphasis on the aesthetic appeal of a manuscript than the robustness of its scientific claims. In doing this, journals are implicitly encouraging authors to do whatever it takes to obtain eye-catching results. To achieve this, researchers can use common research practices that beautify results at the expense of the robustness of those results (e.g., p-hacking). The problem is not engaging in these practices, but failing to disclose them. A car whose carburetor is duct-taped to the rest of the car might work perfectly fine, but the buyer has a right to know about the duct-taping. Without high levels of transparency in scientific publications, consumers of scientific manuscripts are in a similar position as buyers of used cars – they cannot reliably tell the difference between lemons and high quality findings. This phenomenon – quality uncertainty – has been shown to erode trust in economic markets, such as the used car market. The same problem threatens to erode trust in science. The solution is to increase transparency and give consumers of scientific research the information they need to accurately evaluate research. Transparency would also encourage researchers to be more careful in how they conduct their studies and write up their results. To make this happen, we must tie journals’ reputations to their practices regarding transparency. Reviewers hold a great deal of power to make this happen, by demanding the transparency needed to rigorously evaluate scientific manuscripts. The public expects transparency from science, and appropriately so – we should be held to a higher standard than used car salespeople.},
	pages = {1},
	number = {1},
	journaltitle = {Collabra: Psychology},
	author = {Vazire, Simine},
	urldate = {2024-04-30},
	date = {2017-01-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/QFA6JRYR/Vazire - 2017 - Quality Uncertainty Erodes Trust in Science.pdf:application/pdf},
}

@article{simons_value_2014,
	title = {The value of direct replication},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691613514755},
	doi = {10.1177/1745691613514755},
	abstract = {Reproducibility is the cornerstone of science. If an effect is reliable, any competent researcher should be able to obtain it when using the same procedures with adequate statistical power. Two of the articles in this special section question the value of direct replication by other laboratories. In this commentary, I discuss the problematic implications of some of their assumptions and argue that direct replication by multiple laboratories is the only way to verify the reliability of an effect.},
	pages = {76--80},
	number = {1},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Simons, Daniel J.},
	urldate = {2024-04-30},
	date = {2014-01},
	langid = {english},
}

@article{wiebels_leveraging_2021,
	title = {Leveraging containers for reproducible psychological research},
	volume = {4},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/25152459211017853},
	doi = {10.1177/25152459211017853},
	abstract = {Containers have become increasingly popular in computing and software engineering and are gaining traction in scientific research. They allow packaging up all code and dependencies to ensure that analyses run reliably across a range of operating systems and software versions. Despite being a crucial component for reproducible science, containerization has yet to become mainstream in psychology. In this tutorial, we describe the logic behind containers, what they are, and the practical problems they can solve. We walk the reader through the implementation of containerization within a research workflow with examples using Docker and R. Specifically, we describe how to use existing containers, build personalized containers, and share containers alongside publications. We provide a worked example that includes all steps required to set up a container for a research project and can easily be adapted and extended. We conclude with a discussion of the possibilities afforded by the large-scale adoption of containerization, especially in the context of cumulative, open science, toward a more efficient and inclusive research ecosystem.},
	pages = {251524592110178},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Wiebels, Kristina and Moreau, David},
	urldate = {2024-04-30},
	date = {2021-04},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/R92445Q2/Wiebels and Moreau - 2021 - Leveraging Containers for Reproducible Psychological Research.pdf:application/pdf},
}

@article{cristea_improving_2018,
	title = {Improving disclosure of financial conflicts of interest for research on psychosocial interventions},
	volume = {75},
	issn = {2168-622X},
	url = {http://archpsyc.jamanetwork.com/article.aspx?doi=10.1001/jamapsychiatry.2018.0382},
	doi = {10.1001/jamapsychiatry.2018.0382},
	pages = {541},
	number = {6},
	journaltitle = {{JAMA} Psychiatry},
	shortjournal = {{JAMA} Psychiatry},
	author = {Cristea, Ioana-Alina and Ioannidis, John P. A.},
	urldate = {2024-04-30},
	date = {2018-06-01},
	langid = {english},
}

@article{chivers_does_2019,
	title = {Does psychology have a conflict-of-interest problem?},
	volume = {571},
	rights = {http://www.springer.com/tdm},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/d41586-019-02041-5},
	doi = {10.1038/d41586-019-02041-5},
	pages = {20--23},
	number = {7763},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Chivers, Tom},
	urldate = {2024-04-30},
	date = {2019-07},
	langid = {english},
}

@misc{cobey_biomedical_2023,
	title = {Biomedical researchers’ perspectives on the reproducibility of research: a cross-sectional international survey},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.09.18.558287},
	doi = {10.1101/2023.09.18.558287},
	shorttitle = {Biomedical researchers’ perspectives on the reproducibility of research},
	abstract = {Abstract
          We conducted an international cross-sectional survey of biomedical researchers’ perspectives on the reproducibility of research. This study builds on a widely cited 2016 survey on reproducibility, and provides a biomedical-specific and contemporary perspective on reproducibility. To sample the community, we randomly selected 400 journals indexed in {MEDLINE}, from which we extracted the author names and e-mails from all articles published between October 1, 2020 and October 1, 2021. We invited participants to complete an anonymous online survey which collected basic demographic information, perceptions about a reproducibility crisis, perceived causes of irreproducibility of research results, experience conducting replication studies, and knowledge of funding and training for research on reproducibility. A total of 1924 participants accessed our survey, of which 1630 provided useable responses (response rate 7\% of 23,234). Key findings include that 72\% of participants agreed there was a reproducibility crisis in biomedicine, with 27\% of participants indicating the crisis was ‘significant’. The leading perceived cause of irreproducibility was a ‘pressure to publish’ with 62\% of participants indicating it ‘always’ or ‘very often’ contributes. About half of the participants (54\%) had run a replication of their own previously published study while slightly more (57\%) had run a replication of another researcher’s study. Just 16\% of participants indicated their institution had established procedures to enhance the reproducibility of biomedical research; and 67\% felt their institution valued new research over replication studies. Participants also reported few opportunities to obtain funding to attempt to reproduce a study and 83\% perceived it would be harder to do so than to get funding to do a novel study. Our results may be used to guide training and interventions to improve research reproducibility and to monitor rates of reproducibility over time. The findings are also relevant to policy makers and academic leadership looking to create incentives and research cultures that support reproducibility and value research quality.},
	author = {Cobey, Kelly D. and Ebrahimzadeh, Sanam and Page, Matthew J. and Thibault, Robert T. and Nguyen, Phi-Yen and Abu-Dalfa, Farah and Moher, David},
	urldate = {2024-05-02},
	date = {2023-09-21},
	langid = {english},
	file = {Submitted Version:/Users/tom/Zotero/storage/9XL7L2QL/Cobey et al. - 2023 - Biomedical researchers’ perspectives on the reproducibility of research a cross-sectional internati.pdf:application/pdf},
}

@article{eakin_misinformation_2003,
	title = {Misinformation effects in eyewitness memory: The presence and absence of memory impairment as a function of warning and misinformation accessibility.},
	volume = {29},
	issn = {1939-1285, 0278-7393},
	url = {https://doi.apa.org/doi/10.1037/0278-7393.29.5.813},
	doi = {10.1037/0278-7393.29.5.813},
	shorttitle = {Misinformation effects in eyewitness memory},
	pages = {813--825},
	number = {5},
	journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	shortjournal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Eakin, Deborah K. and Schreiber, Thomas A. and Sergent-Marshall, Susan},
	urldate = {2024-05-15},
	date = {2003-09},
	langid = {english},
}

@article{takarangi_modernising_2006,
	title = {Modernising the misinformation effect: the development of a new stimulus set},
	volume = {20},
	rights = {http://onlinelibrary.wiley.com/{termsAndConditions}\#vor},
	issn = {0888-4080, 1099-0720},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/acp.1209},
	doi = {10.1002/acp.1209},
	shorttitle = {Modernising the misinformation effect},
	abstract = {Abstract
            Researchers studying the misinformation effect tend to present the event in one of two formats: slides or video. Both have their advantages and disadvantages. Videos capture much more information than slides, but slides permit easy counterbalancing of event details. We capitalised on digital technology to create a misinformation event that resolves many of the limitations inherent in earlier formats. Copyright © 2006 John Wiley \& Sons, Ltd.},
	pages = {583--590},
	number = {5},
	journaltitle = {Applied Cognitive Psychology},
	shortjournal = {Applied Cognitive Psychology},
	author = {Takarangi, Melanie K.T. and Parker, Sophie and Garry, Maryanne},
	urldate = {2024-05-15},
	date = {2006-07},
	langid = {english},
	file = {Takarangi et al. - 2006 - Modernising the misinformation effect the development of a new stimulus set.pdf:/Users/tom/Zotero/storage/9E3LL8S7/Takarangi et al. - 2006 - Modernising the misinformation effect the development of a new stimulus set.pdf:application/pdf},
}

@article{butler_misinformation_2023,
	title = {The (Mis)Information Game: A social media simulator},
	volume = {56},
	issn = {1554-3528},
	url = {https://link.springer.com/10.3758/s13428-023-02153-x},
	doi = {10.3758/s13428-023-02153-x},
	shorttitle = {The (Mis)Information Game},
	abstract = {Given the potential negative impact reliance on misinformation can have, substantial effort has gone into understanding the factors that influence misinformation belief and propagation. However, despite the rise of social media often being cited as a fundamental driver of misinformation exposure and false beliefs, how people process misinformation on social media platforms has been under-investigated. This is partially due to a lack of adaptable and ecologically valid social media testing paradigms, resulting in an over-reliance on survey software and questionnaire-based measures. To provide researchers with a flexible tool to investigate the processing and sharing of misinformation on social media, this paper presents The Misinformation Game—an easily adaptable, open-source online testing platform that simulates key characteristics of social media. Researchers can customize posts (e.g., headlines, images), source information (e.g., handles, avatars, credibility), and engagement information (e.g., a post’s number of likes and dislikes). The platform allows a range of response options for participants (like, share, dislike, flag) and supports comments. The simulator can also present posts on individual pages or in a scrollable feed, and can provide customized dynamic feedback to participants via changes to their follower count and credibility score, based on how they interact with each post. Notably, no specific programming skills are required to create studies using the simulator. Here, we outline the key features of the simulator and provide a non-technical guide for use by researchers. We also present results from two validation studies. All the source code and instructions are freely available online at https://misinfogame.com.},
	pages = {2376--2397},
	number = {3},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Butler, Lucy H. and Lamont, Padraig and Wan, Dean Law Yim and Prike, Toby and Nasim, Mehwish and Walker, Bradley and Fay, Nicolas and Ecker, Ullrich K. H.},
	urldate = {2024-05-15},
	date = {2023-07-11},
	langid = {english},
	file = {Butler et al. - 2023 - The (Mis)Information Game A social media simulator.pdf:/Users/tom/Zotero/storage/HUJDMELB/Butler et al. - 2023 - The (Mis)Information Game A social media simulator.pdf:application/pdf},
}

@article{ulatowska_format_2016,
	title = {Do format differences in the presentation of information affect susceptibility to memory distortions? The three-stage misinformation procedure reconsidered},
	volume = {129},
	issn = {0002-9556, 1939-8298},
	url = {https://scholarlypublishingcollective.org/ajp/article/129/4/407/258545/Do-Format-Differences-in-the-Presentation-of},
	doi = {10.5406/amerjpsyc.129.4.0407},
	shorttitle = {Do format differences in the presentation of information affect susceptibility to memory distortions?},
	abstract = {Abstract
            To date most studies within the misinformation paradigm have used the visual presentation of a to-be-remembered event that is later tested verbally or visually. However, the well-established encoding specificity hypothesis predicts that congruence between encoding and test phases should lead to fewer memory errors. In Study 1, we examined the susceptibility to misinformation after encoding original information in 1 of 4 different formats: as a film, slides, and as a written or auditory narrative. All participants were tested verbally, and those who encoded original information pictorially (as a video or slides) were more likely to incorrectly accept verbally suggested information. This might be a consequence of encoding–retrieval format match. In Study 2, using either verbal or pictorial modality during encoding, postevent information, and test (fully crossed design), we partially supported the encoding–retrieval format match hypothesis; however, auditory presentation of original or postevent information modified the effect, showing that a memory trace created after auditory description was the strongest.},
	pages = {407--417},
	number = {4},
	journaltitle = {The American Journal of Psychology},
	author = {Ulatowska, Joanna and Olszewska, Justyna and Hanson, Matthew D.},
	urldate = {2024-05-15},
	date = {2016-12-01},
	langid = {english},
	file = {Ulatowska et al. - 2016 - Do format differences in the presentation of information affect susceptibility to memory distortions.pdf:/Users/tom/Zotero/storage/DXRQJKJX/Ulatowska et al. - 2016 - Do format differences in the presentation of information affect susceptibility to memory distortions.pdf:application/pdf},
}

@incollection{garry_misinformation_2007,
	edition = {0},
	title = {Misinformation effects and the suggestibility of eyewitness memory},
	isbn = {978-0-203-77486-1},
	url = {https://www.taylorfrancis.com/books/9781134811861},
	booktitle = {Do Justice and Let the Sky Fall: Elizabeth F. Loftus and Her Contributions to Science, Law, and Academic Freedom},
	publisher = {Psychology Press},
	author = {Zaragoza, Maria and Belli, Robert F. and Payment, K. E.},
	editor = {Garry, Maryanne and Hayne, Harlene},
	urldate = {2024-05-16},
	date = {2007},
	langid = {english},
	doi = {10.4324/9780203774861},
	file = {Garry and Hayne - 2013 - Do Justice and Let the Sky Fall Elizabeth F. Loftus and Her Contributions to Science, Law, and Acad.pdf:/Users/tom/Zotero/storage/2HSGJ3GN/Garry and Hayne - 2013 - Do Justice and Let the Sky Fall Elizabeth F. Loftus and Her Contributions to Science, Law, and Acad.pdf:application/pdf},
}

@article{ferguson_survey_2023,
	title = {Survey of open science practices and attitudes in the social sciences},
	volume = {14},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-41111-1},
	doi = {10.1038/s41467-023-41111-1},
	abstract = {Abstract
            Open science practices such as posting data or code and pre-registering analyses are increasingly prescribed and debated in the applied sciences, but the actual popularity and lifetime usage of these practices remain unknown. This study provides an assessment of attitudes toward, use of, and perceived norms regarding open science practices from a sample of authors published in top-10 (most-cited) journals and {PhD} students in top-20 ranked North American departments from four major social science disciplines: economics, political science, psychology, and sociology. We observe largely favorable private attitudes toward widespread lifetime usage (meaning that a researcher has used a particular practice at least once) of open science practices. As of 2020, nearly 90\% of scholars had ever used at least one such practice. Support for posting data or code online is higher (88\% overall support and nearly at the ceiling in some fields) than support for pre-registration (58\% overall). With respect to norms, there is evidence that the scholars in our sample appear to underestimate the use of open science practices in their field. We also document that the reported lifetime prevalence of open science practices increased from 49\% in 2010 to 87\% a decade later.},
	pages = {5401},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Ferguson, Joel and Littman, Rebecca and Christensen, Garret and Paluck, Elizabeth Levy and Swanson, Nicholas and Wang, Zenan and Miguel, Edward and Birke, David and Pezzuto, John-Henry},
	urldate = {2024-05-30},
	date = {2023-09-05},
	langid = {english},
	file = {Ferguson et al. - 2023 - Survey of open science practices and attitudes in the social sciences.pdf:/Users/tom/Zotero/storage/YYUTQXCR/Ferguson et al. - 2023 - Survey of open science practices and attitudes in the social sciences.pdf:application/pdf},
}

@article{akker_preregistration_2021,
	title = {Preregistration of secondary data analysis: A template and tutorial},
	volume = {5},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2003-2714},
	url = {https://open.lnu.se/index.php/metapsychology/article/view/2625},
	doi = {10.15626/MP.2020.2625},
	shorttitle = {Preregistration of secondary data analysis},
	abstract = {Preregistration has been lauded as one of the solutions to the so-called ‘crisis of confidence’ in the social sciences and has therefore gained popularity in recent years. However, the current guidelines for preregistration have been developed primarily for studies where new data will be collected. Yet, preregistering secondary data analyses--- where new analyses are proposed for existing data---is just as important, given that researchers’ hypotheses and analyses may be biased by their prior knowledge of the data. The need for proper guidance in this area is especially desirable now that data is increasingly shared publicly. In this tutorial, we present a template specifically designed for the preregistration of secondary data analyses and provide comments and a worked example that may help with using the template effectively. Through this illustration, we show that completing such a template is feasible, helps limit researcher degrees of freedom, and may make researchers more deliberate in their data selection and analysis efforts.},
	journaltitle = {Meta-Psychology},
	shortjournal = {{MP}},
	author = {Akker, Olmo van den and Weston, Sara and Campbell, Lorne and Chopik, Bill and Damian, Rodica and Davis-Kean, Pamela and Hall, Andrew and Kosie, Jessica and Kruse, Elliott and Olsen, Jerome and Ritchie, Stuart and Valentine, Kd and Van 'T Veer, Anna and Bakker, Marjan},
	urldate = {2024-06-01},
	date = {2021-11-09},
	file = {Full Text:/Users/tom/Zotero/storage/HNSPLKLI/Van Den Akker et al. - 2021 - Preregistration of secondary data analysis A template and tutorial.pdf:application/pdf},
}

@article{vanpaemel_are_2015,
	title = {Are we wasting a good crisis? The availability of psychological research data after the storm},
	volume = {1},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2376-6832},
	url = {https://online.ucpress.edu/collabra/article/1/1/3/112343/Are-We-Wasting-a-Good-Crisis-The-Availability-of},
	doi = {10.1525/collabra.13},
	shorttitle = {Are we wasting a good crisis?},
	abstract = {To study the availability of psychological research data, we requested data from 394 papers, published in all issues of four {APA} journals in 2012. We found that 38\% of the researchers sent their data immediately or after reminders. These findings are in line with estimates of the willingness to share data in psychology from the recent or remote past. Although the recent crisis of confidence that shook psychology has highlighted the importance of open research practices, and technical developments have greatly facilitated data sharing, our findings make clear that psychology is nowhere close to being an open science.},
	pages = {3},
	number = {1},
	journaltitle = {Collabra},
	author = {Vanpaemel, Wolf and Vermorgen, Maarten and Deriemaecker, Leen and Storms, Gert},
	urldate = {2024-06-02},
	date = {2015-01-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/9C2Y5X6C/Vanpaemel et al. - 2015 - Are We Wasting a Good Crisis The Availability of Psychological Research Data after the Storm.pdf:application/pdf},
}

@article{dulitzki_expanding_2024,
	title = {Expanding the data Ark: an attempt to make the data from highly cited social science papers publicly available},
	volume = {11},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.240016},
	doi = {10.1098/rsos.240016},
	shorttitle = {Expanding the data Ark},
	abstract = {Access to scientific data can enable independent reuse and verification; however, most data are not available and become increasingly irrecoverable over time. This study aimed to retrieve and preserve important datasets from 160 of the most highly-cited social science articles published between 2008–2013 and 2015–2018. We asked authors if they would share data in a public repository—the Data Ark—or provide reasons if data could not be shared. Of the 160 articles, data for 117 (73\%, 95\% {CI} [67\%–80\%]) were not available and data for 7 (4\%, 95\% {CI} [0\%–12\%]) were available with restrictions. Data for 36 (22\%, 95\% {CI} [16\%–30\%]) articles were available in unrestricted form: 29 of these datasets were already available and 7 datasets were made available in the Data Ark. Most authors did not respond to our data requests and a minority shared reasons for not sharing, such as legal or ethical constraints. These findings highlight an unresolved need to preserve important scientific datasets and increase their accessibility to the scientific community.},
	pages = {240016},
	number = {5},
	journaltitle = {Royal Society Open Science},
	shortjournal = {R. Soc. Open Sci.},
	author = {Dulitzki, Coby and Crane, Steven Michael and Hardwicke, Tom E. and Ioannidis, John P. A.},
	urldate = {2024-06-03},
	date = {2024-05},
	langid = {english},
	file = {Dulitzki et al. - 2024 - Expanding the data Ark an attempt to make the data from highly cited social science papers publicly.pdf:/Users/tom/Zotero/storage/TMSF2GWQ/Dulitzki et al. - 2024 - Expanding the data Ark an attempt to make the data from highly cited social science papers publicly.pdf:application/pdf},
}

@article{michie_behaviour_2011,
	title = {The behaviour change wheel: A new method for characterising and designing behaviour change interventions},
	volume = {6},
	rights = {http://www.springer.com/tdm},
	issn = {1748-5908},
	url = {http://implementationscience.biomedcentral.com/articles/10.1186/1748-5908-6-42},
	doi = {10.1186/1748-5908-6-42},
	shorttitle = {The behaviour change wheel},
	abstract = {Background: Improving the design and implementation of evidence-based practice depends on successful behaviour change interventions. This requires an appropriate method for characterising interventions and linking them to an analysis of the targeted behaviour. There exists a plethora of frameworks of behaviour change interventions, but it is not clear how well they serve this purpose. This paper evaluates these frameworks, and develops and evaluates a new framework aimed at overcoming their limitations.
Methods: A systematic search of electronic databases and consultation with behaviour change experts were used to identify frameworks of behaviour change interventions. These were evaluated according to three criteria: comprehensiveness, coherence, and a clear link to an overarching model of behaviour. A new framework was developed to meet these criteria. The reliability with which it could be applied was examined in two domains of behaviour change: tobacco control and obesity.
Results: Nineteen frameworks were identified covering nine intervention functions and seven policy categories that could enable those interventions. None of the frameworks reviewed covered the full range of intervention functions or policies, and only a minority met the criteria of coherence or linkage to a model of behaviour. At the centre of a proposed new framework is a ‘behaviour system’ involving three essential conditions: capability, opportunity, and motivation (what we term the ‘{COM}-B system’). This forms the hub of a ‘behaviour change wheel’ ({BCW}) around which are positioned the nine intervention functions aimed at addressing deficits in one or more of these conditions; around this are placed seven categories of policy that could enable those interventions to occur. The {BCW} was used reliably to characterise interventions within the English Department of Health’s 2010 tobacco control strategy and the National Institute of Health and Clinical Excellence’s guidance on reducing obesity.
Conclusions: Interventions and policies to change behaviour can be usefully characterised by means of a {BCW} comprising: a ‘behaviour system’ at the hub, encircled by intervention functions and then by policy categories. Research is needed to establish how far the {BCW} can lead to more efficient design of effective interventions.},
	pages = {42},
	number = {1},
	journaltitle = {Implementation Science},
	shortjournal = {Implementation Sci},
	author = {Michie, Susan and Van Stralen, Maartje M and West, Robert},
	urldate = {2024-06-19},
	date = {2011-12},
	langid = {english},
	file = {Michie et al. - 2011 - The behaviour change wheel A new method for characterising and designing behaviour change intervent.pdf:/Users/tom/Zotero/storage/G85TIFN6/Michie et al. - 2011 - The behaviour change wheel A new method for characterising and designing behaviour change intervent.pdf:application/pdf},
}

@article{clark_failing_2023,
	title = {Failing to replicate predicts citation declines in psychology},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2304862120},
	doi = {10.1073/pnas.2304862120},
	abstract = {With a sample of 228 psychology papers that failed to replicate, we tested whether the trajectory of citation patterns changes following the publication of a failure to replicate. Across models, we found consistent evidence that failing to replicate predicted lower future citations and that the size of this reduction increased over time. In a 14-y postpublication period, we estimated that the publication of a failed replication was associated with an average citation decline of 14\% for original papers. These findings suggest that the publication of failed replications may contribute to a self-correcting science by decreasing scholars’ reliance on unreplicable original findings.},
	pages = {e2304862120},
	number = {29},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Clark, Cory J. and Connor, Paul and Isch, Calvin},
	urldate = {2024-07-02},
	date = {2023-07-18},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/V9SKLSEU/Clark et al. - 2023 - Failing to replicate predicts citation declines in psychology.pdf:application/pdf},
}

@article{hardwicke_citation_2021-1,
	title = {Citation patterns following a strongly contradictory replication result: four case studies from psychology},
	volume = {4},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/25152459211040837},
	doi = {10.1177/25152459211040837},
	shorttitle = {Citation patterns following a strongly contradictory replication result},
	abstract = {Replication studies that contradict prior findings may facilitate scientific self-correction by triggering a reappraisal of the original studies; however, the research community’s response to replication results has not been studied systematically. One approach for gauging responses to replication results is to examine how they affect citations to original studies. In this study, we explored postreplication citation patterns in the context of four prominent multilaboratory replication attempts published in the field of psychology that strongly contradicted and outweighed prior findings. Generally, we observed a small postreplication decline in the number of favorable citations and a small increase in unfavorable citations. This indicates only modest corrective effects and implies considerable perpetuation of belief in the original findings. Replication results that strongly contradict an original finding do not necessarily nullify its credibility; however, one might at least expect the replication results to be acknowledged and explicitly debated in subsequent literature. By contrast, we found substantial citation bias: The majority of articles citing the original studies neglected to cite relevant replication results. Of those articles that did cite the replication but continued to cite the original study favorably, approximately half offered an explicit defense of the original study. Our findings suggest that even replication results that strongly contradict original findings do not necessarily prompt a corrective response from the research community.},
	pages = {251524592110408},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {Hardwicke, Tom E. and Szűcs, Dénes and Thibault, Robert T. and Crüwell, Sophia and Van Den Akker, Olmo R. and Nuijten, Michèle B. and Ioannidis, John P. A.},
	urldate = {2024-07-04},
	date = {2021-07},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/FNWTUVLH/Hardwicke et al. - 2021 - Citation Patterns Following a Strongly Contradictory Replication Result Four Case Studies From Psyc.pdf:application/pdf},
}

@article{serghiou_media_2021,
	title = {Media and social media attention to retracted articles according to Altmetric},
	volume = {16},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0248625},
	doi = {10.1371/journal.pone.0248625},
	abstract = {The number of retracted articles has grown fast. However, the extent to which researchers and the public are made adequately aware of these retractions and how the media and social media respond to them remains unknown. Here, we aimed to evaluate the media and social media attention received by retracted articles and assess also the attention they receive post-retraction versus pre-retraction. We downloaded all records of retracted literature maintained by the Retraction Watch Database and originally published between January 1, 2010 to December 31, 2015. For all 3,008 retracted articles with a separate {DOI} for the original and its retraction, we downloaded the respective Altmetric Attention Score ({AAS}) (from Altmetric) and citation count (from Crossref), for the original article and its retraction notice on June 6, 2018. We also compared the {AAS} of a random sample of 572 retracted full journal articles available on {PubMed} to that of unretracted full articles matched from the same issue and journal. 1,687 (56.1\%) of retracted research articles received some amount of Altmetric attention, and 165 (5.5\%) were even considered popular ({AAS}{\textgreater}20). 31 (1.0\%) of 2,953 with a record on Crossref received {\textgreater}100 citations by June 6, 2018. Popular articles received substantially more attention than their retraction, even after adjusting for attention received post-retraction (Median difference, 29; 95\% {CI}, 17–61). Unreliable results were the most frequent reason for retraction of popular articles (32; 19\%), while fake peer review was the most common reason (421; 15\%) for the retraction of other articles. In comparison to matched articles, retracted articles tended to receive more Altmetric attention (23/31 matched groups; P-value, 0.01), even after adjusting for attention received post-retraction. Our findings reveal that retracted articles may receive high attention from media and social media and that for popular articles, pre-retraction attention far outweighs post-retraction attention.},
	pages = {e0248625},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Serghiou, Stylianos and Marton, Rebecca M. and Ioannidis, John P. A.},
	editor = {Dorta-González, Pablo},
	urldate = {2024-07-05},
	date = {2021-05-12},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/5A9EXEPL/Serghiou et al. - 2021 - Media and social media attention to retracted articles according to Altmetric.pdf:application/pdf},
}

@article{weinberg_four_2003,
	title = {Four golden lessons},
	volume = {426},
	rights = {http://www.springer.com/tdm},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/426389a},
	doi = {10.1038/426389a},
	pages = {389--389},
	number = {6965},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Weinberg, Steven},
	urldate = {2024-07-25},
	date = {2003-11},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/8AELMK6L/Weinberg - 2003 - Four golden lessons.pdf:application/pdf},
}

@article{schwartz_importance_2008,
	title = {The importance of stupidity in scientific research},
	volume = {121},
	issn = {1477-9137, 0021-9533},
	url = {https://journals.biologists.com/jcs/article/121/11/1771/30038/The-importance-of-stupidity-in-scientific-research},
	doi = {10.1242/jcs.033340},
	pages = {1771--1771},
	number = {11},
	journaltitle = {Journal of Cell Science},
	author = {Schwartz, Martin A.},
	urldate = {2024-07-25},
	date = {2008-06-01},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/UUEF88GI/Schwartz - 2008 - The importance of stupidity in scientific research.pdf:application/pdf},
}

@article{cahill_neurobiology_2001,
	title = {The neurobiology of learning and memory: some reminders to remember},
	author = {Cahill, Larry and {McGaugh}, James L and Weinberger, Norman M},
	date = {2001},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/NK8GS3FX/Cahill et al. - 2001 - The neurobiology of learning and memory some reminders to remember.pdf:application/pdf},
}

@article{jaeger_categorical_2008,
	title = {Categorical data analysis: Away from {ANOVAs} (transformation or not) and towards logit mixed models},
	volume = {59},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0749596X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X07001337},
	doi = {10.1016/j.jml.2007.11.007},
	shorttitle = {Categorical data analysis},
	abstract = {This paper identifies several serious problems with the widespread use of {ANOVAs} for the analysis of categorical outcome variables such as forced-choice variables, question-answer accuracy, choice in production (e.g. in syntactic priming research), et cetera. I show that even after applying the arcsine-square-root transformation to proportional data, {ANOVA} can yield spurious results. I discuss conceptual issues underlying these problems and alternatives provided by modern statistics. Specifically, I introduce ordinary logit models (i.e. logistic regression), which are well-suited to analyze categorical data and offer many advantages over {ANOVA}. Unfortunately, ordinary logit models do not include random effect modeling. To address this issue, I describe mixed logit models (Generalized Linear Mixed Models for binomially distributed outcomes, Breslow \& Clayton, 1993), which combine the advantages of ordinary logit models with the ability to account for random subject and item effects in one step of analysis. Throughout the paper, I use a psycholinguistic data set to compare the different statistical methods.},
	pages = {434--446},
	number = {4},
	journaltitle = {Journal of Memory and Language},
	shortjournal = {Journal of Memory and Language},
	author = {Jaeger, T. Florian},
	urldate = {2024-08-09},
	date = {2008-11},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/IHMA8BCT/Jaeger - 2008 - Categorical data analysis Away from ANOVAs (transformation or not) and towards logit mixed models.pdf:application/pdf},
}

@article{schneider_is_2024,
	title = {Is something rotten in the state of Denmark? Cross-national evidence for widespread involvement but not systematic use of questionable research practices across all fields of research},
	volume = {19},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0304342},
	doi = {10.1371/journal.pone.0304342},
	shorttitle = {Is something rotten in the state of Denmark?},
	abstract = {Questionable research practices ({QRP}) are believed to be widespread, but empirical assessments are generally restricted to a few types of practices. Furthermore, conceptual confusion is rife with use and prevalence of {QRPs} often being confused as the same quantity. We present the hitherto most comprehensive study examining {QRPs} across scholarly fields and knowledge production modes. We survey perception, use, prevalence and predictors of {QRPs} among 3,402 researchers in Denmark and 1,307 in the {UK}, {USA}, Croatia and Austria. Results reveal remarkably similar response patterns among Danish and international respondents (τ = 0.85). Self-reported use indicates whether respondents have used a {QRP} in recent publications. 9 out of 10 respondents admitted using at least one {QRP}. Median use is three out of nine {QRP} items. Self-reported prevalence reflects the frequency of use. On average, prevalence rates were roughly three times lower compared to self-reported use. Findings indicated that the perceived social acceptability of {QRPs} influenced self-report patterns. Results suggest that most researchers use different types of {QRPs} within a restricted time period. The prevalence estimates, however, do not suggest outright systematic use of specific {QRPs}. Perceived pressure was the strongest systemic predictor for prevalence. Conversely, more local attention to research cultures and academic age was negatively related to prevalence. Finally, the personality traits conscientiousness and, to a lesser degree, agreeableness were also inversely associated with self-reported prevalence. Findings suggest that explanations for engagement with {QRPs} are not only attributable to systemic factors, as hitherto suggested, but a complicated mixture of experience, systemic and individual factors, and motivated reasoning.},
	pages = {e0304342},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Schneider, Jesper W. and Allum, Nick and Andersen, Jens Peter and Petersen, Michael Bang and Madsen, Emil B. and Mejlgaard, Niels and Zachariae, Robert},
	urldate = {2024-08-15},
	date = {2024-08-12},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Surveys, Scientific misconduct, Survey research, Peer review, Questionnaires, Research integrity, Personality traits, Chin},
	file = {Full Text PDF:/Users/tom/Zotero/storage/46RSH286/Schneider et al. - 2024 - Is something rotten in the state of Denmark Cross-national evidence for widespread involvement but.pdf:application/pdf},
}

@online{hardwicke_open_2023,
	title = {The Open Science Atlas},
	url = {https://theplosblog.plos.org/2023/03/the-open-science-atlas/},
	abstract = {Einstein Award nominations are open. The Open Science Atlas was a finalist last year},
	titleaddon = {The Official {PLOS} Blog},
	author = {Hardwicke, Tom E. and Stylianos, Serghiou and Thibault, Robert T and Mody, Fallon},
	urldate = {2024-08-16},
	date = {2023-03-08},
	langid = {american},
	file = {Snapshot:/Users/tom/Zotero/storage/4IIQ78HB/the-open-science-atlas.html:text/html},
}

@misc{wicherts_privacy_2022,
	title = {Privacy protection in the era of open science},
	url = {https://osf.io/ybzu9},
	doi = {10.31234/osf.io/ybzu9},
	abstract = {Given the many benefits of sharing data, an increasing number of psychological researchers publicly share the data underlying their research via online repositories. While undoubtedly a positive scientific development that enables greater verification and data re-use, it is important to protect the interests and confidentiality of research participants while doing so. This is particularly relevant when studying sensitive topics, for example related to health, religion, politics, and sexual behaviors. We systematically assessed the risk of identification of individual participants in 2,169 psychological datasets shared alongside articles published in Psychological Science from 2014 - 2019, Judgment and Decision Making from 2011 - 2014, and {PLOS} {ONE} from 2013 - 2015. Results show that individuals could be readily identified by names, {IP} addresses, web identifiers (email, {MTurk} Worker {ID}), birth dates, or {ZIP} codes and initials combined with other demographic variables in 114 (5.3\%) of the datasets. An additional 94 datasets (4.3\%) included (often unnecessary) demographic information that posed some re-identification risk. Moreover, of these datasets with identifying or potentially identifying data, 110 (53\%) also contained data considered sensitive according to the {GDPR}. The majority of cases presenting privacy risks could have been prevented through simple procedures to de-identify datasets without sacrificing valuable information or transparency. We offer practical guidance to improve privacy protections in this transitional period towards greater data openness.},
	publisher = {{OSF}},
	author = {Wicherts, Jelte and Klein, Richard A. and Swaans, Sofie H. F. and Maassen, Esther and Stoevenbelt, Andrea H. and Hartgerink, C. H. J. and Peeters, Victor H. B. T. G. and Jonge, Myrthe de and Rüffer, Franziska},
	urldate = {2024-08-18},
	date = {2022-03-29},
	langid = {english},
	keywords = {open science, open data, data sharing, research ethics, privacy},
	file = {open_data_privacy_preprint:/Users/tom/Zotero/storage/EQCVYV7E/open_data_privacy_preprint.docx:application/vnd.openxmlformats-officedocument.wordprocessingml.document},
}

@misc{abalkina_paper_2022,
	title = {Paper mills: a novel form of publishing malpractice affecting psychology},
	url = {https://osf.io/2yf8z},
	doi = {10.31234/osf.io/2yf8z},
	shorttitle = {Paper mills},
	abstract = {We first describe the phenomenon of the academic paper mill, a kind of large-scale fraud in which authors pay to have work published in reputable journals. We give examples of some known paper mills and discuss ‘red flags’ that characterise their outputs.  Most of the early examples were in biomedical and computational sciences and so paper mills are less familiar to many psychologists.  In the next section, we describe a broker company/paper mill, Tanu.pro, discovered by the first author, which was identified by the use of fake email addresses. This paper mill placed six outputs in the Journal of Community Psychology, a reputable journal from a mainstream publisher. We look in detail at these papers and describe the features that confirm that malpractice was involved in publication. In five cases there was circumstantial evidence of tampering with the peer review process coupled with lack of editorial oversight. These papers have now been retracted. In a final section, we discuss the need for editors of psychology journals to be aware of potential targeting by paper mills and recommend editorial procedures to counteract these},
	publisher = {{OSF}},
	author = {Abalkina, Anna and Bishop, Dorothy},
	urldate = {2024-08-18},
	date = {2022-09-05},
	langid = {english},
	keywords = {Open Science, Publication, Integrity, Editing, Fraud, Paper Mills, Peer Review},
	file = {Submitted Version:/Users/tom/Zotero/storage/A2RGMVI2/Abalkina and Bishop - 2022 - Paper mills a novel form of publishing malpractice affecting psychology.pdf:application/pdf},
}

@article{ioannidis_users_2019,
	title = {A user’s guide to inflated and manipulated impact factors},
	volume = {49},
	issn = {1365-2362},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/eci.13151},
	doi = {10.1111/eci.13151},
	pages = {e13151},
	number = {9},
	journaltitle = {European Journal of Clinical Investigation},
	author = {Ioannidis, John P. A. and Thombs, Brett D.},
	urldate = {2024-08-19},
	date = {2019},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/eci.13151},
	keywords = {bias, guidelines, citations, bibliometrics, impact factor, self-citation},
	file = {PDF:/Users/tom/Zotero/storage/53NQ9XQ3/Ioannidis and Thombs - 2019 - A user’s guide to inflated and manipulated impact factors.pdf:application/pdf;Snapshot:/Users/tom/Zotero/storage/SA6GMLLH/eci.html:text/html},
}

@article{newell_unconscious_2014,
	title = {Unconscious influences on decision making: A critical review},
	volume = {37},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/unconscious-influences-on-decision-making-a-critical-review/86885344F7E8A44457C3FC63CFA3F3AF},
	doi = {10.1017/S0140525X12003214},
	shorttitle = {Unconscious influences on decision making},
	abstract = {To what extent do we know our own minds when making decisions? Variants of this question have preoccupied researchers in a wide range of domains, from mainstream experimental psychology (cognition, perception, social behavior) to cognitive neuroscience and behavioral economics. A pervasive view places a heavy explanatory burden on an intelligent cognitive unconscious, with many theories assigning causally effective roles to unconscious influences. This article presents a novel framework for evaluating these claims and reviews evidence from three major bodies of research in which unconscious factors have been studied: multiple-cue judgment, deliberation without attention, and decisions under uncertainty. Studies of priming (subliminal and primes-to-behavior) and the role of awareness in movement and perception (e.g., timing of willed actions, blindsight) are also given brief consideration. The review highlights that inadequate procedures for assessing awareness, failures to consider artifactual explanations of “landmark” results, and a tendency to uncritically accept conclusions that fit with our intuitions have all contributed to unconscious influences being ascribed inflated and erroneous explanatory power in theories of decision making. The review concludes by recommending that future research should focus on tasks in which participants' attention is diverted away from the experimenter's hypothesis, rather than the highly reflective tasks that are currently often employed.},
	pages = {1--19},
	number = {1},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Newell, Ben R. and Shanks, David R.},
	urldate = {2024-08-20},
	date = {2014-02},
	langid = {english},
	keywords = {decision making, judgment, awareness, conscious, deliberation, intuition, perceptual-motor skills, unconscious},
	file = {Submitted Version:/Users/tom/Zotero/storage/D53IZAIB/Newell and Shanks - 2014 - Unconscious influences on decision making A critical review.pdf:application/pdf},
}

@article{colloca_how_2011,
	title = {How placebo responses are formed: a learning perspective},
	volume = {366},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2010.0398},
	doi = {10.1098/rstb.2010.0398},
	shorttitle = {How placebo responses are formed},
	abstract = {Despite growing scientific interest in the placebo effect and increasing understanding of neurobiological mechanisms, theoretical conceptualization of the placebo effect remains poorly developed. Substantial mechanistic research on this phenomenon has proceeded with little guidance by any systematic theoretical paradigm. This review seeks to present a theoretical perspective on the formation of placebo responses. We focus on information processing, and argue that different kinds of learning along with individuals' genetic make-up evolved as the proximate cause for triggering behavioural and neural mechanisms that enable the formation of individual expectations and placebo responses. Conceptualizing the placebo effect in terms of learning offers the opportunity for facilitating scientific investigation with a significant impact on medical care.},
	pages = {1859--1869},
	number = {1572},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	shortjournal = {Phil. Trans. R. Soc. B},
	author = {Colloca, Luana and Miller, Franklin G.},
	urldate = {2024-08-20},
	date = {2011-06-27},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/PNR4DVHG/Colloca and Miller - 2011 - How placebo responses are formed a learning perspective.pdf:application/pdf},
}

@article{benedetti_conscious_2003,
	title = {Conscious expectation and unconscious conditioning in analgesic, motor, and hormonal placebo/nocebo responses},
	volume = {23},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.23-10-04315.2003},
	doi = {10.1523/JNEUROSCI.23-10-04315.2003},
	abstract = {The placebo and nocebo effect is believed to be mediated by both cognitive and conditioning mechanisms, although little is known about their role in different circumstances. In this study, we first analyzed the effects of opposing verbal suggestions on experimental ischemic arm pain in healthy volunteers and on motor performance in Parkinsonian patients and found that verbally induced expectations of analgesia/hyperalgesia and motor improvement/worsening antagonized completely the effects of a conditioning procedure. We also measured the effects of opposing verbal suggestions on hormonal secretion and found that verbally induced expectations of increase/decrease of growth hormone ({GH}) and cortisol did not have any effect on the secretion of these hormones. However, if a preconditioning was performed with sumatriptan, a 5-{HT}
              1B/1D
              agonist that stimulates {GH} and inhibits cortisol secretion, a significant increase of {GH} and decrease of cortisol plasma concentrations were found after placebo administration, although opposite verbal suggestions were given. These findings indicate that verbally induced expectations have no effect on hormonal secretion, whereas they affect pain and motor performance. This suggests that placebo responses are mediated by conditioning when unconscious physiological functions such as hormonal secretion are involved, whereas they are mediated by expectation when conscious physiological processes such as pain and motor performance come into play, even though a conditioning procedure is performed.},
	pages = {4315--4323},
	number = {10},
	journaltitle = {The Journal of Neuroscience},
	shortjournal = {J. Neurosci.},
	author = {Benedetti, Fabrizio and Pollo, Antonella and Lopiano, Leonardo and Lanotte, Michele and Vighetti, Sergio and Rainero, Innocenzo},
	urldate = {2024-08-20},
	date = {2003-05-15},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/W8RNIM8W/Benedetti et al. - 2003 - Conscious Expectation and Unconscious Conditioning in Analgesic, Motor, and Hormonal PlaceboNocebo.pdf:application/pdf},
}

@article{baribault_metastudies_2018,
	title = {Metastudies for robust tests of theory},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1708285114},
	doi = {10.1073/pnas.1708285114},
	abstract = {We describe and demonstrate an empirical strategy useful for discovering and replicating empirical effects in psychological science. The method involves the design of a metastudy, in which many independent experimental variables—that may be moderators of an empirical effect—are indiscriminately randomized. Radical randomization yields rich datasets that can be used to test the robustness of an empirical claim to some of the vagaries and idiosyncrasies of experimental protocols and enhances the generalizability of these claims. The strategy is made feasible by advances in hierarchical Bayesian modeling that allow for the pooling of information across unlike experiments and designs and is proposed here as a gold standard for replication research and exploratory research. The practical feasibility of the strategy is demonstrated with a replication of a study on subliminal priming.},
	pages = {2607--2612},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Baribault, Beth and Donkin, Chris and Little, Daniel R. and Trueblood, Jennifer S. and Oravecz, Zita and Van Ravenzwaaij, Don and White, Corey N. and De Boeck, Paul and Vandekerckhove, Joachim},
	urldate = {2024-08-20},
	date = {2018-03-13},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/KK32VXIZ/Baribault et al. - 2018 - Metastudies for robust tests of theory.pdf:application/pdf},
}

@article{egorova_not_2015,
	title = {Not seeing or feeling is still believing: conscious and non-conscious pain modulation after direct and observational learning},
	volume = {5},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep16809},
	doi = {10.1038/srep16809},
	shorttitle = {Not seeing or feeling is still believing},
	abstract = {Abstract
            Our experience with the world is shaped not only directly through personal exposure but also indirectly through observing others and learning from their experiences. Using a conditioning paradigm, we investigated how directly and observationally learned information can affect pain perception, both consciously and non-consciously. Differences between direct and observed cues were manifest in higher pain ratings and larger skin conductance responses to directly experienced cues. However, the pain modulation effects produced by conditioning were of comparable magnitude for direct and observational learning. These results suggest that social observation can induce positive and negative pain modulation. Importantly, the fact that cues learned by observation and activated non-consciously still produced a robust conditioning effect that withstood extinction highlights the role of indirect exposure in placebo and nocebo effects.},
	pages = {16809},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Egorova, Natalia and Park, Joel and Orr, Scott P. and Kirsch, Irving and Gollub, Randy L. and Kong, Jian},
	urldate = {2024-08-20},
	date = {2015-11-18},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/AECGW6JM/Egorova et al. - 2015 - Not seeing or feeling is still believing conscious and non-conscious pain modulation after direct a.pdf:application/pdf},
}

@article{jensen_classical_2015,
	title = {Classical conditioning of analgesic and hyperalgesic pain responses without conscious awareness},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1504567112},
	doi = {10.1073/pnas.1504567112},
	abstract = {Significance
            It is unclear to what extent new learning can take place outside of conscious awareness. In the present study, we used psychophysical measures and classical conditioning to establish whether psychologically mediated analgesic and hyperalgesic responses could be acquired by unseen (subliminally presented) stimuli. Our study demonstrates that analgesia and hyperalgesia can be learned without conscious awareness, suggesting that higher-order cognitive processes may be affected by implicit learning mechanisms.
          , 
            
              Pain reduction and enhancement can be produced by means of conditioning procedures, yet the role of awareness during the acquisition stage of classical conditioning is unknown. We used psychophysical measures to establish whether conditioned analgesic and hyperalgesic responses could be acquired by unseen (subliminally presented) stimuli. A 2 × 2 factorial design, including subliminal/supraliminal exposures of conditioning stimuli ({CS}) during acquisition/extinction, was used. Results showed significant analgesic and hyperalgesic responses (
              P
              {\textless} 0.001), and responses were independent of {CS} awareness, as subliminal/supraliminal cues during acquisition/extinction led to comparable outcomes. The effect was significantly larger for hyperalgesic than analgesic responses (
              P
              {\textless} 0.001). Results demonstrate that conscious awareness of the {CS} is not required during either acquisition or extinction of conditioned analgesia or hyperalgesia. Our results support the notion that nonconscious stimuli have a pervasive effect on human brain function and behavior and may affect learning of complex cognitive processes such as psychologically mediated analgesic and hyperalgesic responses.},
	pages = {7863--7867},
	number = {25},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Jensen, Karin and Kirsch, Irving and Odmalm, Sara and Kaptchuk, Ted J. and Ingvar, Martin},
	urldate = {2024-08-20},
	date = {2015-06-23},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/BI2ICJBD/Jensen et al. - 2015 - Classical conditioning of analgesic and hyperalgesic pain responses without conscious awareness.pdf:application/pdf},
}

@article{egorova_face_2017,
	title = {In the face of pain: The choice of visual cues in pain conditioning matters},
	volume = {21},
	rights = {http://onlinelibrary.wiley.com/{termsAndConditions}\#vor},
	issn = {1090-3801, 1532-2149},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ejp.1024},
	doi = {10.1002/ejp.1024},
	shorttitle = {In the face of pain},
	abstract = {Background: Visual cue conditioning is a valuable experimental paradigm to investigate placebo and nocebo effects in pain. However, little attention has been paid to the cues themselves and potential variability of effects (their quantity and quality) stemming from the choice of stimuli. Yet, this seemingly methodological question has important implications for the interpretation of experimental ﬁndings in terms of their signiﬁcance for clinical practice.
Methods: We investigated the effect of heat pain conditioning using different types of visual cues (abstract images, faces and pseudo-words) in a group of 22 healthy volunteers. We analysed conditioning effects calculated as the difference in pain ratings to heat stimuli of identical temperature preceded by conditioned high or low pain cues with (1) subliminal and supraliminal presentation; and (2) immediately after conditioning and following extinction. Awareness manipulation and test following indirect, observational extinction allowed us to assess the strength and robustness of the conditioning effects induced with different cue types.
Results: We observed no differences in conditioning effect magnitudes between images, faces and words when all stimuli were presented supraliminally. With subliminal presentation, only face stimuli elicited a signiﬁcant effect; equally only face cue-induced effect withstood extinction.
Conclusions: Our ﬁndings indicate that face-related associations to pain might be stronger than those elicited with other visual cues, as face cues seem to induce stronger subliminal effects and withstand mild extinction.},
	pages = {1243--1251},
	number = {7},
	journaltitle = {European Journal of Pain},
	shortjournal = {European Journal of Pain},
	author = {Egorova, N. and Park, J. and Kong, J.},
	urldate = {2024-08-20},
	date = {2017-08},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/WPQXMWGR/Egorova et al. - 2017 - In the face of pain The choice of visual cues in pain conditioning matters.pdf:application/pdf},
}

@article{rosen_effects_2017,
	title = {Effects of subtle cognitive manipulations on placebo analgesia – An implicit priming study},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {1090-3801, 1532-2149},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ejp.961},
	doi = {10.1002/ejp.961},
	abstract = {Background: Expectancy is widely accepted as a key contributor to placebo effects. However, it is not known whether non-conscious expectancies achieved through semantic priming may contribute to placebo analgesia. In this study, we investigated if an implicit priming procedure, where participants were unaware of the intended priming inﬂuence, affected placebo analgesia.
Methods: In a double-blind experiment, healthy participants (n = 36) were randomized to different implicit priming types; one aimed at increasing positive expectations and one neutral control condition. First, pain calibration (thermal) and a credibility demonstration of the placebo analgesic device were performed. In a second step, an independent experimenter administered the priming task; Scrambled Sentence Test. Then, pain sensitivity was assessed while telling participants that the analgesic device was either turned on (placebo) or turned off (baseline). Pain responses were recorded on a 0–100 Numeric Response Scale.
Results: Overall, there was a signiﬁcant placebo effect (p {\textless} 0.001), however, the priming conditions (positive/neutral) did not lead to differences in placebo outcome. Prior experience of pain relief (during initial pain testing) correlated signiﬁcantly with placebo analgesia (p {\textless} 0.001) and explained 34\% of placebo variance. Trait neuroticism correlated positively with placebo analgesia (p {\textless} 0.05) and explained 21\% of placebo variance.
Conclusions: Priming is one of many ways to inﬂuence behaviour, and non-conscious activation of positive expectations could theoretically affect placebo analgesia. Yet, we found no {SST} priming effect on placebo analgesia. Instead, our data point to the signiﬁcance of prior experience of pain relief, trait neuroticism and social interaction with the treating clinician.},
	pages = {594--604},
	number = {4},
	journaltitle = {European Journal of Pain},
	shortjournal = {European Journal of Pain},
	author = {Rosén, A. and Yi, J. and Kirsch, I. and Kaptchuk, T.J. and Ingvar, M. and Jensen, K.B.},
	urldate = {2024-08-20},
	date = {2017-04},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/7CCSNII5/Rosén et al. - 2017 - Effects of subtle cognitive manipulations on placebo analgesia – An implicit priming study.pdf:application/pdf},
}

@article{noauthor_yes_nodate,
	title = {‘Yes, but how much smaller?’ A simple observation about p-values in academic fraud detection},
	abstract = {An overlooked feature of p-values is that they are commonly reported as lower than a lowest threshold of interest (i.e. ‘p{\textless}0.001’) instead of reported exactly. We term these {STALT} (‘smaller-than-a-lowest-threshold’) values, which are hidden until recalculated. Recalculation allows scientific papers to be evaluated for plausibility as p-values which are extremely small, such as some of those presented here (e.g. p{\textless}1.7e-10, p{\textless}2.3e-19) are incredibly unlikely. We strongly urge meta-analysts in particular to test, and if necessary discard, papers that report implausibly small p-values when considering them for inclusion in any omnibus analysis.},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/7L7X4MWE/‘Yes, but how much smaller’ A simple observation about p-values in academic fraud detection.pdf:application/pdf},
}

@article{montgomery_how_2018,
	title = {How conditioning on posttreatment variables can ruin your experiment and what to do about it},
	volume = {62},
	issn = {0092-5853, 1540-5907},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ajps.12357},
	doi = {10.1111/ajps.12357},
	abstract = {In principle, experiments o↵er a straightforward method for social scientists to accurately estimate causal e↵ects. However, scholars often unwittingly distort treatment e↵ect estimates by conditioning on variables that could be a↵ected by their experimental manipulation. Typical examples include controlling for post-treatment variables in statistical models, eliminating observations based on post-treatment criteria, or subsetting the data based on post-treatment variables. Though these modeling choices are intended to address common problems encountered when conducting experiments, they can bias estimates of causal e↵ects. Moreover, problems associated with conditioning on post-treatment variables remain largely unrecognized in the ﬁeld, which we show frequently publishes experimental studies using these practices in our discipline’s most prestigious journals. We demonstrate the severity of experimental post-treatment bias analytically and document the magnitude of the potential distortions it induces using visualizations and reanalyses of real-world data. We conclude by providing applied researchers with recommendations for best practice.},
	pages = {760--775},
	number = {3},
	journaltitle = {American Journal of Political Science},
	shortjournal = {American J Political Sci},
	author = {Montgomery, Jacob M. and Nyhan, Brendan and Torres, Michelle},
	urldate = {2024-08-20},
	date = {2018-07},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/7IQHHZMG/Montgomery et al. - 2018 - How Conditioning on Posttreatment Variables Can Ruin Your Experiment and What to Do about It.pdf:application/pdf},
}

@article{fergusson_post-randomisation_2002-2,
	title = {Post-randomisation exclusions: the intention to treat principle and excluding patients from analysis},
	volume = {325},
	issn = {09598138, 14685833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.325.7365.652},
	doi = {10.1136/bmj.325.7365.652},
	shorttitle = {Post-randomisation exclusions},
	pages = {652--654},
	number = {7365},
	journaltitle = {{BMJ}},
	author = {Fergusson, D.},
	urldate = {2024-08-20},
	date = {2002-09-21},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/66MNDQFT/Fergusson - 2002 - Post-randomisation exclusions the intention to treat principle and excluding patients from analysis.pdf:application/pdf},
}

@article{hardwicke_empirical_2024,
	title = {An empirical appraisal of {eLife}’s assessment vocabulary},
	volume = {22},
	abstract = {{ARUes}:{ePalrecahseacrtoinclfiersmpthuabtlaisllhheeaddbinygtlheevejlosuarrneraelperLeisfeenaterdecaocrrceoctmlyp}:anied by short evaluation statements that use phrases from a prescribed vocabulary to evaluate research on 2 dimensions: importance and strength of support. Intuitively, the prescribed phrases appear to be highly synonymous (e.g., important/valuable, compelling/convincing) and the vocabulary’s ordinal structure may not be obvious to readers. We conducted an online repeated-measures experiment to gauge whether the phrases were interpreted as intended. We also tested an alternative vocabulary with (in our view) a less ambiguous structure. A total of 301 participants with a doctoral or graduate degree used a 0\% to 100\% scale to rate the importance and strength of support of hypothetical studies described using phrases from both vocabularies. For the {eLife} vocabulary, most participants’ implied ranking did not match the intended ranking on both the importance (n = 59, 20\% matched, 95\% confidence interval [15\% to 24\%]) and strength of support dimensions (n = 45, 15\% matched [11\% to 20\%]). By contrast, for the alternative vocabulary, most participants’ implied ranking did match the intended ranking on both the importance (n = 188, 62\% matched [57\% to 68\%]) and strength of support dimensions (n = 201, 67\% matched [62\% to 72\%]). {eLife}’s vocabulary tended to produce less consistent between-person interpretations, though the alternative vocabulary still elicited some overlapping interpretations away from the middle of the scale. We speculate that explicit presentation of a vocabulary’s intended ordinal structure could improve interpretation. Overall, these findings suggest that more structured and less ambiguous language can improve communication of research evaluations.},
	pages = {e3002645},
	number = {8},
	journaltitle = {{PLOS} Biology},
	author = {Hardwicke, Tom E and Schiavone, Sarah R and Clarke, Beth and Vazire, Simine},
	date = {2024},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/RPRDJQ6Z/Hardwicke et al. - An empirical appraisal of eLife’s assessment vocabulary.pdf:application/pdf},
}

@article{varma_systematic_2024,
	title = {A systematic review and meta-analysis of experimental methods for modulating intrusive memories following lab-analogue trauma exposure in non-clinical populations},
	rights = {2024 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-024-01956-y},
	doi = {10.1038/s41562-024-01956-y},
	abstract = {Experiencing trauma leads to intrusive memories ({IMs}), a hallmark symptom of post-traumatic stress disorder ({PTSD}), which also occurs transdiagnostically. Understanding why {IMs} increase or decrease is pivotal in developing interventions to support mental health. In this preregistered meta-analysis ({PROSPERO}: {CRD}42021224835), we included 134 articles (131 techniques, 606 effect sizes and 12,074 non-clinical participants) to investigate how experimental techniques alter {IM} frequency, intrusion-related distress and symptoms arising from lab-analogue trauma exposure. Eligible articles were identified by searching eight databases until 12 December 2023. To test potential publication biases, we employed methods including Egger’s test and three-parameter selection models. We employed three-level multilevel modelling and meta-regressions to examine whether and how experimental techniques would modulate {IM} frequency and associated outcomes. Results showed that techniques (behavioural, pharmacological, neuromodulation) significantly reduced intrusion frequency (g = 0.16, 95\% confidence interval [0.09, 0.23]). Notably, techniques aimed to reduce {IMs} also ameliorated intrusion-related distress and symptoms, while techniques that increased {IMs} exacerbated these related outcomes, thus highlighting {IM}’s centrality in {PTSD}-like symptoms. Techniques tapping into mental imagery processing (for example, trauma reminder followed by playing Tetris) reduced intrusions when administered immediately after, or at a delayed time after trauma. Although our meta-analysis is limited to symptoms induced by lab-analogue trauma exposure, some lab-based results have now generalized to real-world trauma and {IMs}, highlighting the promising utility of lab-analogue trauma paradigms for intervention development.},
	pages = {1--20},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Varma, Mohith M. and Zeng, Shengzi and Singh, Laura and Holmes, Emily A. and Huang, Jingyun and Chiu, Man Hey and Hu, Xiaoqing},
	urldate = {2024-08-23},
	date = {2024-08-21},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Post-traumatic stress disorder},
	file = {Full Text PDF:/Users/tom/Zotero/storage/KMQQ6XHD/Varma et al. - 2024 - A systematic review and meta-analysis of experimental methods for modulating intrusive memories foll.pdf:application/pdf},
}

@article{haber_assessing_2024,
	title = {Assessing the effects of a precommitment policy applied during peer review},
	volume = {2},
	issn = {null},
	url = {https://doi.org/10.1080/2833373X.2024.2386955},
	doi = {10.1080/2833373X.2024.2386955},
	abstract = {Improving journal policies and practices is hampered by a lack of experimentation and evidence. Implementing high-powered journal policy experiments to test solutions is difficult because of status quo biases, logistical difficulties, sample size requirements, and ethical considerations. In this article, we introduce a new policy and a new framework for experimental implementation at scale. The policy, called “Registered Revisions”, triggers during traditional peer review when reviewers ask for additional data and/or analysis. Authors are asked to make a Revision Plan for how they will address these comments. If the Revision Plan and responses to standard questions are approved, editors agree to make an in-principle acceptance for publication decision. In other words, the article will be accepted as long as the Revision Plan is carried out as specified, regardless of the results. Registered Revisions enables authors and editors to gain experience of preregistering their methods without them having to proactively commit to a full, two-stage Registered Reports-style publication process. The experimental framework is a meta experiment involving a collaboration of many journals. Each journal implements an experiment based on a shared protocol, logistical guidance, materials, and infrastructure. The journals can use and publish their experiments independently, and also contribute the data and findings to a prospective meta-analysis. This approach presents a potential path forward for evidence-informed journal policy reform.},
	pages = {2386955},
	number = {1},
	journaltitle = {Evidence-Based Toxicology},
	author = {Haber, Noah A. and Errington, Timothy M. and Daley, Macie and Whaley, Paul and Nosek, Brian A.},
	urldate = {2024-08-23},
	date = {2024-12-31},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/2833373X.2024.2386955},
	keywords = {preregistration, peer review, journal policy, Registered reports, randomized experiment},
	file = {Full Text PDF:/Users/tom/Zotero/storage/X4I5HB8C/Haber et al. - 2024 - Assessing the effects of a precommitment policy applied during peer review.pdf:application/pdf},
}

@article{wendelborn_promoting_2024,
	title = {Promoting data sharing: the moral obligations of public funding agencies},
	volume = {30},
	issn = {1471-5546},
	url = {https://doi.org/10.1007/s11948-024-00491-3},
	doi = {10.1007/s11948-024-00491-3},
	shorttitle = {Promoting data sharing},
	abstract = {Sharing research data has great potential to benefit science and society. However, data sharing is still not common practice. Since public research funding agencies have a particular impact on research and researchers, the question arises: Are public funding agencies morally obligated to promote data sharing? We argue from a research ethics perspective that public funding agencies have several pro tanto obligations requiring them to promote data sharing. However, there are also pro tanto obligations that speak against promoting data sharing in general as well as with regard to particular instruments of such promotion. We examine and weigh these obligations and conclude that all things considered funders ought to promote the sharing of data. Even the instrument of mandatory data sharing policies can be justified under certain conditions.},
	pages = {35},
	number = {4},
	journaltitle = {Science and Engineering Ethics},
	shortjournal = {Sci Eng Ethics},
	author = {Wendelborn, Christian and Anger, Michael and Schickhardt, Christoph},
	urldate = {2024-08-23},
	date = {2024-08-06},
	langid = {english},
	keywords = {Research integrity, Data sharing, Artificial Intelligence, Epistemic integrity, Funding agencies, Medical Ethics, Moral obligations, Scientific freedom, Scientific progress, Social value},
	file = {Full Text PDF:/Users/tom/Zotero/storage/HIW3NJFX/Wendelborn et al. - 2024 - Promoting Data Sharing The Moral Obligations of Public Funding Agencies.pdf:application/pdf},
}

@article{alperin_analysis_2024,
	title = {An analysis of the suitability of {OpenAlex} for bibliometric analyses},
	abstract = {Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably {OpenAlex}, have emerged. While many studies have begun using {OpenAlex} as a data source, few critically assess its limitations. This study, conducted in collaboration with the {OpenAlex} team, addresses this gap by comparing {OpenAlex} to Scopus across a number of dimensions. The analysis concludes that {OpenAlex} is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address {OpenAlex}'s limitations. Doing so will be necessary to confidently use {OpenAlex} across a wider set of analyses, including those that are not at all possible with more constrained databases.},
	author = {Alperin, Juan Pablo and Portenoy, Jason and Demes, Kyle and Larivière, Vincent},
	date = {2024},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/EDKY359X/Alperin et al. - 2024 - An analysis of the suitability of OpenAlex for bibliometric analyses.pdf:application/pdf},
}

@misc{haupka_analysis_2024,
	title = {Analysis of the publication and document types in {OpenAlex}, Web of Science, Scopus, Pubmed and Semantic Scholar},
	url = {http://arxiv.org/abs/2406.15154},
	abstract = {This study compares and analyses publication and document types in the following bibliographic databases: {OpenAlex}, Scopus, Web of Science, Semantic Scholar and {PubMed}. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in {OpenAlex}, as {OpenAlex} is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.},
	number = {{arXiv}:2406.15154},
	publisher = {{arXiv}},
	author = {Haupka, Nick and Culbert, Jack H. and Schniedermann, Alexander and Jahn, Najko and Mayr, Philipp},
	urldate = {2024-09-04},
	date = {2024-06-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.15154 [cs]},
	keywords = {Computer Science - Digital Libraries},
	file = {PDF:/Users/tom/Zotero/storage/M3B9FATH/Haupka et al. - 2024 - Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Seman.pdf:application/pdf},
}

@misc{culbert_reference_2024,
	title = {Reference coverage analysis of openalex compared to web of science and scopus},
	url = {http://arxiv.org/abs/2401.16359},
	abstract = {{OpenAlex} is a promising open source of scholarly metadata, and competitor to the established proprietary sources, the Web of Science and Scopus. As {OpenAlex} provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as {OpenAlex} is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this empirical paper, we will study the reference and metadata coverage within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16,788,282 recent publications shared by all three databases, {OpenAlex} has average source reference numbers and internal coverage, respectively, comparable to both Web of Science and Scopus. We also demonstrate that the comparison of other core metadata covered by {OpenAlex} shows mixed results, with {OpenAlex} capturing more {ORCID} identifiers, fewer abstracts and a similar number of Open Access information per article when compared to both Web of Science and Scopus.},
	number = {{arXiv}:2401.16359},
	publisher = {{arXiv}},
	author = {Culbert, Jack and Hobert, Anne and Jahn, Najko and Haupka, Nick and Schmidt, Marion and Donner, Paul and Mayr, Philipp},
	urldate = {2024-09-04},
	date = {2024-02-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2401.16359 [cs]},
	keywords = {Computer Science - Digital Libraries},
	file = {PDF:/Users/tom/Zotero/storage/GVDZT473/Culbert et al. - 2024 - Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus.pdf:application/pdf},
}

@article{gorman_ecological_2014,
	title = {Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis)},
	volume = {9},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0090081},
	doi = {10.1371/journal.pone.0090081},
	pages = {e90081},
	number = {3},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Gorman, Kristen B. and Williams, Tony D. and Fraser, William R.},
	editor = {Chiaradia, André},
	urldate = {2024-09-29},
	date = {2014-03-05},
	langid = {english},
	file = {Full Text:/Users/tom/Zotero/storage/4CC7MZC5/Gorman et al. - 2014 - Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins.pdf:application/pdf},
}

@article{wickham_tidy_2014,
	title = {Tidy data},
	volume = {59},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v59/i10/},
	doi = {10.18637/jss.v059.i10},
	number = {10},
	journaltitle = {Journal of Statistical Software},
	shortjournal = {J. Stat. Soft.},
	author = {Wickham, Hadley},
	urldate = {2024-09-29},
	date = {2014},
	langid = {english},
	file = {PDF:/Users/tom/Zotero/storage/DD7ZGMAM/Wickham - 2014 - Tidy data.pdf:application/pdf},
}
